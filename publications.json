{
  "publications": [
    {
      "title": "MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention",
      "authors": [
        "Amir H. Abdi",
        "Chengruidong Zhang",
        "Dongsheng Li",
        "Huiqiang Jiang",
        "Jianfeng Gao",
        "Lili Qiu",
        "Qianhui Wu",
        "Surin Ahn",
        "Xufang Luo",
        "Yucheng Li",
        "Yuqing Yang"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Human language technologies"
      ],
      "publication_date": "July 2025",
      "abstract": "The integration of long-context capabilities with visual understanding unlocks unprecedented potential for Vision Language Models (VLMs). However, the quadratic attention complexity during the pre-filling phase remains a significant obstacle to real-world deployment. To overcome this limitation, we introduce MMInference (Multimodality Million tokens Inference), a dynamic sparse attention method that accelerates the prefilling stage for long-context multi-modal inputs. First, our analysis reveals that the temporal and spatial locality of video input leads to a unique sparse pattern, the Grid pattern. Simultaneously, VLMs exhibit markedly different sparse distributions across different modalities. We introduce a permutation-based method to leverage the unique Grid pattern and handle modality boundary issues. By offline search the optimal sparse patterns for each head, MMInference constructs the sparse distribution dynamically based on the input. We also provide optimized GPU kernels for efficient sparse computations. Notably, MMInference integrates seamlessly into existing VLM pipelines without any model modifications or fine-tuning. Experiments on multi-modal benchmarks-including Video QA, Captioning, VisionNIAH, and Mixed-Modality NIAH-with state-of-the-art long-context VLMs (LongVila, LlavaVideo, VideoChat-Flash, Qwen2.5-VL) show that MMInference accelerates the pre-filling stage by up to 8.3x at 1M tokens while maintaining accuracy. Our code is available at this https URL.",
      "url": "https://www.microsoft.com/en-us/research/publication/mminference-accelerating-pre-filling-for-long-context-vlms-via-modality-aware-permutation-sparse-attention/"
    },
    {
      "title": "SRC: A Scalable Reliable Connection for RDMA with Decoupled QPs and Connections",
      "authors": [
        "Ran Shu",
        "Yiren Zhao",
        "Yongqiang Xiong"
      ],
      "research_areas": [
        "Hardware and devices",
        "Systems and networking"
      ],
      "publication_date": "August 2025",
      "abstract": "Using Remote Direct Memory Access (RDMA) is the trend in data centers for achieving high throughput and low latency due to the benefits of hardware offloaded stacks. However, RDMA cannot provide consistently high performance at scale due to limited RDMA NIC (RNIC) hardware state capacity. We observe that the high number of required RDMA connections for efficient RDMA system is due to the coupled design of channels between host and RNIC and the network connections. In this paper, we propose a novel RDMA transport concept, SRC, which decouples the network connections from queue pairs. SRC introduces a lightweight mapping scheme for efficient forwarding between QPs and connection on RNIC. Besides, SRC lets software manages the mapping between QPs and connections. Results show that SRC can reduce RDMA states size from 146.20 MB to 0.19 MB for a 512-server cluster running RDMA applications.",
      "url": "https://www.microsoft.com/en-us/research/publication/src-a-scalable-reliable-connection-for-rdma-with-decoupled-qps-and-connections/"
    },
    {
      "title": "Enter, Exit, Page Fault, Leak: Testing Isolation Boundaries for Microarchitectural Leaks",
      "authors": [
        "Boris Köpf",
        "Cédric Fournet",
        "Flavien Solt",
        "Jana Hofmann",
        "Oleksii Oleksenko",
        "Stavros Volos"
      ],
      "research_areas": [
        "Security, privacy, and cryptography"
      ],
      "publication_date": "May 2026",
      "abstract": "CPUs provide isolation mechanisms like virtualization and privilege levels to protect software. Yet these focus on architectural isolation while typically overlooking microarchitectural side channels, exemplified by Meltdown and Foreshadow. Software must therefore supplement architectural defenses with ad-hoc microarchitectural patches, which are constantly evolving as new attacks emerge and defenses are proposed. Such reactive approach makes ensuring complete isolation a daunting task, and leaves room for errors and oversights.\nWe address this problem by developing a tool that stress tests microarchitectural isolation between security domains such as virtual machines, kernel, and processes, with the goal of detecting flaws in the isolation boundaries. The tool extends model-based relational testing (MRT) methodology to enable detection of cross-domain information leakage. We design a new test case generator and execution sandbox to handle multi-domain execution, new leakage models to encode expected leaks, and new analysis techniques to manage nondeterminism.\nWe use this tool to perform an in-depth testing campaign on six x86-64 CPUs for leakage across different isolation boundaries. The testing campaign exposed four new leaks and corroborated numerous known ones, with only two false positives throughout the entire campaign.\nThese results show critical gaps in current isolation mechanisms as well as validate a robust methodology for detecting microarchitectural flaws. As such, this approach enables a shift from reactive patching to proactive security validation in processor design.",
      "url": "https://www.microsoft.com/en-us/research/publication/enter-exit-page-fault-leak-testing-isolation-boundaries-for-microarchitectural-leaks/"
    },
    {
      "title": "Nods of Agreement: Webcam-Driven Avatars Improve Meeting Outcomes and Avatar Satisfaction Over Audio-Driven or Static Avatars in All-Avatar Work Videoconferencing",
      "authors": [
        "Charlie Hewitt",
        "Fang Ma",
        "Ju Zhang",
        "Lev Tankelevitch",
        "Lohit Petikam",
        "Marco Gillies",
        "Marta Wilczkowiak (SHE/HER)",
        "Payod Panda",
        "Sean Rintel",
        "Torang Asadi",
        "Xenui Pan"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Human-computer interaction"
      ],
      "publication_date": "October 2025",
      "abstract": "Avatars are edging into mainstream videoconferencing, but evaluation of how avatar animation modalities contribute to work meeting outcomes has been limited. We report a within-group videoconferencing experiment in which 68 employees of a global technology company, in 16 groups, used the same stylized avatars in three modalities (static picture, audio-animation, and webcam-animation) to complete collaborative decision-making tasks. Quantitatively, for meeting outcomes, webcam-animated avatars improved meeting effectiveness over the picture modality and were also reported to be more comfortable and inclusive than both other modalities. In terms of avatar satisfaction, there was a similar preference for webcam animation as compared to both other modalities. Our qualitative analysis shows participants expressing a preference for the holistic motion of webcam animation, and that meaningful movement outweighs realism for meeting outcomes, as evidenced through a systematic overview of ten thematic factors. We discuss implications for research and commercial deployment and conclude that webcam-animated avatars are a plausible alternative to video in work meetings.\nKEYWORDS: videoconferencing, work, avatar animation modalities, satisfaction, outcomes, effectiveness, alignment, comfort, inclusivity, expression, perception, preference",
      "url": "https://www.microsoft.com/en-us/research/publication/nods-of-agreement-webcam-driven-avatars-improve-meeting-outcomes-and-avatar-satisfaction-over-audio-driven-or-static-avatars-in-all-avatar-work-videoconferencing/"
    },
    {
      "title": "Dorami: Privilege Separating Security Monitor on RISC-V TEEs",
      "authors": [
        "Mark Kuhne",
        "Shweta Shinde",
        "Stavros Volos"
      ],
      "research_areas": [
        "Security, privacy, and cryptography",
        "Systems and networking"
      ],
      "publication_date": "August 2025",
      "abstract": "TEE implementations on RISC-V offer an enclave abstraction by introducing a trusted component called the security monitor (SM). The SM performs critical tasks such as isolating enclaves from each other as well as from the OS by using privileged ISA instructions that enforce the physical memory protection. However, the SM executes at the highest privilege layer on the platform (machine-mode) along side firmware that is not only large in size but also includes third-party vendor code specific to the platform. In this paper, we present DORAMI, a privilege separation approach that isolates the SM from the firmware thus reducing the attack surface on TEEs. DORAMI re-purposes existing ISA features to enforce its isolation and achieves its goals without large overheads.",
      "url": "https://www.microsoft.com/en-us/research/publication/dorami-privilege-separating-security-monitor-on-risc-v-tees/"
    },
    {
      "title": "Auto-Prep: Holistic Prediction of Data Preparation Steps for Self-Service Business Intelligence",
      "authors": [
        "Eugenie Y. Lai",
        "Surajit Chaudhuri",
        "Yeye He"
      ],
      "research_areas": [
        "Data platforms and analytics"
      ],
      "publication_date": "September 2025",
      "abstract": "Business Intelligence (BI) plays a critical role in empowering modern enterprises to make informed data-driven decisions, and has grown into a billion-dollar business. Self-service BI tools like Power BI and Tableau have democratized the “dashboarding” phase of BI, by offering user-friendly, drag-and-drop interfaces that are tailored to non-technical enterprise users. However, despite these advances, we observe that the “data preparation” phase of BI continues to be a key pain point for BI users today.\nIn this work, we systematically study around 2K real BI projects harvested from public sources, focusing on the data-preparation phase of the BI workflows. We observe that users often have to program both (1) data transformation steps and (2) table joins steps, before their raw data can be ready for dashboarding and analysis. A careful study of the BI workflows reveals that transformation and join steps are often intertwined in the same BI project, such that considering both holistically is crucial to accurately predict these steps. Leveraging this observation, we develop an Auto-Prep system to holistically predict transformations and joins, using a principled graph-based algorithm inspired by Steiner-tree, with provable quality guarantees. Extensive evaluations using real BI projects suggest that Auto-Prep can correctly predict over 70\\% transformation and join steps, significantly more accurate than existing algorithms as well as language-models such as GPT-4.",
      "url": "https://www.microsoft.com/en-us/research/publication/auto-prep-holistic-prediction-of-data-preparation-steps-for-self-service-business-intelligence/"
    },
    {
      "title": "A 0.51-Approximation of Maximum Matching in Sublinear n^1.5 Time",
      "authors": [
        "Jakub Tarnawski",
        "Mohammad Roghani",
        "Sepideh Mahabadi"
      ],
      "research_areas": [
        "Algorithms"
      ],
      "publication_date": "July 2025",
      "abstract": "We study the problem of estimating the size of a maximum matching in sublinear time. The problem has been studied extensively in the literature and various algorithms and lower bounds are known for it. Our result is a $0.5109$-approximation algorithm with a running time of $\\tilde{O}(n\\sqrt{n})$. All previous algorithms either provide only a marginal improvement (e.g., $2^{-280}$) over the $0.5$-approximation that arises from estimating a \\emph{maximal} matching, or have a running time that is nearly $n^2$. Our approach is also arguably much simpler than other algorithms beating $0.5$-approximation.",
      "url": "https://www.microsoft.com/en-us/research/publication/a-0-51-approximation-of-maximum-matching-in-sublinear-n1-5-time/"
    },
    {
      "title": "TablePilot: Recommending Human-Preferred Tabular Data Analysis with Large Language Models",
      "authors": [
        "Deyin Yi",
        "Dongmei Zhang",
        "Haoyu Dong",
        "Lang Cao",
        "Mengyu Zhou",
        "Shi Han",
        "Yihao Liu"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Data platforms and analytics",
        "Human-computer interaction",
        "Programming languages and software engineering"
      ],
      "publication_date": "July 2025",
      "abstract": "Tabular data analysis is crucial in many scenarios, yet efficiently identifying relevant queries and results for new tables remains challenging due to data complexity, diverse analytical operations, and high-quality analysis requirements. To address these challenges, we aim to recommend query–code–result triplets tailored for new tables in tabular data analysis workflows. In this paper, we present TablePilot, a pioneering tabular data analysis framework leveraging large language models to autonomously generate comprehensive and superior analytical results without relying on user profiles or prior interactions. Additionally, we propose Rec-Align, a novel method to further improve recommendation quality and better align with human preferences. Experiments on DART, a dataset specifically designed for comprehensive tabular data analysis recommendation, demonstrate the effectiveness of our framework. Based on GPT-4o, the tuned TablePilot achieves 77.0% top-5 recommendation recall. Human evaluations further highlight its effectiveness in optimizing tabular data analysis workflows.",
      "url": "https://www.microsoft.com/en-us/research/publication/tablepilot-recommending-human-preferred-tabular-data-analysis-with-large-language-models/"
    },
    {
      "title": "Supporting Industry Computing Researchers in Assessing, Articulating, and Addressing the Potential Negative Societal Impact of Their Work",
      "authors": [
        "Jennifer Wortman Vaughan",
        "Solon Barocas",
        "Wesley Hanwen Deng"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Human-computer interaction"
      ],
      "publication_date": "November 2025",
      "abstract": "Recent years have witnessed increasing calls for computing researchers to grapple with the societal impacts of their work. Tools such as impact assessments have gained prominence as a method to uncover potential impacts, and a number of publication venues now encourage authors to include an impact statement in their submissions. Despite this push, little is known about the way researchers assess, articulate, and address the potential negative societal impact of their work — especially in industry settings, where research outcomes are often quickly integrated into products. In addition, while there are nascent efforts to support researchers in this task, there remains a dearth of empirically-informed tools and processes. Through interviews with 25 industry computing researchers across different companies and research areas, we first identify four key factors that influence how they grapple with (or choose not to grapple with) the societal impact of their research. To develop an effective impact assessment template tailored to industry computing researchers’ needs, we conduct an iterative co-design process with these 25 industry researchers and an additional 16 researchers and practitioners with prior experience and expertise in reviewing and developing impact assessments or broad responsible computing practices. Through the co-design process, we develop 10 design considerations to facilitate the effective design, development, and adaptation of an impact assessment template for use in industry research settings and beyond, as well as our own “Societal Impact Assessment” template with concrete scaffolds. We explore the effectiveness of this template through a user study with 15 industry research interns, revealing both its strengths and limitations. Finally, we discuss the implications for future researchers and organizations seeking to foster more responsible research practices.",
      "url": "https://www.microsoft.com/en-us/research/publication/supporting-industry-computing-researchers-in-assessing-articulating-and-addressing-the-potential-negative-societal-impact-of-their-work/"
    },
    {
      "title": "TableLoRA: Low-rank Adaptation on Table Structure Understanding for Large Language Models",
      "authors": [
        "Dongmei Zhang",
        "Haoyu Dong",
        "Mengyu Zhou",
        "Shi Han",
        "Xinyi He",
        "Yeye He",
        "Yihao Liu"
      ],
      "research_areas": [
        "Algorithms",
        "Artificial intelligence",
        "Data platforms and analytics",
        "Human language technologies"
      ],
      "publication_date": "July 2025",
      "abstract": "Tabular data are crucial in many fields and their understanding by large language models (LLMs) under high parameter efficiency paradigm is important.\nHowever, directly applying parameter-efficient fine-tuning (PEFT) techniques to tabular tasks presents significant challenges, particularly in terms of better table serialization and the representation of two-dimensional structured information within a one-dimensional sequence.\nTo address this, we propose TableLoRA, a module designed to improve LLMs’ understanding of table structure during PEFT. It incorporates special tokens for serializing tables with special token encoder and uses 2D LoRA to encode low-rank information on cell positions. Experiments on four tabular-related datasets demonstrate that TableLoRA consistently outperforms vanilla LoRA and surpasses various table encoding methods tested in control experiments. These findings reveal that TableLoRA, as a table-specific LoRA, enhances the ability of LLMs to process tabular data effectively, especially in low-parameter settings, demonstrating its potential as a robust solution for handling table-related tasks.",
      "url": "https://www.microsoft.com/en-us/research/publication/tablelora-low-rank-adaptation-on-table-structure-understanding-for-large-language-models/"
    },
    {
      "title": "Warbler: Speculative Distributed Transactions with Geo-Replication",
      "authors": [
        "Sebastian Angel",
        "Shuai Mu",
        "Siddhartha Sen",
        "Weihai Shen",
        "Yang Cui"
      ],
      "research_areas": [
        "Systems and networking"
      ],
      "publication_date": "July 2025",
      "abstract": "Abstract coming soon…",
      "url": "https://www.microsoft.com/en-us/research/publication/warbler-speculative-distributed-transactions-with-geo-replication/"
    },
    {
      "title": "Deriving Semantic Checkers from Tests to Detect Silent Failures in Production Distributed Systems",
      "authors": [
        "Achmad Imam Kistijantoro",
        "Chang Lou",
        "Dimas Shidqi Parikesit",
        "Ding Yuan",
        "Peng Huang",
        "Senapati Diwangkara",
        "Suman Nath",
        "Yujin Huang",
        "Yuzhuo Jing",
        "Zhewen Yang"
      ],
      "research_areas": [
        "Systems and networking"
      ],
      "publication_date": "July 2025",
      "abstract": "Production distributed systems provide rich features, but various defects can cause a system to silently violate its semantics without explicit errors. Such failures cause serious consequences. Yet, they are extremely challenging to detect, as it requires deep domain knowledge and substantial manual efforts to write good checkers. In this paper, we explore a novel approach that directly derive semantic checkers from existing test code in a system. We first present a large-scale feasibility study on existing test cases. Guided by the study findings, we develop T2C, a framework that uses static and dynamic analysis to transform and generalize a test into a runtime checker. We apply T2C on four large, popular distributed systems and successfully derive many checkers. Derived checkers are able to detect 15 out of 20 real-world silent failures with a small overhead incurred during runtime.",
      "url": "https://www.microsoft.com/en-us/research/publication/eriving-semantic-checkers-from-tests-to-detect-silent-failures-in-production-distributed-systems/"
    },
    {
      "title": "STACKFEED: Structured Textual Actor-Critic Knowledge Base Editing with Feedback",
      "authors": [
        "Arjun Radhakrishna",
        "Arun Iyer",
        "Gustavo Soares",
        "Krishna Kariya",
        "Naman Gupta",
        "Priyanshu Gupta",
        "Shashank Kirtania",
        "Sriram Rajamani",
        "Sumit Gulwani",
        "Suresh Parthasarathy"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "July 2025",
      "abstract": "Large Language Models (LLMs) often generate incorrect or outdated information, especially in low-resource settings or when dealing with private data. To address this, Retrieval-Augmented Generation (RAG) uses external knowledge bases (KBs), but these can also suffer from inaccuracies. We introduce STACKFEED, a novel Structured Textual Actor-Critic Knowledge base editing with FEEDback approach that iteratively refines the KB based on expert feedback using a multi-actor, centralized critic reinforcement learning framework. Each document is assigned to an actor, modeled as a ReACT agent, which performs structured edits based on document-specific targeted instructions from a centralized critic. Experimental results show that STACKFEED significantly improves KB quality and RAG system performance, enhancing accuracy by up to 8% over baselines.",
      "url": "https://www.microsoft.com/en-us/research/publication/stackfeed/"
    },
    {
      "title": "LLMoC: Large Language Model Inference at Wafer Scale",
      "authors": [
        "Congjie He",
        "Fan Yang",
        "Jilong Xue",
        "Lingxiao Ma",
        "Luo Mai",
        "Pei Mu",
        "Yeqi Huang",
        "Ziming Miao"
      ],
      "research_areas": [
        "Systems and networking"
      ],
      "publication_date": "July 2025",
      "abstract": "Emerging AI accelerators increasingly adopt wafer-scale manufacturing technologies, integrating hundreds of thousands of AI cores in a mesh architecture with large distributed on-chip memory (tens of GB in total) and ultra-high on-chip memory bandwidth (tens of PB/s). However, current LLM inference systems, optimized for shared memory architectures like GPUs, fail to exploit these accelerators fully.\nWe introduce WaferLLM, the first wafer-scale LLM inference system. WaferLLM is guided by a novel PLMR model (pronounced as “Plummer”) that captures the unique hardware characteristics of wafer-scale architectures. Leveraging this model, WaferLLM pioneers wafer-scale LLM parallelism, optimizing the utilization of hundreds of thousands of on-chip cores. It also introduces MeshGEMM and MeshGEMV, the first GEMM and GEMV implementations designed to scale effectively on wafer-scale accelerators.\nEvaluations show that WaferLLM achieves up to 200× higher accelerator utilization than state-of-the-art methods. Leveraging a wafer-scale accelerator (Cerebras WSE2), WaferLLM delivers GEMV operations 606× faster and 16× more energy-efficient than on an NVIDIA A100 GPU. For full LLM inference, WaferLLM achieves 10-20× speedups over A100 GPU clusters running SGLang and vLLM. These advantages are expected to grow as wafer-scale AI models, software, and hardware continue to mature. WaferLLM is open-sourced at https://github.com/MeshInfra/WaferLLM.",
      "url": "https://www.microsoft.com/en-us/research/publication/llmoc-large-language-model-inference-at-wafer-scale/"
    },
    {
      "title": "PoWER Never Corrupts: Tool-Agnostic Verification of Crash Consistency and Corruption Detection",
      "authors": [
        "Cheng Huang",
        "Chris Hawblitzel",
        "Hayley LeBlanc",
        "Jay Lorch",
        "Nickolai Zeldovich",
        "Vijay Chidambaram",
        "Yiheng Tao"
      ],
      "research_areas": [
        "Programming languages and software engineering",
        "Systems and networking"
      ],
      "publication_date": "July 2025",
      "abstract": "Storage systems must maintain integrity even after rare and difficult-to-test-for conditions like power losses and media errors. Formal verification presents a promising avenue to ensure storage systems are resilient, but current approaches involve significant complexity and rely on verification constructs or forms of logic beyond what most verifiers natively support. In this paper, we present two new verification techniques that rely only on standard constructs provided by most verification tools such as Hoare logic, ghost variables, and quantifiers. First, we introduce PoWER (Preconditions on Writes Enforcing Recoverability), a novel approach to verifying crash consistency that encodes its requirements in the preconditions of storage API methods. Second, we present a new model of media corruption for provable corruption detection on any type of storage device. To demonstrate the power of these new techniques, we use them to build two verified storage systems using two different verification frameworks. We build and verify the key-value (KV) store CapybaraKV using Verus and the notary server CapybaraNS using Dafny. Both systems are built for persistent memory (PM), which we target due to new challenges it presents to building resilient storage systems. We develop new techniques to address these challenges, including the corruption-detecting Boolean, a new primitive for atomic checksum updates. Both systems verify in under a minute, and CapybaraKV achieves performance competitive with similar unverified PM KV stores.",
      "url": "https://www.microsoft.com/en-us/research/publication/power-never-corrupts-tool-agnostic-verification-of-crash-consistency-and-corruption-detection/"
    },
    {
      "title": "Kamino: Efficient VM Allocation at Scale with Latency-Driven Cache-Aware Scheduling",
      "authors": [
        "Abhisek Pan",
        "David Dion",
        "David Domingo",
        "Hugo Barbalho",
        "Ishai Menache",
        "Kuan Liu",
        "Marco Molinaro",
        "Sudarsun Kannan",
        "Thomas Moscibroda"
      ],
      "research_areas": [
        "Systems and networking"
      ],
      "publication_date": "July 2025",
      "abstract": "In virtual machine (VM) allocation systems, caching repetitive and similar VM allocation requests and associated resolution rules is crucial for reducing computational costs and meeting strict latency requirements. While modern allocation systems distribute requests among multiple allocator agents\nand use caching to improve performance, current schedulers often neglect the cache state and latency considerations when assigning each new request to an agent. Due to the high variance in costs of cache hits and misses and the associated processing overheads of updating the caches, simple\nload-balancing and cache-aware mechanisms result in high latencies. We introduce Kamino, a high-performance, latency-driven and cache-aware request scheduling system aimed at minimizing end-to-end latencies. Kamino employs a novel scheduling algorithm grounded in theory which uses partial\nindicators from the cache state to assign each new request to the agent with the lowest estimated latency. Evaluation of Kamino using a high-fidelity simulator on large-scale production workloads shows a 42% reduction in average request latencies. Our deployment of Kamino in the control plane of a large public cloud confirms these improvements, with a 33% decrease in cache miss rates and 17% reduction in memory usage.",
      "url": "https://www.microsoft.com/en-us/research/publication/kamino-efficient-vm-allocation-at-scale-with-latency-driven-cache-aware-scheduling/"
    },
    {
      "title": "CollabLLM: From Passive Responders to Active Collaborators",
      "authors": [
        "Baolin Peng",
        "Gavin Li",
        "Hao Cheng",
        "J. Leskovec",
        "James Zou",
        "Jianfeng Gao",
        "Michel Galley",
        "Shirley Wu",
        "Weixin Cai",
        "Yao Dou"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Human language technologies",
        "Human-computer interaction"
      ],
      "publication_date": "July 2025",
      "abstract": "Large Language Models are typically trained with next-turn rewards, limiting their ability to optimize for long-term interaction. As a result, they often respond passively to ambiguous or open-ended user requests, failing to help users reach their ultimate intents and leading to inefficient conversations. To address these limitations, we introduce CollabLLM, a novel and general training framework that enhances multiturn human-LLM collaboration. Its key innovation is a collaborative simulation that estimates the long-term contribution of responses using Multiturn-aware Rewards. By reinforcement fine-tuning these rewards, CollabLLM goes beyond responding to user requests, and actively uncovers user intent and offers insightful suggestions-a key step towards more human-centered AI. We also devise a multiturn interaction benchmark with three challenging tasks such as document creation. CollabLLM significantly outperforms our baselines with averages of 18.5% higher task performance and 46.3% improved interactivity by LLM judges. Finally, we conduct a large user study with 201 judges, where CollabLLM increases user satisfaction by 17.6% and reduces user spent time by 10.4%.",
      "url": "https://www.microsoft.com/en-us/research/publication/collabllm-from-passive-responders-to-active-collaborators/"
    },
    {
      "title": "LongRoPE2: Near-Lossless LLM Context Window Scaling",
      "authors": [
        "Fan Yang",
        "Gaokai Zhang",
        "Gilsinia Lopez",
        "Li Lyna Zhang",
        "Mao Yang",
        "Ning Shang",
        "Siyuan Wang",
        "Weizhu Chen"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Systems and networking"
      ],
      "publication_date": "July 2025",
      "abstract": "LongRoPE2 is a novel approach that extends the effective context window of pre-trained large language models (LLMs) to the target length, while preserving the performance on the original shorter context window. This is achieved by three contributions: (1) a hypothesis that insufficient training in higher RoPE dimensions contributes to the persistent out-of-distribution (OOD) issues observed in existing methods; (2) an effective RoPE rescaling algorithm that adopts evolutionary search guided by “needle-driven” perplexity to address the insufficient training problem; (3) a mixed context window training approach that fine-tunes model weights to adopt rescaled RoPE for long-context sequences while preserving the short-context performance with the original RoPE. Extensive experiments on LLaMA3-8B and Phi3-mini-3.8B across various benchmarks validate the hypothesis and demonstrate the effectiveness of LongRoPE2. Remarkably, LongRoPE2 extends LLaMA3-8B to achieve a 128K effective context length while retaining over 98.5% of short-context performance, using only 10B tokens — 80x fewer than Meta’s approach, which fails to reach the target effective context length.",
      "url": "https://www.microsoft.com/en-us/research/publication/longrope2-near-lossless-llm-context-window-scaling/"
    },
    {
      "title": "DPImageBench: A Unified Benchmark for Differentially Private Image Synthesis",
      "authors": [
        "Chen Gong",
        "Kecen Li",
        "Tianhao Wang",
        "Zinan Lin"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Security, privacy, and cryptography"
      ],
      "publication_date": "July 2025",
      "abstract": "Differentially private (DP) image synthesis aims to generate artificial images that retain the properties of sensitive images while protecting the privacy of individual images within the dataset. Despite recent advancements, we find that inconsistent–and sometimes flawed–evaluation protocols have been applied across studies. This not only impedes the understanding of current methods but also hinders future advancements.\nTo address the issue, this paper introduces DPImageBench for DP image synthesis, with thoughtful design across several dimensions: (1) Methods. We study eleven prominent methods and systematically characterize each based on model architecture, pretraining strategy, and privacy mechanism. (2) Evaluation. We include nine datasets and seven fidelity and utility metrics to thoroughly assess them. Notably, we find that a common practice of selecting downstream classifiers based on the highest accuracy on the sensitive test set not only violates DP but also overestimates the utility scores. DPImageBench corrects for these mistakes. (3) Platform. Despite the methods and evaluation protocols, DPImageBench provides a standardized interface that accommodates current and future implementations within a unified framework. With DPImageBench, we have several noteworthy findings. For example, contrary to the common wisdom that pretraining on public image datasets is usually beneficial, we find that the distributional similarity between pretraining and sensitive images significantly impacts the performance of the synthetic images and does not always yield improvements. In addition, adding noise to low-dimensional features, such as the high-level characteristics of sensitive images, is less affected by the privacy budget compared to adding noise to high-dimensional features, like weight gradients. The former methods perform better than the latter under a low privacy budget.",
      "url": "https://www.microsoft.com/en-us/research/publication/dpimagebench-a-unified-benchmark-for-differentially-private-image-synthesis/"
    },
    {
      "title": "NextCoder: Robust Adaptation of Code LMs to Diverse Code Edits",
      "authors": [
        "Abhijeet Awasthi",
        "Aditya Kanade",
        "Nagarajan Natarajan",
        "Swayam Singh*",
        "Tushar Aggarwal*"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Programming languages and software engineering"
      ],
      "publication_date": "July 2025",
      "abstract": "Software engineering activities frequently involve edits to existing code. However, contemporary code language models (LMs) lack the ability to handle diverse types of code-edit requirements. In this work, we attempt to overcome this shortcoming through (1) a novel synthetic data generation pipeline and (2) a robust model adaptation algorithm. Starting with seed code examples and diverse editing criteria, our pipeline generates high-quality samples comprising original and modified code, along with natural language instructions in different styles and verbosity. Today’s code LMs come bundled with strong abilities, such as code generation and instruction following, which should not be lost due to fine-tuning. To ensure this, we propose a novel adaptation algorithm, SeleKT, that (a) leverages a dense gradient-based step to identify the weights that are most important for code editing, and (b) does a sparse projection onto the base model to avoid overfitting. Using our approach, we obtain a new series of models NextCoder (adapted from QwenCoder-2.5) that achieves strong results on five code-editing benchmarks, outperforming comparable size models and even several larger ones. We show the generality of our approach on two model families (DeepSeekCoder and QwenCoder), compare against other fine-tuning approaches, and demonstrate robustness by showing retention of code generation and general problem-solving abilities post adaptation. We opensource the models, synthetic dataset, and implementation at huggingface (opens in new tab) [coming soon!].",
      "url": "https://www.microsoft.com/en-us/research/publication/nextcoder-robust-adaptation-of-code-lms-to-diverse-code-edits/"
    },
    {
      "title": "PipeThreader: Software-Defined Pipelining for Efficient DNN Execution",
      "authors": [
        "Fan Yang",
        "Feiyang Chen",
        "Jilong Xue",
        "Lei Wang",
        "Lingxiao Ma",
        "Mao Yang",
        "Yang Wang",
        "Yining Shi",
        "Yu Cheng",
        "Yuqing Xia",
        "Zhi Yang",
        "Zhiwen Mo"
      ],
      "research_areas": [
        "Systems and networking"
      ],
      "publication_date": "July 2025",
      "abstract": "To effectively utilize heterogeneous specialized hardware units in modern GPUs, such as TensorCores and Tensor Memory Accelerators, this paper introduces PipeThreader, a new DNN compiler. PipeThreader proposes shifting scheduling functionality from hardware to software so as to enable more efficient and sophisticated computation pipelining with minimal manual effort. This is achieved through sTask-graph, a new DNN computation abstraction, a hierarchical hardware abstraction that captures the capabilities of specialized units, and new scheduling primitives. As a result, PipeThreader can discover efficient pipeline scheduling for well-studied DNN architectures like FlashAttention, achieving comparable or even superior performance. Additionally, it can uncover novel pipeline schemes for emerging models like Mamba2, delivering significantly better performance compared to state-of-the-art hand-crafted implementations. The code is open-sourced at https://github.com/tile-ai/tilelang (opens in new tab).",
      "url": "https://www.microsoft.com/en-us/research/publication/pipethreader-software-defined-pipelining-for-efficient-dnn-execution/"
    },
    {
      "title": "Offline Learning for Combinatorial Multi-armed Bandits",
      "authors": [
        "Carlee Joe-Wong",
        "Jinhang Zuo",
        "John C.S. Lui",
        "Siwei Wang",
        "Wei Chen",
        "Xiangxiang Dai",
        "Xutong Liu"
      ],
      "research_areas": [
        "Algorithms",
        "Artificial intelligence"
      ],
      "publication_date": "July 2025",
      "abstract": "The combinatorial multi-armed bandit (CMAB) is a fundamental sequential decision-making framework, extensively studied over the past decade. However, existing work primarily focuses on the online setting, overlooking the substantial costs of online interactions and the readily available offline datasets. To overcome these limitations, we introduce Off-CMAB, the first offline learning framework for CMAB. Central to our framework is the combinatorial lower confidence bound (CLCB) algorithm, which combines pessimistic reward estimations with combinatorial solvers. To characterize the quality of offline datasets, we propose two novel data coverage conditions and prove that, under these conditions, CLCB achieves a near-optimal suboptimality gap, matching the theoretical lower bound up to a logarithmic factor. We validate Off-CMAB through practical applications, including learning to rank, large language model (LLM) caching, and social influence maximization, showing its ability to handle nonlinear reward functions, general feedback models, and out-of-distribution action samples that exclude optimal or even feasible actions. Extensive experiments on synthetic and real-world datasets for these applications further highlight the superior performance of CLCB.",
      "url": "https://www.microsoft.com/en-us/research/publication/offline-learning-for-combinatorial-multi-armed-bandits/"
    },
    {
      "title": "System comparison using automated generation of relevance judgements in multiple languages",
      "authors": [
        "Dawn Lawrie",
        "Douglas W Oard",
        "Eugene Yang",
        "James Mayfield",
        "Paul Thomas"
      ],
      "research_areas": [
        "Search and information retrieval"
      ],
      "publication_date": "July 2025",
      "abstract": "Recent work has shown that Large Language Models (LLMs) can produce relevance judgements for English retrieval that are useful as a basis for system comparison, and they do so at vastly reduced cost compared to human assessors. Using relevance judgements and ranked retrieval runs from the TREC NeuCLIR track, this paper shows that LLMs can also produce reliable assessments in other languages, even when the topic description or the prompt are in a language different from the documents. Results with Chinese, Persian and Russian documents show that although document language affects both agreement with human assessors on graded relevance and on preference ordering among systems, prompt-language and topic-language effects are negligible. This has implications for the design of multilingual test collections, suggesting that prompts and topic descriptions can be developed in any convenient language.",
      "url": "https://www.microsoft.com/en-us/research/publication/system-comparison-using-automated-generation-of-relevance-judgements-in-multiple-languages/"
    },
    {
      "title": "CAD-Editor: A Locate-then-Infill Framework with Automated Training Data Synthesis for Text-Based CAD Editing",
      "authors": [
        "Jiang Bian",
        "Qi Liu",
        "Shizhao Sun",
        "Yu Yuan"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Computer vision"
      ],
      "publication_date": "July 2025",
      "abstract": "Computer Aided Design (CAD) is indispensable across various industries. \\emph{Text-based CAD editing}, which automates the modification of CAD models based on textual instructions, holds great potential but remains underexplored. Existing methods primarily focus on design variation generation or text-based CAD generation, either lacking support for text-based control or neglecting existing CAD models as constraints. We introduce \\emph{CAD-Editor}, the first framework for text-based CAD editing. To address the challenge of demanding triplet data with accurate correspondence for training, we propose an automated data synthesis pipeline. This pipeline utilizes design variation models to generate pairs of original and edited CAD models and employs Large Vision-Language Models (LVLMs) to summarize their differences into editing instructions. To tackle the composite nature of text-based CAD editing, we propose a locate-then-infill framework that decomposes the task into two focused sub-tasks: locating regions requiring modification and infilling these regions with appropriate edits. Large Language Models (LLMs) serve as the backbone for both sub-tasks, leveraging their capabilities in natural language understanding and CAD knowledge. Experiments show that CAD-Editor achieves superior performance both quantitatively and qualitatively.",
      "url": "https://www.microsoft.com/en-us/research/publication/cad-editor-a-locate-then-infill-framework-with-automated-training-data-synthesis-for-text-based-cad-editing/"
    },
    {
      "title": "Mapping global floods with 10 years of satellite radar data",
      "authors": [
        "Amit Misra",
        "Juan M. Lavista Ferres",
        "Kevin White",
        "Simone Fobi Nsutezo",
        "William Straka III"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Computer vision",
        "Ecology and environment"
      ],
      "publication_date": "July 2025",
      "abstract": "Floods cause extensive global damage annually, making effective monitoring essential. While satellite observations have proven invaluable for flood detection and tracking, comprehensive global flood datasets spanning extended time periods remain scarce. In this study, we introduce a deep learning flood detection model that leverages the cloud-penetrating capabilities of Sentinel-1 Synthetic Aperture Radar (SAR) satellite imagery, enabling consistent flood extent mapping through cloud cover and in both day and night conditions. By applying this model to 10 years of SAR data, we create a unique, longitudinal global flood extent dataset with predictions unaffected by cloud coverage, offering comprehensive and consistent insights into historically flood-prone areas over the past decade. We use our model predictions to identify historically flood-prone areas in Ethiopia and demonstrate real-time disaster response capabilities during the May 2024 floods in Kenya. Additionally, our longitudinal analysis reveals potential increasing trends in global flood extent over time, although further validation is required to explore links to climate change. To maximize impact, we provide public access to both our model predictions and a code repository, empowering researchers and practitioners worldwide to advance flood monitoring and enhance disaster response strategies.",
      "url": "https://www.microsoft.com/en-us/research/publication/mapping-global-floods-with-10-years-of-satellite-radar-data/"
    },
    {
      "title": "Designing Interfaces that Support Temporal Work Across  Meetings with Generative AI",
      "authors": [
        "Ava Elizabeth Scott",
        "Lev Tankelevitch",
        "Payod Panda",
        "Rishi Vanukuru",
        "Sean Rintel",
        "Xinyue Chen"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Human-computer interaction",
        "Social sciences"
      ],
      "publication_date": "July 2025",
      "abstract": "Temporal work is an essential part of the modern knowledge workplace, where multiple threads of meetings and projects are connected across time by the acts of looking back (retrospection) and ahead (prospection). As we develop Generative AI interfaces to support knowledge work, this lens of temporality can help ground design in real workplace needs. Building upon research in routine dynamics and cognitive science, and an exploratory analysis of real recurring meetings, we develop a framework and a tool for the synergistic exploration of temporal work and the capabilities of Generative AI. We then use these to design a series of interface concepts and prototypes to better support work that spans multiple scales of time. Through this approach, we demonstrate how the design of new Generative AI tools can be guided by our understanding of how work really happens across meetings and projects.\nKEYWORDS: videoconferencing, meetings, goal, recurring, temporal work, retrospection, prospection, intentionality, generative AI, support, tools\n\nRELATED RESEARCH\n\n\nFormative studies\n\n\n\nMental Models of Meeting Goals: Supporting Intentionality in Meeting Technology\n\n\n\nPrototype studies\n\n\n\nBefore meetings: What Does Success Look Like? Catalyzing Meeting Intentionality with AI-Assisted Prospective Reflection\n\n\nBefore and during meetings: The CoExplorer Technology Probe: A Generative AI-Powered Adaptive Interface to Support Intentionality in Planning and Running Video Meetings\n\n\nDuring meetings: Are We On Track? AI-Assisted Active and Passive Goal Reflection During Meetings – Microsoft Research\n\n\nBetween meetings: Designing Interfaces that Support Temporal Work Across Meetings with Generative AI\n",
      "url": "https://www.microsoft.com/en-us/research/publication/designing-interfaces-that-support-temporal-work-across-meetings-with-generative-ai/"
    },
    {
      "title": "Graph-Based Algorithms for Diverse Similarity Search",
      "authors": [
        "Haike Xu",
        "Kiran Shiragur",
        "Piotr Indyk",
        "Piyush Anand",
        "Ravishankar Krishnaswamy",
        "Sepideh Mahabadi",
        "Vikas C. Raykar"
      ],
      "research_areas": [
        "Algorithms",
        "Search and information retrieval"
      ],
      "publication_date": "July 2025",
      "abstract": "Nearest neighbor search is a fundamental data structure problem with many applications in machine learning, computer vision, recommendation systems and other fields. Although the main objective of the data structure is to quickly report data points that are closest to a given query, it has long been noted (Carbonell and Goldstein, 1998) that without additional constraints the reported answers can be redundant and/or duplicative. This issue is typically addressed in two stages: in the first stage, the algorithm retrieves a (large) number r of points closest to the query, while in the second stage, the r points are post-processed and a small subset is selected to maximize the desired diversity objective. Although popular, this method suffers from a fundamental efficiency bottleneck, as the set of points retrieved in the first stage often needs to be much larger than the final output.\nIn this paper we present provably efficient algorithms for approximate nearest neighbor search with diversity constraints that bypass this two stage process. Our algorithms are based on popular graph-based methods, which allows us to “piggy-back” on the existing efficient implementations. These are the first graph-based algorithms for nearest neighbor search with diversity constraints. For data sets with low intrinsic dimension, our data structures report a diverse set of k points approximately closest to the query, in time that only depends on k and \\log \\Delta, where \\Delta is the ratio of the diameter to the closest pair distance in the data set. This bound is qualitatively similar to the best known bounds for standard (non-diverse) graph-based algorithms. Our experiments show that the search time of our algorithms is substantially lower than that using the standard two-stage approach.",
      "url": "https://www.microsoft.com/en-us/research/publication/graph-based-algorithms-for-diverse-similarity-search/"
    },
    {
      "title": "Guessing Efficiently for Constrained Subspace Approximation",
      "authors": [
        "Aditya Bhaskara",
        "Ali Vakilian",
        "David Woodruff",
        "Madhusudhan Reddy Pittu",
        "Sepideh Mahabadi"
      ],
      "research_areas": [
        "Algorithms"
      ],
      "publication_date": "July 2025",
      "abstract": "In this paper we study constrained subspace approximation problem. Given a set of points {a_1,…,a_n} in R^d , the goal of the subspace approximation problem is to find a k-dimensional subspace that best approximates the input points. More precisely, for a given p>=1, we aim to minimize the pth power of the l_p norm of the error vector (||a_1- Pa_1||,…,||a_n-Pa_n||), where P denotes the projection matrix onto the subspace and the norms are Euclidean. In constrained subspace approximation (CSA), we additionally have constraints on the projection matrix P. In its most general form, we require P to belong to a given subset S that is described explicitly or implicitly.\nWe introduce a general framework for constrained subspace approximation. Our approach, that we term coreset-guess-solve, yields either (1+ε)-multiplicative or ε-additive approximations for a variety of constraints. We show that it provides new algorithms for partition-constrained subspace approximation with applications to fair subspace approximation, k-means clustering, and projected non-negative matrix factorization, among others. Specifically, while we reconstruct the best known bounds for k-means clustering in Euclidean spaces, we improve the known results for the remainder of the problems.",
      "url": "https://www.microsoft.com/en-us/research/publication/guessing-efficiently-for-constrained-subspace-approximation/"
    },
    {
      "title": "Position: To Make Text-to-Image Models that Work for Marginalized Communities, We Need New Measurement Practices for the Long Tail",
      "authors": [
        "Cecily Morrison",
        "Daniela Massiceti",
        "Deepthi Sudharsan",
        "Hamna .",
        "Jennifer Wortman Vaughan",
        "Nari Johnson",
        "Samantha Dalal",
        "Siobhan Mackenzie Hall",
        "Theo Holroyd"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Computer vision",
        "Human-computer interaction"
      ],
      "publication_date": "July 2025",
      "abstract": "While the capabilities of frontier text-to-image models are rapidly improving, they often fail to represent the low data, long tail concepts that matter to historically marginalized communities. Effective measurement is a critical first step towards identifying and addressing these errors, yet little work has validated if existing T2I evaluation metrics work for the long tail. In this paper, we draw upon two community-based case studies to identify challenges with applying best practices to validate T2I metrics using human preference data. We show that available approaches to create and validate evaluation metrics break down when applied to tail concepts because of the need for community knowledge (scaling community annotations) and challenges achieving a range of good and bad images (shades of bad). We take the position that methodological innovation is needed to develop measurement practices that work for the long tail. We outline directions for future work that moves beyond traditional approaches to measurement towards imagining new ways to center community expertise throughout the measurement process.",
      "url": "https://www.microsoft.com/en-us/research/publication/position-to-make-text-to-image-models-that-work-for-marginalized-communities-we-need-new-measurement-practices-for-the-long-tail/"
    },
    {
      "title": "Text-to-CAD Generation Through Infusing Visual Feedback in Large Language Models",
      "authors": [
        "Jiang Bian",
        "Ruiyu Wang",
        "Shizhao Sun",
        "Yu Yuan"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Computer vision"
      ],
      "publication_date": "July 2025",
      "abstract": "Creating Computer-Aided Design (CAD) models requires significant expertise and effort. Text-to-CAD, which converts textual descriptions into CAD parametric sequences, is crucial in streamlining this process. Recent studies have utilized ground-truth parametric sequences, known as sequential signals, as supervision to achieve this goal. However, CAD models are inherently multimodal, comprising parametric sequences and corresponding rendered visual objects. Besides,the rendering process from parametric sequences to visual objects is many-to-one. Therefore, both sequential and visual signals are critical for effective training. In this work, we introduce CADFusion, a framework that uses Large Language Models (LLMs) as the backbone and alternates between two training stages: the sequential learning (SL) stage and the visual feedback (VF) stage. In the SL stage, we train LLMs using ground-truth parametric sequences, enabling the generation of logically coherent parametric sequences. In the VF stage, we reward parametric sequences that render into visually preferred objects and penalize those that do not, allowing LLMs to learn how rendered visual objects are perceived and evaluated. These two stages alternate throughout the training, ensuring balanced learning and preserving benefits of both signals. Experiments demonstrate that CADFusion significantly improves performance, both qualitatively and quantitatively.",
      "url": "https://www.microsoft.com/en-us/research/publication/text-to-cad-generation-through-infusing-visual-feedback-in-large-language-models/"
    },
    {
      "title": "Demographically-inspired query variants using an LLM",
      "authors": [
        "Falk Scholer",
        "Mark Sanderson",
        "Marwah Alaofi",
        "Nicola Ferro",
        "Paul Thomas"
      ],
      "research_areas": [
        "Search and information retrieval"
      ],
      "publication_date": "July 2025",
      "abstract": "This study proposes a method to diversify queries in existing test collections to reflect some of the diversity of search engine users, aligning with an earlier vision of an ‘ideal’ test collection. A Large Language Model (LLM) is used to create query variants: alternative\nqueries that have the same meaning as the original. These variants represent user profiles characterised by different properties, such as language and domain proficiency, which are known in the Information Retrieval (IR) literature to influence query formulation.\nThe LLM’s ability to generate query variants that align with user profiles is empirically validated, and the variants’ utility is further explored for IR system evaluation. Results demonstrate that the variants impact how systems are ranked and show that user profiles experience significantly different levels of system effectiveness. This method enables an alternative perspective on system evaluation where we can observe both the impact of user profiles on system rankings and how system performance varies across users.",
      "url": "https://www.microsoft.com/en-us/research/publication/demographically-inspired-query-variants-using-an-llm/"
    },
    {
      "title": "ReproCopilot: LLM-Driven Failure Reproduction with Dynamic Refinement",
      "authors": [
        "Fazle Faisal",
        "Suman Nath",
        "Tanakorn Leesatapornwongsa"
      ],
      "research_areas": [
        "Systems and networking"
      ],
      "publication_date": "June 2025",
      "abstract": "Failure reproduction is a crucial step for debugging software systems, but it is often challenging and time-consuming, especially when the failures depend on complex inputs, states, or environments. In this paper, we present ReproCopilot, a tool that leverages program analysis and a large language model (LLM) to generate failure reproduction code and inputs. ReproCopilot proposes two novel techniques: state-oriented code generation and dynamic refinement that iteratively guide the LLM with program analysis feedback until the generated code can successfully reproduce the failure. We evaluate ReproCopilot on 37 real-world cases from 15 open-source projects, and show that it can reproduce 78% of them, significantly outperforming the-state-of-the-art solutions.",
      "url": "https://www.microsoft.com/en-us/research/publication/reprocopilot-llm-driven-failure-reproduction-with-dynamic-refinement/"
    },
    {
      "title": "Accurate and scalable exchange-correlation with deep learning",
      "authors": [
        "Abylay Katbashev",
        "Amir Karton",
        "Bálint Máté",
        "Chin-Wei Huang",
        "Christopher Bishop",
        "David B. Williams-Young",
        "Deniz Gunceler",
        "Derk Kooi",
        "Giulia Luise",
        "Jan Hermann",
        "Jose Garrido Torres",
        "K.J.H. Giesbertz",
        "Lin Huang",
        "Megan Stanley",
        "Paola Gori-Giorgi",
        "Rianne van den Berg",
        "Roberto Sordillo",
        "Rodrigo Chavez Zavaleta",
        "Sebastian Ehlert",
        "Stephanie Lanius",
        "Sékou-Oumar Kaba",
        "Thijs Vogels",
        "Wessel Bruinsma",
        "Xinran wei",
        "Yingrong Chen"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "June 2025",
      "abstract": "Density Functional Theory (DFT) is the most widely used electronic structure method for predicting the properties of molecules and materials. Although DFT is, in principle, an exact reformulation of the Schrödinger equation, practical applications rely on approximations to the unknown exchange-correlation (XC) functional. Most existing XC functionals are constructed using a limited set of increasingly complex, hand-crafted features that improve accuracy at the expense of computational efficiency. Yet, no current approximation achieves the accuracy and generality for predictive modeling of laboratory experiments at chemical accuracy — typically defined as errors below 1 kcal/mol. In this work, we present Skala, a modern deep learning-based XC functional that bypasses expensive hand-designed features by learning representations directly from data. Skala achieves chemical accuracy for atomization energies of small molecules while retaining the computational efficiency typical of semi-local DFT. This performance is enabled by training on an unprecedented volume of high-accuracy reference data generated using computationally intensive wavefunction-based methods. Notably, Skala systematically improves with additional training data covering diverse chemistry. By incorporating a modest amount of additional high-accuracy data tailored to chemistry beyond atomization energies, Skala achieves accuracy competitive with the best-performing hybrid functionals across general main group chemistry, at the cost of semi-local DFT. As the training dataset continues to expand, Skala is poised to further enhance the predictive power of first-principles simulations.",
      "url": "https://www.microsoft.com/en-us/research/publication/accurate-and-scalable-exchange-correlation-with-deep-learning/"
    },
    {
      "title": "Reduction Fusion for Optimized Distributed Data-Parallel Computations via Inverse Recomputation",
      "authors": [
        "Haoxiang Lin",
        "Hongyu Zhang",
        "Mao Yang",
        "Ming Wu",
        "Yang Wang",
        "Yanjie Gao"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Programming languages and software engineering",
        "Systems and networking"
      ],
      "publication_date": "June 2025",
      "abstract": "Distributed data-parallel computations are critical for both traditional big data applications and emerging large language model tasks. The efficiency of these computations largely depends on reducer performance, particularly in handling extensive data access. This paper introduces a novel reduction fusion algorithm that optimizes distributed data-parallel programs by fusing dependent reducers and mappers into a single, unified reducer. Employing inverse recomputation, the algorithm preserves partial aggregation and reduces storage, network I/O, memory, and cache overheads. Our preliminary evaluation reveals performance improvements of up to 2.47×, demonstrating the practicality and effectiveness of this approach, while also highlighting its potential to address challenges posed by extensive data access in modern distributed computing environments.",
      "url": "https://www.microsoft.com/en-us/research/publication/reduction-fusion-for-optimized-distributed-data-parallel-computations-via-inverse-recomputation/"
    },
    {
      "title": "PadChest-GR: A Bilingual Chest X-ray Dataset for Grounded Radiology Report Generation",
      "authors": [
        "Antonio Pertusa",
        "Aurelia Bustos",
        "Daniel Coelho de Castro",
        "Javier Alvarez-Valle",
        "Joaquín Galant-Herrero",
        "José María Salinas-Serrano",
        "Kenji Takeda",
        "Kenza Bouzid",
        "Lara Jaques-Pérez",
        "Lourdes Pérez-Rodríguez",
        "Maria Dolores Sánchez-Valverde",
        "Maria Teodora Wetscherek",
        "Shruthi Bannur",
        "Stephanie Hyland"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Medical, health and genomics"
      ],
      "publication_date": "June 2025",
      "abstract": "Background\nArtificial intelligence (AI)–powered radiology report generation (RRG) aims to create free-text radiology reports from clinical imaging. Grounded radiology report generation (GRRG) augments RRG by including the localization of individual findings on the image. Currently, to our knowledge, no manually annotated chest x-ray (CXR) datasets exist on which to train GRRG models.\nMethods\nIn this article, we present a dataset called PadChest-GR (grounded reporting), which is derived from the CXR dataset, PadChest, and aimed at training GRRG models to analyze CXR images. First, we selected a subset of studies from PadChest that contained images with frontal projection; studies that were originally labeled as suboptimal and those involving pediatric patients were excluded. Then, using Generative Pretrained Transformer 4 in Microsoft Azure OpenAI Service, we processed reports to extract sentences with single findings, translate them from Spanish into English, link them to the existing PadChest finding and location labels, and classify the finding progression. A team of 14 radiologists discarded studies with poor image quality or issues relating to the report or findings list and then manually annotated the findings using bounding boxes to surround regions of interest in each image.\nResults\nWe curated a public bilingual dataset of 4555 CXR studies with grounded reports, of which 3099 were abnormal and 1456 were normal. Each report contains complete lists of sentences describing individual present (positive) findings and absent (negative) findings in English and Spanish. In total, PadChest-GR contains 7037 positive-finding sentences and 3422 negative-finding sentences. Every positive-finding sentence is associated with up to two independent sets of bounding boxes labeled by different readers and has categorical labels for finding type, locations, and progression.\nConclusions\nPadChest-GR is a manually curated dataset designed to train GRRG models to understand and interpret radiological images and generated text. By including detailed localization and comprehensive annotations of all clinically relevant findings, PadChest-GR provides a valuable resource for developing and evaluating GRRG models from CXR images.",
      "url": "https://www.microsoft.com/en-us/research/publication/padchest-gr/"
    },
    {
      "title": "An Empirical Study of Issues in Large Language Model Training Systems",
      "authors": [
        "Haoxiang Lin",
        "Ruiming Lu",
        "Yanjie Gao",
        "Yueguo Chen"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Programming languages and software engineering",
        "Systems and networking"
      ],
      "publication_date": "June 2025",
      "abstract": "Large language models (LLMs) have gained significant traction in recent years, driving advancements in various applications. The training and evaluation of these models depend heavily on specialized LLM training systems, which are deployed across numerous GPUs, partition LLMs, and process large datasets. However, issues in LLM training systems can lead to program crashes or unexpected behavior, reducing development productivity and wasting valuable resources such as GPUs and storage.\nThis paper presents the first comprehensive empirical study of issues in LLM training systems. We conducted a manual analysis of 300 high-quality issue reports and corresponding fix commits from the GitHub repositories of three prominent LLM training systems: Microsoft DeepSpeed, NVIDIA Megatron-LM, and Hugging Face Transformers. Our analysis identified common symptoms, root causes, typical fixes, and debugging and testing practices in LLM training systems. Our major findings include: (1) LLM training systems exhibit issues and trends that are uncommon in traditional deep learning, such as Concurrency Error and Tensor Management Error occurring in parallel training, which are particularly difficult to diagnose and resolve. (2) The primary root causes of these issues are API Misuse (19.67%), Configuration Error (18.33%), and General Code Error (16.33%), respectively. Such issues often arise from the rapid evolution of the systems, the integration of complex external dependencies, and a configuration-driven development paradigm. (3) Current testing and debugging practices are often insufficient for identifying issues related to parallel training and large-scale numerical computations. Based on our findings, we propose several research topics and tooling improvements that can facilitate the future development of LLMs.",
      "url": "https://www.microsoft.com/en-us/research/publication/an-empirical-study-of-issues-in-large-language-model-training-systems/"
    },
    {
      "title": "dl²: Detecting Communication Deadlocks in Deep Learning Jobs",
      "authors": [
        "Haoxiang Lin",
        "Hongyu Zhang",
        "Jiyu Luo",
        "Mao Yang",
        "Ming Wu",
        "Yanjie Gao"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Programming languages and software engineering",
        "Systems and networking"
      ],
      "publication_date": "June 2025",
      "abstract": "In recent years, deep learning has seen widespread adoption across various domains, giving rise to large-scale models such as large language models. Training these models, particularly in distributed environments, presents substantial computational and communication challenges. A critical issue is the communication deadlock—a state in which processes become indefinitely stalled while awaiting network messages from others, which leads to resource wastage and reduced productivity. Current approaches to deadlock handling are either unsuitable for deep learning due to its unique hybrid programming paradigm or limit optimization opportunities. This paper presents dl2, a novel dynamic analysis tool designed to detect communication deadlocks in deep learning jobs. dl2 models the runtime trace of a job as an execution graph, detects unmatched communications, and constructs a wait-for graph to identify deadlock cycles. dl2 can also handle nondeterministic communication behaviors, providing replay and diagnostic support for root cause analysis. We evaluate dl2 using PyTorch with a combination of synthetic test cases and real-world deep learning workloads. The experimental results show that dl2 successfully detects all communication deadlocks, achieving 100% precision and recall, which highlights its effectiveness.",
      "url": "https://www.microsoft.com/en-us/research/publication/dl²-detecting-communication-deadlocks-in-deep-learning-jobs/"
    },
    {
      "title": "Rockhopper: A Robust Optimizer for Spark Configuration Tuning in Production Environment",
      "authors": [
        "Aditya Lakra",
        "Andreas Mueller",
        "Arshdeep Sekhon",
        "Ashit Gosalia",
        "Brian Kroth",
        "Carlo Curino",
        "Dario Bernal",
        "Dhruv Relwani",
        "Estera Kot",
        "Karuna Sagar Krishna",
        "Long Tian",
        "Mo Liu",
        "Rahul Challapalli",
        "Rathijit Sen",
        "Rui Fang",
        "Sergiy Matusevych",
        "Shaily Fozdar",
        "Subru Krishnan",
        "Sule Kahraman",
        "Tengfei Huang",
        "Weihan Tang",
        "Xin He",
        "Yiwen Zhu"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Data platforms and analytics"
      ],
      "publication_date": "June 2025",
      "abstract": "Apache Spark, renowned for its scalability and ease of use, has become the standard for big data processing. However, optimizing Spark performance in production environments poses significant challenges. Traditional machine learning-based configuration tuning methods often necessitate extensive resources, lengthy experimentation, and risk performance regressions. Observational noise in production environments further complicates the tuning process, leading to suboptimal results. This paper presents an adaptive, robust learning approach leveraging insights from benchmark workloads to improve production tuning strategies. We propose a Centroid Learning algorithm resilient to noise, minimizing regressions and prioritizing promising configurations, combined with a workload embedding technique for context-aware adaptation and transfer learning. Evaluations using benchmark and customer workloads show consistent performance gains. Released in June 2024 as part of the Microsoft Fabric Spark offering, even with dynamic and evolving workloads, the system delivers approximately a 20% performance improvement in production for customer workloads by only tuning three query-level configurations.",
      "url": "https://www.microsoft.com/en-us/research/publication/rockhopper-a-robust-optimizer-for-spark-configuration-tuning-in-production-environment/"
    },
    {
      "title": "Autotuning Systems: Techniques, Challenges, and Opportunities",
      "authors": [
        "Brian Kroth",
        "Sergiy Matusevych",
        "Yiwen Zhu"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Data platforms and analytics",
        "Systems and networking"
      ],
      "publication_date": "June 2025",
      "abstract": "The rapid growth of cloud computing and systems has introduced significant complexity in managing and optimizing configurations to meet diverse workload demands across a wide array of hardware. Autotuning systems, leveraging advancements in machine learning and optimization, offer an effective solution to these challenges. By automating configuration tuning, these systems can dynamically adapt to workload changes, optimize performance in real time, and reduce the burden on system administrators. This tutorial provides both theoretical foundations and practical demonstrations of systems autotuning software, with a focus on offline and online optimization methodologies. We present a comprehensive review of state-of-the-art autotuning systems and discuss how they address key challenges, such as handling large configuration spaces, mitigating noise in real-world environments, and ensuring safe and efficient exploration during tuning. To conclude, we offer a hands-on session where participants can experiment with an open-source system and gain experience with real-world tuning scenarios.",
      "url": "https://www.microsoft.com/en-us/research/publication/autotuning-systems-techniques-challenges-and-opportunities/"
    },
    {
      "title": "Preaching to the ChoIR: Lessons IR should share with AI",
      "authors": [
        "Arjen P de Vries",
        "Chirag Shah",
        "Claudia Hauff",
        "Damiano Spina",
        "Falk Scholer",
        "Gianluca Demartini",
        "Guido Zuccon",
        "Kevin Roitero",
        "Mark Sanderson",
        "Matthew Lease",
        "Paul Thomas",
        "Stefano Mizzaro"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Search and information retrieval"
      ],
      "publication_date": "July 2025",
      "abstract": "The field of Information Retrieval (IR) changed profoundly at the end of the 1990s with the rise of Web Search, and there are parallels with developments in Artificial Intelligence (AI) happening today with the advent of ChatGPT, Large Language Models, and Generative AI.\nWe acknowledge that there are clear differences between IR and AI. For example, IR is a much smaller field, and new problems arise, like data contamination that may affect benchmark-based evaluation of AI systems. But looking through the lens of an IR researcher, there are many striking similarities between the two fields of IR (25 years ago) and AI (today), and many topics appearing in discussions in AI resemble those of 25 years ago in IR: benchmark reliability and robust evaluation, reproducibility of results for non-public models, privacy and copyright issues, efficiency and scalability, etc. In this paper, we discuss similarities and differences between IR and AI and then derive some lessons learned in the field of IR as a list of recommendations — urging the IR community to reflect on, discuss, and convey these lessons to the AI field.\nWe believe that a joint community effort by all IR researchers is both necessary and dutiful to obtain a fruitful discussion and research advancements with the AI community.",
      "url": "https://www.microsoft.com/en-us/research/publication/preaching-to-the-choir-lessons-ir-should-share-with-ai/"
    },
    {
      "title": "Z3Guide: A Scalable, Student-Centered, and Extensible  Educational Environment for Logic Modeling",
      "authors": [
        "Ayana Monroe",
        "Jonathan \"Peli\" de Halleux",
        "Nikolaj Bjørner",
        "Ruanqianqian (Lisa) Huang",
        "Sorin Lerner"
      ],
      "research_areas": [
        "Human-computer interaction",
        "Programming languages and software engineering"
      ],
      "publication_date": "June 2025",
      "abstract": "Constraint-satisfaction problems (CSPs) are ubiquitous, ranging from budgeting for grocery shopping to verifying software behavior. Logic modeling helps solve CSPs programmatically using SMT solvers. Despite its importance in many Computer Science disciplines, resources for teaching and learning logic modeling are scarce and scattered, and challenges remain in designing educational environments for logic modeling that are accessible and meet the needs of teachers and students. This paper explores how to design such an environment and probes the impact of the design on the learning experience. From a need-finding interview study and a design iteration with teachers of logic modeling, we curated 10 design guidelines spanning three main requirements: providing easy access, supporting various educational modalities, and allowing extensions for customized pedagogical needs. We implemented nine guidelines in Z3Guide, an open-source browser-based tool. Using Z3Guide in a logic modeling learning workshop with more than 100 students, we gathered positive feedback on its support for learning and identified opportunities for future improvements.",
      "url": "https://www.microsoft.com/en-us/research/publication/z3guide-a-scalable-student-centered-and-extensible-educational-environment-for-logic-modeling/"
    },
    {
      "title": "Principal Type Inference under a Prefix",
      "authors": [
        "Daan Leijen",
        "Wenjia Ye"
      ],
      "research_areas": [
        "Programming languages and software engineering"
      ],
      "publication_date": "June 2025",
      "abstract": "At the heart of the Damas-Hindley-Milner (HM) type system lies\nthe abstraction rule which derives a function type for a lambda expression.\nIn this rule, the type of the parameter can be “guessed”, and can be any\ntype that fits the derivation. The beauty of the HM system is that there always exists a most general type that\nencompasses all possible derivations — Algorithm W is used to infer these most\ngeneral types in practice.\nUnfortunately, this property is also the bane of the HM type rules. Many languages\nextend HM typing with additional features which often require complex side\nconditions to the type rules to maintain principal types. For example, various type systems for\nimpredicative type inference, like HMF, FreezeML, or Boxy types, require\nlet-bindings to always assign most general types. Such a restriction is difficult\nto specify as a logical deduction rule though, as it ranges over all possible\nderivations. Despite these complications, the actual implementations of various\ntype inference algorithms are usually straightforward extensions of algorithm\\ W,\nand from an implementation perspective, much of the complexity of various type\nsystem extensions, like boxes or polymorphic weights, is in some sense\nartificial.\nIn this article we rephrase the HM type rules as _type inference under a\nprefix_, called HMQ. HMQ is sound and complete with respect to the HM type\nrules, but always derives principal types that correspond to the types inferred\nby algorithm W. The HMQ type rules are close to the clarity of the declarative HM\ntype rules, but also specific enough to “read off” an inference algorithm, and\ncan form an excellent basis to describe type system extensions in practice.\nWe show in particular how to describe the FreezeML and HMF systems in terms\nof inference under a prefix, and how we no longer require complex side conditions.\nWe also show a novel formalization of static overloading in HMQ as implemented\nin Koka language.",
      "url": "https://www.microsoft.com/en-us/research/publication/principal-type-inference-under-a-prefix/"
    },
    {
      "title": "Code Researcher: Deep Research Agent for Large Systems Code and Commit History",
      "authors": [
        "Abhav Mehrotra",
        "Aditya Kanade",
        "Nagarajan Natarajan",
        "Nalin Wadhwa",
        "Ramakrishna Bairi",
        "Ramneet Singh∗",
        "Sathvik Joel∗"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Programming languages and software engineering"
      ],
      "publication_date": "June 2025",
      "abstract": "ArXiv link: https://arxiv.org/abs/2506.11060 (opens in new tab)\nLarge Language Model (LLM)-based coding agents have shown promising results on coding benchmarks, but their effectiveness on systems code remains underexplored. Due to the size and complexities of systems code, making changes to a systems codebase is a daunting task, even for humans. It requires researching about many pieces of context, derived from the large codebase and its massive commit history, before making changes. Inspired by the recent progress on deep research agents, we design the first deep research agent for code, called Code Researcher, and apply it to the problem of generating patches for mitigating crashes reported in systems code. Code Researcher performs multi-step reasoning about semantics, patterns, and commit history of code to gather sufficient context. The context is stored in a structured memory which is used for synthesizing a patch. We evaluate Code Researcher on kBenchSyz [23], a benchmark of Linux kernel crashes, and show that it significantly outperforms strong baselines, achieving a crash-resolution rate of 58%, compared to 37.5% by SWE-agent [40]. On an average, Code Researcher explores 10 files in each trajectory whereas SWE-agent explores only 1.33 files, highlighting Code Researcher’s ability to deeply explore the codebase. Through another experiment on an open-source multimedia software, we show the generalizability of Code Researcher. Our experiments highlight the importance of global context gathering and multi-faceted reasoning for large codebases.",
      "url": "https://www.microsoft.com/en-us/research/publication/code-researcher-deep-research-agent-for-large-systems-code-and-commit-history/"
    },
    {
      "title": "Direct Reasoning Optimization: LLMs Can Reward And Refine Their Own Reasoning for Open-Ended Tasks",
      "authors": [
        "Emre Kiciman",
        "Leonardo Nunes",
        "Ranveer Chandra",
        "Songwu Lu",
        "Srinagesh Sharma",
        "Tusher Chakraborty",
        "Yifei Xu"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "June 2025",
      "abstract": "Recent advances in Large Language Models (LLMs) have showcased impressive reasoning abilities in structured tasks like mathematics and programming, largely driven by Reinforcement Learning with Verifiable Rewards (RLVR), which uses outcome-based signals that are scalable, effective, and robust against reward hacking. However, applying similar techniques to open-ended long-form reasoning tasks remains challenging due to the absence of generic, verifiable reward signals. To address this, we propose Direct Reasoning Optimization (DRO), a reinforcement learning framework for fine-tuning LLMs on open-ended, particularly long-form, reasoning tasks, guided by a new reward signal: the Reasoning Reflection Reward (R3). At its core, R3 selectively identifies and emphasizes key tokens in the reference outcome that reflect the influence of the model’s preceding chain-of-thought reasoning, thereby capturing the consistency between reasoning and reference outcome at a fine-grained level. Crucially, R3 is computed internally using the same model being optimized, enabling a fully self-contained training setup. Additionally, we introduce a dynamic data filtering strategy based on R3 for open-ended reasoning tasks, reducing cost while improving downstream performance. We evaluate DRO on two diverse datasets — ParaRev, a long-form paragraph revision task, and FinQA, a math-oriented QA benchmark — and show that it consistently outperforms strong baselines while remaining broadly applicable across both open-ended and structured domains.",
      "url": "https://www.microsoft.com/en-us/research/publication/direct-reasoning-optimization-llms-can-reward-and-refine-their-own-reasoning-for-open-ended-tasks/"
    },
    {
      "title": "Sulfonated Benzo[c]cinnolines for Alkaline Redox-Flow Batteries",
      "authors": [
        "Allison J. Gatz",
        "Anne J. McNeil",
        "Bichlien Nguyen",
        "Bryan R. Goldsmith",
        "Cameron Gruich",
        "David G. Kwabi",
        "Jake Smith",
        "Jason Dong",
        "Jessica L. Tami",
        "Siddhant Singh"
      ],
      "research_areas": [
        "Hardware and devices",
        "Technology for emerging markets"
      ],
      "publication_date": "June 2025",
      "abstract": "Aqueous organic redox-flow batteries (RFBs) are promising for grid-scale energy storage, but the development of stable, low-cost electrolytes remains challenging. Herein, azobenzenes were investigated as negative electrolytes because of their low molecular weight and commercial availability. The electrochemical properties and cycling performance of methyl red, an azobenzene dye, were evaluated under alkaline conditions. Analyzing the decomposition products revealed degradation through hydrazo bond cleavage. To attenuate degradation, we tested a more structurally rigid cis-azobenzene with connected aromatic rings, known as benzo[c]cinnoline (BC). The sulfonation of BC enabled a higher aqueous solubility, with the disulfonated product (ds-BC) exhibiting a reduction potential of −0.84 V vs Ag/AgCl. In full cells, ds-BC was paired with ferrocyanide–ferricyanide, producing a 1.11 V battery with a capacity fade rate of 0.77%/day. Density functional theory calculations predicted that reduced BCs exhibit greater thermodynamic stability than azobenzenes. This work introduces benzo[c]cinnoline as a promising molecular scaffold for RFBs.",
      "url": "https://www.microsoft.com/en-us/research/publication/sulfonated-benzoccinnolines-for-alkaline-redox-flow-batteries/"
    },
    {
      "title": "Fast and Reliable N – k Contingency Screening with Input-Convex Neural Networks",
      "authors": [
        "Baosen Zhang",
        "Nicolas Christianson",
        "Steven Low",
        "Weiwei Yang",
        "Wenqi Cui"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "June 2025",
      "abstract": "Power system operators must ensure that dispatch decisions remain feasible in case of grid outages or contingencies to prevent cascading failures and ensure reliable operation. However, checking the feasibility of all contingencies — every possible simultaneous failure of grid components — is computationally intractable for even small, requiring system operators to resort to heuristic screening methods. Because of the increase in uncertainty and changes in system behaviors, heuristic lists might not include all relevant contingencies, generating false negatives in which unsafe scenarios are misclassified as safe. In this work, we propose to use input-convex neural networks (ICNNs) for contingency screening. We show that ICNN reliability can be determined by solving a convex optimization problem, and by scaling model weights using this problem as a differentiable optimization layer during training, we can learn an ICNN classifier that is both data-driven and has provably guaranteed reliability. Namely, our method can ensure a zero false negative rate. We empirically validate this methodology in a case study on the IEEE 39-bus test network, observing that it yields substantial (10-20x) speedups while having excellent classification accuracy.",
      "url": "https://www.microsoft.com/en-us/research/publication/fast-and-reliable-n-k-contingency-screening-with-input-convex-neural-networks/"
    },
    {
      "title": "Optimizing Cloud-to-GPU Throughput for Deep Learning With Earth Observation Data",
      "authors": [
        "Akram Zaytar",
        "Anthony Ortiz",
        "Caleb Robinson",
        "Gilles Quentin Hacheme",
        "Girmaw Abebe Tadesse",
        "Juan M. Lavista Ferres",
        "Rahul Dodhia",
        "Tammy Glazer"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "June 2025",
      "abstract": "Training deep learning models on petabyte-scale Earth observation (EO) data requires separating compute resources from data storage. However, standard PyTorch data loaders cannot keep modern GPUs utilized when streaming GeoTIFF files directly from cloud storage. In this work, we benchmark GeoTIFF loading throughput from both cloud object storage and local SSD, systematically testing different loader configurations and data parameters. We focus on tile-aligned reads and worker thread pools, using Bayesian optimization to find optimal settings for each storage type. Our optimized configurations increase remote data loading throughput by 20x and local throughput by 4x compared to default settings. On three public EO benchmarks, models trained with optimized remote loading achieve the same accuracy as local training within identical time budgets. We improve validation IoU by 6-15% and maintain 85-95% GPU utilization versus 0-30% with standard configurations. Code is publicly available at https://github.com/microsoft/pytorch-cloud-geotiff-optimization",
      "url": "https://www.microsoft.com/en-us/research/publication/optimizing-cloud-to-gpu-throughput-for-deep-learning-with-earth-observation-data/"
    },
    {
      "title": "Accurate Chemistry Collection: Coupled cluster atomization energies for broad chemical space",
      "authors": [
        "Amir Karton",
        "Chin-Wei Huang",
        "Derk Kooi",
        "Giulia Luise",
        "Jan Hermann",
        "Kenji Takeda",
        "Marwin Segler",
        "Paola Gori-Giorgi",
        "Rianne van den Berg",
        "Sebastian Ehlert",
        "Stephanie Lanius",
        "Thijs Vogels",
        "Victor Garcia Satorras"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "June 2025",
      "abstract": "Accurate thermochemical data with sub-chemical accuracy (i.e., within ±1 kcal/mol from sufficiently accurate experimental or theoretical reference data) is essential for the development and improvement of computational chemistry methods. Challenging thermochemical properties such as heats of formation and total atomization energies (TAEs) are of particular interest because they rigorously test the ability of computational chemistry methods to accurately describe complex chemical transformations involving multiple bond rearrangements. Yet, existing thermochemical datasets that confidently reach this level of accuracy are limited in either size or scope. Datasets with highly accurate reference values include a small number of data points, and larger datasets provide less accurate data or only cover a narrow portion of the chemical space. The existing datasets are therefore insufficient for developing data-driven methods with predictive accuracy over a large chemical space. The Microsoft Research Accurate Chemistry Collection (MSR-ACC) will address this challenge. Here, it offers the MSR-ACC/TAE25 dataset of 76,879 total atomization energies obtained at the CCSD(T)/CBS level via the W1-F12 thermochemical protocol. The dataset is constructed to exhaustively cover chemical space for all elements up to argon by enumerating and sampling chemical graphs, thus avoiding bias towards any particular subspace of the chemical space (such as drug-like, organic, or experimentally observed molecules). With this first dataset in MSR-ACC, we enable data-driven approaches for developing predictive computational chemistry methods with unprecedented accuracy and scope.",
      "url": "https://www.microsoft.com/en-us/research/publication/accurate-chemistry-collection-coupled-cluster-atomization-energies-for-broad-chemical-space/"
    },
    {
      "title": "Implicit Language Models are RNNs: Balancing Parallelization and Expressivity",
      "authors": [
        "Babak Rahmani",
        "Fabian Falck",
        "Heiner Kremer",
        "Hitesh Ballani",
        "Jannes Gladrow",
        "Mark Schöne"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "June 2025",
      "abstract": "State-space models (SSMs) and transformers dominate the language modeling landscape. However, they are constrained to a lower computational complexity than classical recurrent neural networks (RNNs), limiting their expressivity. In contrast, RNNs lack parallelization during training, raising fundamental questions about the trade off between parallelization and expressivity. We propose implicit SSMs, which iterate a transformation until convergence to a fixed point. Theoretically, we show that implicit SSMs implement the non-linear state-transitions of RNNs. Empirically, we find that only approximate fixed-point convergence suffices, enabling the design of a scalable training curriculum that largely retains parallelization, with full convergence required only for a small subset of tokens. Our approach demonstrates superior state-tracking capabilities on regular languages, surpassing transformers and SSMs. We further scale implicit SSMs to natural language reasoning tasks and pretraining of large-scale language models up to 1.3B parameters on 207B tokens – representing, to our knowledge, the largest implicit model trained to date. Notably, our implicit models outperform their explicit counterparts on standard benchmarks.",
      "url": "https://www.microsoft.com/en-us/research/publication/implicit-language-models-are-rnns-balancing-parallelization-and-expressivity/"
    },
    {
      "title": "NTIRE 2025 Challenge on Video Quality Enhancement for Video Conferencing: Datasets, Methods and Results",
      "authors": [
        "Henrik Turbell",
        "Louis Florentin",
        "Quan Zou",
        "Radu Timofte",
        "Sandeep Siddhartha",
        "Varun Jain",
        "Zongwei Wu"
      ],
      "research_areas": [
        "Computer vision"
      ],
      "publication_date": "June 2025",
      "abstract": "This paper presents a comprehensive review of the 1st Challenge on Video Quality Enhancement for Video Conferencing held at the NTIRE workshop at CVPR 2025, and highlights the problem statement, datasets, proposed solutions, and results. The aim of this challenge was to design a Video Quality Enhancement (VQE) model to enhance video quality in video conferencing scenarios by (a) improving lighting, (b) enhancing colors, (c) reducing noise, and (d) enhancing sharpness – giving a professional studio-like effect. Participants were given a differentiable Video Quality Assessment (VQA) model, training, and test videos. A total of 91 participants registered for the challenge. We received 10 valid submissions that were evaluated in a crowd-sourced framework.",
      "url": "https://www.microsoft.com/en-us/research/publication/ntire-2025-challenge-on-video-quality-enhancement-for-video-conferencing-datasets-methods-and-results/"
    },
    {
      "title": "Examining the Expanding Role of Synthetic Data Throughout the AI Development Pipeline",
      "authors": [
        "Alex Kessler",
        "Jennifer Wortman Vaughan",
        "Shivani Kapania",
        "Stephanie Ballard"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "June 2025",
      "abstract": "Alongside the growth of generative AI, we are witnessing a surge in the use of synthetic data across all stages of the AI development pipeline. It is now common practice for researchers and practitioners to use one large generative model (which we refer to as an auxiliary model) to generate synthetic data that is used to train or evaluate another, reconfiguring AI workflows and reshaping the very nature of data. While scholars have raised concerns over the risks of synthetic data, policy guidance and best practices for its responsible use have not kept up with these rapidly evolving industry trends, in part because we lack a clear picture of current practices and challenges. Our work aims to address this gap. Through 29 interviews with AI practitioners and responsible AI experts, we examine the expanding role of synthetic data in AI development. Our findings reveal how auxiliary models are now widely used across the AI development pipeline. Practitioners describe synthetic data as crucial for addressing data scarcity and providing a competitive edge, noting that evaluation of generative AI systems at scale would be infeasible without auxiliary models. However, they face challenges controlling the outputs of auxiliary models, generating data that accurately depict underrepresented groups, and scaling data validation practices that are based primarily on manual inspection. We detail general limitations of and ethical considerations for synthetic data and conclude with a proposal of concrete steps towards the development of best practices for its responsible use.",
      "url": "https://www.microsoft.com/en-us/research/publication/examining-the-expanding-role-of-synthetic-data-throughout-the-ai-development-pipeline/"
    },
    {
      "title": "Dynamic Region Ownership for Concurrency Safety",
      "authors": [
        "Brandt Bucher",
        "Eric Snow",
        "Fridtjof Peer Stoldt",
        "Guido Van Rossum",
        "Matthew J. Parkinson",
        "Matthew Johnson",
        "Sylvan Clebsch",
        "Tobias Wrigstad"
      ],
      "research_areas": [
        "Programming languages and software engineering"
      ],
      "publication_date": "June 2025",
      "abstract": "When a program, or programming language, graduates from sequential to concurrent execution it can be a painful process. The ways in which the components of a program interact with each other in a concurrent setting can be considerably more complex than in a sequential setting. The core problem is unrestricted shared mutable state. An alternative to unrestricted shared mutable state is to restrict the sharing using Ownership. Ownership can turn what would have been a race into a deterministic failure that can be explained to the programmer. However, Ownership has predominantly taken place in statically typed languages.\nIn this paper, we explore retrofitting an existing dynamically typed programming language with an ownership model based on regions. Our core aim is to provide safe concurrency, that is, the ownership model should provide deterministic dynamic failures of ownership that can be explained to the programmer. We present a dynamic model of ownership that provides ownership of groups objects called regions. We provide dynamic enforcement of our region discipline, which we have implemented in a simple interpreter that provides a Python-like syntax and semantics, and report on our first steps into integrating it into an existing language, Python.",
      "url": "https://www.microsoft.com/en-us/research/publication/dynamic-region-ownership-for-concurrency-safety/"
    },
    {
      "title": "Reason-before-Retrieve: One-Stage Reflective Chain-of-Thoughts for Training-Free Zero-Shot Composed Image Retrieval",
      "authors": [
        "Dongmei Zhang",
        "Gang Xiong",
        "Gaopeng Gou",
        "Jing Yu",
        "Jue Zhang",
        "Qi Wu",
        "Qingwei Lin 林庆维",
        "Saravan Rajmohan",
        "Xiaoting Qin",
        "Yuanmin Tang"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Computer vision"
      ],
      "publication_date": "June 2025",
      "abstract": "Composed Image Retrieval (CIR) aims to retrieve target images that closely resemble a reference image while integrating user-specified textual modifications, thereby capturing user intent more precisely. Existing training-free zero-shot CIR (ZS-CIR) methods often employ a two-stage process: they first generate a caption for the reference image and then use Large Language Models for reasoning to obtain a target description. However, these methods suffer from missing critical visual details and limited reasoning capabilities, leading to suboptimal retrieval performance. To address these challenges, we propose a novel, training-free one-stage method, One-Stage Reflective Chain-of-Thought Reasoning for ZS-CIR (OSrCIR), which employs Multimodal Large Language Models to retain essential visual information in a single-stage reasoning process, eliminating the information loss seen in two-stage methods. Our Reflective Chain-of-Thought framework further improves interpretative accuracy by aligning manipulation intent with contextual cues from reference images. OSrCIR achieves performance gains of 1.80% to 6.44% over existing training-free methods across multiple tasks, setting new state-of-the-art results in ZS-CIR and enhancing its utility in vision-language applications.",
      "url": "https://www.microsoft.com/en-us/research/publication/reason-before-retrieve-one-stage-reflective-chain-of-thoughts-for-training-free-zero-shot-composed-image-retrieval/"
    },
    {
      "title": "Hybridization-Encoded DNA Tags with Paper-Based Readout for Anti-Forgery Raw Material Tracking",
      "authors": [
        "Alex Crown",
        "Jiaming Li",
        "Karin Strauss",
        "Peter Ney",
        "Ranveer Chandra",
        "Sergey Yekhanin",
        "Yuan-Jyue Chen",
        "…"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "June 2025",
      "abstract": "Abstract coming soon…",
      "url": "https://www.microsoft.com/en-us/research/publication/hybridization-encoded-dna-tags-with-paper-based-readout-for-anti-forgery-raw-material-tracking/"
    },
    {
      "title": "From Elements to Design: A Layered Approach for Automatic Graphic Design Composition",
      "authors": [
        "Danqing Huang",
        "Ji Li",
        "Jiang Bian",
        "Jiawei Lin",
        "Shizhao Sun",
        "Ting Liu"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Computer vision"
      ],
      "publication_date": "June 2025",
      "abstract": "In this work, we investigate automatic design composition from multimodal graphic elements. Although recent studies have developed various generative models for graphic design, they usually face the following limitations: they only focus on certain subtasks and are far from achieving the design composition task; they do not consider the hierarchical information of graphic designs during the generation process. To tackle these issues, we introduce the layered design principle into Large Multimodal Models (LMMs) and propose a novel approach, called LaDeCo, to accomplish this challenging task. Specifically, LaDeCo first performs layer planning for a given element set, dividing the input elements into different semantic layers according to their contents. Based on the planning results, it subsequently predicts element attributes that control the design composition in a layer-wise manner, and includes the rendered image of previously generated layers into the context. With this insightful design, LaDeCo decomposes the difficult task into smaller manageable steps, making the generation process smoother and clearer. The experimental results demonstrate the effectiveness of LaDeCo in design composition. Furthermore, we show that LaDeCo enables some interesting applications in graphic design, such as resolution adjustment, element filling, design variation, etc. In addition, it even outperforms the specialized models in some design subtasks without any task-specific training.",
      "url": "https://www.microsoft.com/en-us/research/publication/from-elements-to-design-a-layered-approach-for-automatic-graphic-design-composition/"
    },
    {
      "title": "The Power of Migrations in Dynamic Bin Packing",
      "authors": [
        "Konstantina Mellou",
        "Marco Molinaro",
        "Rudy Zhou"
      ],
      "research_areas": [
        "Algorithms"
      ],
      "publication_date": "June 2025",
      "abstract": "In the Dynamic Bin Packing problem, n items arrive and depart the system in an online manner, and the goal is to maintain a good packing throughout. We consider the objective of minimizing the total active time, i.e., the sum of the number of open bins over all times. An important tool for maintaining an efficient packing in many applications is the use of migrations; e.g., transferring computing jobs across different machines. However, there are large gaps in our understanding of the approximability of dynamic bin packing with migrations. Prior work has covered the power of no migrations and >n migrations, but we ask the question: What is the power of limited (≤n) migrations?\nOur first result is a dichotomy between no migrations and linear migrations: Using a sublinear number of migrations is asymptotically equivalent to doing zero migrations, where the competitive ratio grows with μ, the ratio of the largest to smallest item duration. On the other hand, we prove that for every α∈(0,1], there is an algorithm that does ≈αn migrations and achieves competitive ratio ≈1/α (in particular, independent of μ); we also show that this tradeoff is essentially best possible. This fills in the gap between zero migrations and >n migrations in Dynamic Bin Packing.\nFinally, in light of the above impossibility results, we introduce a new model that more directly captures the impact of migrations. Instead of limiting the number of migrations, each migration adds a delay of C time units to the item’s duration; this commonly appears in settings where a blackout or set-up time is required before the item can restart its execution in the new bin. In this new model, we prove a O(min(√C,μ))-approximation, and an almost matching lower bound.",
      "url": "https://www.microsoft.com/en-us/research/publication/the-power-of-migrations-in-dynamic-bin-packing/"
    },
    {
      "title": "Auto-Test: Learning Semantic-Domain Constraints for Unsupervised  Error Detection in Tables",
      "authors": [
        "Dongmei Zhang",
        "Haidong Zhang",
        "Qixu Chun",
        "Song Ge",
        "Surajit Chaudhuri",
        "Weiwei Cui",
        "Yeye He (yeyehe)"
      ],
      "research_areas": [
        "Data platforms and analytics"
      ],
      "publication_date": "June 2025",
      "abstract": "Data cleaning is a long-standing challenge in data management. While powerful logic and statistical algorithms have been developed to detect and repair data errors in tables, existing algorithms predominantly rely on domain-experts to first manually specify data-quality constraints specific to a given table, before data cleaning algorithms can be applied.\nIn this work, we propose a new class of data-quality constraints that we call Semantic-Domain Constraints, which can be reliably inferred and automatically applied to any tables, without requiring domain-experts to manually specify on a per-table basis.\nWe develop a principled framework to systematically learn such constraints from table corpora using large-scale statistical tests, which can further be distilled into a core set of constraints using our optimization framework, with provable quality guarantees. Extensive evaluations show that this new class of constraints can be used to both (1) directly detect errors on real tables in the wild, and (2) augment existing expert-driven data-cleaning techniques as a new class of complementary constraints.\nOur extensively labeled benchmark dataset with 2400 real data columns, as well as our code are available at here (opens in new tab) to facilitate future research.",
      "url": "https://www.microsoft.com/en-us/research/publication/auto-test-learning-semantic-domain-constraints-for-unsupervised-error-detection-in-tables/"
    },
    {
      "title": "Expanding smallholder irrigation in central Kenya demonstrates the importance of protecting grassland landscapes",
      "authors": [
        "Bernice Sainepo",
        "Caleb Robinson",
        "Elizah Peter",
        "Elkanah Kipkoech",
        "Gilles Quentin Hacheme",
        "Girmaw Abebe Tadesse",
        "Renatus Magesa",
        "Stephen Andrew Wood"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Ecology and environment"
      ],
      "publication_date": "June 2025",
      "abstract": "The rapid expansion of agriculture in Kenya, driven by the country’s growing population, poses critical environmental challenges such as climate change and biodiversity loss. While deforestation has received much attention, the equally vital grasslands are under significant threat. Monitoring efforts have often prioritized the extent of land conversion, with limited focus on the intensity of conversion, particularly in non-forest biomes. This study addresses this gap by assessing the expansion of row crop and irrigated agriculture in the grasslands of central Kenya from 2018 to 2022. The region, particularly around Laikipia County, has seen a rapid increase in irrigated agriculture, encroaching on natural grasslands and threatening local wildlife through excessive water extraction. We mapped the expansion of both smallholder and large-scale croplands and developed a model based on vegetation indices and proximity to key water sources to estimate irrigation likelihood. Our results show a 27% increase in likely irrigated croplands from 2018 to 2022, with projections indicating that, if current trends persist, irrigation areas will double by 2030, probably increasing water consumption substantially. This study underscores the urgent need to protect Kenya’s native grasslands, which are facing active threats with far-reaching environmental consequences.",
      "url": "https://www.microsoft.com/en-us/research/publication/expanding-smallholder-irrigation-in-central-kenya-demonstrates-the-importance-of-protecting-grassland-landscapes/"
    },
    {
      "title": "GUI-Actor: Coordinate-Free Visual Grounding for GUI Agents",
      "authors": [
        "Baolin Peng",
        "Bo Qiao",
        "Chaoyun Zhang",
        "Dongmei Zhang",
        "Huan Zhang",
        "Huiqiang Jiang",
        "Jian Mu",
        "Jianbing Zhang",
        "Jianfeng Gao",
        "Jianwei Yang",
        "Kanzhi Cheng",
        "Lars Liden",
        "Qianhui Wu",
        "Qingwei Lin 林庆维",
        "Reuben Tan",
        "Rui Yang",
        "Si Qin",
        "Tong Zhang"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Computer vision",
        "Human language technologies"
      ],
      "publication_date": "June 2025",
      "abstract": "One of the principal challenges in building VLM-powered GUI agents is visual grounding—localizing the appropriate screen region for action execution based on both the visual content and the textual plans. Most existing work formulates this as a text-based coordinate generation task. However, these approaches suffer from several limitations: weak spatial-semantic alignment due to the lack of explicit spatial supervision; inability to handle ambiguous supervision targets, as single-point predictions penalize valid variations; and a mismatch between the dense nature of screen coordinates and the coarse, patch-level granularity of visual features extracted by models like Vision Transformers. In this paper, we propose GUI-Actor, a VLM-based method for coordinate-free GUI grounding. At its core, GUI-Actor introduces an attention-based action head that learns to align a dedicated  token with all relevant visual patch tokens, enabling the model to propose one or more action regions in a single forward pass. In line with this, we further design a grounding verifier to evaluate and select the most plausible action region from the candidates proposed for action execution. Extensive experiments show that GUI-Actor outperforms prior state-of-the-art methods on multiple GUI action grounding benchmarks, with improved generalization to unseen screen resolutions and layouts. Notably, GUI-Actor-7B achieves scores of 40.7 with Qwen2-VL and 44.6 with Qwen2.5-VL as backbones, outperforming UI-TARS-72B (38.1) on ScreenSpot-Pro, with significantly fewer parameters and training data. Furthermore, by incorporating the verifier, we find that fine-tuning only the newly introduced action head (~100M parameters for 7B model) while keeping the VLM backbone frozen is sufficient to achieve performance comparable to previous state-of-the-art models, highlighting that GUI-Actor can endow the underlying VLM with effective grounding capabilities without compromising its general-purpose strengths.",
      "url": "https://www.microsoft.com/en-us/research/publication/gui-actor-coordinate-free-visual-grounding-for-gui-agents/"
    },
    {
      "title": "EcoServe: Designing Carbon-Aware AI Inference Systems",
      "authors": [
        "Esha Choukse",
        "G. Edward Suh",
        "Rodrigo Fonseca",
        "Udit Gupta",
        "Yueying Li",
        "Zhanqiu Hu"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Systems and networking"
      ],
      "publication_date": "June 2025",
      "abstract": "The rapid increase in LLM ubiquity and scale levies unprecedented demands on computing infrastructure. These demands not only incur large compute and memory resources, but also significant energy, yielding large operational and embodied carbon emissions. In this work, we present two main observations. First, while GPUs dominate operational carbon, host processing systems (e.g., CPUs, memory, storage) dominate embodied carbon. Second, based on traces from production deployment of two Generative AI services in the cloud, offline, batch-inference accounts for a significant portion (up to 55\\%) of serving capacity. We propose four pillars of carbon-conscious infrastructure design for LLM serving systems: \\textbf{\\textit{Reduce, Reuse, Rightsize, and Recycle}}. We demonstrate that EcoServe can lower carbon emissions by up to 47\\%, compared to performance, energy, and cost-optimized design points, while maintaining performance targets and SLOs.",
      "url": "https://www.microsoft.com/en-us/research/publication/ecoserve-designing-carbon-aware-ai-inference-systems/"
    },
    {
      "title": "I2VGuard: Safeguarding Images against Misuse in Diffusion-based Image-to-Video Models",
      "authors": [
        "Dongnan Gui",
        "Wengang Zhou",
        "Xun Guo",
        "Yan Lu"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "June 2025",
      "abstract": "Recent advances in image-to-video generation have enabled animation of still images and offered pixel-level controllability. While these models hold great potential to transform single images into vivid and dynamic videos, they also carry risks of misuse that could impact privacy, security, and copyright protection. This paper proposes a novel approach that applies imperceptible perturbations on images to degrade the quality of the generated videos, thereby protecting images from misuse in white-box image-to-video diffusion models. Specifically, we function our approach as an adversarial attack, incorporating spatial, temporal, and diffusion attack modules. The spatial attack shifts image features from their original distribution to a lower-quality target distribution, reducing visual fidelity. The temporal attack disrupts coherent motion by interfering with temporal attention maps that guide motion generation. To enhance the robustness of our approach across different models, we further propose a diffusion attack module leveraging contrastive loss. Our approach can be easily integrated with mainstream diffusion-based I2V models. Extensive experiments on SVD, CogVideoX, and ControlNeXt demonstrate that our method significantly impairs generation quality in terms of visual clarity and motion consistency, while introducing only minimal artifacts to the images. To the best of our knowledge, we are the first to explore adversarial attacks on image-to-video generation for security purposes.",
      "url": "https://www.microsoft.com/en-us/research/publication/i2vguard-safeguarding-images-against-misuse-in-diffusion-based-image-to-video-models/"
    },
    {
      "title": "Enhancing Food Security with High-Quality Land-Use and Land-Cover Maps: A Local Model Approach",
      "authors": [
        "Caleb Robinson",
        "Charles Mwangi",
        "Esther Maina",
        "Gilles Quentin Hacheme",
        "Girmaw Abebe Tadesse",
        "Hamed Alemohammad",
        "Joshua Nyakundi",
        "Juan M. Lavista Ferres",
        "Luana Marotti",
        "Rahul Dodhia"
      ],
      "research_areas": [
        "Computer vision"
      ],
      "publication_date": "June 2025",
      "abstract": "In 2023, 58.0% of the African population experienced moderate to severe food insecurity, with 21.6% facing severe food insecurity. Land-use and land-cover maps enable informed resource management, urban planning, environment monitoring to enhance food security. The development of global land cover maps has been facilitated by the increasing availability of earth observation data and advancements in geospatial machine learning. However, these global maps exhibit lower accuracy and inconsistencies in Africa, partly due to the lack of representative training data. To address this issue, we propose a data-centric framework with a teacher-student model setup, which uses diverse data sources of satellite images and label examples to produce local land-cover maps. Our method trains a high-resolution teacher model on images with a resolution of 0.331 m/pixel and a low-resolution student model on publicly available images with a resolution of 10 m/pixel. The student model also utilizes the teacher model’s output as its weak label examples as a form of outcome-based knowledge distillation. We evaluated our framework using Murang’a County in Kenya, renowned for its agricultural productivity, as a use case. Our local models achieved higher quality maps, with improvements of 0.14 in the F1 score and 0.21 in Intersection-over-Union, compared to the best global model. Our evaluation also revealed inconsistencies in existing global maps, with a maximum agreement rate of 0.30 among themselves. Our work provides valuable guidance to decision makers for driving informed decisions to enhance food security.",
      "url": "https://www.microsoft.com/en-us/research/publication/enhancing-food-security-with-high-quality-land-use-and-land-cover-maps-a-local-model-approach/"
    },
    {
      "title": "Keeping Humans in the Loop: Human-Centered Automated Annotation with Generative AI",
      "authors": [
        "Nick Pangakis",
        "Sam Wolken"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Social sciences"
      ],
      "publication_date": "June 2025",
      "abstract": "Automated text annotation is a compelling use case for generative large language models (LLMs) in social media research. Recent work suggests that LLMs can achieve strong performance on annotation tasks; however, these studies evaluate LLMs on a small number of tasks and likely suffer from contamination due to a reliance on public benchmark datasets. Here, we test a human-centered framework for responsibly evaluating artificial intelligence tools used in automated annotation. We use GPT-4 to replicate 27 annotation tasks across 11 password-protected datasets from recently published computational social science articles in high-impact journals. For each task, we compare GPT-4 annotations against human-annotated ground-truth labels and against annotations from separate supervised classification models fine-tuned on human-generated labels. Although the quality of LLM labels is generally high, we find significant variation in LLM performance across tasks, even within datasets. Our findings underscore the importance of a human-centered workflow and careful evaluation standards: Automated annotations significantly diverge from human judgment in numerous scenarios, despite various optimization strategies such as prompt tuning. Grounding automated annotation in validation labels generated by humans is essential for responsible evaluation.",
      "url": "https://www.microsoft.com/en-us/research/publication/keeping-humans-in-the-loop-human-centered-automated-annotation-with-generative-ai/"
    },
    {
      "title": "What Does Success Look Like? Catalyzing Meeting Intentionality with AI-Assisted Prospective Reflection",
      "authors": [
        "Ava Elizabeth Scott",
        "Lev Tankelevitch",
        "Payod Panda",
        "Rishi Vanukuru",
        "Sean Rintel",
        "Xinyue Chen"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Human-computer interaction"
      ],
      "publication_date": "June 2025",
      "abstract": "Despite decades of HCI and Meeting Science research, complaints about ineffective meetings are still pervasive. We argue that meeting technologies lack support for prospective reflection, that is, thinking about why a meeting is needed and what might happen. To explore this, we designed a Meeting Purpose Assistant (MPA) technology probe to coach users to articulate their meeting’s purpose and challenges, and act accordingly. The MPA used Generative AI to support personalized and actionable prospective reflection across the diversity of meeting contexts. Using a participatory prompting methodology, 18 employees of a global technology company reflected with the MPA on upcoming meetings. Observed impacts were: clarifying meeting purposes, challenges, and success conditions; changing perspectives and flexibility; improving preparation and communication; and proposing changed plans. We also identify perceived social, temporal, and technological barriers to using the MPA. We present system and workflow design considerations for developing AI-assisted reflection support for meetings. KEYWORDS: videoconferencing, meetings, goals, purpose, intentionality, workplace, prospective reflection, generative AI, participatory prompting\n\nRELATED RESEARCH\n\n\nFormative studies\n\n\n\nMental Models of Meeting Goals: Supporting Intentionality in Meeting Technology\n\n\n\nPrototype studies\n\n\n\nBefore meetings: What Does Success Look Like? Catalyzing Meeting Intentionality with AI-Assisted Prospective Reflection\n\n\nBefore and during meetings: The CoExplorer Technology Probe: A Generative AI-Powered Adaptive Interface to Support Intentionality in Planning and Running Video Meetings\n\n\nDuring meetings: Are We On Track? AI-Assisted Active and Passive Goal Reflection During Meetings – Microsoft Research\n\n\nBetween meetings: Designing Interfaces that Support Temporal Work Across Meetings with Generative AI",
      "url": "https://www.microsoft.com/en-us/research/publication/what-does-success-look-like-catalyzing-meeting-intentionality-with-ai-assisted-prospective-reflection/"
    },
    {
      "title": "Dynamic Prompt Middleware: Contextual Prompt Refinement Controls for Comprehension Tasks",
      "authors": [
        "Advait Sarkar",
        "Ian Drosos",
        "Jack Williams",
        "Nick Wilson",
        "Payod Panda",
        "Sean Rintel"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Human-computer interaction",
        "Social sciences"
      ],
      "publication_date": "June 2025",
      "abstract": "Effective prompting of generative AI is challenging for many users, particularly in expressing context for comprehension tasks such as explaining spreadsheet formulas, Python code, and text passages. Prompt middleware aims to address this barrier by assisting in prompt construction, but barriers remain for users in expressing adequate control so that they can receive AI-responses that match their preferences. We conduct a formative survey (n=38) investigating user needs for control over AI-generated explanations in comprehension tasks, which uncovers a trade-off between standardized but predictable support for prompting, and adaptive but unpredictable support tailored to the user and task. To explore this trade-off, we implement two prompt middleware approaches: Dynamic Prompt Refinement Control (Dynamic PRC) and Static Prompt Refinement Control (Static PRC). The Dynamic PRC approach generates context-specific UI elements that provide prompt refinements based on the user’s prompt and user needs from the AI, while the Static PRC approach offers a preset list of generally applicable refinements. We evaluate these two approaches with a controlled user study (n=16) to assess the impact of these approaches on user control of AI responses for crafting better explanations. Results show a preference for the Dynamic PRC approach as it afforded more control, lowered barriers to providing context, and encouraged exploration and reflection of the tasks, but that reasoning about the effects of different generated controls on the final output remains challenging. Drawing on participant feedback, we discuss design implications for future Dynamic PRC systems that enhance user control of AI responses. Our findings suggest that dynamic prompt middleware can improve the user experience of generative AI workflows by affording greater control and guide users to a better AI response.\nKEYWORDS: Dynamic UX Generation, Prompt Middleware",
      "url": "https://www.microsoft.com/en-us/research/publication/dynamic-prompt-middleware-contextual-prompt-refinement-controls-for-comprehension-tasks/"
    },
    {
      "title": "Autoformalizing Mathematical Statements by  Symbolic Equivalence and Semantic Consistency",
      "authors": [
        "Fan Yang",
        "Xian Zhang",
        "Xiaoxing Ma",
        "Xinming Wei",
        "Yifan Wu",
        "Zenan Li",
        "Zhaoyu Li"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "",
      "abstract": "Autoformalization, the task of automatically translating natural language descriptions into a formal language, poses a significant challenge across various domains, especially in mathematics. Recent advancements in large language models (LLMs) have unveiled their promising capabilities to formalize even competition-level math problems. However, we observe a considerable discrepancy between pass@1 and pass@k accuracies in LLM-generated formalizations. To address this gap, we introduce a novel framework that scores and selects the best result from k autoformalization candidates based on two complementary self-consistency methods: symbolic equivalence and semantic consistency. Elaborately, symbolic equivalence identifies the logical homogeneity among autoformalization candidates using automated theorem provers, and semantic consistency evaluates the preservation of the original meaning by informalizing the candidates and computing the similarity between the embeddings of the original and informalized texts. Our extensive experiments on the MATH and miniF2F datasets demonstrate that our approach significantly enhances autoformalization accuracy, achieving up to 0.22-1.35x relative improvements across various LLMs and baseline methods.",
      "url": "https://www.microsoft.com/en-us/research/publication/autoformalizing-mathematical-statements-by-symbolic-equivalence-and-semantic-consistency/"
    },
    {
      "title": "Node Similarities under Random Projections: Limits and Pathological Cases",
      "authors": [
        "Cassiano Becker",
        "Jennifer Neville",
        "Tvrtko Tadić"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Mathematics"
      ],
      "publication_date": "June 2025",
      "abstract": "Random Projections have been widely used to generate embeddings for various graph learning tasks due to their computational efficiency. The majority of applications have been justified through the Johnson-Lindenstrauss Lemma. In this paper, we take a step further and investigate how well dot product and cosine similarity are preserved by random projections when these are applied over the rows of the graph matrix. Our analysis provides new asymptotic and finite-sample results, identifies pathological cases, and tests them with numerical experiments. We specialize our fundamental results to a ranking application by computing the probability of random projections flipping the node ordering induced by their embeddings. We find that, depending on the degree distribution, the method produces especially unreliable embeddings for the dot product, regardless of whether the adjacency or the normalized transition matrix is used. With respect to the statistical noise introduced by random projections, we show that cosine similarity produces remarkably more precise approximations.",
      "url": "https://www.microsoft.com/en-us/research/publication/node-similarities-under-random-projections-limits-and-pathological-cases-2/"
    },
    {
      "title": "Scaling Textual Gradients via Sampling-Based Momentum",
      "authors": [
        "Jiachen T. Wang",
        "Junyuan Hong",
        "Yuxin Chen",
        "Zhangyang Wang",
        "Zinan Lin",
        "Zixin Ding"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "June 2025",
      "abstract": "As prompts play an increasingly critical role in large language models (LLMs), optimizing textual prompts has become a crucial challenge. The Textual Gradient Descent (TGD) framework has emerged as a promising data-driven approach that iteratively refines textual prompts using LLM – suggested updates (or textual gradients) over minibatches of training samples. In this paper, we empirically demonstrate that scaling the number of training examples initially improves but later degrades TGD’s performance across multiple downstream NLP tasks. However, while data scaling improves results for most tasks, it also significantly increases the computational cost when leveraging LLMs. To address this, we draw inspiration from numerical gradient descent and propose Textual Stochastic Gradient Descent with Momentum (TSGD-M) – a method that facilitates scalable in-context learning by reweighting prompt sampling based on past batch distributions. Across nine NLP tasks spanning three domains – including BIG-Bench Hard (BBH), natural language understanding tasks, and reasoning tasks – TSGD-M significantly outperforms TGD baselines that do not incorporate reweighted sampling, while also reducing variance in most tasks.",
      "url": "https://www.microsoft.com/en-us/research/publication/scaling-textual-gradients-via-sampling-based-momentum/"
    },
    {
      "title": "PulseCore: An Impredicative Concurrent Separation Logic for Dependently Typed Programs",
      "authors": [
        "Aseem Rastogi",
        "Gabriel Ebner",
        "Guido Martínez",
        "Megan Frisella",
        "Nikhil Swamy",
        "Tahina Ramananandro",
        "Thibault Dardinier"
      ],
      "research_areas": [
        "Programming languages and software engineering"
      ],
      "publication_date": "June 2025",
      "abstract": "PulseCore is a new program logic suitable for intrinsic proofs of higher-order, stateful, concurrent, dependently typed programs. It provides many of the features of a modern, concurrent separation logic, including dynamically allocated impredicative invariants, higher-order ghost state, step-indexing with later credits, and support for user-defined ghost state constructions. PulseCore is developed foundationally within the F* programming language with fully mechanized proofs, and is applicable to F* programs itself.\nTo evaluate our work, we use Pulse, a surface language within F* for PulseCore, to develop a range of program proofs. Illustrating its suitability for proving higher-order concurrent programs, we present a verified library for task pools in the style of OCaml5, together with some verified task-parallel programs. Next, we present various data structures and synchronization primitives, including a barrier that requires the use of higher-order ghost state. Finally, we present a verified implementation of the DICE Protection Environment, an industry standard secure boot protocol. Taken together, our evaluation consists of more than 31,000 lines of verified code in a range of settings, providing evidence that PulseCore is both highly expressive as well as practical for a variety of program proof applications.",
      "url": "https://www.microsoft.com/en-us/research/publication/pulsecore-an-impredicative-concurrent-separation-logic-for-dependently-typed-programs/"
    },
    {
      "title": "Self-reflecting Large Language Models: A Hegelian Dialectical Approach",
      "authors": [
        "Can Goksen",
        "Julie E. Maybee",
        "Kazuhito Koishida",
        "Michael Solodko",
        "Saeed Amizadeh",
        "Sara Abdali"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Human language technologies",
        "Mathematics",
        "Social sciences"
      ],
      "publication_date": "June 2025",
      "abstract": "Investigating NLP through a philosophical lens has recently caught researchers’ eyes, as it bridges computational methods with classical schools of philosophy. This paper introduces a philosophical framework inspired by the Hegelian Dialectic to enable LLMs’ self-reflection, utilizing a self-dialectical approach to emulate internal critiques and synthesize new scientific ideas (spanning do mains such as mathematics, physics, and more). Additionally, we explore the effect of generation temperature in LLMs by introducing a dynamic annealing approach, which encourages creativity in the early stages and gradually focuses on refinement and nuance, as well as a constant temperature strategy. Furthermore, we implement a Multi-Agent Majority Voting (MAMV) strategy to assess the validity and novelty of the generated ideas, which proves useful in the absence of domain experts. We also evaluate the effectiveness of our method in generating novel scientific ideas and improving LLMs’ reasoning capabilities. Our experiments demonstrate promising results in ideation, along with significant improvements in mathematical and symbolic reasoning.",
      "url": "https://www.microsoft.com/en-us/research/publication/self-reflecting-large-language-models-a-hegelian-dialectical-approach-2/"
    },
    {
      "title": "COVID-19 Pandemic-Era Changes in Sudden Unexpected Infant Death in the United States",
      "authors": [
        "Bill Weeks",
        "Darren Tanner",
        "Edwin A. Mitchell",
        "Jan-Marino Ramirez",
        "Juan M. Lavista Ferres"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Medical, health and genomics"
      ],
      "publication_date": "May 2025",
      "abstract": "OBJECTIVE\nWe aimed to investigate COVID-19 pandemic-era changes in postperinatal sudden unexpected infant death (SUID) and their association with maternal sociodemographic factors.\nMETHODS\nWe conducted a nationwide cohort study using Centers for Disease Control and Prevention National Center for Health Statistics data for US births from 2016 to 2021. SUID cases were identified using International Classification of Diseases, Tenth Edition codes R95, R99, and W75; we defined pandemic-era births as those between April 2020 and December 2021. We compared postperinatal SUID rates before and during the pandemic and conducted counterfactual analyses to identify whether the pandemic was associated with changes in SUID rate trends. Analyses were stratified by maternal race and ethnicity, age, education, and insurance status.\nRESULTS\nOverall postperinatal SUID rates increased from 87.6 per 100 000 births prepandemic to 95.3 after pandemic onset (rate difference, 7.7; 95% CI, 4.9–10.5). Significant rate increases were found among infants born to non-Hispanic Black mothers, younger mothers, mothers with lower educational attainment, and Medicaid recipients. Counterfactual analyses showed higher than expected pandemic-era rates for these groups and additionally infants born to non-Hispanic white mothers and mothers with private insurance. Only 0.86% of pandemic-era SUID cases included COVID-19 as an additional cause of death.\nCONCLUSION\nPostperinatal SUID rates increased after the onset of the COVID-19 pandemic, with the largest increases in sociodemographic groups already at high risk for SUID. Secondary effects of the pandemic, rather than direct viral impact, may have contributed to the rise in SUID rates. The study underscores the need for further research to pinpoint specific factors and develop interventions to mitigate these increases.",
      "url": "https://www.microsoft.com/en-us/research/publication/covid-19-pandemic-era-changes-in-sudden-unexpected-infant-death-in-the-united-states/"
    },
    {
      "title": "Dion: Distributed Orthonormalized Updates",
      "authors": [
        "Byron Xu",
        "John Langford",
        "Kwangjun Ahn",
        "Natalie Abreu"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "May 2025",
      "abstract": "Recent work has shown that orthonormal matrix updates speed up neural network optimization, improve training stability, and offer better hyperparameter transfer across model sizes. Applying these updates efficiently when model weights and optimizer states are sharded across a large-scale distributed LLM training system remains a major challenge. We introduce Dion (DIstributed OrthoNormalization), a scalable and communication-efficient orthonormalizing optimizer. Dion leverages low-rank approximation and decoupled momentum buffers, eliminating the need for full gradient synchronization while producing numerically equivalent results. It is compatible with simultaneous DDP, FSDP, and TP parallelism, and it computes an orthonormalized update without unsharding a full parameter matrix on any single device. We evaluate Dion on language models from 120M to 3B parameters and find that its benefits improve with increasing model size and batch size.",
      "url": "https://www.microsoft.com/en-us/research/publication/dion-distributed-orthonormalized-updates/"
    },
    {
      "title": "Decision Trees with Short Explainable Rules",
      "authors": [
        "E. Laber",
        "F. Cicalese",
        "M. Molinaro",
        "Marco Molinaro",
        "Victor Feitosa Souza"
      ],
      "research_areas": [
        "Algorithms"
      ],
      "publication_date": "May 2025",
      "abstract": "Decision trees are widely used in many settings where interpretable models are preferred or required. As confirmed by recent empirical studies, the interpretability/explainability of a decision tree critically depends on some of its structural parameters, like size and the average/maximum depth of its leaves. There is indeed a vast literature on the design and analysis of decision tree algorithms that aim at optimizing these parameters.\nThis paper contributes to this important line of research: we propose as a novel criterion of measuring the interpretability of a decision tree, the sparsity of the set of attributes that are required to explain the classification of the examples. We give a tight characterization of the best possible guarantees achievable by a decision tree built to optimize both our new measure (which we call the explanation size) and the more classical measures of worst-case and average depth. We also show that from our characterizations it is possible to obtain polynomial algorithms that guarantee O(log n)-approximation (hence optimal if P!=NP) for the minimization of both the average/worst-case explanation size and the average/worst-case depth.",
      "url": "https://www.microsoft.com/en-us/research/publication/decision-trees-with-short-explainable-rules-2/"
    },
    {
      "title": "Prototypical Human-AI Collaboration Behaviors from LLM-Assisted Writing in the Wild",
      "authors": [
        "Bahar Sarrafzadeh",
        "Debarati Das",
        "Hancheng Cao",
        "Sheshera Mysore"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Human language technologies"
      ],
      "publication_date": "May 2025",
      "abstract": "As large language models (LLMs) are used in complex writing workflows, users engage in multi-turn interactions to steer generations to better fit their needs. Rather than passively accepting output, users actively refine, explore, and co-construct text. We conduct a large-scale analysis of this collaborative behavior for users engaged in writing tasks in the wild with two popular AI assistants, Bing Copilot and WildChat. Our analysis goes beyond simple task classification or satisfaction estimation common in prior work and instead characterizes how users interact with LLMs through the course of a session. We identify prototypical behaviors in how users interact with LLMs in prompts following their original request. We refer to these as Prototypical Human-AI Collaboration Behaviors (PATHs) and find that a small group of PATHs explain a majority of the variation seen in user-LLM interaction. These PATHs span users revising intents, exploring texts, posing questions, adjusting style or injecting new content. Next, we find statistically significant correlations between specific writing intents and PATHs, revealing how users’ intents shape their collaboration behaviors. We conclude by discussing the implications of our findings on LLM alignment.",
      "url": "https://www.microsoft.com/en-us/research/publication/prototypical-human-ai-collaboration-behaviors-from-llm-assisted-writing-in-the-wild/"
    },
    {
      "title": "GeoVision Labeler: Zero-Shot Geospatial Classification with Vision and Language Models",
      "authors": [
        "Akram Zaytar",
        "Caleb Robinson",
        "Gilles Quentin Hacheme",
        "Girmaw Abebe Tadesse",
        "Juan M. Lavista Ferres",
        "Rahul Dodhia"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Computer vision",
        "Ecology and environment"
      ],
      "publication_date": "May 2025",
      "abstract": "Classifying geospatial imagery remains a major bottleneck for applications such as disaster response and land-use monitoring-particularly in regions where annotated data is scarce or unavailable. Existing tools (e.g., RS-CLIP) that claim zero-shot classification capabilities for satellite imagery nonetheless rely on task-specific pretraining and adaptation to reach competitive performance. We introduce GeoVision Labeler (GVL), a strictly zero-shot classification framework: a vision Large Language Model (vLLM) generates rich, human-readable image descriptions, which are then mapped to user-defined classes by a conventional Large Language Model (LLM). This modular, and interpretable pipeline enables flexible image classification for a large range of use cases. We evaluated GVL across three benchmarks-SpaceNet v7, UC Merced, and RESISC45. It achieves up to 93.2% zero-shot accuracy on the binary Buildings vs. No Buildings task on SpaceNet v7. For complex multi-class classification tasks (UC Merced, RESISC45), we implemented a recursive LLM-driven clustering to form meta-classes at successive depths, followed by hierarchical classification first resolving coarse groups, then finer distinctions to deliver competitive zero-shot performance. GVL is open-sourced at this https URL to catalyze adoption in real-world geospatial workflows.",
      "url": "https://www.microsoft.com/en-us/research/publication/geovision-labeler-zero-shot-geospatial-classification-with-vision-and-language-models/"
    },
    {
      "title": "Not only what, but also when: Understanding Brazilian political comments on legislative bills over time through Stance Detection and Topic Modeling",
      "authors": [
        "Matheus Cerqueira"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "May 2025",
      "abstract": "Legislative public spaces are important structures for participatory democracy, allowing citizens’ voices to get engaged with politic decisions. As a consequence of the popularization of information and communication technologies, internet based tools have played an important role to improve public participation in political decisions, known as e-Democracy. These tools are usually composed of a set of functionalities or small services, named microservices. The better the microservices, the higher the citizen participation. This work investigates how to extract useful knowledge from citizen participation in the microservices of the public portal of the Brazilian Chamber of Deputies. For such, it analyzes public comments incorporating Natural Language Processing and Artificial Intelligence techniques in a platform named Ulysses. The tasks developed on this paper focus on a temporal analysis of comments on bills in the portal through Stance Detection and dynamic Topic Modeling tasks. For the first task, OxêSD, a BERTimbau-based model, was trained on two different corpora, one of them translated into Portuguese, and its predictive performance was evaluated using the F1 and ROC-AUC metrics, achieving 73% for both on our proposed Political-BRSD a mixed dataset containing both translated content from a bigger multilingual dataset (adapted from x-Stance) and bill-specific content (adapted from Ulysses-SD); for the second, BERTopic, a Topic Modeling framework, was used. Visualization tools to analyze how the proposed approach addressed the task were also used to explore the knowledge extracted. They allow the user to understand over time how the comments relate to each other and how the comments relate to a given legislative bill.",
      "url": "https://www.microsoft.com/en-us/research/publication/not-only-what-but-also-when-understanding-brazilian-political-comments-on-legislative-bills-over-time-through-stance-detection-and-topic-modeling/"
    },
    {
      "title": "Beyond Metrics: Evaluating LLMs’ Effectiveness in Culturally Nuanced, Low-Resource Real-World Scenarios",
      "authors": [
        "Jacki O'Neill",
        "Jindong Wang",
        "Kalika Bali",
        "Keshet Ronen",
        "Millicent Ochieng",
        "Sunayana Sitaram",
        "Varun Gumma",
        "Vishrav Chaudhary"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Human language technologies",
        "Human-computer interaction"
      ],
      "publication_date": "May 2025",
      "abstract": "The deployment of Large Language Models (LLMs) in real-world applications presents both opportunities and challenges, particularly in multilingual and code-mixed communication settings. This research evaluates the performance of seven leading LLMs in sentiment analysis on a dataset derived from multilingual and code-mixed WhatsApp chats, including Swahili, English and Sheng. Our evaluation includes both quantitative analysis using metrics like F1 score and qualitative assessment of LLMs’ explanations for their predictions. We find that, while Mistral-7b and Mixtral-8x7b achieved high F1 scores, they and other LLMs such as GPT-3.5-Turbo, Llama-2-70b, and Gemma-7b struggled with understanding linguistic and contextual nuances, as well as lack of transparency in their decision-making process as observed from their explanations. In contrast, GPT-4 and GPT-4-Turbo excelled in grasping diverse linguistic inputs and managing various contextual information, demonstrating high consistency with human alignment and transparency in their decision-making process. The LLMs however, encountered difficulties in incorporating cultural nuance especially in non-English settings with GPT-4s doing so inconsistently. The findings emphasize the necessity of continuous improvement of LLMs to effectively tackle the challenges of culturally nuanced, low-resource real-world settings and the need for developing evaluation benchmarks for capturing these issues.",
      "url": "https://www.microsoft.com/en-us/research/publication/beyond-metrics-evaluating-llms-effectiveness-in-culturally-nuanced-low-resource-real-world-scenarios/"
    },
    {
      "title": "The Agentic Economy",
      "authors": [
        "Aleksandrs Slivkins",
        "Brendan Lucier",
        "Daniel G. Goldstein",
        "David Rothschild",
        "Eleanor Dillon",
        "Jake Hofman",
        "Markus Mobius",
        "Matthew Vogel",
        "Nicole Immorlica",
        "Sonia Jaffe"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Economics"
      ],
      "publication_date": "May 2025",
      "abstract": "Generative AI has transformed human-computer interaction by enabling natural language interfaces and the emergence of autonomous agents capable of acting on users’ behalf. While early applications have improved individual productivity, these gains have largely been confined to predefined tasks within existing workflows. We argue that the more profound economic impact lies in reducing communication frictions between consumers and businesses. This shift could reorganize markets, redistribute power, and catalyze the creation of new products and services. We explore the implications of an agentic economy, where assistant agents act on behalf of consumers and service agents represent businesses, interacting programmatically to facilitate transactions. A key distinction we draw is between unscripted interactions — enabled by technical advances in natural language and protocol design — and unrestricted interactions, which depend on market structures and governance. We examine the current limitations of siloed and end-to-end agents, and explore future scenarios shaped by technical standards and market dynamics. These include the potential tension between agentic walled gardens and an open web of agents, implications for advertising and discovery, the evolution of micro-transactions, and the unbundling and rebundling of digital goods. Ultimately, we argue that the architecture of agentic communication will determine the extent to which generative AI democratizes access to economic opportunity.",
      "url": "https://www.microsoft.com/en-us/research/publication/the-agentic-economy/"
    },
    {
      "title": "GenTool: Enhancing Tool Generalization in Language Models through Zero-to-One and Weak-to-Strong Simulation",
      "authors": [
        "Hui Liu",
        "Jeff Z. Pan",
        "Jennifer Neville",
        "Jie He",
        "Longqi Yang",
        "Mengting Wan",
        "Pei Zhou",
        "Xia Song",
        "Xiaofeng Xu"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "May 2025",
      "abstract": "Large Language Models (LLMs) can enhance their capabilities as AI assistants by integrating external tools, allowing them to access a wider range of information. While recent LLMs are typically fine-tuned with tool usage examples during supervised fine-tuning (SFT), questions remain about their ability to develop robust tool-usage skills and can effectively generalize to unseen queries and tools. In this work, we present GenTool, a novel training framework that prepares LLMs for diverse generalization challenges in tool utilization. Our approach addresses two fundamental dimensions critical for real-world applications: Zero-to-One Generalization, enabling the model to address queries initially lacking a suitable tool by adopting and utilizing one when it becomes available, and Weak-to-Strong Generalization, allowing models to leverage enhanced versions of existing tools to solve queries. To achieve this, we develop synthetic training data simulating these two dimensions of tool usage and introduce a two-stage fine-tuning approach: optimizing tool ranking, then refining tool selection. Through extensive experiments across four generalization scenarios, we demonstrate that our method significantly enhances the tool-usage capabilities of LLMs ranging from 1B to 8B parameters, achieving performance that surpasses GPT-4o. Furthermore, our analysis also provides valuable insights into the challenges LLMs encounter in tool generalization.",
      "url": "https://www.microsoft.com/en-us/research/publication/gentool-enhancing-tool-generalization-in-language-models-through-zero-to-one-and-weak-to-strong-simulation/"
    },
    {
      "title": "VeriTrail: Closed-Domain Hallucination Detection with Traceability",
      "authors": [
        "Dasha Metropolitansky",
        "Jonathan Larson"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Human language technologies"
      ],
      "publication_date": "May 2025",
      "abstract": "Even when instructed to adhere to source material, Language Models often generate unsubstantiated content – a phenomenon known as “closed-domain hallucination.” This risk is amplified in processes with multiple generative steps (MGS), compared to processes with a single generative step (SGS). However, due to the greater complexity of MGS processes, we argue that detecting hallucinations in their final outputs is necessary but not sufficient: it is equally important to trace where hallucinated content was likely introduced and how faithful content may have been derived from the source through intermediate outputs. To address this need, we present VeriTrail, the first closed-domain hallucination detection method designed to provide traceability for both MGS and SGS processes. We also introduce the first datasets to include all intermediate outputs as well as human annotations of final outputs’ faithfulness for their respective MGS processes. We demonstrate that VeriTrail outperforms baseline methods on both datasets.",
      "url": "https://www.microsoft.com/en-us/research/publication/veritrail-closed-domain-hallucination-detection-with-traceability/"
    },
    {
      "title": "AIOpsLab: A Holistic Framework for Evaluating AI Agents for Enabling Autonomous Cloud",
      "authors": [
        "Chetan Bansal",
        "Gagan Somashekar",
        "Jonathan Mace",
        "Manish Shetty",
        "Minghua Ma",
        "Rujia Wang",
        "Saravan Rajmohan",
        "Yinfang Chen",
        "Yogesh Simmhan"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Data platforms and analytics",
        "Systems and networking"
      ],
      "publication_date": "May 2025",
      "abstract": "AI for IT Operations (AIOps) aims to automate complex operational tasks, such as fault localization and root cause analysis, to reduce human workload and minimize customer impact. While traditional DevOps tools and AIOps algorithms often focus on addressing isolated operational tasks, recent advances in Large Language Models (LLMs) and AI agents are revolutionizing AIOps by enabling end-to-end and multitask automation. This paper envisions a future where AI agents autonomously manage operational tasks throughout the entire incident lifecycle, leading to self-healing cloud systems, a paradigm we term AgentOps. Realizing this vision requires a comprehensive framework to guide the design, development, and evaluation of these agents. To this end, we present AIOPSLAB, a framework that not only deploys microservice cloud environments, injects faults, generates workloads, and exports telemetry data but also orchestrates these components and provides interfaces for interacting with and evaluating agents. We discuss the key requirements for such a holistic framework and demonstrate how AIOPSLAB can facilitate the evaluation of next-generation AIOps agents. Through evaluations of state-of-the-art LLM agents within the benchmark created by AIOPSLAB, we provide insights into their capabilities and limitations in handling complex operational tasks in cloud environments. The source code for AIOPSLAB is publicly available at https://aka.ms/aiopslab-repo.",
      "url": "https://www.microsoft.com/en-us/research/publication/aiopslab-a-holistic-framework-for-evaluating-ai-agents-for-enabling-autonomous-cloud/"
    },
    {
      "title": "Identifying Incoherent Search Sessions: Search Click Fraud Remediation Under Real-World Constraints",
      "authors": [
        "Brendan Saltaformaggio",
        "Cormac Herley",
        "David Oygenblik",
        "Haichuan Xu",
        "Mingxuan Yao",
        "Paul England",
        "Ranjita Pai Sridhar",
        "Runze Zhang",
        "Vacha Dave",
        "Zheng Yang"
      ],
      "research_areas": [
        "Security, privacy, and cryptography"
      ],
      "publication_date": "May 2025",
      "abstract": "Search engines and advertisers continuously suffer substantial financial losses from click fraud, which poses challenges to existing detection algorithms. Even more concerning, despite ongoing advancements, our understanding of click fraud remains limited, leaving room for sophisticated fraudulent techniques to bypass existing detection measures. In this study, we pivot from examining individual search requests to analyzing search sessions, defined as sequences of consecutive search queries made by the same user. We found that benign users exhibit coherent behavior patterns within these sessions, which contrast clearly with those of fraudulent actors. Specifically, legitimate users tend to conduct searches focused on a single topic at a time. In contrast, fraudsters or automated bots often exhibit diverse, illogical, and incoherent search behaviors within a session. To address this behavioral distinction, we propose CoSeC, a system designed to quantify the “incoherence index” of search sessions. CoSeC integrates literal semantic, temporal, and ad-click behavioral features to evaluate sessions’ coherence quantitatively. Our evaluation of CoSeC demonstrates high efficacy, achieving a precision of 95.79% and a recall of 92.40% in identifying incoherent sessions, highlighting CoSeC’s substantial potential to enhance real-world click fraud detection.",
      "url": "https://www.microsoft.com/en-us/research/publication/identifying-incoherent-search-sessions-search-click-fraud-remediation-under-real-world-constraints/"
    },
    {
      "title": "Online Scheduling for LLM Inference with KV Cache Constraints",
      "authors": [
        "Chara Podimata",
        "Jiashuo Jiang",
        "Konstantina Mellou",
        "Marco Molinaro",
        "Patrick Jaillet",
        "Zijie Zhou"
      ],
      "research_areas": [
        "Algorithms",
        "Artificial intelligence"
      ],
      "publication_date": "May 2025",
      "abstract": "Large Language Model (LLM) inference, where a trained model generates text one word at a time in response to user prompts, is a computationally intensive process requiring efficient scheduling to optimize latency and resource utilization. A key challenge in LLM inference is the management of the Key-Value (KV) cache, which reduces redundant computations but introduces memory constraints. In this work, we model LLM inference with KV cache constraints theoretically and propose a novel batching and scheduling algorithm that minimizes inference latency while effectively managing the KV cache’s memory.\nMore specifically, we make the following contributions. First, to evaluate the performance of online algorithms for scheduling in LLM inference, we introduce a hindsight optimal benchmark, formulated as an integer program that computes the minimum total inference latency under full future information. Second, we prove that no deterministic online algorithm can achieve a constant competitive ratio when the arrival process is arbitrary. Third, motivated by the computational intractability of solving the integer program at scale, we propose a polynomial-time online scheduling algorithm and show that under certain conditions it can achieve a constant competitive ratio. We also demonstrate our algorithm’s strong empirical performance by comparing it to the hindsight optimal in a synthetic dataset. Finally, we conduct empirical evaluations on a real-world public LLM inference dataset, simulating the Llama2-70B model on A100 GPUs, and show that our algorithm significantly outperforms the benchmark algorithms. Overall, our results offer a path toward more sustainable and cost-effective LLM deployment.",
      "url": "https://www.microsoft.com/en-us/research/publication/online-scheduling-for-llm-inference-with-kv-cache-constraints/"
    },
    {
      "title": "ProtoRAIL: A Risk-cognizant Imitation Agent for Adaptive vCPU Oversubscription In The Cloud",
      "authors": [
        "Bo Qiao",
        "Chetan Bansal",
        "Dongmei Zhang",
        "Eli Cortez",
        "Fangkai Yang",
        "Hang Dong",
        "Lu Wang",
        "Mayukh Das",
        "Qingwei Lin 林庆维",
        "Saravan Rajmohan",
        "Si Qin",
        "Victor Ruehle",
        "Íñigo Goiri"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Systems and networking"
      ],
      "publication_date": "May 2025",
      "abstract": "Safe optimization of operating costs is one of the holy grails of successful revenue-generating cloud systems and capacity/resource efficiency is a key factor in making that a reality. Among other strategies for resource efficiency across major cloud providers, Oversubscription is an extremely prevalent practice where more virtual resources are offered than actual physical capacity to minimize revenue loss against redundant capacity. While resources can be of any type, including compute, memory, power or network bandwidth, we highlight the scenario of virtual CPU (vCPU) oversubscription since vCPU cores are primarily the billable units for cloud services and has substantial impact on business as well as users. For a seamless cloud experience, while being cost-efficient for the provider, suitable policies for controlling oversubscription margins are crucial. Narrow margins lead to redundant expenditure on under-utilized resource capacity, and wider margins lead to under-provisioning where customer workloads may suffer from resource contention.\nMost oversubscription policies today are engineered either with tribal knowledge or with static heuristics about the system, which lead to catastrophic overloading or stranded/under-utilized resources. Smart oversubscription policies that can adapt to demand/utilization patterns across time and granularity to jointly optimize cost benefits and risks is a non-trivial, largely, unsolved problem. We address this challenge with our proposed novel Prototypical Risk-cognizant Active Imitation Learning (ProtoRAIL) framework that exploits approximate symmetries in utilization patterns to learn suitable policies. The active knowledge-in-the-loop (KITL) module de-risks the learned policies. Our empirical investigations and real deployments on X company’s internal (1st party) cloud service, show orders of magnitude reduction (≈≥ 90×) in risk and significant increase in benefits (saved stranded resources: in a range of ≈ 7 to 10%).",
      "url": "https://www.microsoft.com/en-us/research/publication/protorail-a-risk-cognizant-imitation-agent-for-adaptive-vcpu-oversubscription-in-the-cloud/"
    },
    {
      "title": "Apiary: An OS for the Modern FPGA",
      "authors": [
        "Baris Kasikci",
        "Irene Zhang",
        "Katie Lim",
        "Matthew Giordano",
        "Thomas Anderson"
      ],
      "research_areas": [
        "Programming languages and software engineering"
      ],
      "publication_date": "May 2025",
      "abstract": "Many datacenter operators have deployed FPGAs as hardware accelerators because their reconfigurability allows them to be repurposed as the application mix changes. Directly attaching the FPGA to the network further reduces latency, improves cost-performance, and reduces energy use relative to mediating network communications with CPUs. However, building accelerated applications or services for direct-attached FPGAs is challenging, especially with the complex I/O and multi-accelerator capacity of modern FPGAs. To address this, we propose Apiary, a microkernel operating system for direct-attached FPGA accelerators. The key idea in Apiary is to raise the level of abstraction for accelerated application code, with security, virtualization, threaded execution, and interprocess communication provided by the hardware OS layer.",
      "url": "https://www.microsoft.com/en-us/research/publication/apiary-an-os-for-the-modern-fpga/"
    },
    {
      "title": "Forecasting acute childhood malnutrition in Kenya using machine learning and diverse sets of indicators",
      "authors": [
        "Bistra Dilkina",
        "Caleb Robinson",
        "Girmaw Abebe Tadesse",
        "Herbert Wanyonyi",
        "Juan M. Lavista Ferres",
        "Laura Ferguson",
        "Rahul Dodhia",
        "Samuel Mburu",
        "Samuel Murage",
        "Shiphrah Kuria"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "May 2025",
      "abstract": "Objectives\nMalnutrition is a leading cause of morbidity and mortality for children under-5 globally. Low- and middle-income countries, such as Kenya, bear the greatest burden of malnutrition. The Kenyan government has been collecting clinical indicators, including on malnutrition, using District Health Information Software-2 (DHIS2) for over a decade. We aim to address the existing gap in decision-makers’ ability to develop and utilize malnutrition forecasting capabilities for timely interventions. Specifically, our objectives include: develop a spatio-temporal machine learning model to forecast acute malnutrition among children in Kenya using DHIS2 data, enhance forecasting capability by integrating external complementary indicators, such as publicly available satellite imagery-driven signals, and forecast acute malnutrition at various stages and time horizons, including moderate, severe, and aggregated cases.\n\n\nMethods\nWe propose a framework to forecast malnutrition risk for each sub-county in Kenya based on clinical indicators and remote sensory data. To achieve this, we first aggregate clinical indicators and remotely sensed satellite data, specifically gross primary productivity measurements, to the sub-county level. We then label the rate of children diagnosed with acute malnutrition at the sub-county level using the standard Integrated Food Security Phase Classification for Acute Malnutrition. We then apply and compare several methods for forecasting malnutrition risk in Kenya using data collected from January 2019 to February 2024. As a baseline, we used a Window Average model, which captures the current practice at the Kenyan Ministry of Health. We also trained machine learning models, such as Logistic Regression and Gradient Boosting, to forecast acute malnutrition risk based on observed indicators from prior months. Different metrics, mainly Area Under Receiver Operating Characteristic Curve (AUC), were used to evaluate the forecasting performance by comparing their forecast values to known values on a hold-out test set.\n\n\nResults\nWe found that machine learning based models consistently outperform the Window Average baselines on forecasting sub-county malnutrition rates in Kenya. For example, the Gradient Boosting model achieves a mean AUC of 0.86 when forecasting with a 6-month time horizon, compared to an AUC of 0.73 achieved by the Window Average model. The Window Average method particularly fails to correctly forecast malnutrition in parts of West and Central Kenya where the acute malnutrition rate is variable over time and typically less than . We further found that machine learning models with satellite-based features alone also outperform Window Averaging baselines, while not needing clinical data at inference time. Finally, we found that recently observed outcomes and the remotely sensed data are key indicators. Our results demonstrate the ability of machine learning models to accurately forecast malnutrition in Kenya at a sub-county level from a variety of indicators.\n\n\nConclusions\nTo the best of the authors’ knowledge, this work is the first to use clinical indicators collected via DHIS2 to forecast acute malnutrition in childhood at the sub-county level in Kenya. This work represents a foundational step in developing a broader childhood malnutrition forecasting framework, capable of monitoring malnutrition trends and identifying impending malnutrition peaks across more than 80 low- and middle-income countries collecting similar DHIS2 datasets.\n",
      "url": "https://www.microsoft.com/en-us/research/publication/forecasting-acute-childhood-malnutrition-in-kenya-using-machine-learning-and-diverse-sets-of-indicators/"
    },
    {
      "title": "VoLUT: Efficient Volumetric Streaming Enhanced By LUT-Based Super-Resolution",
      "authors": [
        "Lili Qiu",
        "Xinyang Jiang",
        "Yifan Yang",
        "Yuqing Yang"
      ],
      "research_areas": [
        "Computer vision",
        "Systems and networking"
      ],
      "publication_date": "May 2025",
      "abstract": "3D volumetric video provides immersive experience and is gaining traction in digital media. Despite its rising popularity, the streaming of volumetric video content poses significant challenges due to the high data bandwidth requirement. A natural approach to mitigate the bandwidth issue is to reduce the volumetric video’s data rate by downsampling the content prior to transmission. The video can then be upsampled at the receiver’s end using a super-resolution (SR) algorithm to reconstruct the high-resolution details. While super-resolution techniques have been extensively explored and advanced for 2D video content, there is limited work on SR algorithms tailored for volumetric videos. To address this gap and the growing need for efficient volumetric video streaming, we have developed VoLUT with a new SR algorithm specifically designed for volumetric content. Our algorithm uniquely harnesses the power of lookup tables (LUTs) to facilitate the efficient and accurate upscaling of low-resolution volumetric data. The use of LUTs enables our algorithm to quickly reference precomputed high-resolution values, thereby significantly reducing the computational complexity and time required for upscaling. We further apply adaptive video bit rate algorithm (ABR) to dynamically determine the downsampling rate according to the network condition and stream the selected video rate to the receiver. Compared to related work, VoLUT is the first to enable high-quality 3D SR on commodity mobile devices at line-rate. Our evaluation shows VoLUT can reduce bandwidth usage by 70% , boost QoE by 36.7% for volumetric video streaming and achieve 8.4× 3D SR speed-up with no quality compromise.",
      "url": "https://www.microsoft.com/en-us/research/publication/volut-efficient-volumetric-streaming-enhanced-by-lut-based-super-resolution/"
    },
    {
      "title": "Lost in Transmission: When and Why LLMs Fail to Reason Globally",
      "authors": [
        "Adith Swaminathan",
        "Jennifer Neville",
        "Kiran Tomlinson",
        "Tobias Schnabel"
      ],
      "research_areas": [
        "Algorithms",
        "Artificial intelligence"
      ],
      "publication_date": "May 2025",
      "abstract": "Despite their many successes, transformer-based large language models (LLMs) continue to struggle with tasks that require complex reasoning over large parts of their input. We argue that these failures arise due to capacity limits on the accurate flow of information within LLMs. To formalize this issue, we introduce the bounded attention prefix oracle (BAPO) model, a new computational framework that models bandwidth constraints on attention heads, the mechanism for internal communication in LLMs. We show that several important reasoning problems like graph reachability require high communication bandwidth for BAPOs to solve; we call these problems BAPO-hard. Our experiments corroborate our theoretical predictions: GPT-4, Claude, and Gemini succeed on BAPO-easy tasks and fail even on relatively small BAPO-hard tasks. BAPOs also reveal another benefit of chain of thought (CoT): we prove that breaking down a task using CoT can turn any BAPO-hard problem into a BAPO-easy one. Our results offer principled explanations for key LLM failures and suggest directions for architectures and inference methods that mitigate bandwidth limits.",
      "url": "https://www.microsoft.com/en-us/research/publication/lost-in-transmission-when-and-why-llms-fail-to-reason-globally/"
    },
    {
      "title": "A Fourier Space Perspective on Diffusion Models",
      "authors": [
        "Edward Meeds",
        "Fabian Falck",
        "Javier Zazo",
        "Kiarash Zahirnia",
        "Rachel Lawrence",
        "Richard Turner",
        "Sushrut Karmalkar",
        "Teodora Pandeva"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Computer vision"
      ],
      "publication_date": "May 2025",
      "abstract": "Diffusion models are state-of-the-art generative models on data modalities such as images, audio, proteins and materials. These modalities share the property of exponentially decaying variance and magnitude in the Fourier domain. Under the standard Denoising Diffusion Probabilistic Models (DDPM) forward process of additive white noise, this property results in high-frequency components being corrupted faster and earlier in terms of their Signal-to-Noise Ratio (SNR) than low-frequency ones. The reverse process then generates low-frequency information before high-frequency details. In this work, we study the inductive bias of the forward process of diffusion models in Fourier space. We theoretically analyse and empirically demonstrate that the faster noising of high-frequency components in DDPM results in violations of the normality assumption in the reverse process. Our experiments show that this leads to degraded generation quality of high-frequency components. We then study an alternate forward process in Fourier space which corrupts all frequencies at the same rate, removing the typical frequency hierarchy during generation, and demonstrate marked performance improvements on datasets where high frequencies are primary, while performing on par with DDPM on standard imaging benchmarks.",
      "url": "https://www.microsoft.com/en-us/research/publication/a-fourier-space-perspective-on-diffusion-models/"
    },
    {
      "title": "Modeling protective meningococcal antibody responses and factors influencing antibody persistence following vaccination with MenAfriVac® using machine learning",
      "authors": [
        "Anthony Marfin",
        "Bill Weeks",
        "Brian Taliesin",
        "Juan M. Lavista Ferres",
        "Mark Alderson",
        "Md Nasir",
        "Niranjan Bhat",
        "Rahul Dodhia",
        "Shahrzad Gholami",
        "Troy Leader"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Medical, health and genomics"
      ],
      "publication_date": "May 2025",
      "abstract": "Meningococcal meningitis poses a significant public health burden in the meningitis belt region of sub-Saharan Africa. The introduction of the meningococcal PsA-TT vaccine (MenAfriVac®) has successfully eliminated Neisseria meningitidis serogroup A (NmA) cases in the region. However, the duration of post-vaccination immunity and the need for booster doses remain uncertain. To address this knowledge gap, we developed computational models using machine learning techniques to improve the effectiveness of modeling in guiding vaccination strategies for the African meningitis belt. Using serologic data from previous clinical trials of PsA-TT, we proposed a short-term and a long-term model that integrated demographic and medical variables (such as age, height and weight) with previous antibody titer levels and vaccination information to predict NmA antibody titer levels following vaccination. In the short-term model, we found moderately high performance (R-squared = 0.59) for out-of-training-data subjects and even better performance (R squared = 0.83) in the long-term evaluation. Our models estimated the half-life of the vaccine to be 13.9 years for the study population overall, similar to previously reported estimates. Machine learning techniques offer several advantages over previous approaches, as they do not require multiple readings from the same subject, can be rigorously validated using a subset of subject data not used for training. The proposed approach also facilitates the interpretation of the relationship between input variables and antibody levels at a population level. By incorporating subject-specific demographic and medical variables, our models could potentially be used to tailor vaccination schedules to at-risk populations.",
      "url": "https://www.microsoft.com/en-us/research/publication/modeling-protective-meningococcal-antibody-responses-and-factors-influencing-antibody-persistence-following-vaccination-with-menafrivac-using-machine-learning/"
    },
    {
      "title": "Storage Class Memory is Dead,  All Hail Managed-Retention Memory:  Rethinking Memory for the AI Era",
      "authors": [
        "Ant Rowstron",
        "Burcu Canakci",
        "Dushyanth Narayanan",
        "Ioan Stefanovici",
        "Junyi Liu",
        "Paolo Costa",
        "Richard Black",
        "Sergey Legtchenko",
        "Xingbo Wu"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Systems and networking"
      ],
      "publication_date": "May 2025",
      "abstract": "AI clusters today are one of the major uses of High Bandwidth Memory (HBM). However, HBM is suboptimal for AI workloads for several reasons. Analysis shows HBM is overprovisioned on write performance, but underprovisioned on density and read bandwidth, and also has significant energy per bit overheads. It is also expensive, with lower yield than DRAM due to manufacturing complexity. We propose a new memory class: Managed-Retention Memory (MRM), which is more optimized to store key data structures for AI inference workloads. We believe that MRM may finally provide a path to viability for technologies that were originally proposed to support Storage Class Memory (SCM). These technologies traditionally offered long-term persistence (10+ years) but provided poor IO performance and/or endurance. MRM makes different trade-offs, and by understanding the workload IO patterns, MRM foregoes long-term data retention and write performance for better potential performance on the metrics important for these workloads.",
      "url": "https://www.microsoft.com/en-us/research/publication/storage-class-memory-is-dead-all-hail-managed-retention-memory-rethinking-memory-for-the-ai-era/"
    },
    {
      "title": "Towards Cloud Efficiency with Large-scale Workload Characterization",
      "authors": [
        "A. Parayil",
        "Chetan Bansal",
        "Jue Zhang",
        "Xiaoting Qin",
        "Íñigo Goiri"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Systems and networking"
      ],
      "publication_date": "May 2025",
      "abstract": "Cloud providers introduce features (e.g., Spot VMs, Harvest VMs, and Burstable VMs) and optimizations (e.g., oversubscription, auto-scaling, power harvesting, and overclocking) to improve efficiency and reliability. To effectively utilize these features, it’s crucial to understand the characteristics of workloads running in the cloud. However, workload characteristics can be complex and depend on multiple signals, making manual characterization difficult and unscalable.\nIn this study, we conduct the first large-scale examination of first-party workloads at Microsoft to understand their characteristics. Through an empirical study, we aim to answer the following questions: (1) What are the critical workload characteristics that impact efficiency and reliability on cloud platforms? (2) How do these characteristics vary across different workloads? (3) How can cloud platforms leverage these insights to efficiently characterize all workloads at scale? This study provides a deeper understanding of workload characteristics and their impact on cloud performance, which can aid in optimizing cloud services. Additionally, it identifies potential areas for future research.",
      "url": "https://www.microsoft.com/en-us/research/publication/towards-cloud-efficiency-with-large-scale-workload-characterization/"
    },
    {
      "title": "Good things come in small packages: Should we build AI clusters with Lite-GPUs?",
      "authors": [
        "Ant Rowstron",
        "Burcu Canakci",
        "Dushyanth Narayanan",
        "Junyi Liu",
        "Nathanael Cheriere",
        "Paolo Costa",
        "Sergey Legtchenko",
        "Xingbo Wu"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Hardware and devices",
        "Systems and networking"
      ],
      "publication_date": "May 2025",
      "abstract": "To match the blooming demand of generative AI workloads, GPU designers have so far been trying to pack more and more compute and memory into single complex and expensive packages. However, there is growing uncertainty about the scalability of individual GPUs and thus AI clusters, as state-of-the-art GPUs are already displaying packaging, yield, and cooling limitations. We propose to rethink the design and scaling of AI clusters through efficiently-connected large clusters of Lite-GPUs, GPUs with single, small dies and a fraction of the capabilities of larger GPUs. We think recent advances in co-packaged optics can enable distributing AI workloads onto many Lite-GPUs through high bandwidth and efficient communication. In this paper, we present the key benefits of Lite-GPUs on manufacturing cost, blast radius, yield, and power efficiency; and discuss systems opportunities and challenges around resource, workload, memory, and network management.",
      "url": "https://www.microsoft.com/en-us/research/publication/good-things-come-in-small-packages-should-we-adopt-lite-gpus-in-ai-infrastructure/"
    },
    {
      "title": "LLMs Get Lost In Multi-Turn Conversation",
      "authors": [
        "Hiroaki Hayashi",
        "Jennifer Neville",
        "Philippe Laban",
        "Yingbo Zhou"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Human language technologies"
      ],
      "publication_date": "May 2025",
      "abstract": "Large Language Models (LLMs) are conversational interfaces. As such, LLMs have the potential to assist their users not only when they can fully specify the task at hand, but also to help them define, explore, and refine what they need through multi-turn conversational exchange. Although analysis of LLM conversation logs has confirmed that underspecification occurs frequently in user instructions, LLM evaluation has predominantly focused on the single-turn, fully-specified instruction setting. In this work, we perform large-scale simulation experiments to compare LLM performance in single- and multi-turn settings. Our experiments confirm that all the top open- and closed-weight LLMs we test exhibit significantly lower performance in multi-turn conversations than single-turn, with an average drop of 39% across six generation tasks. Analysis of 200,000+ simulated conversations decomposes the performance degradation into two components: a minor loss in aptitude and a significant increase in unreliability. We find that LLMs often make assumptions in early turns and prematurely attempt to generate final solutions, on which they overly rely. In simpler terms, we discover that *when LLMs take a wrong turn in a conversation, they get lost and do not recover*.",
      "url": "https://www.microsoft.com/en-us/research/publication/llms-get-lost-in-multi-turn-conversation/"
    },
    {
      "title": "PolySAT: Word-level Bit-vector Reasoning in Z3",
      "authors": [
        "Clemens Eisenhofer",
        "Daniela Kaufmann",
        "Jakob Rath",
        "Laura Kov'acs",
        "Nikolaj Bjørner"
      ],
      "research_areas": [
        "Algorithms",
        "Programming languages and software engineering"
      ],
      "publication_date": "May 2025",
      "abstract": "PolySAT is a word-level decision procedure supporting bit-precise SMT reasoning over polynomial arithmetic with large bit-vector operations. Addressing challenges of verified software, PolySAT integrates the theoretical development of SMT-based calculi with a proof of concept implementation and empirical evaluation. The PolySAT calculus extends conflict-driven clause learning modulo theories with two key components: (i) a bit-vector plugin to the equality graph, and (ii) a theory solver for bit-vector arithmetic with non-linear polynomials. PolySAT implements dedicated procedures to extract bit-vector intervals from polynomial inequalities. For conflict analysis and resolution, PolySAT comes with on-demand lemma generation over non-linear bit-vector arithmetic. PolySAT is integrated into the SMT solver Z3 and has applications in model checking and smart contract verification where bit-blasting techniques on multipliers/divisions do not scale.",
      "url": "https://www.microsoft.com/en-us/research/publication/polysat-word-level-bit-vector-reasoning-in-z3/"
    },
    {
      "title": "Towards Resource-Efficient Compound AI Systems",
      "authors": [
        "Adam Belay",
        "Esha Choukse",
        "Gohar Irfan Choudhry",
        "Ricardo Bianchini",
        "Rodrigo Fonseca",
        "Íñigo Goiri"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "May 2025",
      "abstract": "Compound AI Systems, integrating multiple interacting components like models, retrievers, and external tools, have emerged as essential for addressing complex AI tasks. However, current implementations suffer from inefficient resource utilization due to tight coupling between application logic and execution details, a disconnect between orchestration and resource management layers, and the perceived exclusiveness between efficiency and quality. We propose a vision for resource-efficient Compound AI Systems through a declarative workflow programming model and an adaptive runtime system for dynamic scheduling and resource-aware decision-making. Decoupling application logic from low-level details exposes levers for the runtime to flexibly configure the execution environment and resources, without compromising on quality. Enabling collaboration between the workflow orchestration and cluster manager enables higher efficiency through better scheduling and resource management. We are building a prototype system, called Murakkab, to realize this vision. Our preliminary evaluation demonstrates speedups up to ∼ 3.4× in workflow completion times while delivering ∼ 4.5× higher energy efficiency, showing promise in optimizing resources and advancing AI system design ",
      "url": "https://www.microsoft.com/en-us/research/publication/towards-resource-efficient-compound-ai-systems/"
    },
    {
      "title": "Media Bias Detector: Designing and Implementing a Tool for Real-Time Selection and Framing Bias Analysis in News Coverage",
      "authors": [
        "Amir Tohidi",
        "Anushkaa Gupta",
        "Christopher Callison-Burch",
        "David Rothschild",
        "Duncan J Watts",
        "Jennifer Wang",
        "Samar Haider",
        "Yuxuan Zhang"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Human-computer interaction"
      ],
      "publication_date": "May 2025",
      "abstract": "Mainstream media, through their decisions on what to cover and how to frame the stories they cover, can mislead readers without using outright falsehoods. Therefore, it is crucial to have tools that expose these editorial choices underlying media bias. In this paper, we introduce the Media Bias Detector, a tool for researchers, journalists, and news consumers. By integrating large language models, we provide near real-time granular insights into the topics, tone, political lean, and facts of news articles aggregated to the publisher level. We assessed the tool’s impact by interviewing 13 experts from journalism, communications, and political science, revealing key insights into usability and functionality, practical applications, and AI’s role in powering media bias tools. We explored this in more depth with a follow-up survey of 150 news consumers. This work highlights opportunities for AI-driven tools that empower users to critically engage with media content, particularly in politically charged environments.",
      "url": "https://www.microsoft.com/en-us/research/publication/media-bias-detector-designing-and-implementing-a-tool-for-real-time-selection-and-framing-bias-analysis-in-news-coverage/"
    },
    {
      "title": "Computer-Aided Detection (CADe) of Small Metastatic   Prostate Cancer Lesions on 3D PSMA PET Volumes Using   Multi-Angle Maximum Intensity Projections",
      "authors": [
        "Amirhosein Toosi",
        "Bill Weeks",
        "Carlos F. Uribe",
        "Felipe Oviedo",
        "François Bénard",
        "Ghasemali Divband",
        "Juan M. Lavista Ferres",
        "Rahul Dodhia",
        "Sara Harsini"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Medical, health and genomics"
      ],
      "publication_date": "May 2025",
      "abstract": "Objectives: We aimed to develop and evaluate a novel computer-aided detection (CADe) approach for identifying small metastatic biochemically recurrent (BCR) prostate cancer (PCa) lesions on PSMA-PET images, utilizing multi-angle Maximum Intensity Projections (MA-MIPs) and state-of-the-art (SOTA) object detection algorithms.\nMethods: We fine-tuned and evaluated 16 SOTA object detection algorithms (selected across four main categories of model types) applied to MA-MIPs as extracted from rotated 3D PSMA-PET volumes. Predicted 2D bounding boxes were back-projected to the original 3D space using the Ordered Subset Expectation Maximization (OSEM) algorithm. A fine-tuned Medical Segment-Anything Model (MedSAM) was then also used to segment the identified lesions within the bounding boxes.\nResults: The proposed method achieved a high detection performance for this difficult task, with the FreeAnchor model reaching an F1-score of 0.69 and a recall of 0.74. It outperformed several 3D methods in efficiency while maintaining comparable accuracy. Strong recall rates were observed for clinically relevant areas, such as local relapses (0.82) and bone metastases (0.80).\nConclusion: Our fully automated CADe tool shows promise in assisting physicians as a “second reader” for detecting small metastatic BCR PCa lesions on PSMA-PET images. By leveraging the strength and computational efficiency of 2D models while preserving 3D spatial information of the PSMA-PET volume, the proposed approach has the potential to improve detectability and reduce workload in cancer diagnosis and management.",
      "url": "https://www.microsoft.com/en-us/research/publication/computer-aided-detection-cade-of-small-metastatic-prostate-cancer-lesions-on-3d-psma-pet-volumes-using-multi-angle-maximum-intensity-projections/"
    },
    {
      "title": "“I use video calling in all areas of my life”: Understanding the Video Calling Experiences of Chronically Ill People",
      "authors": [
        "Ann Paradiso",
        "Denae Ford",
        "Ed Cutrell",
        "Erin Beneteau",
        "Humphrey Curtis",
        "John Tang",
        "Martez Mott",
        "Sasa Junuzovic"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Human-computer interaction",
        "Medical, health and genomics"
      ],
      "publication_date": "May 2025",
      "abstract": "Since the Covid-19 pandemic, video calling (VC) has become a staple means of daily communication. Beyond socializing, VC in the United States (U.S.) now supports remote work, healthcare and education. The sudden ubiquity of VC could have presented both advantages and challenges for chronically ill people. However, our understanding of chronically ill people’s experiences with VC remains limited. To address this gap, we conducted the largest online survey study (N=55) on chronically ill people’s VC experiences in the U.S.–investigating their routines, facilitators and barriers. Our quantitative and qualitative findings established that chronically ill people heavily depend on VC to cope with everyday life. At the same time, VC can also detrimentally exacerbate cognitive (e.g., brain fog), emotional (e.g., self-consciousness) and physical challenges (e.g., migraines) for chronically ill people. In response, we offer actionable design opportunities to improve the accessibility and experience of VC for chronically ill people.",
      "url": "https://www.microsoft.com/en-us/research/publication/i-use-video-calling-in-all-areas-of-my-life-understanding-the-video-calling-experiences-of-chronically-ill-people/"
    },
    {
      "title": "Learning Identifiable Structures Helps Avoid Bias in DNN-based Supervised Causal Learning",
      "authors": [
        "Dongmei Zhang",
        "Haibing Guan",
        "Huang Bojun",
        "Jiaru Zhang",
        "Qiang Fu",
        "Rui Ding",
        "Shi Han",
        "Yang Hua",
        "Zizhen Deng"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "May 2025",
      "abstract": "Causal discovery is a structured prediction task that aims to predict causal relations among variables based on their data samples. Supervised Causal Learning (SCL) is an emerging paradigm in this field. Existing Deep Neural Network (DNN)-based methods commonly adopt the “Node-Edge approach”, in which the model first computes an embedding vector for each variable-node, then uses these variable-wise representations to concurrently and independently predict for each directed causal-edge. In this paper, we first show that this architecture has some systematic bias that cannot be mitigated regardless of model size and data size. We then propose SiCL, a DNN-based SCL method that predicts a skeleton matrix together with a v-tensor (a third-order tensor representing the v-structures). According to the Markov Equivalence Class (MEC) theory, both the skeleton and the v-structures are identifiable causal structures under the canonical MEC setting, so predictions about skeleton and vstructures do not suffer from the identifiability limit in causal discovery, thus SiCL can avoid the systematic bias in Node-Edge architecture, and enable consistent estimators for causal discovery. Moreover, SiCL is also equipped with a specially designed pairwise encoder module with a unidirectional attention layer to model both internal and external relationships of pairs of nodes. Experimental results on both synthetic and real-world benchmarks show that SiCL significantly outperforms other DNN-based SCL approaches.",
      "url": "https://www.microsoft.com/en-us/research/publication/learning-identifiable-structures-helps-avoid-bias-in-dnn-based-supervised-causal-learning/"
    },
    {
      "title": "Core-Set Selection for Data-efficient Land Cover Segmentation",
      "authors": [
        "Akram Zaytar",
        "Anthony Ortiz",
        "Caleb Robinson",
        "Juan M. Lavista Ferres",
        "Keiller Nogueira",
        "Oktay Karakuş",
        "Paul L Rosin",
        "Rahul Dodhia",
        "Ribana Roscher",
        "Ronny Hänsch",
        "Simone Fobi Nsutezo",
        "Wanli Ma"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "May 2025",
      "abstract": "The increasing accessibility of remotely sensed data and the potential of such data to inform large-scale decision-making has driven the development of deep learning models for many Earth Observation tasks. Traditionally, such models must be trained on large datasets. However, the common assumption that broadly larger datasets lead to better outcomes tends to overlook the complexities of the data distribution, the potential for introducing biases and noise, and the computational resources required for processing and storing vast datasets. Therefore, effective solutions should consider both the quantity and quality of data. In this paper, we propose six novel core-set selection methods for selecting important subsets of samples from remote sensing image segmentation datasets that rely on imagery only, labels only, and a combination of each. We benchmark these approaches against a random-selection baseline on three commonly used land cover classification datasets: DFC2022, Vaihingen, and Potsdam. In each of the datasets, we demonstrate that training on a subset of samples outperforms the random baseline, and some approaches outperform training on all available data. This result shows the importance and potential of data-centric learning for the remote sensing domain. The code is available at https://github.com/keillernogueira/data-centric-rs-classification/.",
      "url": "https://www.microsoft.com/en-us/research/publication/core-set-selection-for-data-efficient-land-cover-segmentation/"
    },
    {
      "title": "Rollbaccine : Herd Immunity against Storage Rollback Attacks in TEEs",
      "authors": [
        "Aditya Balasubramanian",
        "David C. Y. Chu",
        "Dee Bao",
        "Heidi Howard",
        "Lucky E. Katahanas",
        "Natacha Crooks",
        "Soujanya Ponnapalli"
      ],
      "research_areas": [
        "Security, privacy, and cryptography",
        "Systems and networking"
      ],
      "publication_date": "May 2025",
      "abstract": "Today, users can “lift-and-shift” unmodified applications into modern, VM-based Trusted Execution Environments (TEEs) in order to gain hardware-based security guarantees. However, TEEs do not protect applications against disk rollback attacks, where persistent storage can be reverted to an earlier state after a crash; existing rollback resistance solutions either only support a subset of applications or require code modification. Our key insight is that restoring disk consistency after a rollback attack guarantees rollback resistance for any application. We present Rollbaccine, a device mapper that provides automatic rollback resistance for all applications by provably preserving disk consistency. Rollbaccine intercepts and replicates writes to disk, restores lost state from backups during recovery, and minimizes overheads by taking advantage of the weak, multi-threaded semantics of disk operations. Across benchmarks over two real applications (PostgreSQL and HDFS) and two file systems (ext4 and xfs), Rollbaccine adds only 19% overhead, except for the fsync-heavy Filebench Varmail. In addition, Rollbaccine outperforms the state-of-the-art, non-automatic rollback resistant solution by 208x.",
      "url": "https://www.microsoft.com/en-us/research/publication/rollbaccine-herd-immunity-against-storage-rollback-attacks-in-tees/"
    },
    {
      "title": "AI on My Shoulder: Supporting Emotional Labor in Front-Office Roles with an LLM-based Empathetic Coworker",
      "authors": [
        "Jash R. Parekh",
        "Javier Hernandez",
        "Jina Suh",
        "Koustuv Saha",
        "Mary Czerwinski",
        "Qiuyue",
        "Roy Zimmerman",
        "V. D. Swain",
        "Varun Mishra",
        "Yechan Jeon"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Human-computer interaction"
      ],
      "publication_date": "April 2025",
      "abstract": "Client-Service Representatives (CSRs) are vital to organizations. Frequent interactions with disgruntled clients, however, disrupt their mental well-being. To help CSRs regulate their emotions while interacting with uncivil clients, we designed Care-Pilot, an LLM-powered assistant, and evaluated its efficacy, perception, and use. Our comparative analyses between 665 human and Care-Pilot-generated support messages highlight Care-Pilot’s ability to adapt to and demonstrate empathy in various incivility incidents. Additionally, 143 CSRs assessed Care-Pilot’s empathy as more sincere and actionable than human messages. Finally, we interviewed 20 CSRs who interacted with Care-Pilot in a simulation exercise. They reported that Care-Pilot helped them avoid negative thinking, recenter thoughts, and humanize clients; showing potential for bridging gaps in coworker support. Yet, they also noted deployment challenges and emphasized the indispensability of shared experiences. We discuss future designs and societal implications of AI-mediated emotional labor, underscoring empathy as a critical function for AI assistants for worker mental health.",
      "url": "https://www.microsoft.com/en-us/research/publication/ai-on-my-shoulder-supporting-emotional-labor-in-front-office-roles-with-an-llm-based-empathetic-coworker/"
    },
    {
      "title": "AllHands: Ask Me Anything on Large-scale Verbatim Feedback via Large Language Models",
      "authors": [
        "Chaoyun Zhang",
        "Dongmei Zhang",
        "Minghua Ma",
        "Qi Zhang",
        "Qingwei Lin 林庆维",
        "Saravan Rajmohan",
        "Shilin He",
        "Si Qin",
        "Xiaoting Qin",
        "Xiaoyu Gou",
        "Yajie Xue",
        "Yu Kang",
        "Yuhao Wu",
        "Yuyi Liang",
        "Zicheng Ma"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "May 2025",
      "abstract": "Verbatim feedback constitutes a valuable repository of user experiences, opinions, and requirements essential for software development. Effectively and efficiently extracting valuable insights from such data poses a challenging task. This paper introduces Allhands , an innovative analytic framework designed for large-scale feedback analysis through a natural language interface, leveraging large language models (LLMs). Allhands adheres to a conventional feedback analytic workflow, initially conducting classification and topic modeling on the feedback to convert them into a structurally augmented format, incorporating LLMs to enhance accuracy, robustness, generalization, and user-friendliness. Subsequently, an LLM agent is employed to interpret users’ diverse questions in natural language on feedback, translating them into Python code for execution, and delivering comprehensive multi-modal responses, including text, code, tables, and images. We evaluate Allhands across three diverse feedback datasets. The experiments demonstrate that Allhands achieves superior efficacy at all stages of analysis, including classification and topic modeling, eventually providing users with an “ask me anything” experience with comprehensive, correct and human-readable response. To the best of our knowledge, Allhands stands as the first comprehensive feedback analysis framework that supports diverse and customized requirements for insight extraction through a natural language interface.",
      "url": "https://www.microsoft.com/en-us/research/publication/allhands-ask-me-anything-on-large-scale-verbatim-feedback-via-large-language-models/"
    },
    {
      "title": "MicroNova: Folding-based arguments with efficient (on-chain) verification",
      "authors": [
        "Greg Zaverucha",
        "Jiaxing Zhao",
        "Srinath Setty",
        "Weidong Cui"
      ],
      "research_areas": [
        "Security, privacy, and cryptography"
      ],
      "publication_date": "May 2025",
      "abstract": "We describe the design and implementation of MicroNova, a folding-based recursive argument for producing proofs of incremental computations of the form y=F(ℓ)(x), where F is a possibly non-deterministic computation (encoded using a constraint system such as R1CS), x is the initial input, y is the output, and ℓ>0. The proof of an ℓ-step computation is produced step-by-step such that the proof size nor the time to verify it depends on ℓ. The proof at the final iteration is then compressed, to achieve further succinctness in terms of proof size and verification time. Compared to prior folding-based arguments, a distinguishing aspect of MicroNova is the concrete efficiency of the verifier—even in a resource-constrained environment such as Ethereum’s blockchain. In particular, the compressed proof consists of O(log⁡N) group elements and it can be verified with O(log⁡N) group scalar multiplications and two pairing operations, where N is the number of constraints for a single invocation of F. MicroNova requires a universal trusted setup and can employ any existing setup material created for the popular KZG univariate polynomial commitment scheme. Finally, we implement and experimentally evaluate MicroNova. We find that MicroNova’s proofs can be efficiently verified on the Ethereum blockchain with ≈2.2M gas. Furthermore, MicroNova’s prover incurs minimal overheads atop its baseline Nova’s prover.",
      "url": "https://www.microsoft.com/en-us/research/publication/micronova-folding-based-arguments-with-efficient-on-chain-verification/"
    },
    {
      "title": "Robust Optical Transceiver Manipulation in Cluttered Cable Environments Using 3D Scene Understanding and Planning",
      "authors": [
        "Andromachi Chatzieleftheriou",
        "Ant Rowstron",
        "Bohong Weng",
        "Chenyu Liu",
        "David Sweeney",
        "Fabian Otto",
        "Iason Sarantopoulos",
        "Jiaolong Yang",
        "Sicheng Xu",
        "Xin Tong",
        "Yizhong Zhang"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Computer vision",
        "Hardware and devices",
        "Systems and networking"
      ],
      "publication_date": "May 2025",
      "abstract": "Robotic manipulation in cluttered environments presents significant challenges, particularly when the clutter includes thin, deformable objects like cables, which complicate perception and decision-making processes. In the context of datacenters, the automation of networking tasks often involves the manipulation of optical transceivers within densely packed cable configurations. Such environments are characterized by an abundance of delicate, overlapping, and intersecting cables, leading to frequent occlusions. This paper introduces an innovative system designed for the manipulation of optical transceivers in environments cluttered by cables. Our integrated approach combines advanced 3D scene understanding with a heuristic-based pushing policy to effectively manipulate optical transceivers amidst clutter. The system’s perception component utilizes image segmentation and 3D reconstruction to accurately model the transceivers and surrounding cables. Meanwhile, the planning aspect employs a search algorithm with task-specific heuristics, to navigate the gripper, displace obstructing cables, and safely achieve a precise pre-grasp position in front of the target transceiver. We have conducted extensive evaluations of our methodology in both simulated and real-world settings, demonstrating its high success rates, robustness, and proficiency in addressing the unique challenges posed by cable-occluded environments within datacenters.\n",
      "url": "https://www.microsoft.com/en-us/research/publication/transceiver-manipulation-in-clutter/"
    },
    {
      "title": "AI, Help Me Think—but for Myself: Assisting People in Complex Decision-Making by Providing Different Kinds of Cognitive Support",
      "authors": [
        "Elisabeth von Oswald",
        "Leon Reicherts",
        "Mariam Hassib",
        "Yuanting Liu",
        "Yvonne Rogers",
        "Zelun Tony Zhang"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Human-computer interaction",
        "Social sciences"
      ],
      "publication_date": "May 2025",
      "abstract": "How can we design AI tools that effectively support human decision-making by complementing and enhancing users’ reasoning processes? Common recommendation-centric approaches face challenges such as inappropriate reliance or a lack of integration with users’ decision-making processes. Here, we explore an alternative interaction model in which the AI outputs build upon users’ own decision-making rationales. We compare this approach, which we call ExtendAI, with a recommendation-based AI. Participants in our mixed-methods user study interacted with both AIs as part of an investment decision-making task. We found that the AIs had different impacts, with ExtendAI integrating better into the decision-making process and people’s own thinking and leading to slightly better outcomes. RecommendAI was able to provide more novel insights while requiring less cognitive effort. We discuss the implications of these and other findings along with three tensions of AI-assisted decision-making which our study revealed.",
      "url": "https://www.microsoft.com/en-us/research/publication/ai-help-me-think-but-for-myself-assisting-people-in-complex-decision-making-by-providing-different-kinds-of-cognitive-support/"
    },
    {
      "title": "A Scalable Algorithm for Fair Influence Maximization with Unbiased Estimator",
      "authors": [
        "Hao Peng",
        "Philip S. Yu",
        "Wei Chen",
        "Xiaobin Rui",
        "Zhixiao Wang"
      ],
      "research_areas": [
        "Algorithms",
        "Artificial intelligence",
        "Social sciences"
      ],
      "publication_date": "May 2025",
      "abstract": "This paper studies the fair influence maximization problem with efficient algorithms. In particular, given a graph G, a community structure C consisting of disjoint communities, and a budget k, the problem asks to select a seed set S (|S| = k) that maximizes the influence spread while narrowing the influence gap between different communities. This problem derives from some significant social scenarios, such as health interventions (e.g. suicide/HIV prevention) where individuals from racial minorities or LGBTQ communities may be disproportionately excluded from the benefits of the intervention. To depict the concept of fairness in the context of influence maximization, researchers have proposed various notions of fairness, where the welfare fairness notion that better\nbalances fairness level and influence spread has shown promising effectiveness. However, the lack of efficient algorithms for optimizing the objective function under welfare fairness restricts its application to networks of only a few hundred nodes. In this paper, we modify the objective function of welfare fairness to maximize the exponentially weighted sum and the logarithmically weighted sum over all communities’ influenced fractions (utility). To achieve efficient algorithms with theoretical guarantees, we first introduce two unbiased estimators: one for the fractional power of the arithmetic mean and the other for the logarithm of the arithmetic mean. Then, by adapting the Reverse Influence Sampling (RIS) approach, we convert the optimization problem to a weighted maximum coverage problem. We also analyze the number of reverse reachable sets needed to approximate the fair influence at a high probability. Finally, we present an efficient algorithm that guarantees 1 − 1/e − ε (positive objective function) or 1 + 1/e + ε (negative objective function) approximation for any small ε > 0. Experiments demonstrate that our proposed algorithm could efficiently handle large-scale networks with good performance.",
      "url": "https://www.microsoft.com/en-us/research/publication/a-scalable-algorithm-for-fair-influence-maximization-with-unbiased-estimator/"
    },
    {
      "title": "3DiFACE: Synthesizing and Editing Holistic 3D Facial Animation",
      "authors": [
        "Balamurugan Thambiraja",
        "Darren Cosker",
        "Justus Thies",
        "Malte Prinzler",
        "Sadegh Aliakbarian"
      ],
      "research_areas": [
        "Computer vision"
      ],
      "publication_date": "May 2025",
      "abstract": "Creating an animation of a specific person with audio-synced lip motions, realistic head motion and editing via artist-defined keyframes are a set of tasks that challenge existing speech-driven 3D facial animation methods. Especially, editing 3D facial animation is a complex and time-consuming task carried out by highly skilled animators. Also, most existing works overlook the inherent one-to-many relationship between speech and facial motion, where multiple plausible lip and head animations could sync with the audio input. To this end, we present 3DiFACE, a novel method for holistic speech-driven 3D facial animation, which produces diverse plausible lip and head motions for a single audio input, while also allowing editing via keyframing and interpolation. 3DiFACE is a lightweight audio-conditioned diffusion model, which can be fine-tuned to generate personalized 3D facial animation requiring only a short video of the subject. Specifically, we leverage the viseme-level diversity in our training corpus to train a fully-convolutional diffusion model that produces diverse sequences for single audio input. Additionally, we employ a modified guided motion diffusion to enable head-motion synthesis and editing using masking. Through quantitative and qualitative evaluations, we demonstrate that our method is capable of generating and editing diverse holistic 3D facial animations given a single audio input, with control between high fidelity and diversity.",
      "url": "https://www.microsoft.com/en-us/research/publication/3diface-synthesizing-and-editing-holistic-3d-facial-animation/"
    },
    {
      "title": "Data Formulator 2: Iterative Creation of Data Visualizations, with AI Transforming Data Along the Way",
      "authors": [
        "Bongshin Lee",
        "Chenglong Wang",
        "Dan Marshall",
        "Jianfeng Gao",
        "Steven Drucker"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Human-computer interaction"
      ],
      "publication_date": "April 2025",
      "abstract": "Data analysts often need to iterate between data transformations and chart designs to create rich visualizations for exploratory data analysis. Although many AI-powered systems have been introduced to reduce the effort of visualization authoring, existing systems are not well suited for iterative authoring. They typically require analysts to provide, in a single turn, a text-only prompt that fully describe a complex visualization. We introduce Data Formulator 2 (DF2 for short), an AI-powered visualization system designed to overcome this limitation. DF2 blends graphical user interfaces and natural language inputs to enable users to convey their intent more effectively, while delegating data transformation to AI. Furthermore, to support efficient iteration, DF2 lets users navigate their iteration history and reuse previous designs, eliminating the need to start from scratch each time. A user study with eight participants demonstrated that DF2 allowed participants to develop their own iteration styles to complete challenging data exploration sessions.",
      "url": "https://www.microsoft.com/en-us/research/publication/data-formulator-2-iteratively-creating-rich-visualizations-with-ai/"
    },
    {
      "title": "Need Help? Designing Proactive AI Assistants for Programming",
      "authors": [
        "Alan Zhu",
        "Ameet Talwalkar",
        "David Sontag",
        "Hussein Mozannar",
        "Sebastian Zhao",
        "Valerie Chen"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Human-computer interaction"
      ],
      "publication_date": "April 2025",
      "abstract": "While current chat-based AI assistants primarily operate reactively, responding only when prompted by users, there is significant potential for these systems to proactively assist in tasks without explicit invocation, enabling a mixed-initiative interaction. This work explores the design and implementation of proactive AI assistants powered by large language models. We first outline the key design considerations for building effective proactive assistants. As a case study, we propose a proactive chat-based programming assistant that automatically provides suggestions and facilitates their integration into the programmer’s code. The programming context provides a shared workspace enabling the assistant to offer more relevant suggestions. We conducted a randomized experimental study examining the impact of various design elements of the proactive assistant on programmer productivity and user experience. Our findings reveal significant benefits of incorporating proactive chat assistants into coding environments, while also uncovering important nuances that influence their usage and effectiveness.",
      "url": "https://www.microsoft.com/en-us/research/publication/need-help-designing-proactive-ai-assistants-for-programming/"
    },
    {
      "title": "Intent Tagging: Exploring Micro-Prompting Interactions for Supporting Granular Human-GenAI Co-Creation Workflows",
      "authors": [
        "Asta Roseway",
        "David Brown",
        "Frederic Gmeiner",
        "Hugo Romat",
        "Ken Hinckley",
        "Kenneth Holstein",
        "Michael Bentley",
        "Michel Pahud",
        "Nathalie Henry Riche",
        "Nicolai Marquardt",
        "Nikolas Martelaro"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Human-computer interaction"
      ],
      "publication_date": "April 2025",
      "abstract": "Despite Generative AI (GenAI) systems’ potential for enhancing content creation, users often struggle to effectively integrate GenAI into their creative workflows. Core challenges include misalignment of AI-generated content with user intentions (intent elicitation and alignment), user uncertainty around how to best communicate their intents to the AI system (prompt formulation), and insufficient flexibility of AI systems to support diverse creative workflows (workflow flexibility). Motivated by these challenges, we created IntentTagger: a system for slide creation based on the notion of Intent Tags – small, atomic conceptual units that encapsulate user intent – for exploring granular and non-linear micro-prompting interactions for Human-GenAI co-creation workflows. Our user study with 12 participants provides insights into the value of flexibly expressing intent across varying levels of ambiguity, meta-intent elicitation, and the benefits and challenges of intent tag-driven workflows. We conclude by discussing the broader implications of our findings and design considerations for GenAI-supported content creation workflows.",
      "url": "https://www.microsoft.com/en-us/research/publication/intent-tagging-exploring-micro-prompting-interactions-for-supporting-granular-human-genai-co-creation-workflows/"
    },
    {
      "title": "Phi-4-reasoning Technical Report",
      "authors": [
        "Ahmed Awadallah",
        "Arindam Mitra",
        "Besmira Nushi",
        "Caio César Teodoro Mendes",
        "Dimitris Papailiopoulos",
        "Guoqing Zheng",
        "Gustavo de Rosa",
        "Harkirat Behl",
        "Lingjiao Chen (lingjiaochen)",
        "Marah Abdin",
        "Mojan Javaheripi",
        "Neel Joshi",
        "Olli Saarikivi",
        "Piero Kauffmann",
        "Safoora Yousefi",
        "Sahaj Agarwal",
        "Shital Shah",
        "Suriya Gunasekar",
        "Vaishnavi Shrivastava",
        "Vibhav Vineet",
        "Vidhisha Balachandran",
        "Yash Lara",
        "Yue Wu"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "April 2025",
      "abstract": "We introduce Phi-4-reasoning, a 14-billion parameter reasoning model that achieves strong performance on complex reasoning tasks. Trained via supervised fine-tuning of Phi-4 on carefully curated set of “teachable” prompts–selected for the right level of complexity and diversity–and reasoning demonstrations generated using o3-mini, Phi-4-reasoning generates detailed reasoning chains that effectively leverage inference time compute. We further develop Phi-4-reasoning-plus, a variant enhanced through a short phase of outcome-based reinforcement learning that offers higher performance by generating longer reasoning traces. Across a wide range of reasoning tasks, both models outperform significantly larger open-weight models such as DeepSeekR1-Distill-Llama-70B model and approach the performance levels of full DeepSeek R1 model. Our comprehensive evaluations span benchmarks in math and scientific reasoning, coding, algorithmic problem solving, planning, and spatial understanding. Interestingly, we observe a non-trivial transfer of improvements to general-purpose benchmarks as well. In this report, we provide insights into our training data, our training methodologies, and our evaluations. We show that the benefit of careful data curation for supervised fine-tuning (SFT) extends to reasoning language models, and can be further amplified by reinforcement learning (RL). Finally, our evaluation points to opportunities for improving how we assess the performance and robustness of reasoning models.",
      "url": "https://www.microsoft.com/en-us/research/publication/phi-4-reasoning-technical-report/"
    },
    {
      "title": "Sonora: Human-AI Co-Creation of 3D Audio Worlds and its Impact on Anxiety and Cognitive Load",
      "authors": [
        "Andrew D. Wilson",
        "Fernanda De La Torre",
        "Javier Hernandez",
        "Judith Amores"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Human-computer interaction"
      ],
      "publication_date": "April 2025",
      "abstract": "Soundscapes are widely used for relaxation, but their potential for personalized, navigable experiences remains under-explored. To address this, we developed Sonora, an AI tool that enables real-time generation of synthetic, spatialized soundscapes, allowing users to navigate immersive auditory environments and customize soundscapes using voice commands. Sonora’s architecture integrates audio diffusion models and LLMs within Unity. A between-subjects study with 32 participants investigated its effects on anxiety and user experience, compared to a control condition involving passive listening to a soundscape. Participants who interacted with Sonora reported higher entertainment than the control group. A positive correlation was found between state anxiety and user requests for Sonora, suggesting anxious users engaged more. Participants with moderate to high trait anxiety experienced significant reductions in state anxiety across both conditions, with no significant difference in cognitive load. Our findings highlight Sonora’s potential to promote relaxation, emphasizing the value of personalized experiences for mental health.",
      "url": "https://www.microsoft.com/en-us/research/publication/sonora-human-ai-co-creation-of-3d-audio-worlds-and-its-impact-on-anxiety-and-cognitive-load/"
    },
    {
      "title": "CacheBlend: Fast Large Language Model Serving for RAG with Cached Knowledge Fusion",
      "authors": [
        "Hanchen Li",
        "Jiayi Yao",
        "Junchen Jiang",
        "Kuntai Du",
        "Qizheng Zhang",
        "Shan Lu",
        "Siddhant Ray",
        "Yihua Cheng",
        "Yuhan Liu"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Systems and networking"
      ],
      "publication_date": "April 2025",
      "abstract": "Large language models (LLMs) often incorporate multiple text chunks in their inputs to provide the necessary contexts. To speed up the prefill of the long LLM inputs, one can pre-compute the KV cache of a text and re-use the KV cache when the context is reused as the prefix of another LLM input. However, the reused text chunks are not always the input prefix, which makes precomputed KV caches not directly usable since they ignore the text’s cross-attention with the preceding texts. Thus, the benefits of reusing KV caches remain largely unrealized.\nThis paper tackles just one question: when an LLM input contains multiple text chunks, how to quickly combine their precomputed KV caches in order to achieve the same generation quality as the expensive full prefill (i.e., without reusing KV cache)? We present CacheBlend, a scheme that reuses the pre-computed KV caches, regardless prefix or not, and selectively recomputes the KV values of a small subset of tokens to partially update each reused KV cache. In the meantime, the small extra delay for recomputing some tokens can be pipelined with the retrieval of KV caches within the same job, allowing CacheBlend to store KV caches in slower devices with more storage capacity while retrieving them without increasing the inference delay. By comparing CacheBlend with the state-of-the-art KV cache reusing schemes on three open-source LLMs of various sizes and four popular benchmark datasets of different tasks, we show that CacheBlend reduces time-to-first-token (TTFT) by 2.2–3.3× and increases the inference throughput by 2.8-5× from full KV recompute without compromising generation quality.",
      "url": "https://www.microsoft.com/en-us/research/publication/you-only-prefill-once-combining-cached-knowledge-for-large-language-model-serving-with-cacheblend/"
    },
    {
      "title": "FlexCAD: Unified and Versatile Controllable CAD Generation with Fine-tuned Large Language Models",
      "authors": [
        "Deng Cai",
        "Jiang Bian (jiabia)",
        "Shizhao Sun",
        "Wenxiao Wang",
        "Zhanwei Zhang"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Computer vision"
      ],
      "publication_date": "April 2025",
      "abstract": "Recently, there is a growing interest in creating computer-aided design (CAD) models based on user intent, known as controllable CAD generation. Existing work offers limited controllability and needs separate models for different types of control, reducing efficiency and practicality. To achieve controllable generation across all CAD construction hierarchies, such as sketch-extrusion, extrusion, sketch, face, loop and curve, we propose FlexCAD, a unified model by fine-tuning large language models (LLMs). First, to enhance comprehension by LLMs, we represent a CAD model as a structured text by abstracting each hierarchy as a sequence of text tokens. Second, to address various controllable generation tasks in a unified model, we introduce a hierarchy-aware masking strategy. Specifically, during training, we mask a hierarchy-aware field in the CAD text with a mask token. This field, composed of a sequence of tokens, can be set flexibly to represent various hierarchies. Subsequently, we ask LLMs to predict this masked field. During inference, the user intent is converted into a CAD text with a mask token replacing the part the user wants to modify, which is then fed into FlexCAD to generate new CAD models. Comprehensive experiments on public dataset demonstrate the effectiveness of FlexCAD in both generation quality and controllability. Code will be available at https://github.com/microsoft/FlexCAD.",
      "url": "https://www.microsoft.com/en-us/research/publication/flexcad-unified-and-versatile-controllable-cad-generation-with-fine-tuned-large-language-models/"
    },
    {
      "title": "DreamGarden: A Designer Assistant for Growing Games from a Single Prompt",
      "authors": [
        "Andrzej Banburski-Fahey",
        "Sam Earle",
        "Samyak Parajuli"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Human-computer interaction"
      ],
      "publication_date": "April 2025",
      "abstract": "Coding assistants are increasingly leveraged in game design, both generating code and making high-level plans. To what degree can these tools align with developer workflows, and what new modes of human-computer interaction can emerge from their use? We present DreamGarden, an AI system capable of assisting with the development of diverse game environments in Unreal Engine. At the core of our method is an LLM-driven planner, capable of breaking down a single, high-level prompt — a dream, memory, or imagined scenario provided by a human user — into a hierarchical action plan, which is then distributed across specialized submodules facilitating concrete implementation. This system is presented to the user as a garden of plans and actions, both growing independently and responding to user intervention via seed prompts, pruning, and feedback. Through a user study, we explore design implications of this system, charting courses for future work in semi-autonomous assistants and open-ended simulation design.",
      "url": "https://www.microsoft.com/en-us/research/publication/dreamgarden-a-designer-assistant-for-growing-games-from-a-single-prompt/"
    },
    {
      "title": "Effects of LLM-based Search on Decision Making: Speed, Accuracy, and Overreliance",
      "authors": [
        "Daniel G. Goldstein",
        "David Rothschild",
        "Jake Hofman",
        "Sofia Eleni Spatharioti"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Human-computer interaction"
      ],
      "publication_date": "April 2025",
      "abstract": "Recent advances in large language models (LLMs) are transforming online applications, including search tools that accommodate complex natural language queries and provide direct responses. There are, however, concerns about the veracity of LLM-generated content due to potential for LLMs to “hallucinate”. In two online experiments, we examined how LLM-based search affects behavior compared to traditional search and explored ways to reduce overreliance on incorrect LLM-based output. Participants assigned to LLM-based search completed tasks more quickly, with fewer but more complex queries, and reported a more satisfying experience. While decision accuracy was comparable when the LLM was correct, users overrelied on incorrect information when the model erred. In a second experiment, a color-coded highlighting system helped users detect errors, improving decision accuracy without affecting other outcomes. These findings suggest that LLM-based search tools have promise as decision aids but also highlight the importance of effectively communicating uncertainty to mitigate overreliance.",
      "url": "https://www.microsoft.com/en-us/research/publication/effects-of-llm-based-search-on-decision-making-speed-accuracy-and-overreliance/"
    },
    {
      "title": "Canvil: Designerly Adaptation for LLM-Powered User Experiences",
      "authors": [
        "Amy Zhang",
        "David W. McDonald",
        "Jennifer Wortman Vaughan",
        "K. Feng",
        "Q. Vera Liao",
        "Ziang Xiao"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Human-computer interaction"
      ],
      "publication_date": "April 2025",
      "abstract": "Advancements in large language models (LLMs) are sparking a proliferation of LLM-powered user experiences (UX). In product teams, designers often craft UX to meet user needs, but it is unclear how they engage with LLMs as a novel design material. Through a formative study with 12 designers, we find that designers seek a translational process that enables design requirements to shape and be shaped by LLM behavior, motivating a need for designerly adaptation to facilitate this translation. We then built Canvil, a Figma widget that operationalizes designerly adaptation. We used Canvil as a probe to study designerly adaptation in a group-based design study (6 groups, N=17), finding that designers constructively iterated on both adaptation approaches and interface designs to enhance end-user interaction with LLMs. Furthermore, designers identified promising collaborative workflows for designerly adaptation. Our work opens new avenues for processes and tools that foreground designers’ human-centered expertise when developing LLM-powered applications.",
      "url": "https://www.microsoft.com/en-us/research/publication/canvil-designerly-adaptation-for-llm-powered-user-experiences/"
    },
    {
      "title": "Towards Neural Synthesis for SMT-Assisted Proof-Oriented Programming",
      "authors": [
        "Gabriel Ebner",
        "Nikhil Swamy",
        "Saikat Chakraborty",
        "Sakina Fatima",
        "Sarah Fakhoury",
        "Shuvendu Lahiri",
        "Siddharth Bhat"
      ],
      "research_areas": [
        "Programming languages and software engineering"
      ],
      "publication_date": "April 2025",
      "abstract": "Proof-oriented programs mix computational content with proofs of program correctness. However, the human effort involved in programming and proving is still substantial, despite the use of Satisfiability Modulo Theories (SMT) solvers to automate proofs in languages such as F*. Seeking to spur research on using AI to automate the construction of proof-oriented programs, we curate a dataset of 600K lines of open-source F* programs and proofs, including software used in production systems ranging from Windows and Linux to Python and Firefox. Our dataset includes around 32K top-level F* definitions, each representing a type-directed program and proof synthesis problem producing a definition given a formal specification expressed as an F* type. We provide a program fragment checker that queries F* to check the correctness of candidate solutions. We also report on an extended version of our dataset containing a total of 940K lines of programs and proofs, with a total of 54k top-level F* definitions. We believe this is the largest corpus of SMT-assisted program proofs coupled with a reproducible program-fragment checker. Grounded in this dataset, we investigate the use of AI to synthesize programs and their proofs in F*, with promising results. Our main finding in that the performance of fine-tuned smaller language models (such as Phi-2 or StarCoder) compare favorably with large language models (such as GPT-4), at a much lower computational cost. We also identify various type-based retrieval augmentation techniques and find that they boost performance significantly. With detailed error analysis and case studies, we identify potential strengths and weaknesses of models and techniques and suggest directions for future improvements.",
      "url": "https://www.microsoft.com/en-us/research/publication/towards-neural-synthesis-for-smt-assisted-proof-oriented-programming/"
    },
    {
      "title": "AI-Enabled Screening for Retinopathy of Prematurity in Low-Resource Settings",
      "authors": [
        "Anthony Ortiz",
        "Bill Weeks",
        "Brenda Peña",
        "Carlos Serafin",
        "Gabriela Saidman",
        "Guillermo Monteoliva",
        "Jehú Torres",
        "Juan M. Lavista Ferres",
        "Juan Mármol",
        "María Ana Martinez-Castellanos",
        "Rahul Dodhia",
        "Susana Patiño",
        "Vanina Schbib"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Computer vision"
      ],
      "publication_date": "April 2025",
      "abstract": "Importance: Retinopathy of prematurity (ROP) is the leading cause of preventable childhood blindness worldwide. If detected and treated early, ROP-associated blindness is preventable; however, identifying patients who might respond to treatment requires screening over time, which is challenging in low-resource settings where access to pediatric ophthalmologists and pediatric ocular imaging cameras is limited.\nObjective: To develop and assess the performance of a machine learning algorithm that uses smartphone-collected videos to perform retinal screening for ROP in low-resource settings.\nDesign, Setting, and Participants: This diagnostic study used smartphone-obtained videos of fundi in premature neonates with and without ROP in Mexico and Argentina between May 12, 2020, and October 31, 2023. Machine-learning (ML)–driven algorithms were developed to process a video, identify the best frames within the video, and use those frames to determine whether ROP was likely or not. Eligible neonates born with gestational age less than 36 weeks or birth weight less than 1500 g were included on the study.\nExposures: An ML algorithm applied to a smartphone-obtained video.\nMain Outcomes and Measures: The ML algorithms’ ability to identify high-quality retinal images and classify those images as indicating ROP or not at the frame and patient levels, measured by accuracy, specificity, and sensitivity, compared with classifications from 3 pediatric ophthalmologists.\nResults: A total of 524 videos were collected for 512 neonates with median gestational age of 32 weeks (range, 25-36 weeks) and median birth weight of 1610 g (range, 580-2800 g). The frame selection model identified high-quality retinal images from 397 of 456 videos (87.1%; 95% CI, 84.0%-90.1%) reserved for testing model performance. Across all test videos, 97.4% (95% CI, 96.7%-98.1%) of high-quality retinal images selected by the model contained fundus images. At the frame level, the ROP classifier model had a sensitivity of 76.7% (95% CI, 69.9%-83.5%); at the patient level, the classifier model had a sensitivity of 93.3% (95% CI, 86.4%-100%). At both levels, the model’s sensitivity was higher than that for the panel of pediatric ophthalmologists (frame level: 71.4% [95% CI, 64.1%-78.7%]; patient level: 73.3% [95% CI, 61.0%-85.6%]). Specificity and accuracy were higher for ophthalmologist classification vs the ML model.\nConclusions and Relevance: In this diagnostic study, a process that used smartphone-collected videos of premature neonates’ fundi to determine whether high-quality retinal images were present had high sensitivity to classify such images as indicating or not indicating ROP but lower specificity and accuracy than ophthalmologist assessment. This process costs a fraction of the current process for retinal image collection and classification and could be used to expand access to ROP screening in low-resource settings, with potential to help prevent the most common cause of preventable childhood blindness.",
      "url": "https://www.microsoft.com/en-us/research/publication/ai-enabled-screening-for-retinopathy-of-prematurity-in-low-resource-settings/"
    },
    {
      "title": "Social by Nature: How Socio-tecture Shapes the Work of SMBs and Considerations for Reimagining Collaborative Human-AI Systems",
      "authors": [
        "Elizabeth Ankrah",
        "Gillian R Hayes",
        "Jacki O'Neill",
        "Kagonya Awori",
        "Mark Kariuki",
        "Mercy Muchai",
        "Millicent Ochieng",
        "Stephanie Nyairo"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Human-computer interaction"
      ],
      "publication_date": "April 2025",
      "abstract": "Globally, small and medium-sized businesses (SMBs) have had to adapt to rapid digital changes, a shift accelerated by the COVID-19 pandemic. In Kenya, this transition has involved a significant move towards digital management tools. While many had already experienced marked digitalization over the last few decades, they completed this work differently from their European and North American counterparts. This study explores how Kenyan SMBs continue to navigate these changes and considers the potential of Generative AI in this context. Applying the concept of socio-tecture—which emphasizes social networks, relational business practices, and employees as knowledge producers—we analyze how these elements influence SMB operations in Nairobi. We highlight how socio-tecture affects business performance and growth, and discuss how an Afro-centric strengths-based approach might offer unique opportunities and challenges with the influx of new technologies like Generative AI.",
      "url": "https://www.microsoft.com/en-us/research/publication/social-by-nature-how-socio-tecture-shapes-the-work-of-smbs-and-considerations-for-reimagining-collaborative-human-ai-systems/"
    },
    {
      "title": "Reflecting on Design Paradigms of Animated Data Video Tools",
      "authors": [
        "Haotian Li",
        "Huamin Qu",
        "Leixian Shen",
        "Yun Wang"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Human-computer interaction"
      ],
      "publication_date": "April 2025",
      "abstract": "Animated data videos have gained significant popularity in recent years. However, authoring data videos remains challenging due to the complexity of creating and coordinating diverse components (e.g., visualization, animation, audio, etc.). Although numerous tools have been developed to streamline the process, there is a lack of comprehensive understanding and reflection of their design paradigms to inform future development. To address this gap, we propose a framework for understanding data video creation tools along two dimensions: what data video components to create and coordinate, including visual, motion, narrative, and audio components, and how to support the creation and coordination. By applying the framework to analyze 46 existing tools, we summarized key design paradigms of creating and coordinating each component based on the varying work distribution for humans and AI in these tools. Finally, we share our detailed reflections, highlight gaps from a holistic view, and discuss future directions to address them.",
      "url": "https://www.microsoft.com/en-us/research/publication/reflecting-on-design-paradigms-of-animated-data-video-tools/"
    },
    {
      "title": "ProductMeta: An Interactive System for Metaphorical Product Design Ideation with Multimodal Large Language Models",
      "authors": [
        "Jie Deng",
        "Qinyi Zhou",
        "Sai Ma",
        "Scarlett Li",
        "Yan Xia",
        "Yang Ou",
        "Yingqing Xu",
        "Yu Liu",
        "Yun Wang",
        "Zhicong Lu"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Human-computer interaction"
      ],
      "publication_date": "April 2025",
      "abstract": "Product metaphors, which involve creating products that convey meaning through metaphorical associations, are a powerful tool in product design. However, according to our formative study, novice designers often struggle to establish coherent links between target and source, to manage the complexity of diverse mapping possibilities and to balance product usability with metaphorical expression. To address these challenges, we introduce ProductMeta, a creativity support tool designed to support novice designers in exploring and developing metaphorical product designs. ProductMeta incorporates domain knowledge and decomposes the design process into iterative modules and framework-based interfaces, fostering both divergent and convergent thinking. Through user studies, we demonstrate that ProductMeta enables novice designers to generate diverse and contextually relevant design ideas by facilitating structured exploration. We conclude with design implications for human-AI co-creation.",
      "url": "https://www.microsoft.com/en-us/research/publication/productmeta-an-interactive-system-for-metaphorical-product-design-ideation-with-multimodal-large-language-models/"
    },
    {
      "title": "InterLink: Linking Text with Code and Output in Computational Notebooks",
      "authors": [
        "Dominik Moritz",
        "Haotian Li",
        "Huamin Qu",
        "Leni Yang",
        "Yanna Lin"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Human-computer interaction"
      ],
      "publication_date": "April 2025",
      "abstract": "Computational notebooks, widely used for ad-hoc analysis and often shared with others, can be difficult to understand because the standard linear layout is not optimized for reading. In particular, related text, code, and outputs may be spread across the UI making it difficult to draw connections. In response, we introduce InterLink, a plugin designed to present the relationships between text, code, and outputs, thereby making notebooks easier to understand. In a formative study, we identify pain points and derive design requirements for identifying and navigating relationships among various pieces of information within notebooks. Based on these requirements, InterLink features a new layout that separates text from code and outputs into two columns. It uses visual links to signal relationships between text and associated code and outputs and offers interactions for navigating related pieces of information. In a user study with 12 participants, those using InterLink were 13.6% more accurate at finding and integrating information from complex analyses in computational notebooks. These results show the potential of notebook layouts that make them easier to understand.",
      "url": "https://www.microsoft.com/en-us/research/publication/interlink-linking-text-with-code-and-output-in-computational-notebooks/"
    },
    {
      "title": "Iterative Self-Tuning LLMs for Enhanced Jailbreaking Capabilities",
      "authors": [
        "Aidan San",
        "Chung-En Sun",
        "Hao Cheng",
        "Jianfeng Gao",
        "Michel Galley",
        "Tsui-Wei Weng",
        "Weiwei Yang",
        "Xiaodong Liu"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Security, privacy, and cryptography"
      ],
      "publication_date": "April 2025",
      "abstract": "Recent research has shown that Large Language Models (LLMs) are vulnerable to automated jailbreak attacks, where adversarial suffixes crafted by algorithms appended to harmful queries bypass safety alignment and trigger unintended responses. Current methods for generating these suffixes are computationally expensive and have low Attack Success Rates (ASR), especially against well-aligned models like Llama2 and Llama3. To overcome these limitations, we introduce ADV-LLM, an iterative self-tuning process that crafts adversarial LLMs with enhanced jailbreak ability. Our framework significantly reduces the computational cost of generating adversarial suffixes while achieving nearly 100% ASR on various open-source LLMs. Moreover, it exhibits strong attack transferability to closed-source models, achieving 99% ASR on GPT-3.5 and 49% ASR on GPT-4, despite being optimized solely on Llama3. Beyond improving jailbreak ability, ADV-LLM provides valuable insights for future safety alignment research through its ability to generate large datasets for studying LLM safety.\n",
      "url": "https://www.microsoft.com/en-us/research/publication/iterative-self-tuning-llms-for-enhanced-jailbreaking-capabilities/"
    },
    {
      "title": "TAPAS: Thermal- and Power-Aware Scheduling for LLM Inference in Cloud Platforms",
      "authors": [
        "Chaojie Zhang",
        "Esha Choukse",
        "Haoran Qiu",
        "Josep Torrellas",
        "Jovan Stojkovic",
        "Ricardo Bianchini",
        "Rodrigo Fonseca",
        "Íñigo Goiri"
      ],
      "research_areas": [
        "Systems and networking"
      ],
      "publication_date": "April 2025",
      "abstract": "The rising demand for generative large language models (LLMs) poses challenges for thermal and power management in cloud datacenters. Traditional techniques often are inadequate for LLM inference due to the fine-grained, millisecond-scale execution phases, each with distinct performance, thermal, and power profiles. Additionally, LLM inference workloads are sensitive to various configuration parameters (e.g., model parallelism, size, and quantization) that involve trade-offs between performance, temperature, power, and output quality. Moreover, clouds often co-locate SaaS and IaaS workloads, each with different levels of visibility and flexibility. We propose TAPAS, a thermal- and power-aware framework designed for LLM inference clusters in the cloud. TAPAS enhances cooling and power oversubscription capabilities, reducing the total cost of ownership (TCO) while effectively handling emergencies (e.g., cooling and power failures). The system leverages historical temperature and power data, along with the adaptability of SaaS workloads, to: (1) efficiently place new GPU workload VMs within cooling and power constraints, (2) route LLM inference requests across SaaS VMs, and (3) reconfigure SaaS VMs to manage load spikes and emergency situations. Our evaluation on a large GPU cluster demonstrates significant reductions in thermal and power throttling events, boosting system efficiency.",
      "url": "https://www.microsoft.com/en-us/research/publication/tapas-thermal-and-power-aware-scheduling-for-llm-inference-in-cloud-platforms/"
    },
    {
      "title": "As Confidence Aligns: Exploring the Effect of AI Confidence on Human Self-confidence in Human-AI Decision Making",
      "authors": [
        "Jingshu Li",
        "Junti Zhang",
        "Q. Vera Liao",
        "Yi-chieh Lee",
        "Yitian Yang"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Human-computer interaction"
      ],
      "publication_date": "April 2025",
      "abstract": "Complementary collaboration between humans and AI is essential for human-AI decision making. One feasible approach to achieving it involves accounting for the calibrated confidence levels of both AI and users. However, this process would likely be made more difficult by the fact that AI confidence may influence users’ self-confidence and its calibration. To explore these dynamics, we conducted a randomized behavioral experiment. Our results indicate that in human-AI decision-making, users’ self-confidence aligns with AI confidence and such alignment can persist even after AI ceases to be involved. This alignment then affects users’ self-confidence calibration. We also found the presence of real-time correctness feedback of decisions reduced the degree of alignment. These findings suggest that users’ self-confidence is not independent of AI confidence, which practitioners aiming to achieve better human-AI collaboration need to be aware of. We call for research focusing on the alignment of human cognition and behavior with AI.",
      "url": "https://www.microsoft.com/en-us/research/publication/as-confidence-aligns-exploring-the-effect-of-ai-confidence-on-human-self-confidence-in-human-ai-decision-making/"
    },
    {
      "title": "ASHABot: An LLM-Powered Chatbot to Support the Informational Needs of Community Health Workers",
      "authors": [
        "Aditya Vashistha",
        "Bhuvan Sachdeva",
        "Hamid Abdullah",
        "Mahendra Meena",
        "Mehak Chhokar",
        "Mohit Jain",
        "Pragnya Ramjee",
        "Ruchit Nagar"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Human-computer interaction",
        "Medical, health and genomics"
      ],
      "publication_date": "April 2025",
      "abstract": "Community health workers (CHWs) provide last-mile healthcare services but face challenges due to limited medical knowledge and training. This paper describes the design, deployment, and evaluation of ASHABot, an LLM-powered, experts-in-the-loop, WhatsApp-based chatbot to address the information needs of CHWs in India. Through interviews with CHWs and their supervisors and log analysis, we examine factors affecting their engagement with ASHABot, and ASHABot’s role in addressing CHWs’ informational needs. We found that ASHABot provided a private channel for CHWs to ask rudimentary and sensitive questions they hesitated to ask supervisors. CHWs trusted the information they received on ASHABot and treated it as an authoritative resource. CHWs’ supervisors expanded their knowledge by contributing answers to questions ASHABot failed to answer, but were concerned about demands on their workload and increased accountability. We emphasize positioning LLMs as supplemental fallible resources within the community healthcare ecosystem, instead of as replacements for supervisor support.",
      "url": "https://www.microsoft.com/en-us/research/publication/ashabot-an-llm-powered-chatbot-to-support-the-informational-needs-of-community-health-workers/"
    },
    {
      "title": "Exploring Early Adopters’ Use of AI Driven Multi-Agent Systems to Inform Human-Agent Interaction Design: Insights from Industry Practice",
      "authors": [
        "Amanda K. Hall",
        "Amanda Snellinger",
        "Austin L. Toombs",
        "Scott Saponas",
        "Suchismita Naik"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Human-computer interaction"
      ],
      "publication_date": "April 2025",
      "abstract": "This case study explores the experiences of Microsoft employees, who are early adopters of multi-agent generative AI systems, as they experiment with these technologies to design, test, and deploy new tools attempting to bridge the gap between existing Microsoft products and emerging AI capabilities. Thirteen developers and creators participated in 60-minute semi-structured interviews to elicit their challenges, use cases, and lessons learned from their experimentation with multi-agent AI frameworks. A thematic qualitative analysis process was conducted to analyze the interview data. Participants reported building multi-agent AI tools to address tasks in team collaboration, productivity, customer support, creative processes, and security. Strategies for managing complexity, enhancing transparency, and balancing agent autonomy with human oversight were found to be important human-agent interaction design considerations. Findings from this study highlight the capabilities and limitations of specialized multi-agents within the contexts of participants’ use cases and provide insights to inform the human-agent interaction design of future multi-agent generative AI systems.",
      "url": "https://www.microsoft.com/en-us/research/publication/exploring-early-adopters-use-of-ai-driven-multi-agent-systems-to-inform-human-agent-interaction-design-insights-from-industry-practice/"
    },
    {
      "title": "Local Prompt Optimization",
      "authors": [
        "Vishal Chowdhary",
        "Yash Jain"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Programming languages and software engineering",
        "Technology for emerging markets"
      ],
      "publication_date": "April 2025",
      "abstract": "In recent years, the use of prompts to guide the output of Large Language Models have increased dramatically. However, even the best of experts struggle to choose the correct words to stitch up a prompt for the desired task. To solve this, LLM driven prompt optimization emerged as an important problem. Existing prompt optimization methods optimize a prompt globally, where in all the prompt tokens have to be optimized over a large vocabulary while solving a complex task. The large optimization space (tokens) leads to insufficient guidance for a better prompt. In this work, we introduce Local Prompt Optimization (LPO) that integrates with any general automatic prompt engineering method. We identify the optimization tokens in a prompt and nudge the LLM to focus only on those tokens in its optimization step. We observe remarkable performance improvements on Math Reasoning (GSM8k and MultiArith) and BIG-bench Hard benchmarks across various automatic prompt engineering methods. Further, we show that LPO converges to the optimal prompt faster than global methods.",
      "url": "https://www.microsoft.com/en-us/research/publication/local-prompt-optimization/"
    },
    {
      "title": "Interactive Debugging and Steering of Multi-Agent AI Systems",
      "authors": [
        "Adam Fourney (adamfo)",
        "Erkang (Eric) Zhu",
        "Gagan Bansal",
        "Jack Gerrits",
        "Saleema Amershi",
        "Victor Dibia",
        "Will Epperson"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Human-computer interaction"
      ],
      "publication_date": "April 2025",
      "abstract": "Fully autonomous teams of LLM-powered AI agents are emerging that collaborate to perform complex tasks for users. What challenges do developers face when trying to build and debug these AI agent teams? In formative interviews with five AI agent developers, we identify core challenges: difficulty reviewing long agent conversations to localize errors, lack of support in current tools for interactive debugging, and the need for tool support to iterate on agent configuration. Based on these needs, we developed an interactive multi-agent debugging tool, AGDebugger, with a UI for browsing and sending messages, the ability to edit and reset prior agent messages, and an overview visualization for navigating complex message histories. In a two-part user study with 14 participants, we identify common user strategies for steering agents and highlight the importance of interactive message resets for debugging. Our studies deepen understanding of interfaces for debugging increasingly important agentic workflows.",
      "url": "https://www.microsoft.com/en-us/research/publication/interactive-debugging-and-steering-of-multi-agent-ai-systems/"
    },
    {
      "title": "Collective Meaning Cascades but Strange Ducks Swim Upstream: Facilitating Collective Meaning-making through Co-development of AI Models",
      "authors": [
        "Aaron L Halfaker",
        "Ciell Brusse",
        "Haiyi Zhu",
        "Kenneth Holstein",
        "Tzu-Sheng Kuo"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Human-computer interaction"
      ],
      "publication_date": "April 2025",
      "abstract": "Communities of practice operate by developing, sharing, and formalizing concepts together — collective meaning-making — thereby enabling all their community members to work together effectively. In the context of Wikipedia, these concepts include article quality, vandalism, and other subjective aspects of collective work. AI and machine learning have proven to be powerful tools for facilitating collaboration at scale by modeling and applying shared concepts. We examine meaning-making in parallel with aligning AI behavior. We describe a case study of modeling the quality of articles in Dutch Wikipedia using an AI model, while engaging in a meaning-making process with Dutch Wikipedians. This case blurs the line between social governance and how meaning is reshaped in an AI model. Based on the case study, we present the Collective Meaning Cycle, a framework that describes the bidirectional relationship between modeling and meaning-making. We also provide implications for the practice of participatory AI design.",
      "url": "https://www.microsoft.com/en-us/research/publication/collective-meaning-cascades-but-strange-ducks-swim-upstream-facilitating-collective-meaning-making-through-co-development-of-ai-models/"
    },
    {
      "title": "AI-Instruments: Embodying Prompts as Instruments to Abstract & Reflect Graphical Interface Commands as General-Purpose Tools",
      "authors": [
        "Anna Offenwanger",
        "David Brown",
        "Frederic Gmeiner",
        "Hugo Romat",
        "Ken Hinckley",
        "Kori Inkpen",
        "Michel Pahud",
        "Nathalie Henry Riche",
        "Nicolai Marquardt"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Human-computer interaction"
      ],
      "publication_date": "April 2025",
      "abstract": "Chat-based prompts respond with verbose linear-sequential texts, making it difficult to explore and refine ambiguous intents, back up and reinterpret, or shift directions in creative AI-assisted design work. AI-Instruments instead embody”prompts”as interface objects via three key principles: (1) Reification of user-intent as reusable direct-manipulation instruments; (2) Reflection of multiple interpretations of ambiguous user-intents (Reflection-in-intent) as well as the range of AI-model responses (Reflection-in-response) to inform design”moves”towards a desired result; and (3) Grounding to instantiate an instrument from an example, result, or extrapolation directly from another instrument. Further, AI-Instruments leverage LLM’s to suggest, vary, and refine new instruments, enabling a system that goes beyond hard-coded functionality by generating its own instrumental controls from content. We demonstrate four technology probes, applied to image generation, and qualitative insights from twelve participants, showing how AI-Instruments address challenges of intent formulation, steering via direct manipulation, and non-linear iterative workflows to reflect and resolve ambiguous intents.",
      "url": "https://www.microsoft.com/en-us/research/publication/ai-instruments-embodying-prompts-as-instruments-to-abstract-reflect-graphical-interface-commands-as-general-purpose-tools/"
    },
    {
      "title": "3DGen: AI-Assisted Generation of Provably Correct Binary Format Parsers",
      "authors": [
        "Markus Kuppe",
        "Nikhil Swamy",
        "Sarah Fakhoury",
        "Shuvendu Lahiri",
        "Tahina Ramananandro"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Programming languages and software engineering"
      ],
      "publication_date": "April 2025",
      "abstract": "Improper parsing of attacker-controlled input is a leading source of software security vulnerabilities, especially when programmers transcribe informal format descriptions in RFCs into efficient parsing logic in low-level, memory unsafe languages. Several researchers have proposed formal specification languages for data formats from which efficient code can be extracted. However, distilling informal requirements into formal specifications is challenging and, despite their benefits, new, formal languages are hard for people to learn and use. In this work, we present 3DGen, a framework that makes use of AI agents to transform mixed informal input, including natural language documents (i.e., RFCs) and example inputs into format specifications in a language called 3D. To support humans in understanding and trusting the generated specifications, 3DGen uses symbolic methods to also synthesize test inputs that can be validated against an external oracle. Symbolic test generation also helps in distinguishing multiple plausible solutions. Through a process of repeated refinement, 3DGen produces a 3D specification that conforms to a test suite, and which yields safe, efficient, provably correct, parsing code in C. We have evaluated 3DGen on 20 Internet standard formats, demonstrating the potential for AI-agents to produce formally verified C code at a non-trivial scale. A key enabler is the use of a domain-specific language to limit AI outputs to a class for which automated, symbolic analysis is tractable.",
      "url": "https://www.microsoft.com/en-us/research/publication/3dgen-ai-assisted-generation-of-provably-correct-binary-format-parsers/"
    },
    {
      "title": "MotionBlocks: Modular Geometric Motion Remapping for More Accessible Upper Body Movement in Virtual Reality",
      "authors": [
        "Alessandra Luz",
        "Daniel Vogel",
        "Johann Wentzel",
        "Martez Mott"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Human-computer interaction"
      ],
      "publication_date": "April 2025",
      "abstract": "Movement-based spatial interaction in VR can present significant challenges for people with limited mobility, particularly due to the mismatch between the upper body motion a VR app requires and the user’s capabilities. We describe MotionBlocks, an approach which enables 3D spatial input with smaller motions or simpler input devices using modular geometric motion remapping. A formative study identifies common accessibility issues within VR motion design, and informs a design language of VR motions that fall within simple geometric primitives. These 3D primitives enable collapsing spatial or non-spatial input into a normalized input vector, which is then expanded into a second 3D primitive representing larger, more complex 3D motions. An evaluation with people with mobility limitations found that using geometric primitives for highly customized upper body input remapping reduced physical workload, temporal workload, and perceived effort.",
      "url": "https://www.microsoft.com/en-us/research/publication/motionblocks-modular-geometric-motion-remapping-for-more-accessible-upper-body-movement-in-virtual-reality/"
    },
    {
      "title": "A Taxonomy of Linguistic Expressions That Contribute To Anthropomorphism of Language Technologies",
      "authors": [
        "Alexandra Olteanu",
        "Alicia DeVrio",
        "Lisa Egede",
        "Myra Cheng",
        "Su Lin Blodgett"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Human-computer interaction"
      ],
      "publication_date": "April 2025",
      "abstract": "Recent attention to anthropomorphism — the attribution of human-like qualities to non-human objects or entities — of language technologies like LLMs has sparked renewed discussions about potential negative impacts of anthropomorphism. To productively discuss the impacts of this anthropomorphism and in what contexts it is appropriate, we need a shared vocabulary for the vast variety of ways that language can be anthropomorphic. In this work, we draw on existing literature and analyze empirical cases of user interactions with language technologies to develop a taxonomy of textual expressions that can contribute to anthropomorphism. We highlight challenges and tensions involved in understanding linguistic anthropomorphism, such as how all language is fundamentally human and how efforts to characterize and shift perceptions of humanness in machines can also dehumanize certain humans. We discuss ways that our taxonomy supports more precise and effective discussions of and decisions about anthropomorphism of language technologies.",
      "url": "https://www.microsoft.com/en-us/research/publication/a-taxonomy-of-linguistic-expressions-that-contribute-to-anthropomorphism-of-language-technologies/"
    },
    {
      "title": "Fostering Appropriate Reliance on Large Language Models: The Role of Explanations, Sources, and Inconsistencies",
      "authors": [
        "Jennifer Wortman Vaughan",
        "Olga Russakovsky",
        "Q. Vera Liao",
        "Sunnie S. Y. Kim",
        "Tania Lombrozo"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Human-computer interaction"
      ],
      "publication_date": "April 2025",
      "abstract": "Large language models (LLMs) can produce erroneous responses that sound fluent and convincing, raising the risk that users will rely on these responses as if they were correct. Mitigating such overreliance is a key challenge. Through a think-aloud study in which participants use an LLM-infused application to answer objective questions, we identify several features of LLM responses that shape users’ reliance: explanations (supporting details for answers), inconsistencies in explanations, and sources. Through a large-scale, pre-registered, controlled experiment (N=308), we isolate and study the effects of these features on users’ reliance, accuracy, and other measures. We find that the presence of explanations increases reliance on both correct and incorrect responses. However, we observe less reliance on incorrect responses when sources are provided or when explanations exhibit inconsistencies. We discuss the implications of these findings for fostering appropriate reliance on LLMs.",
      "url": "https://www.microsoft.com/en-us/research/publication/fostering-appropriate-reliance-on-large-language-models-the-role-of-explanations-sources-and-inconsistencies/"
    },
    {
      "title": "Towards Energy Efficient 5G vRAN Servers",
      "authors": [
        "Anuj Kalia",
        "Bozidar Radunovic",
        "Francis Y. Yan",
        "Leyang Xue",
        "Nikita Lazarev",
        "Xenofon Foukas"
      ],
      "research_areas": [
        "Systems and networking"
      ],
      "publication_date": "April 2025",
      "abstract": "We study the problem of improving energy efficiency in virtualized radio access network (vRAN) servers, focusing on CPUs. Two distinct characteristics of vRAN software—strict real-time sub-millisecond deadlines and its proprietary black-box nature—preclude the use of existing general-purpose CPU energy management techniques. This paper presents RENC, a system that saves energy by adjusting CPU frequency in response to sub-second variations in cellular workloads, using the following techniques. First, despite large fluctuations in vRAN CPU load at sub-ms timescales, RENC establishes safe low-load intervals, e.g., by coupling Media Access Control (MAC) layer rate limiting with CPU frequency changes. This prevents high traffic during low-power operation, which would otherwise cause deadline misses. Second, we design techniques to compute CPU frequencies that are safe for these low-load intervals, achieved by measuring the slack in vRAN threads’ deadlines using Linux eBPF hooks, or minor binary rewriting of the vRAN software. Third, we demonstrate the need to handle CPU load spikes triggered by control operations, such as new users attaching to the network. Our evaluation in a state-of-the-art vRAN testbed shows that our techniques reduces a vRAN server’s CPU power consumption by up to 45% (29% server-wide).",
      "url": "https://www.microsoft.com/en-us/research/publication/towards-energy-efficient-5g-vran-servers/"
    },
    {
      "title": "TALES: Text Adventure Learning Environment Suite",
      "authors": [
        "Christopher Zhang Cui",
        "Marc-Alexandre Côté",
        "Prithviraj Ammanabrolu",
        "Xingdi Yuan",
        "Zhang Xiao"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "April 2025",
      "abstract": "Reasoning is an essential skill to enable Large Language Models (LLMs) to interact with the world. As tasks become more complex, they demand increasingly sophisticated and diverse reasoning capabilities for sequential decision-making, requiring structured reasoning over the context history to determine the next best action. We introduce TALES, a diverse collection of synthetic and human-written text-adventure games designed to challenge and evaluate diverse reasoning capabilities. We present results over a range of LLMs, open- and closed-weights, performing a qualitative analysis on the top performing models. Despite an impressive showing on synthetic games, even the top LLM-driven agents fail to achieve 15% on games designed for human enjoyment. Code and visualization of the experiments can be found at https://microsoft.github.io/tale-suite.",
      "url": "https://www.microsoft.com/en-us/research/publication/tales-text-adventure-learning-environment-suite/"
    },
    {
      "title": "Time Warp: The Gap Between Developers’ Ideal vs Actual Workweeks in an AI-Driven Era",
      "authors": [
        "B. Ashok",
        "Brian Houck",
        "Chetan Bansal",
        "Drishti Goel",
        "Sukrit Kumar",
        "Tom Zimmermann"
      ],
      "research_areas": [
        "Programming languages and software engineering"
      ],
      "publication_date": "April 2025",
      "abstract": "Software developers balance a variety of different tasks in a workweek, yet the allocation of time often differs from what they consider ideal. Identifying and addressing these deviations is crucial for organizations aiming to enhance the productivity and well-being of the developers. In this paper, we present the findings from a survey of 484 software developers at Microsoft, which aims to identify the key differences between how developers would like to allocate their time during an ideal workweek versus their actual workweek. Our analysis reveals significant deviations between a developer’s ideal workweek and their actual workweek, with a clear correlation: as the gap between these two workweeks widens, we observe a decline in both productivity and satisfaction. By examining these deviations in specific activities, we assess their direct impact on the developers’ satisfaction and productivity. Additionally, given the growing adoption of AI tools in software engineering, both in the industry and academia, we identify specific tasks and areas that could be strong candidates for automation. In this paper, we make three key contributions: 1) We quantify the impact of workweek deviations on developer productivity and satisfaction 2) We identify individual tasks that disproportionately affect satisfaction and productivity. 3) We provide actual data-driven insights to guide future AI automation efforts in software engineering, aligning them with the developers’ requirements and ideal workflows for maximizing their productivity and satisfaction.",
      "url": "https://www.microsoft.com/en-us/research/publication/time-warp-the-gap-between-developers-ideal-vs-actual-workweeks-in-an-ai-driven-era/"
    },
    {
      "title": "Enabling Silent Telemetry Data Transmission with InvisiFlow",
      "authors": [
        "Gianni Antichi",
        "Liangcheng Yu",
        "Ran Ben Basat",
        "Vincent Liu",
        "Yinda Zhang"
      ],
      "research_areas": [
        "Systems and networking"
      ],
      "publication_date": "April 2025",
      "abstract": "Network applications from traffic engineering to path tracing often rely on the ability to transmit fine-grained telemetry data from network devices to a set of collectors. Unfortunately, prior work has observed—and we validate—that existing transmission methods for such data can result in significant overhead to user traffic and/or loss of telemetry data, particularly when the network is heavily loaded.\nIn this paper, we introduce InvisiFlow, a novel communication substrate for collecting network telemetry data, silently. In contrast to previous systems that always push telemetry packets to collectors based on the shortest path, InvisiFlow dynamically seeks out spare network capacity by leveraging opportunistic sending and congestion gradients, thus minimizing both the loss rate of telemetry data and overheads on user traffic. In a FatTree topology, InvisiFlow can achieve near-zero loss rate even under high-load scenarios (around 33.8x lower loss compared to the state-of-the-art transmission methods used by systems like Everflow and Planck).",
      "url": "https://www.microsoft.com/en-us/research/publication/enabling-silent-telemetry-data-transmission-with-invisiflow/"
    },
    {
      "title": "Efficient Multi-WAN Transport for 5G with OTTER",
      "authors": [
        "Gerry Wan",
        "Mary Hogan",
        "Rachee Singh",
        "Ryan Beckett",
        "Sharad Agarwal",
        "Victor Bahl",
        "Yiming Qiu"
      ],
      "research_areas": [
        "Systems and networking"
      ],
      "publication_date": "April 2025",
      "abstract": "In the ongoing cloudiﬁcation of 5G, software network functions (NFs) are replacing ﬁxed-function network hardware, allowing 5G network operators to leverage the beneﬁts of cloud computing. The migration of NFs and their management to the cloud causes 5G trafﬁc to traverse an operator’s wide-area network (WAN) to the cloud WAN that hosts the datacenters (DCs) running 5G NFs and applications. However, achieving end-to-end (E2E) performance for 5G trafﬁc across two WANs is hard. Placing 5G ﬂows across two WANs with different performance and reliability characteristics, edge and DC resource constraints, and interference from other ﬂows is different and more challenging than single-WAN trafﬁc engineering. We address this challenge and show that orchestrating E2E paths across a multi-WAN overlay allows us to achieve average 13% more throughput, 15% less RTT, 45% less jitter, or reduce average loss from 0.06% to under 0.001%. We implement our multi-WAN 5G ﬂow placement in a scalable optimization prototype that allocates 26%–45% more bytes on the network than greedy baselines while also satisfying the service demands of more ﬂows.\n\n\n\n\n\n",
      "url": "https://www.microsoft.com/en-us/research/publication/efficient-multi-wan-transport-for-5g-with-otter/"
    },
    {
      "title": "Towards Flood Extent Forecasting: Evaluating a Weather Foundation Model and U-Net for Flood Forecasting",
      "authors": [
        "Eric Wanjau",
        "Samuel Chege Maina"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Technology for emerging markets"
      ],
      "publication_date": "April 2025",
      "abstract": "This study explores a data-driven approach that combines flood forcing factors from observation and reanalysis datasets, antecedent flood extent maps, and deep learning to forecast daily flood extents in Rwanda. We extend the architecture used in ClimaX (transformer weather and climate foundation model), investigate its pretrained representations for flood forecasting, and compare performance against a U-Net baseline. Our results demonstrate that a ClimaX variant trained from scratch with a linear projection decoder outperforms the U-Net and other ClimaX variants, highlighting its potential as an effective tool for flood extent forecasting. This work underscores the potential of data-driven deep learning models for flood extent forecasting with implications for improving disaster preparedness and flood risk assessment in vulnerable regions.",
      "url": "https://www.microsoft.com/en-us/research/publication/towards-flood-extent-forecasting-evaluating-a-weather-foundation-model-and-u-net-for-flood-forecasting/"
    },
    {
      "title": "TarDiff: Target-Oriented Diffusion Guidance for Synthetic Electronic Health Record Time Series Generation",
      "authors": [
        "Bowen Deng",
        "Chang Xu",
        "Hao Li",
        "Jiang Bian",
        "Min Hou",
        "Yuhao Huang"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "April 2025",
      "abstract": "Synthetic Electronic Health Record (EHR) time-series generation is crucial for advancing clinical machine learning models, as it helps address data scarcity by providing more training data. However, most existing approaches focus primarily on replicating statistical distributions and temporal dependencies of real-world data. We argue that fidelity to observed data alone does not guarantee better model performance, as common patterns may dominate, limiting the representation of rare but important conditions. This highlights the need for generate synthetic samples to improve performance of specific clinical models to fulfill their target outcomes. To address this, we propose TarDiff, a novel target-oriented diffusion framework that integrates task-specific influence guidance into the synthetic data generation process. Unlike conventional approaches that mimic training data distributions, TarDiff optimizes synthetic samples by quantifying their expected contribution to improving downstream model performance through influence functions. Specifically, we measure the reduction in task-specific loss induced by synthetic samples and embed this influence gradient into the reverse diffusion process, thereby steering the generation towards utility-optimized data. Evaluated on six publicly available EHR datasets, TarDiff achieves state-of-the-art performance, outperforming existing methods by up to 20.4% in AUPRC and 18.4% in AUROC. Our results demonstrate that TarDiff not only preserves temporal fidelity but also enhances downstream model performance, offering a robust solution to data scarcity and class imbalance in healthcare analytics.",
      "url": "https://www.microsoft.com/en-us/research/publication/tardiff-target-oriented-diffusion-guidance-for-synthetic-electronic-health-record-time-series-generation/"
    },
    {
      "title": "SeCom: On Memory Construction and Retrieval for Personalized Conversational Agents",
      "authors": [
        "Chin-Yew Lin",
        "Dongsheng Li",
        "H. Vicky Zhao",
        "Hao Cheng",
        "Huiqiang Jiang",
        "Jianfeng Gao",
        "Lili Qiu",
        "Qianhui Wu",
        "Xufang Luo",
        "Yuqing Yang",
        "Zhuoshi Pan"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Human-computer interaction"
      ],
      "publication_date": "April 2025",
      "abstract": "To deliver coherent and personalized experiences in long-term conversations, existing approaches typically perform retrieval augmented response generation by constructing memory banks from conversation history at either the turn-level, session-level, or through summarization. In this paper, we present two key findings: (1) The granularity of memory unit matters: Turn-level, session-level, and summarization-based methods each exhibit limitations in both memory retrieval accuracy and the semantic quality of the retrieved content. (2) Prompt compression methods, such as LLMLingua-2, can effectively serve as a denoising mechanism, enhancing memory retrieval accuracy across different granularities.\nBuilding on these insights, we propose SeCom, a method that constructs the memory bank at segment level by introducing a conversation Segmentation model that partitions long-term conversations into topically coherent segments, while applying Compression based denoising on memory units to enhance memory retrieval. Experimental results show that SeCom exhibits a significant performance advantage over baselines on long-term conversation benchmarks LOCOMO and Long-MT-Bench+. Additionally, the proposed conversation segmentation method demonstrates superior performance on dialogue segmentation datasets such as DialSeg711, TIAGE, and SuperDialSeg.",
      "url": "https://www.microsoft.com/en-us/research/publication/secom-on-memory-construction-and-retrieval-for-personalized-conversational-agents/"
    },
    {
      "title": "Walk the Talk? Measuring the Faithfulness of Large Language Model Explanations",
      "authors": [
        "Emre Kiciman",
        "John Guttag",
        "Katie Matton",
        "Robert Osazuwa Ness"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "April 2025",
      "abstract": "Large language models (LLMs) are capable of generating plausible explanations of how they arrived at an answer to a question. However, these explanations can misrepresent the model’s “reasoning” process, i.e., they can be unfaithful. This, in turn, can lead to over-trust and misuse. We introduce a new approach for measuring the faithfulness of LLM explanations. First, we provide a rigorous definition of faithfulness. Since LLM explanations mimic human explanations, they often reference high-level concepts in the input question that purportedly influenced the model. We define faithfulness in terms of the difference between the set of concepts that the LLM’s explanations imply are influential and the set that truly are. Second, we present a novel method for estimating faithfulness that is based on: (1) using an auxiliary LLM to modify the values of concepts within model inputs to create realistic counterfactuals, and (2) using a hierarchical Bayesian model to quantify the causal effects of concepts at both the example- and dataset-level. Our experiments show that our method can be used to quantify and discover interpretable patterns of unfaithfulness. On a social bias task, we uncover cases where LLM explanations hide the influence of social bias. On a medical question answering task, we uncover cases where LLMs provide false claims about which pieces of evidence influenced its decisions.",
      "url": "https://www.microsoft.com/en-us/research/publication/walk-the-talk-measuring-the-faithfulness-of-large-language-model-explanations/"
    },
    {
      "title": "Reinforcement Learning from Automatic Feedback for High-Quality Unit Test Generation",
      "authors": [
        "Alexey Svyatkovskiy",
        "Benjamin Steenhoek",
        "Michele Tufano",
        "Neel Sundaresan"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Programming languages and software engineering"
      ],
      "publication_date": "April 2025",
      "abstract": "Software testing is a crucial but time-consuming aspect of software development, and recently, Large Language Models (LLMs) have gained popularity for automated test case generation. However, because LLMs are trained on vast amounts of open-source code, they often generate test cases that do not adhere to best practices and may even contain test smells (anti-patterns). To address this issue, we propose Reinforcement Learning from Static Quality Metrics (RLSQM), wherein we utilize Reinforcement Learning to generate high-quality unit tests based on static analysis-based quality metrics. First, we analyzed LLM-generated tests and show that LLMs frequently do generate undesirable test smells — up to 37% of the time. Then, we implemented lightweight static analysis-based reward model and trained LLMs using this reward model to optimize for five code quality metrics. Our experimental results demonstrate that the RL-optimized Codex model consistently generated higher-quality test cases than the base LLM, improving quality metrics by up to 23%, and generated nearly 100% syntactically-correct code. RLSQM also outperformed GPT-4 on all code quality metrics, in spite of training a substantially cheaper Codex model. We provide insights into how reliably utilize RL to improve test generation quality and show that RLSQM is a significant step towards enhancing the overall efficiency and reliability of automated software testing.",
      "url": "https://www.microsoft.com/en-us/research/publication/reinforcement-learning-from-automatic-feedback-for-high-quality-unit-test-generation/"
    },
    {
      "title": "A Meta-learner for Heterogeneous Effects in Difference-in-Differences",
      "authors": [
        "Eleanor Dillon",
        "Haoge Chang",
        "Hui Lan",
        "Vasilis Syrgkanis"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "April 2025",
      "abstract": "We address the problem of estimating heterogeneous treatment effects in panel data, adopting the popular Difference-in-Differences (DiD) framework under the conditional parallel trends assumption. We propose a novel doubly robust meta-learner for the Conditional Average Treatment Effect on the Treated (CATT), reducing the estimation to a convex risk minimization problem involving a set of auxiliary models. Our framework allows for the flexible estimation of the CATT, when conditioning on any subset of variables of interest using generic machine learning. Leveraging Neyman orthogonality, our proposed approach is robust to estimation errors in the auxiliary models. As a generalization to our main result, we develop a meta-learning approach for the estimation of general conditional functionals under covariate shift. We also provide an extension to the instrumented DiD setting with non-compliance. Empirical results demonstrate the superiority of our approach over existing baselines.",
      "url": "https://www.microsoft.com/en-us/research/publication/a-meta-learner-for-heterogeneous-effects-in-difference-in-differences/"
    },
    {
      "title": "New employee Copilot usage: Insights into productivity and socialization",
      "authors": [
        "Amy Heger",
        "Kathleen Walker",
        "Mihaela Vorvoreanu",
        "Shipi Dhanorkar",
        "Sydney Graham"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Human-computer interaction"
      ],
      "publication_date": "April 2025",
      "abstract": "This report summarizes a mixed-methods study examining how a new generation of employees interacts with the generative AI assistant Microsoft Copilot for productivity when acclimating to a professional environment. Through a series of surveys, interviews, and a diary study, 125 Microsoft interns in a variety of roles provided insights into their usage and effects of Copilot in their new-employee roles. \nTop findings: \n\nWe observed an association between frequency of Copilot use and workplace integration. Interns who used Copilot more frequently felt better socialized and identified with their teams more strongly.  \n\n\nGreater usage of Copilot correlated with an increasingly favorable perception of the AI assistant. \n\n\nThe most frequent and valued Copilot use cases were Information retrieval, writing assistance, and coding assistance. \n\n\nParticipants highlighted multiple ways in which Copilot helped their productivity: (1) the benefit of seamless access to an AI assistant integrated across their work apps in the M365 suite; and Copilot capabilities that (2) helped new employees save time to focus on their primary responsibilities, (3) aided their getting “unstuck” on tasks, (4) stimulated creativity, and (5) served as a nonjudgmental assistant as they adapted to their new professional environment.  \n\n\nInterns learned how to use Copilot by trial and error, picking up tips from peers, or accessing training documentation and resources.  \n\n \nCite as:\n\n\n\n\n\n\nVorvoreanu, M., Graham, S., Heger, A., Dhanorkar, S., & Walker, K. (2025). New employee Copilot usage: Insights into productivity and socialization. Microsoft Technical Report MSR-TR-2025-24. https://www.microsoft.com/en-us/research/publication/new-employee-copilot-usage-insights-into-productivity-and-socialization/\n\n\n\n\n\n",
      "url": "https://www.microsoft.com/en-us/research/publication/new-employee-copilot-usage-insights-into-productivity-and-socialization/"
    },
    {
      "title": "Esc: An Early-stopping Checker for Budget-aware Index Tuning (Extended Version)",
      "authors": [
        "Surajit Chaudhuri",
        "Vivek Narasayya",
        "Wentao Wu",
        "Xiaoying Wang"
      ],
      "research_areas": [
        "Data platforms and analytics"
      ],
      "publication_date": "April 2025",
      "abstract": "Index tuning is a time-consuming process. One major performance bottleneck in existing index tuning systems is the large amount of “what-if” query optimizer calls that estimate the cost of a given pair of query and index configuration without materializing the indexes. There has been recent work on budget-aware index tuning that limits the amount of what-if calls allowed in index tuning. Existing budget-aware index tuning algorithms, however, typically make fast progress early on in terms of the best configuration found but slow down when more and more what-if calls are allocated. This observation of “diminishing return” on index quality leads us to introduce early stopping for budget-aware index tuning, where user specifies a threshold on the tolerable loss of index quality and we stop index tuning if the projected loss with the remaining budget is below the threshold. We further propose Esc, a low-overhead early-stopping checker that realizes this new functionality. Experimental evaluation on top of both industrial benchmarks and real customer workloads demonstrate that Esc can significantly reduce the number of what-if calls made during budget-aware index tuning while incur little or zero improvement loss and little extra computational overhead compared to the overall index tuning time.",
      "url": "https://www.microsoft.com/en-us/research/publication/esc-an-early-stopping-checker-for-budget-aware-index-tuning-2/"
    },
    {
      "title": "ALMTokenizer: A Low-bitrate and Semantic-rich Audio Codec Tokenizer for Audio Language Modeling",
      "authors": [
        "Dongchao Yang",
        "Haohan Guo",
        "Helen M. Meng",
        "Helin Wang",
        "Jiankun Zhao",
        "Songxiang Liu",
        "Xixin Wu",
        "Xu Tan",
        "Xubo Liu",
        "Xueyuan Chen",
        "Yuanyuan Wang",
        "Zeqian Ju"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Audio and Acoustics"
      ],
      "publication_date": "April 2025",
      "abstract": "Recent advancements in audio language models have underscored the pivotal role of audio tokenization, which converts audio signals into discrete tokens, thereby facilitating the application of language model architectures to the audio domain. In this study, we introduce ALMTokenizer, a novel low-bitrate and semantically rich audio codec tokenizer for audio language models. Prior methods, such as Encodec, typically encode individual audio frames into discrete tokens without considering the use of context information across frames. Unlike these methods, we introduce a novel query-based compression strategy to capture holistic information with a set of learnable query tokens by explicitly modeling the context information across frames. This design not only enables the codec model to capture more semantic information but also encodes the audio signal with fewer token sequences. Additionally, to enhance the semantic information in audio codec models, we introduce the following: (1) A masked autoencoder (MAE) loss, (2) Vector quantization based on semantic priors, and (3) An autoregressive (AR) prediction loss. As a result, ALMTokenizer achieves competitive reconstruction performance relative to state-of-the-art approaches while operating at a lower bitrate. Within the same audio language model framework, ALMTokenizer outperforms previous tokenizers in audio understanding and generation tasks.",
      "url": "https://www.microsoft.com/en-us/research/?post_type=msr-research-item&p=1140623"
    },
    {
      "title": "Imaging Transformer for MRI Denoising: a Scalable Model Architecture that enables SNR<<1 Imaging",
      "authors": [
        "A. Campbell-Washburn",
        "Charlotte Manisty",
        "Hui Xue",
        "Iain Pierce",
        "James C. Moon",
        "John Stairs",
        "Joseph Naegele",
        "Michael Hansen",
        "Peter Kellman",
        "Rhodri H. Davies",
        "Sarah M. Hooper",
        "T. Treibel"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Computer vision",
        "Medical, health and genomics"
      ],
      "publication_date": "April 2025",
      "abstract": "Purpose: To propose a flexible and scalable imaging transformer (IT) architecture with three attention modules for multi-dimensional imaging data and apply it to MRI denoising with very low input SNR.\nMethods: Three independent attention modules were developed: spatial local, spatial global, and frame attentions. They capture long-range signal correlation and bring back the locality of information in images. An attention-cell-block design processes 5D tensors ([B, C, F, H, W]) for 2D, 2D+T, and 3D image data. A High Resolution (HRNet) backbone was built to hold IT blocks. Training dataset consists of 206,677 cine series and test datasets had 7,267 series. Ten input SNR levels from 0.05 to 8.0 were tested. IT models were compared to seven convolutional and transformer baselines. To test scalability, four IT models 27m to 218m parameters were trained. Two senior cardiologists reviewed IT model outputs from which the EF was measured and compared against the ground-truth.\nResults: IT models significantly outperformed other models over the tested SNR levels. The performance gap was most prominent at low SNR levels. The IT-218m model had the highest SSIM and PSNR, restoring good image quality and anatomical details even at SNR 0.2. Two experts agreed at this SNR or above, the IT model output gave the same clinical interpretation as the ground-truth. The model produced images that had accurate EF measurements compared to ground-truth values. Conclusions: Imaging transformer model offers strong performance, scalability, and versatility for MR denoising. It recovers image quality suitable for confident clinical reading and accurate EF measurement, even at very low input SNR of 0.2.",
      "url": "https://www.microsoft.com/en-us/research/publication/imaging-transformer-for-mri-denoising-a-scalable-model-architecture-that-enables-snr1-imaging/"
    },
    {
      "title": "Fields of The World: A Machine Learning Benchmark Dataset For Global Agricultural Field Boundary Segmentation",
      "authors": [
        "Adeel Ahmad",
        "Aninda Ghosh",
        "Caleb Robinson",
        "Chris Holmes",
        "Eddie Choi",
        "Hannah Kerner",
        "Jennifer Marcus",
        "Juan M. Lavista Ferres",
        "Matthias Mohr",
        "Nathan Jacobs",
        "Rahul Dodhia",
        "Snehal Chaudhari"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Ecology and environment"
      ],
      "publication_date": "April 2025",
      "abstract": "Crop field boundaries are foundational datasets for agricultural monitoring and assessments but are expensive to collect manually. Machine learning (ML) methods for automatically extracting field boundaries from remotely sensed images could help realize the demand for these datasets at a global scale. However, current ML methods for field instance segmentation lack sufficient geographic coverage, accuracy, and generalization capabilities. Further, research on improving ML methods is restricted by the lack of labeled datasets representing the diversity of global agricultural fields. We present Fields of The World (FTW) — a novel ML benchmark dataset for agricultural field instance segmentation spanning 24 countries on four continents (Europe, Africa, Asia, and South America). FTW is an order of magnitude larger than previous datasets with 70,462 samples, each containing instance and semantic segmentation masks paired with multi-date, multi-spectral Sentinel-2 satellite images. We provide results from baseline models for the new FTW benchmark, show that models trained on FTW have better zero-shot and fine-tuning performance in held-out countries than models that aren’t pre-trained with diverse datasets, and show positive qualitative zero-shot results of FTW models in a real-world scenario — running on Sentinel-2 scenes over Ethiopia.",
      "url": "https://www.microsoft.com/en-us/research/publication/fields-of-the-world-a-machine-learning-benchmark-dataset-for-global-agricultural-field-boundary-segmentation/"
    },
    {
      "title": "Agentic Reasoning and Tool Integration for LLMs via Reinforcement Learning",
      "authors": [
        "Akshay Nambi",
        "Joykirat Singh",
        "Raghav Magazine",
        "Yash Pandya"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "April 2025",
      "abstract": "Large language models (LLMs) have achieved remarkable progress in complex reasoning tasks, yet they remain fundamentally limited by their reliance on static internal knowledge and text-only reasoning. Real-world problem solving often demands dynamic, multi-step reasoning, adaptive decision making, and the ability to interact with external tools and environments. In this work, we introduce ARTIST (Agentic Reasoning and Tool Integration in Self-improving Transformers), a unified framework that tightly couples agentic reasoning, reinforcement learning, and tool integration for LLMs. ARTIST enables models to autonomously decide when, how, and which tools to invoke within multi-turn reasoning chains, leveraging outcome-based RL to learn robust strategies for tool use and environment interaction without requiring step-level supervision. Extensive experiments on mathematical reasoning and multi-turn function calling benchmarks show that ARTIST consistently outperforms state-of-the-art baselines, with up to 22% absolute improvement over base models and strong gains on the most challenging tasks. Detailed studies and metric analyses reveal that agentic RL training leads to deeper reasoning, more effective tool use, and higher-quality solutions. Our results establish agentic RL with tool integration as a powerful new frontier for robust, interpretable, and generalizable problem-solving in LLMs.",
      "url": "https://www.microsoft.com/en-us/research/publication/agentic-reasoning-and-tool-integration-for-llms-via-reinforcement-learning/"
    },
    {
      "title": "Audio Entailment: Assessing Deductive Reasoning for Audio Understanding",
      "authors": [
        "Benjamin Elizalde",
        "Bhiksha Raj",
        "Hannes Gamper",
        "Hazim T. Bukhari",
        "Rita Singh",
        "Shuo Han",
        "Soham Deshmukh"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Audio and Acoustics"
      ],
      "publication_date": "April 2025",
      "abstract": "Recent literature uses language to build foundation models for audio. These Audio–Language Models (ALMs) are trained on a vast number of audio–text pairs and show remarkable performance in tasks including Text-to-Audio Retrieval, Captioning, and Question Answering. However, their ability to engage in more complex open-ended tasks, like Interactive Question-Answering, requires proficiency in logical reasoning—a skill not yet benchmarked. We introduce the novel task of Audio Entailment to evaluate an ALM’s deductive reasoning ability. This task assesses whether a text description (hypothesis) of audio content can be deduced from an audio recording (premise), with potential conclusions being entailment, neutral, or contradiction, depending on the sufficiency of the evidence. We create two datasets for this task with audio recordings sourced from two audio captioning datasets—AudioCaps and Clotho—and hypotheses generated using Large Language Models (LLMs). We benchmark state-of-the-art ALMs and find deficiencies in logical reasoning with both zero-shot and linear probe evaluations. Finally, we propose “caption-before-reason”, an intermediate step of captioning that improves the Zero-Shot and linear-probe performance of ALMs by an absolute 6% and 3%, respectively.\nDatasets — https://github.com/microsoft/AudioEntailment",
      "url": "https://www.microsoft.com/en-us/research/publication/audio-entailment-assessing-deductive-reasoning-for-audio-understanding/"
    },
    {
      "title": "Analyzing decades-long environmental changes in Namibia using archival aerial photography and deep learning",
      "authors": [
        "Akram Zaytar",
        "Caleb Robinson",
        "Emmanuel H. Kreike",
        "Gilles Quentin Hacheme",
        "Girmaw Abebe Tadesse",
        "Juan M. Lavista Ferres",
        "Rahul Dodhia",
        "Tsering Wangyal Shawa"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Computer vision",
        "Ecology and environment"
      ],
      "publication_date": "April 2025",
      "abstract": "This study explores object detection in historical aerial photographs of Namibia to identify long-term environmental changes. Specifically, we aim to identify key objects – Waterholes, Omuti homesteads, and Big trees – around Oshikango in Namibia using sub-meter gray-scale aerial imagery from 1943 and 1972. In this work, we propose a workflow for analyzing historical aerial imagery using a deep semantic segmentation model on sparse hand-labels. To this end, we employ a number of strategies including class-weighting, pseudo-labeling and empirical p-value-based filtering to balance skewed and sparse representations of objects in the ground truth data. Results demonstrate the benefits of these different training strategies resulting in an average F1 = 0.661 and F1 = 0.755 over the three objects of interest for the 1943 and 1972 imagery, respectively. We also identified that the average size of Waterhole and Big trees increased while the average size of Omutis decreased between 1943 and 1972 reflecting some of the local effects of the massive post-Second World War economic, agricultural, demographic, and environmental changes. This work also highlights the untapped potential of historical aerial photographs in understanding long-term environmental changes beyond Namibia (and Africa). With the lack of adequate satellite technology in the past, archival aerial photography offers a great alternative to uncover decades-long environmental changes.",
      "url": "https://www.microsoft.com/en-us/research/publication/analyzing-decades-long-environmental-changes-in-namibia-using-archival-aerial-photography-and-deep-learning/"
    },
    {
      "title": "Retrieval with Learned Similarities",
      "authors": [
        "Bailu Ding",
        "Jiaqi Zhai"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Data platforms and analytics"
      ],
      "publication_date": "April 2025",
      "abstract": "Retrieval plays a fundamental role in recommendation systems, search, and natural language processing (NLP) by efficiently finding relevant items from a large corpus given a query. Dot products have been widely used as the similarity function in such tasks, enabled by Maximum Inner Product Search (MIPS) algorithms for efficient retrieval. However, state-of-the-art retrieval algorithms have migrated to learned similarities. These advanced approaches encompass multiple query embeddings, complex neural networks, direct item ID decoding via beam search, and hybrid solutions. Unfortunately, we lack efficient solutions for retrieval in these state-of-the-art setups. Our work addresses this gap by investigating efficient retrieval techniques with expressive learned similarity functions. We establish Mixture-of-Logits (MoL) as a universal approximator of similarity functions, demonstrate that MoL’s expressiveness can be realized empirically to achieve superior performance on diverse retrieval scenarios, and propose techniques to retrieve the approximate top-k results using MoL with tight error bounds. Through extensive experimentation, we show that MoL, enhanced by our proposed mutual information-based load balancing loss, sets new state-of-the-art results across heterogeneous scenarios, including sequential retrieval models in recommendation systems and finetuning language models for question answering; and our approximate top-k algorithms outperform baselines by up to 66x in latency while achieving >.99 recall rate compared to exact algorithms.\n",
      "url": "https://www.microsoft.com/en-us/research/publication/retrieval-with-learned-similarities/"
    },
    {
      "title": "EyeO: Autocalibrating Gaze Output with Gaze Input for Gaze Typing",
      "authors": [
        "Akanksha Saran",
        "Ann Paradiso",
        "Cyril Zhang",
        "Danielle Bragg",
        "Jacob Alber",
        "John Langford"
      ],
      "research_areas": [
        "Algorithms",
        "Artificial intelligence",
        "Hardware and devices",
        "Human language technologies",
        "Human-computer interaction"
      ],
      "publication_date": "April 2025",
      "abstract": "Gaze tracking devices have the potential to expand interactivity greatly, yet miscalibration remains a significant barrier to use. As devices miscalibrate, people tend to compensate by intentionally offsetting their gaze, which makes detecting miscalibration from eye signals difficult. To help address this problem, we propose a novel approach to seamless calibration based on the insight that the system’s model of eye gaze can be updated during reading (user does not compensate) to improve calibration for typing (user might compensate). To explore this approach, we built an auto-calibrating gaze typing prototype called EyeO and ran a user study with 20 participants. Our user study results suggest that seamless autocalibration can significantly improve typing efficiency and user experience.",
      "url": "https://www.microsoft.com/en-us/research/publication/eyeo-autocalibrating-gaze-output-with-gaze-input-for-gaze-typing/"
    },
    {
      "title": "Causal Order: The Key to Leveraging Imperfect Experts in Causal Inference",
      "authors": [
        "Abbavaram Gowtham Reddy",
        "Abhinav Kumar",
        "Amit Sharma",
        "Aniket Vashishtha",
        "Saketh Bachu",
        "Vineeth N. Balasubramanian"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "April 2025",
      "abstract": "Large Language Models (LLMs) have recently been used as experts to infer causal graphs, often by repeatedly applying a pairwise prompt that asks about the causal relationship of each variable pair. However, such experts, including human domain experts, cannot distinguish between direct and indirect effects given a pairwise prompt. Therefore, instead of the graph, we propose that causal order be used as a more stable output interface for utilizing expert knowledge. When querying a perfect expert with a pairwise prompt, we show that the inferred graph can have significant errors whereas the causal order is always correct. In practice, however, LLMs are imperfect experts and we find that pairwise prompts lead to multiple cycles and do not yield a valid order. Hence, we propose a prompting strategy that introduces an auxiliary variable for every variable pair and instructs the LLM to avoid cycles within this triplet. We show, both theoretically and empirically, that such a triplet prompt leads to fewer cycles than the pairwise prompt. Across multiple real-world graphs, the triplet prompt yields a more accurate order using both LLMs and human annotators as experts. By querying the expert with different auxiliary variables for the same variable pair, it also increases robustness—triplet method with much smaller models such as Phi-3 and Llama-3 8B outperforms a pairwise prompt with GPT-4. For practical usage, we show how the estimated causal order from the triplet method can be used to reduce error in downstream discovery and effect inference tasks.",
      "url": "https://www.microsoft.com/en-us/research/publication/causal-order-the-key-to-leveraging-imperfect-experts-in-causal-inference/"
    },
    {
      "title": "Enhancing Large Language Model Performance with Gradient-Based Parameter Selection",
      "authors": [
        "Haoling Li",
        "Peng Cheng",
        "Qi Chen",
        "Xiao Liu",
        "Xin Zhang",
        "Yeyun Gong",
        "Yifan Wang"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "April 2025",
      "abstract": "Large language models (LLMs) have revolutionized numerous fields of research, driving significant advancements in natural language processing, machine translation, and beyond. Although the extensive number of parameters contributes a lot to the great success, existing studies indicate that not all model parameters hold equal importance, which further leads to redundancy during the parameter update process. Recent works for reducing redundant parameter updates for LLMs either lack task-specific data information, may leading to suboptimal model performance, or discard transformer components or insignificant parameters, limiting the model’s scalability across different tasks and potentially compromising the LLM structure. To address these issues and further enhance the performance of LLMs, we propose Gradient-Mask Tuning (GMT), a method that selectively updates parameters based on gradient information, which is specific to the target tasks. Specifically, after calculating gradients during back propagation, we measure their absolute values and mask those with small absolute values. Our empirical results in various training paradigms like SFT and DPO for various domains of tasks demonstrate that GMT not only preserves the original network structure but also enhances the potential performance of LLMs. Further analysis indicates that GMT exhibits insensitivity to mask ratio and possesses computational efficiency comparable to vanilla training approach.",
      "url": "https://www.microsoft.com/en-us/research/publication/enhancing-large-language-model-performance-with-gradient-based-parameter-selection/"
    },
    {
      "title": "Proving Olympiad Inequalities by Synergizing LLMs and Symbolic Reasoning",
      "authors": [
        "Fan Yang",
        "Kaiyu Yang",
        "Wen Tang",
        "Xian Zhang",
        "Xiaoxing Ma",
        "Xujie Si",
        "Yuan Yao",
        "Zenan Li",
        "Zhaoyu Li"
      ],
      "research_areas": [
        "Systems and networking"
      ],
      "publication_date": "April 2025",
      "abstract": "Large language models (LLMs) can prove mathematical theorems formally by generating proof steps (\\textit{a.k.a.} tactics) within a proof system. However, the space of possible tactics is vast and complex, while the available training data for formal proofs is limited, posing a significant challenge to LLM-based tactic generation. To address this, we introduce a neuro-symbolic tactic generator that synergizes the mathematical intuition learned by LLMs with domain-specific insights encoded by symbolic methods. The key aspect of this integration is identifying which parts of mathematical reasoning are best suited to LLMs and which to symbolic methods. While the high-level idea of neuro-symbolic integration is broadly applicable to various mathematical problems, in this paper, we focus specifically on Olympiad inequalities (Figure~1). We analyze how humans solve these problems and distill the techniques into two types of tactics: (1) scaling, handled by symbolic methods, and (2) rewriting, handled by LLMs. In addition, we combine symbolic tools with LLMs to prune and rank the proof goals for efficient proof search. We evaluate our framework on 161 challenging inequalities from multiple mathematics competitions, achieving state-of-the-art performance and significantly outperforming existing LLM and symbolic approaches without requiring additional training data.",
      "url": "https://www.microsoft.com/en-us/research/publication/proving-olympiad-inequalities-by-synergizing-llms-and-symbolic-reasoning/"
    },
    {
      "title": "Closing the Gap: A User Study on the Real-world Usefulness of AI-powered Vulnerability Detection & Repair in the IDE",
      "authors": [
        "Benjamin Steenhoek",
        "Renata Saldivar",
        "Roshanak Zilouchian Moghaddam",
        "Siva Sivaraman",
        "Wei Le",
        "Yevhen Mohylevskyy"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Programming languages and software engineering",
        "Security, privacy, and cryptography"
      ],
      "publication_date": "April 2025",
      "abstract": "Security vulnerabilities impose significant costs on users and organizations. Detecting and addressing these vulnerabilities early is crucial to avoid exploits and reduce development costs. Recent studies have shown that deep learning models can effectively detect security vulnerabilities. Yet, little research explores how to adapt these models from benchmark tests to practical applications, and whether they can be useful in practice. This paper presents the first empirical study of a vulnerability detection and fix tool with professional software developers on real projects that they own.\nWe implemented DeepVulGuard, an IDE-integrated tool based on state-of-the-art detection and fix models, and show that it has promising performance on benchmarks of historic vulnerability data. DeepVulGuard scans code for vulnerabilities (including identifying the vulnerability type and vulnerable region of code), suggests fixes, provides natural- language explanations for alerts and fixes, leveraging chat interfaces.\nWe recruited 17 professional software developers, observed their usage of the tool on their code, and conducted interviews to assess the tool’s usefulness, speed, trust, relevance, and workflow integration. We also gathered detailed qualitative feedback on users’ perceptions and their desired features. Study participants scanned a total of 24 projects, 6.9k files, and over 1.7 million lines of source code, and generated 170 alerts and 50 fix suggestions. We find that although state-of-the-art AI-powered detection and fix tools show promise, they are not yet practical for real-world use due to a high rate of false positives and non-applicable fixes. User feedback reveals several actionable pain points, ranging from incomplete context to lack of customization for the user’s codebase. Additionally, we explore how AI features, including confidence scores, explanations, and chat interaction, can apply to vulnerability detection and fixing. Based on these insights, we offer practical recommendations for evaluating and deploying AI detection and fix models.\n ",
      "url": "https://www.microsoft.com/en-us/research/publication/closing-the-gap-a-user-study-on-the-real-world-usefulness-of-ai-powered-vulnerability-detection-repair-in-the-ide/"
    },
    {
      "title": "SoundTRC: DNN-based Acoustic Target Region Control",
      "authors": [
        "Andrew Markham",
        "Okan Köpüklü",
        "Yuhang He"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Audio and Acoustics"
      ],
      "publication_date": "April 2025",
      "abstract": "We propose a deep neural network based automatic acoustic target region control framework, where the goal is to maintain all the relevant speech in the designated target region while muting all speech outside the target region in a multi-speaker conferencing room. We discuss three target regions: angle, angle-distance and distance that reflect common region-based speech control requests, and further propose a unified target region encoding strategy to encode the three different target regions into discriminative and compact target region vector. We propose a unified Cross-Attention Transformer based deep neural network, which takes the mixed speech and corresponding target region description as input and outputs ideal ratio mask that is responsible of masking out speech that is outside of the target region while suppressing the noise simultaneously. We run experiments on both simulated shoe-box like 3D room scenes and photo-realistic and complex 3D room scenes, showing the advantage of our proposed framework.",
      "url": "https://www.microsoft.com/en-us/research/publication/soundtrc-dnn-based-acoustic-target-region-control-2/"
    },
    {
      "title": "Heterogeneous Graph Neural Network on Semantic Tree",
      "authors": [
        "Elnaz Nouri",
        "Fuchen Liu",
        "Jack W. Stokes",
        "Mingyu Guan",
        "Purvanshi Mehta",
        "Qinlong Luo",
        "Taesoo Kim"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "April 2025",
      "abstract": "The recent past has seen an increasing interest in Heterogeneous Graph Neural Networks (HGNNs), since many real-world graphs are heterogeneous in nature, from citation graphs to email graphs. However, existing methods ignore a tree hierarchy among metapaths, naturally constituted by different node types and relation types. In this paper, we present HetTree, a novel HGNN that models both the graph structure and heterogeneous aspects in a scalable and effective manner. Specifically, HetTree builds a semantic tree data structure to capture the hierarchy among metapaths. To effectively encode the semantic tree, HetTree uses a novel subtree attention mechanism to emphasize metapaths that are more helpful in encoding parent-child relationships. Moreover, HetTree proposes carefully matching pre-computed features and labels correspondingly, constituting a complete metapath representation. Our evaluation of HetTree on a variety of real-world datasets demonstrates that it outperforms all existing baselines on open benchmarks and efficiently scales to large real-world graphs with millions of nodes and edges.",
      "url": "https://www.microsoft.com/en-us/research/publication/heterogeneous-graph-neural-network-on-semantic-tree/"
    },
    {
      "title": "Generation Probabilities Are Not Enough: Uncertainty Highlighting in AI Code Completions",
      "authors": [
        "Adam Fourney",
        "Gagan Bansal",
        "Helena Vasconcelos",
        "Jennifer Wortman Vaughan",
        "Q. Vera Liao"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Human-computer interaction"
      ],
      "publication_date": "April 2025",
      "abstract": "Large-scale generative models enabled the development of AI-powered code completion tools to assist programmers in writing code. However, much like other AI-powered tools, AI-powered code completions are not always accurate, potentially introducing bugs or even security vulnerabilities into code if not properly detected and corrected by a human programmer. One technique that has been proposed and implemented to help programmers identify potential errors is to highlight uncertain tokens. However, there have been no empirical studies exploring the effectiveness of this technique– nor investigating the different and not-yet-agreed-upon notions of uncertainty in the context of generative models. We explore the question of whether conveying information about uncertainty enables programmers to more quickly and accurately produce code when collaborating with an AI-powered code completion tool, and if so, what measure of uncertainty best fits programmers’ needs. Through a mixed-methods study with 30 programmers, we compare three conditions: providing the AI system’s code completion alone, highlighting tokens with the lowest likelihood of being generated by the underlying generative model, and highlighting tokens with the highest predicted likelihood of being edited by a programmer. We find that highlighting tokens with the highest predicted likelihood of being edited leads to faster task completion and more targeted edits, and is subjectively preferred by study participants. In contrast, highlighting tokens according to their probability of being generated does not provide any benefit over the baseline with no highlighting. We further explore the design space of how to convey uncertainty in AI-powered code completion tools, and find that programmers prefer highlights that are granular, informative, interpretable, and not overwhelming.",
      "url": "https://www.microsoft.com/en-us/research/publication/generation-probabilities-are-not-enough-exploring-the-effectiveness-of-uncertainty-highlighting-in-ai-powered-code-completions/"
    },
    {
      "title": "WABER: Evaluating Reliability and Efficiency of Web Agents with Existing Benchmarks",
      "authors": [
        "Fazle Faisal",
        "Su Kara",
        "Suman Nath"
      ],
      "research_areas": [
        "Systems and networking"
      ],
      "publication_date": "April 2025",
      "abstract": "Most existing web agent benchmarks evaluate agents solely based on their task completion rate, excluding other crucial aspects of agent behavior that impact their usability and deployability in real-world. We propose incorporating two important metrics into a web agent benchmark: reliability that assesses how consistently the agent completes tasks despite transient web unreliability that are common in the wild, and efficiency that measures the speed and cost-effectiveness of the agent’s task completion. Developing new benchmarks to measure these metrics would take significant efforts. To address this, we introduce a novel network proxy-based solution called WABER, which enables the evaluation of these two metrics on existing agents and benchmarks without requiring any modifications to them. This allows agent developers to adopt it effortlessly on any agent and benchmark, with zero developer effort. Using our WABER prototype, we evaluated two existing agents on the WebArena benchmark: Stacked LLM Policy and Agent Workflow Memory. Our results show that current SoTA agents struggle to complete tasks on the WABER framework, demonstrating the need to design agents that are able to generalize to real-word, unreliable scenarios.",
      "url": "https://www.microsoft.com/en-us/research/publication/waber-evaluating-reliability-and-efficiency-of-web-agents-with-existing-benchmarks/"
    },
    {
      "title": "Functional Near-Infrared Spectroscopy Feature Extraction with Application in Workload Estimation",
      "authors": [
        "Andre Golard",
        "David Johnston",
        "Dimitra Emmanouilidou",
        "Elisabeth R. M. Heremans",
        "Ivan Tashev",
        "Ryen White"
      ],
      "research_areas": [
        "Audio and Acoustics",
        "Hardware and devices",
        "Human-computer interaction"
      ],
      "publication_date": "April 2025",
      "abstract": "Functional near-infrared spectroscopy (fNIRS) is a brain imaging technique used to estimate neuronal activity by measuring blood oxygenation. In this paper, we develop and evaluate an extensive set of fNIRS features for workload estimation, combining them with respiration and heartbeat signals. Our subject- and session-independent workload estimator is validated in a virtual flight simulator, where workload is objectively assessed based on task performance. We experiment with various regression models and feature ablations, identifying the most effective fNIRS features. The best fNIRS-based model achieves a correlation of 0.3188 with objective workload labels, improving to 0.3268 when incorporating breathing signals. This study demonstrates the value of our novel fNIRS feature set for workload estimation.",
      "url": "https://www.microsoft.com/en-us/research/publication/functional-near-infrared-spectroscopy-feature-extraction-with-application-in-workload-estimation/"
    },
    {
      "title": "SoundTRC: DNN-based Acoustic Target Region Control",
      "authors": [
        "Andrew Markham",
        "Okan Köpüklü",
        "Yuhang He"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "April 2025",
      "abstract": "We propose a deep neural network based automatic acoustic target region control framework, where the goal is to maintain all the relevant speech in the designated target region while muting all speech outside the target region in a multi-speaker conferencing room. We discuss three target regions: angle, angle-distance and distance that reflect common region-based speech control requests, and further propose a unified target region encoding strategy to encode the three different target regions into discriminative and compact target region vector. We propose a unified Cross-Attention Transformer based deep neural network, which takes the mixed speech and corresponding target region description as input and outputs ideal ratio mask that is responsible of masking out speech that is outside of the target region while suppressing the noise simultaneously. We run experiments on both simulated shoe-box like 3D room scenes and photo-realistic and complex 3D room scenes, showing the advantage of our proposed framework.",
      "url": "https://www.microsoft.com/en-us/research/publication/soundtrc-dnn-based-acoustic-target-region-control/"
    },
    {
      "title": "RuAG: Learned-rule-augmented Generation for Large Language Models",
      "authors": [
        "Chaoyun Zhang",
        "Dongmei Zhang",
        "Lu Wang",
        "Meng Fang",
        "Mykola Pechenizkiy",
        "Pei Xiao",
        "Qi Zhang",
        "Qingwei Lin 林庆维",
        "Randolph Yao",
        "Saravan Rajmohan",
        "Si Qin",
        "Yali Du",
        "Yevgeniy Puzyrev",
        "Yudi Zhang"
      ],
      "research_areas": [
        "Algorithms",
        "Artificial intelligence"
      ],
      "publication_date": "April 2025",
      "abstract": "In-context learning (ICL) and Retrieval-Augmented Generation (RAG) have gained attention for their ability to enhance LLMs’ reasoning by incorporating external knowledge but suffer from limited contextual window size, leading to insufficient information injection. To this end, we propose a novel framework to automatically distill large volumes of offline data into interpretable first-order logic rules, which are injected into LLMs to boost their reasoning capabilities. Our method begins by formulating the search process relying on LLMs’ commonsense, where LLMs automatically define head and body predicates. Then, we apply Monte Carlo Tree Search (MCTS) to address the combinational searching space and efficiently discover logic rules from data. The resulting logic rules are translated into natural language, allowing targeted knowledge injection and seamless integration into LLM prompts for LLM’s downstream task reasoning. We evaluate our framework on public and private industrial tasks, including Natural Language Processing (NLP), time-series, decision-making, and industrial tasks, demonstrating its effectiveness in enhancing LLM’s capability over diverse tasks.",
      "url": "https://www.microsoft.com/en-us/research/publication/ruag-learned-rule-augmented-generation-for-large-language-models/"
    },
    {
      "title": "Smart Casual Verification of the Confidential Consortium Framework",
      "authors": [
        "Amaury Chamayou",
        "Eddy Ashton",
        "Heidi Howard",
        "Markus Kuppe",
        "Natacha Crooks"
      ],
      "research_areas": [
        "Programming languages and software engineering",
        "Systems and networking"
      ],
      "publication_date": "April 2025",
      "abstract": "The Confidential Consortium Framework (CCF) is an open-source platform for developing trustworthy and reliable cloud applications. CCF powers Microsoft’s Azure Confidential Ledger service and as such it is vital to build confidence in the correctness of CCF’s design and implementation. This paper reports our experiences applying smart casual verification to validate the correctness of CCF’s novel distributed protocols, focusing on its unique distributed consensus protocol and its custom client consistency model. We use the term smart casual verification to describe our hybrid approach, which combines the rigor of formal specification and model checking with the pragmatism of automated testing, in our case binding the formal specification in TLA+ to the C++ implementation. While traditional formal methods approaches require substantial buy-in and are often one-off efforts by domain experts, we have integrated our smart casual verification approach into CCF’s CI pipeline, allowing contributors to continuously validate CCF as it evolves. We describe the challenges we faced in applying smart casual verification to a complex existing codebase and how we overcame them to find six subtle bugs in the design and implementation before they could impact production.",
      "url": "https://www.microsoft.com/en-us/research/publication/smart-casual-verification-of-ccfs-distributed-consensus-and-consistency-protocols/"
    },
    {
      "title": "Shifting Work Patterns with Generative AI",
      "authors": [
        "Christopher Stanton",
        "Eleanor Dillon",
        "Nicole Immorlica",
        "Sonia Jaffe"
      ],
      "research_areas": [
        "Economics"
      ],
      "publication_date": "April 2025",
      "abstract": "We present evidence on how generative AI changes the work patterns of knowledge workers using data from a 6-month-long, cross-industry, randomized field experiment. Half of the 6,000 workers in the study received access to a generative AI tool integrated into the applications they already used for emails, document creation, and meetings. We find that access to the AI tool during the first year of its release primarily impacted behaviors that could be changed independently and not behaviors that required coordination to change: workers who used the tool spent 3 fewer hours, or 25% less time on email each week (intent to treat estimate is 1.4 hours) and seemed to complete documents moderately faster, but did not significantly change time spent in meetings.",
      "url": "https://www.microsoft.com/en-us/research/publication/shifting-work-patterns-with-generative-ai/"
    },
    {
      "title": "Distillation and Pruning for Scalable Self-Supervised Representation-Based Speech Quality Assessment",
      "authors": [
        "Benjamin Stahl",
        "Hannes Gamper"
      ],
      "research_areas": [
        "Audio and Acoustics"
      ],
      "publication_date": "April 2025",
      "abstract": "Distill-MOS is a compact and efficient speech quality assessment model learned from a larger speech quality assessment model based on wav2vec2.0 XLS-R embeddings. Model weights and inference code are available on GitHub (opens in new tab).\nInstall Distill-MOS via\npip install distillmos\nThe work is described in the paper: “Distillation and Pruning for Scalable Self-Supervised Representation-Based Speech Quality Assessment”.\nAbstract:\nIn this paper, we investigate distillation and pruning methods to reduce model size for non-intrusive speech quality assessment based on self-supervised representations. Our experiments build on XLS-R-SQA, a speech quality assessment model using wav2vec 2.0 XLS-R embeddings. We retrain this model on a large compilation of mean opinion score datasets, encompassing over 100,000 labeled clips. For distillation, using this model as a teacher, we generate pseudo-labels on unlabeled degraded speech signals and train student models of varying sizes. For pruning, we use a data-driven strategy. While data-driven pruning performs better at larger model sizes, distillation on unlabeled data is more effective for smaller model sizes. Distillation can halve the gap between the baseline’s correlation with ground-truth MOS labels and that of the XLS-R-based teacher model, while reducing model size by two orders of magnitude compared to the teacher model.\nTable summarizing speech quality estimation results",
      "url": "https://www.microsoft.com/en-us/research/publication/distill-mos-a-compact-speech-quality-assessment-model/"
    },
    {
      "title": "Steering Large Language Models between Code Execution and Textual Reasoning",
      "authors": [
        "Chi Wang",
        "Chuchu Fan",
        "Harsh Jhamtani",
        "Srinagesh Sharma",
        "Yongchao Chen"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "April 2025",
      "abstract": "While a lot of recent research focuses on enhancing the textual reasoning capabilities of Large Language Models (LLMs) by optimizing the multi-agent framework or reasoning chains, several benchmark tasks can be solved with 100% success through direct coding, which is more scalable and avoids the computational overhead associated with textual iterating and searching. Textual reasoning has inherent limitations in solving tasks with challenges in math, logics, optimization, and searching, which is unlikely to be solved by simply scaling up the model and data size. The recently released OpenAI GPT Code Interpreter and multi-agent frameworks such as AutoGen have demonstrated remarkable proficiency of integrating code generation and execution to solve complex tasks using LLMs. However, based on our experiments on 7 existing popular methods for steering code/text generation in both single- and multi-turn settings with 14 tasks and 6 types of LLMs (including the new O1-preview), currently there is no optimal method to correctly steer LLMs to write code when needed. We discover some interesting patterns on when models use code vs. textual reasoning with the evolution to task complexity and model sizes, which even result in an astonishingly inverse scaling law. We also discover that results from LLM written code are not always better than using textual reasoning, even if the task could be solved through code. To mitigate the above issues, we propose three methods to better steer LLM code/text generation and achieve a notable improvement. The costs of token lengths and runtime are thoroughly discussed for all the methods. We believe the problem of steering LLM code/text generation is critical for future research and has much space for further improvement. Project Page, Datasets, and Codes are available at https://yongchao98.github.io/CodeSteer/.",
      "url": "https://www.microsoft.com/en-us/research/publication/steering-large-language-models-between-code-execution-and-textual-reasoning/"
    },
    {
      "title": "IrokoBench: A New Benchmark for African Languages in the Age of Large Language Models",
      "authors": [
        "Andiswa Bukula",
        "Blessing Sibanda",
        "Chiamaka Chukwuneke",
        "David Ifeoluwa Adelani",
        "En-Shiun Annie Lee",
        "Foutse Yuehgoh",
        "Godson Kalipe",
        "Happy Buzaaba",
        "Israel Abebe Azime",
        "Jessica Ojo",
        "Jesujoba O. Alabi",
        "Jian Yun Zhuang",
        "Jonathan Mukiibi",
        "Lolwethu Ndolela",
        "Millicent Ochieng",
        "Mmasibidi Setaka",
        "Nkiruka Odu",
        "Pontus Stenetorp",
        "Rooweither Mabuya",
        "Salomey Osei",
        "Salomon Kabongo",
        "Sara Hooker",
        "Shamsuddeen Hassan Muhammad",
        "Sokhar Samb",
        "Tadesse Kebede Guge",
        "Tombekai Vangoni Sherman",
        "Xuanli He"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Human language technologies"
      ],
      "publication_date": "April 2025",
      "abstract": "Despite the widespread adoption of Large language models (LLMs), their remarkable capabilities remain limited to a few high-resource languages. Additionally, many low-resource languages (\\eg African languages) are often evaluated only on basic text classification tasks due to the lack of appropriate or comprehensive benchmarks outside of high-resource languages. In this paper, we introduce IrokoBench — a human-translated benchmark dataset for 17 typologically-diverse low-resource African languages covering three tasks: natural language inference~(AfriXNLI), mathematical reasoning~(AfriMGSM), and multi-choice knowledge-based question answering~(AfriMMLU). We use IrokoBench to evaluate zero-shot, few-shot, and translate-test settings~(where test sets are translated into English) across 10 open and six proprietary LLMs. Our evaluation reveals a significant performance gap between high-resource languages~(such as English and French) and low-resource African languages. We observe a significant performance gap between open and proprietary models, with the highest performing open model, Gemma 2 27B only at 63\\% of the best-performing proprietary model GPT-4o performance. In addition, machine translating the test set to English before evaluation helped to close the gap for larger models that are English-centric, such as Gemma 2 27B and LLaMa 3.1 70B. These findings suggest that more efforts are needed to develop and adapt LLMs for African languages.",
      "url": "https://www.microsoft.com/en-us/research/publication/irokobench-a-new-benchmark-for-african-languages-in-the-age-of-large-language-models/"
    },
    {
      "title": "Boosting Large Language Model for Speech Synthesis: An Empirical Study",
      "authors": [
        "Furu Wei",
        "Hongkun Hao",
        "Jinyu Li",
        "Long Zhou",
        "Rui Wang",
        "Shujie Hu",
        "Shujie Liu"
      ],
      "research_areas": [
        "Human language technologies"
      ],
      "publication_date": "April 2025",
      "abstract": "Large language models (LLMs) have made significant advancements in natural language processing and are concurrently extending the language ability to other modalities, such as speech and vision. Nevertheless, most of the previous work focuses on prompting LLMs with perception abilities like auditory comprehension, and the effective approach for augmenting LLMs with speech synthesis capabilities remains ambiguous. In this paper, we conduct a comprehensive empirical exploration of boosting LLMs with the ability to generate speech, by combining pre-trained LLM LLaMA/OPT and text-to-speech synthesis model VALL-E. We compare three integration methods between LLMs and speech synthesis models, including directly fine-tuned LLMs, superposed layers of LLMs and VALL-E, and coupled LLMs and VALL-E using LLMs as a powerful text encoder. Experimental results show that, using LoRA method to fine-tune LLMs directly to boost the speech synthesis capability does not work well, and superposed LLMs and VALL-E can improve the quality of generated speech both in speaker similarity and word error rate (WER). Among these three methods, coupled methods leveraging LLMs as the text encoder can achieve the best performance, making it outperform original speech synthesis models with a consistently better speaker similarity and a significant (10.9%) WER reduction.",
      "url": "https://www.microsoft.com/en-us/research/publication/boosting-large-language-model-for-speech-synthesis-an-empirical-study/"
    },
    {
      "title": "ChatBench: From Static Benchmarks to Human-AI Evaluation",
      "authors": [
        "Ashton Anderson",
        "Jake Hofman",
        "Serina Chang"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Human-computer interaction"
      ],
      "publication_date": "April 2025",
      "abstract": "With the rapid adoption of LLM-based chatbots, there is a pressing need to evaluate what humans and LLMs can achieve together. However, standard benchmarks, such as MMLU, measure LLM capabilities in isolation (i.e., “AI-alone”). Here, we design and conduct a user study to convert MMLU questions into user-AI conversations, by seeding the user with the question and having them carry out a conversation with the LLM to answer their question. We release ChatBench (opens in new tab), a new dataset with AI-alone, user-alone, and user-AI data for 396 questions and two LLMs, including 144K answers and 7,336 user-AI conversations. We find that AI-alone accuracy fails to predict user-AI accuracy, with significant differences across multiple subjects (math, physics, and moral reasoning), and we analyze the user-AI conversations to provide insight into how they diverge from AI-alone benchmarks. Finally, we show that fine-tuning a user simulator on a subset of ChatBench improves its ability to estimate user-AI accuracies, increasing correlation on held-out questions by more than 20 points, creating possibilities for scaling interactive evaluation.",
      "url": "https://www.microsoft.com/en-us/research/publication/chatbench-from-static-benchmarks-to-human-ai-evaluation/"
    },
    {
      "title": "YES AND: A Generative AI Multi-Agent Framework for Enhancing Diversity of Thought in Individual Ideation for Problem-Solving Through Confidence-Based Agent Turn-Taking",
      "authors": [
        "Pratik Ghosh",
        "Sean Rintel"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Human-computer interaction",
        "Social sciences"
      ],
      "publication_date": "April 2025",
      "abstract": "Diversity of thought is crucial in ideation for problem-solving, yet professionals in organisational settings often face challenges such as limited access to varied expertise and resource constraints which hinder the ideation process. To address this issue, we propose YES AND, a Generative AI based multi-agent framework that simulates diverse perspectives through AI agents for ideation with a single user. Leveraging a unique confidence-based turn-taking model, these agents organically take turns as they build on ideas, pose clarification questions to the user for improved contextual understanding, and allow the user to interject and steer the conversation. Beyond addressing the limitations of traditional ideation, this framework offers a novel approach to leveraging Generative AI for ideation, moving away from the rigidity of pre-defined interaction rules towards a more dynamic and creative process that enables serendipitous development of ideas.",
      "url": "https://www.microsoft.com/en-us/research/publication/yes-and-a-generative-ai-multi-agent-framework-for-enhancing-diversity-of-thought-in-individual-ideation-for-problem-solving-through-confidence-based-agent-turn-taking/"
    },
    {
      "title": "Securing Public Cloud Networks with Efficient Role-based Micro-Segmentation",
      "authors": [
        "Kevin Hsieh*",
        "Ranveer Chandra",
        "Santiago Segarra",
        "Sathiya Kumaran Mani*",
        "Srikanth Kandula",
        "Yajie Zhou"
      ],
      "research_areas": [
        "Systems and networking"
      ],
      "publication_date": "April 2025",
      "abstract": "Securing network traffic within data centers is a critical and daunting challenge due to the increasing complexity and scale of modern public clouds. Micro-segmentation offers a promising solution by implementing fine-grained, workload-specific network security policies to mitigate potential attacks. However, the dynamic nature and large scale of deployments present significant obstacles in crafting precise security policies, limiting the practicality of this approach. To address these challenges, we introduce a novel system that efficiently processes vast volumes of network flow logs and effectively infers the roles of network endpoints. Our method integrates domain knowledge and communication patterns in a principled manner, facilitating the creation of micro-segmentation policies at a large scale. Evaluations with real-world deployment demonstrate that our solution significantly surpasses existing algorithms in role inference accuracy. We implement our solution as an end-to-end system and demonstrate that it is up to 21.5X more cost-efficient than Apache Flink, a widely used open-source stream processing system.\n \n(* = equal contributions)",
      "url": "https://www.microsoft.com/en-us/research/publication/securing-public-cloud-networks-with-efficient-role-based-micro-segmentation/"
    },
    {
      "title": "Node Similarities under Random Projections: Limits and Pathological Cases",
      "authors": [
        "Cassiano Becker",
        "Jennifer Neville",
        "Tvrtko Tadić"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Mathematics"
      ],
      "publication_date": "April 2025",
      "abstract": "Random Projections have been widely used to generate embeddings for various graph learning tasks due to their computational efficiency. The majority of applications have been justified through the Johnson-Lindenstrauss Lemma. In this paper, we take a step further and investigate how well dot product and cosine similarity are preserved by random projections when these are applied over the rows of the graph matrix. Our analysis provides new asymptotic and finite-sample results, identifies pathological cases, and tests them with numerical experiments. We specialize our fundamental results to a ranking application by computing the probability of random projections flipping the node ordering induced by their embeddings. We find that, depending on the degree distribution, the method produces especially unreliable embeddings for the dot product, regardless of whether the adjacency or the normalized transition matrix is used. With respect to the statistical noise introduced by random projections, we show that cosine similarity produces remarkably more precise approximations.",
      "url": "https://www.microsoft.com/en-us/research/publication/node-similarities-under-random-projections-limits-and-pathological-cases/"
    },
    {
      "title": "Efficient and Adaptive Diffusion Model Inference Through Lookup Table on Mobile Devices",
      "authors": [
        "Qipeng Wang",
        "Ruiqi Liu",
        "Shiqi Jiang",
        "Ting Cao",
        "Xuanzhe Liu",
        "Yifan Yang",
        "Yuanchun Li"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Systems and networking"
      ],
      "publication_date": "April 2025",
      "abstract": "Diffusion models have revolutionized image synthesis applications. Many studies focus on using approximate computation such as model quantization to reduce inference costs on mobile devices. However, due to their extensive model parameters and autoregressive inference fashion, the overhead of diffusion models remains high, which is challenging for mobile devices to handle. To reduce the inference overhead of diffusion models on mobile devices, we propose LUT-Diff, an algorithm-system co-design specifically tailored for mobile device diffusion model inference optimization. LUT-Diff optimizes using lookup tables and can efficiently generate a series of lookup table candidates for diffusion models without end-to-end training. During inference, LUT-Diff adaptively selects the best inference strategy based on the application/user’s latency budget. Additionally, LUT-Diff includes a parallel inference engine that rapidly completes model inference through CPU-GPU co-scheduling. Extensive experiments demonstrate that LUT-Diff can generate images comparable to the original model, with an up to 0.012 MSE in generated images. LUT-Diff can also achieve up to 9.1× inference acceleration and reduce the inference memory footprint by up to 70.9% compared to baseline methods. Moreover, LUT-Diff can save at least 3281× the learning cost of lookup tables.",
      "url": "https://www.microsoft.com/en-us/research/publication/efficient-and-adaptive-diffusion-model-inference-through-lookup-table-on-mobile-devices/"
    },
    {
      "title": "TeCoFeS: Text Column Featurization using Semantic Analysis",
      "authors": [
        "Ananya Singha",
        "Ashish Tiwari",
        "Chris Parnin",
        "Mukul Singh",
        "Sumit Gulwani",
        "Vu Le"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Data platforms and analytics",
        "Programming languages and software engineering"
      ],
      "publication_date": "April 2025",
      "abstract": "Extracting insights from text columns can be challenging and time-intensive. Existing methods for topic modeling and feature extraction are based on syntactic features and often overlook the semantics. We introduce the semantic text column featurization problem, and present a scalable approach for automatically solving it. We extract a small sample smartly, use a large language model (LLM) to label only the sample, and then lift the labeling to the whole column using text embeddings. We evaluate our approach by turning existing text classification benchmarks into semantic categorization benchmarks. Our approach performs better than baselines and naive use of LLMs.",
      "url": "https://www.microsoft.com/en-us/research/publication/tecofes-text-column-featurization-using-semantic-analysis/"
    },
    {
      "title": "Ultra-Low Latency Speech Enhancement -A Comprehensive Study",
      "authors": [
        "Haibin Wu",
        "Sebastian Braun"
      ],
      "research_areas": [
        "Audio and Acoustics"
      ],
      "publication_date": "April 2025",
      "abstract": "Speech enhancement models should meet very low latency requirements typically smaller than 5 ms for hearing assistive devices. While various low-latency techniques have been proposed, comparing these methods in a controlled setup using DNNs remains blank. Previous papers have variations in task, training data, scripts, and evaluation settings, which make fair comparison impossible. Moreover, all methods are tested on small, simulated datasets, making it difficult to fairly assess their performance in real-world conditions, which could impact the reliability of scientific findings. To address these issues, we comprehensively investigate various low-latency techniques using consistent training on large-scale data and evaluate with more relevant metrics on real-world data. Specifically, we explore the effectiveness of asymmetric windows, learnable windows, adaptive time domain filterbanks, and the future-frame prediction technique. Additionally, we examine whether increasing the model size can compensate for the reduced window size.",
      "url": "https://www.microsoft.com/en-us/research/publication/ultra-low-latency-speech-enhancement-a-comprehensive-study/"
    },
    {
      "title": "Tools for Thought: Research and Design for Understanding, Protecting, and Augmenting Human Cognition with Generative AI",
      "authors": [
        "Advait Sarkar",
        "Aniket Kittur",
        "Elena L. Glassman",
        "Gonzalo Ramos",
        "Hari Subramonyam",
        "Jessica He",
        "Lev Tankelevitch",
        "Majeed Kazemitabaar",
        "Mina Lee",
        "Srishti Palani",
        "Yvonne Rogers"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Human language technologies",
        "Human-computer interaction"
      ],
      "publication_date": "April 2025",
      "abstract": "CHI 2025 Workshop on Tools for Thought: Research and Design for Understanding, Protecting, and Augmenting Human Cognition with Generative AI.\nThe workshop is part of the ACM (Association of Computing Machinery) CHI conference on Human Factors in Computing Systems.\nThe workshop takes place Saturday, April 26, 2025 — 9AM-5:50PM JST — Yokohama, Japan.\nCHI takes place in Yokohama, Japan, from 26 April to 1 May 2025.\nWorkshop abstract: We invite researchers, designers, practitioners, and provocateurs to explore what it means to understand and shape the impact of Generative AI (GenAI) on human cognition. GenAI radically widens the scope and capability of automation for work, learning, and creativity. While impactful, it also changes workflows and the quality of thinking involved, raising questions about its effects on cognition, including critical thinking and learning. Yet, GenAI also offers opportunities for designing tools for thought that protect and augment cognition. Such systems provoke critical thinking, provide personalized tutoring, or enable novel ways of sensemaking, among other approaches. How does GenAI change workflows and human cognition? What are opportunities and challenges for designing GenAI systems that protect and augment human cognition? Which theories, perspectives, and methods are relevant? This workshop aims to develop a multidisciplinary community interested in exploring these questions to protect against the erosion, and fuel the augmentation, of human cognition using GenAI.\nWorkshop website: https://aka.ms/toolsforthoughtworkshop (opens in new tab)",
      "url": "https://www.microsoft.com/en-us/research/publication/tools-for-thought-research-and-design-for-understanding-protecting-and-augmenting-human-cognition-with-generative-ai/"
    },
    {
      "title": "Target word activity detector: An approach to obtain ASR word boundaries without lexicon",
      "authors": [
        "Eric Sun",
        "Jing Pan",
        "Jinyu Li",
        "Sunit Sivasankaran",
        "Yan Huang"
      ],
      "research_areas": [
        "Human language technologies"
      ],
      "publication_date": "April 2025",
      "abstract": "Obtaining word timestamp information from end-to-end (E2E) ASR models remains challenging due to the lack of explicit time alignment during training. This issue is further complicated in multilingual models. Existing methods, either rely on lexicons or introduce additional tokens, leading to scalability issues and increased computational costs. In this work, we propose a new approach to estimate word boundaries without relying on lexicons. Our method leverages word embeddings from sub-word token units and a pretrained ASR model, requiring only word alignment information during training. Our proposed method can scale-up to any number of languages without incurring any additional cost. We validate our approach using a multilingual ASR model trained on five languages and demonstrate its effectiveness against a strong baseline.",
      "url": "https://www.microsoft.com/en-us/research/publication/target-word-activity-detector-an-approach-to-obtain-asr-word-boundaries-without-lexicon/"
    },
    {
      "title": "Make Some Noise: Towards LLM audio reasoning and generation using sound tokens",
      "authors": [
        "Hannes Gamper",
        "Nebojsa Jojic",
        "Shivam Mehta"
      ],
      "research_areas": [
        "Audio and Acoustics"
      ],
      "publication_date": "April 2025",
      "abstract": "Integrating audio comprehension and generation into large language models (LLMs) remains challenging due to the continuous nature of audio and the resulting high sampling rates. Here, we introduce a novel approach that combines Variational Quantization with Conditional Flow Matching to convert audio into ultra-low bitrate discrete tokens of 0.23kpbs, allowing for seamless integration with text tokens in LLMs. We fine-tuned a pretrained text-based LLM using Low-Rank Adaptation (LoRA) to assess its effectiveness in achieving true multimodal capabilities, i.e., audio comprehension and generation. Our tokenizer outperforms a traditional VQ-VAE across various datasets with diverse acoustic events. Despite the substantial loss of fine-grained details through audio tokenization, our multimodal LLM trained with discrete tokens achieves competitive results in audio comprehension with state-of-the-art methods, though audio generation is poor. Our results highlight the need for larger, more diverse datasets and improved evaluation metrics to advance multimodal LLM performance.\nArchitecture of audio tokenizer containing frozen autoencoder follow by a causal encoder and a conditional flowmatching-based decoder with Diffusion Transformer to reconstruct representations from quantised vectors",
      "url": "https://www.microsoft.com/en-us/research/publication/make-some-noise-towards-llm-audio-reasoning-and-generation-using-sound-tokens/"
    },
    {
      "title": "Structural-Entropy-Based Sample Selection for Efficient and Effective Learning",
      "authors": [
        "Guozu Ma",
        "Jiangning Zhu",
        "Minzhi Lin",
        "Shixia Liu",
        "Tianchi Xie",
        "Wei Chen",
        "Weikai Yang"
      ],
      "research_areas": [
        "Algorithms",
        "Artificial intelligence",
        "Data platforms and analytics"
      ],
      "publication_date": "April 2025",
      "abstract": "Sample selection improves the efficiency and effectiveness of machine learning models by providing informative and representative samples. Typically, samples can be modeled as a sample graph, where nodes are samples and edges represent their similarities. Most existing methods are based on local information, such as the training difficulty of samples, thereby overlooking global  information, such as connectivity patterns. This oversight can result in suboptimal selection because global information is crucial for ensuring that the selected samples well represent the structural properties of the graph. To address this issue, we employ structural entropy to quantify global information and losslessly decompose it from the whole graph to individual nodes using the Shapley value. Based on the decomposition, we present Structural-Entropy-based sample Selection (SES), a method that integrates both global and local information to select informative and representative samples. SES begins by constructing a kNN-graph among samples based on their similarities. It then measures sample importance by combining structural entropy (global metric) with training difficulty (local metric). Finally, SES applies importance-biased blue noise sampling to select a set of diverse and representative samples. Comprehensive experiments in three learning scenarios—supervised learning, active learning, and continual learning— clearly demonstrate the effectiveness of our method.",
      "url": "https://www.microsoft.com/en-us/research/publication/structural-entropy-based-sample-selection-for-efficient-and-effective-learning/"
    },
    {
      "title": "LLM Assistance for Memory Safety",
      "authors": [
        "Akash Lal",
        "Aseem Rastogi",
        "Nausheen Mohammed",
        "Rahul Sharma",
        "Subhajit Roy"
      ],
      "research_areas": [
        "Programming languages and software engineering"
      ],
      "publication_date": "April 2025",
      "abstract": "Memory safety violations in low-level code, written in languages like C, continues to remain one of the major sources of software vulnerabilities. One method of removing such violations by construction is to port C code to a safe C dialect. Such dialects rely on programmer-supplied annotations to guarantee safety with minimal runtime overhead. This porting, however, is a manual process that imposes significant burden on the programmer and, hence, there has been limited adoption of this technique.\nThe task of porting not only requires inferring annotations, but may also need refactoring/rewriting of the code to make it amenable to such annotations. In this paper, we use Large Language Models (LLMs) towards addressing both these concerns. We show how to harness LLM capabilities to do complex code reasoning as well as rewriting of large codebases. We also present a novel framework for whole-program transformations that leverages lightweight static analysis to break the transformation into smaller steps that can be carried out effectively by an LLM. We implement our ideas in a tool called MSA that targets the Checked C dialect. We evaluate MSA on several micro-benchmarks, as well as real-world code ranging up to 20K lines of code. We showcase superior performance compared to a vanilla LLM baseline, as well as demonstrate improvement over a state-of-the-art symbolic (non-LLM) technique.",
      "url": "https://www.microsoft.com/en-us/research/publication/llm-assistance-for-memory-safety/"
    },
    {
      "title": "Fidelity of Cloud Emulators: The Imitation Game of Testing Cloud-based Software",
      "authors": [
        "Anna Mazhar",
        "Saad Sher Alam",
        "Suman Nath",
        "Tianyin Xu",
        "William Zheng",
        "Yinfang Chen"
      ],
      "research_areas": [
        "Programming languages and software engineering",
        "Systems and networking"
      ],
      "publication_date": "April 2025",
      "abstract": "Modern software projects have been increasingly using cloud services as important components. The cloud-based programming practice greatly simplifies software development by harvesting cloud benefits (e.g., high availability and elasticity). However, it imposes new challenges for software testing and analysis, due to opaqueness of cloud backends and monetary cost of invoking cloud services for continuous integration and deployment. As a result, cloud emulators are developed for offline development and testing, before online testing and deployment. This paper presents a systematic analysis of cloud emulators from the perspective of cloud-based software testing. Our goal is to (1) understand the discrepancies introduced by cloud emula- tion with regard to software quality assurance and deployment safety and (2) address inevitable gaps between emulated and real cloud services. The analysis results are concerning. Among 255 APIs of five cloud services from Azure and Amazon Web Services (AWS), we detected discrepant behavior between the emulated and real services in 94 (37%) of the APIs. These discrepancies lead to inconsistent testing results, threatening deployment safety, introducing false alarms, and creating debuggability issues. The root causes are diverse, including accidental implementation defects and essential emulation challenges. We discuss potential solutions and develop a practical mitigation technique to address discrepancies of cloud emulators for software testing.",
      "url": "https://www.microsoft.com/en-us/research/publication/fidelity-of-cloud-emulators-the-imitation-game-of-testing-cloud-based-software/"
    },
    {
      "title": "UFO: A UI-Focused Agent for Windows OS Interaction",
      "authors": [
        "Bo Qiao",
        "Chaoyun Zhang",
        "Dongmei Zhang",
        "Liqun Li",
        "Minghua Ma",
        "Qi Zhang",
        "Qingwei Lin 林庆维",
        "Saravan Rajmohan",
        "Shilin He",
        "Si Qin",
        "Xu Zhang",
        "Yu Kang"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Programming languages and software engineering",
        "Systems and networking"
      ],
      "publication_date": "April 2025",
      "abstract": "We introduce UFO, an innovative UI-Focused agent to fulfill user requests tailored to applications on Windows OS, harnessing the capabilities of GPT-Vision. UFO employs a dual-agent framework to meticulously observe and analyze the graphical user interface (GUI) and control information of Windows applications. This enables the agent to seamlessly navigate and operate within individual applications and across them to fulfill user requests, even when spanning multiple applications. The framework incorporates a control interaction module, facilitating action grounding without human intervention and enabling fully automated execution. Consequently, UFO transforms arduous and time-consuming processes into simple tasks achievable solely through natural language commands. We conducted testing of UFO across 9 popular Windows applications, encompassing a variety of scenarios reflective of users’ daily usage. The results, derived from both quantitative metrics and real-case studies, underscore the superior effectiveness of UFO in fulfilling user requests. To the best of our knowledge, UFO stands as the first UI agent specifically tailored for task completion within the Windows OS environment. The open-source code for UFO is available on this https URL.",
      "url": "https://www.microsoft.com/en-us/research/publication/ufo-a-ui-focused-agent-for-windows-os-interaction/"
    },
    {
      "title": "OG-RAG: Ontology-Grounded Retrieval-Augmented Generation For Large Language Models",
      "authors": [
        "Kartik Sharma",
        "Peeyush Kumar"
      ],
      "research_areas": [
        "Algorithms",
        "Artificial intelligence"
      ],
      "publication_date": "April 2025",
      "abstract": "This paper presents OG-RAG, an Ontology-Grounded Retrieval Augmented Generation method designed to enhance LLM-generated responses by anchoring retrieval processes in domain-specific ontologies. While LLMs are widely used for tasks like question answering and search, they struggle to adapt to specialized knowledge, such as industrial workflows or knowledge work, without expensive fine-tuning or sub-optimal retrieval methods. Existing retrieval-augmented models, such as RAG, offer improvements but fail to account for structured domain knowledge, leading to suboptimal context generation. Ontologies, which conceptually organize domain knowledge by defining entities and their interrelationships, offer a structured representation to address this gap. OG-RAG constructs a hypergraph representation of domain documents, where each hyperedge encapsulates clusters of factual knowledge grounded using domain-specific ontology. An optimization algorithm then retrieves the minimal set of hyperedges that constructs a precise, conceptually grounded context for the LLM. This method enables efficient retrieval while preserving the complex relationships between entities. OG-RAG applies to domains where fact-based reasoning is essential, particularly in tasks that require workflows or decision-making steps to follow predefined rules and procedures. These include industrial workflows in healthcare, legal, and agricultural sectors, as well as knowledge-driven tasks such as news journalism, investigative research, consulting and more. Our evaluations demonstrate that OG-RAG increases the recall of accurate facts by 55% and improves response correctness by 40% across four different LLMs. Additionally, OG-RAG enables 30% faster attribution of responses to context and boosts fact-based reasoning accuracy by 27% compared to baseline methods.",
      "url": "https://www.microsoft.com/en-us/research/publication/og-rag-ontology-grounded-retrieval-augmented-generation-for-large-language-models/"
    },
    {
      "title": "Concept Distillation from Strong to Weak Models via Hypotheses-to-Theories Prompting",
      "authors": [
        "Ashwin Srinivasan",
        "Cassiano Becker",
        "Ehi Nosakhare",
        "Emmanuel Aboah Boateng",
        "Kabir Walia",
        "Nabiha Asghar",
        "Soundararajan Srinivasan",
        "Victor Dibia"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Human language technologies"
      ],
      "publication_date": "April 2025",
      "abstract": "Hand-crafting high quality prompts to optimize the performance of language models is a complicated and labor-intensive process. Furthermore, when migrating to newer, smaller, or weaker models (possibly due to latency or cost gains), prompts need to be updated to re-optimize the task performance. We propose Concept Distillation (CD), an automatic prompt optimization technique for enhancing weaker models on complex tasks. CD involves: (1) collecting mistakes made by weak models with a base prompt (initialization), (2) using a strong model to generate reasons for these mistakes and create rules/concepts for weak models (induction), and (3) filtering these rules based on validation set performance and integrating them into the base prompt (deduction/verification). We evaluated CD on NL2Code and mathematical reasoning tasks, observing significant performance boosts for small and weaker language models. Notably, Mistral-7B’s accuracy on Multi-Arith increased by 20%, and Phi-3-mini-3.8B’s accuracy on HumanEval rose by 34%. Compared to other automated methods, CD offers an effective, cost-efficient strategy for improving weak models’ performance on complex tasks and enables seamless workload migration across different language models without compromising performance.",
      "url": "https://www.microsoft.com/en-us/research/publication/concept-distillation-from-strong-to-weak-models-via-hypotheses-to-theories-prompting/"
    },
    {
      "title": "“Put Your Hands Up”: How Joint Attention Is Initiated Between Blind Children And Their Sighted Peers",
      "authors": [
        "Cecily Morrison",
        "Martin Grayson"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Human-computer interaction",
        "Social sciences"
      ],
      "publication_date": "April 2025",
      "abstract": "Initiating joint attention (JA) is a fundamental first step in social interactions. In sighted individuals, it relies predominantly on visual cues, such as gaze and hand gestures. These features can reduce opportunities for blind and visually impaired (BVI) and sighted people to interact. Understanding the strategies to navigate these challenges is necessary to develop technology that can facilitate more inclusive JA. To address this, we conducted a longitudinal case study of five children with mixed visual abilities engaging in activities rich with JA opportunities. In a teacher-led classroom, the children experimented with the use of an AI-powered headset designed to support BVI people in social situations. Interaction analysis established that situational complexity affects the children’s responses to initiation attempts. Furthermore, the headset adds to this complexity, affecting the frequency and reactions to attempts to initiate JA. The findings informed the creation of a JA initiation framework and suggestions for future design.",
      "url": "https://www.microsoft.com/en-us/research/publication/put-your-hands-up-how-joint-attention-is-initiated-between-blind-children-and-their-sighted-peers/"
    },
    {
      "title": "V2SFlow: Video-to-Speech Generation with Speech Decomposition and Rectified Flow",
      "authors": [
        "Jeongsoo Choi",
        "Ji-Hoon Kim",
        "Jinyu Li",
        "Joon Son Chung",
        "Shujie Liu"
      ],
      "research_areas": [
        "Human language technologies"
      ],
      "publication_date": "April 2025",
      "abstract": "In this paper, we introduce V2SFlow, a novel Video-to-Speech (V2S) framework designed to generate natural and intelligible speech directly from silent talking face videos. While recent V2S systems have shown promising results on constrained datasets with limited speakers and vocabularies, their performance often degrades on real-world, unconstrained datasets due to the inherent variability and complexity of speech signals. To address these challenges, we decompose the speech signal into manageable subspaces (content, pitch, and speaker information), each representing distinct speech attributes, and predict them directly from the visual input. To generate coherent and realistic speech from these predicted attributes, we employ a rectified flow matching decoder built on a Transformer architecture, which models efficient probabilistic pathways from random noise to the target speech distribution. Extensive experiments demonstrate that V2SFlow significantly outperforms state-of-the-art methods, even surpassing the naturalness of ground truth utterances.",
      "url": "https://www.microsoft.com/en-us/research/publication/v2sflow-video-to-speech-generation-with-speech-decomposition-and-rectified-flow/"
    },
    {
      "title": "IRL Dittos: Embodied Multimodal AI Agent Interactions in Open Spaces",
      "authors": [
        "Asta Roseway",
        "Denae Ford",
        "Ed Cutrell",
        "John Tang",
        "Kori Inkpen",
        "Sasa Junuzovic",
        "Seonghee Lee"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Human-computer interaction"
      ],
      "publication_date": "April 2025",
      "abstract": "We introduce the In Real Life (IRL) Ditto, an AI-driven embodied agent designed to represent remote colleagues in shared office spaces, creating opportunities for real-time exchanges even in their absence. IRL Ditto offers a unique hybrid experience by allowing in-person colleagues to encounter a digital version of their remote teammates, initiating greetings, updates, or small talk as they might in person. Our research question examines: How can the IRL Ditto influence interactions and relationships among colleagues in a shared office space? Through a four-day study, we assessed IRL Ditto’s ability to strengthen social ties by simulating presence and enabling meaningful interactions across different levels of social familiarity. We find that enhancing social relationships depended deeply on the foundation of the relationship participants had with the source of the IRL Ditto. This study provides insights into the role of embodied agents in enriching workplace dynamics for distributed teams.",
      "url": "https://www.microsoft.com/en-us/research/publication/irl-dittos-embodied-multimodal-ai-agent-interactions-in-open-spaces/"
    },
    {
      "title": "Robust Root Cause Diagnosis using In-Distribution Interventions",
      "authors": [
        "Amit Sharma",
        "Ashutosh Srivastava",
        "Lokesh Nagalapatti",
        "Sunita Sarawagi"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "April 2025",
      "abstract": "Diagnosing the root cause of an anomaly in a complex interconnected system is a pressing problem in today’s cloud services and industrial operations. We propose In-Distribution Interventions (IDI), a novel algorithm that predicts root cause as nodes that meet two criteria: 1) Anomaly: root cause nodes should take on anomalous values; 2) Fix: had the root cause nodes assumed usual values, the target node would not have been anomalous. Prior methods of assessing the fix condition rely on counterfactuals inferred from a Structural Causal Model (SCM) trained on historical data. But since anomalies are rare and fall outside the training distribution, the fitted SCMs yield unreliable counterfactual estimates. IDI overcomes this by relying on interventional estimates obtained by solely probing the fitted SCM at in-distribution inputs. We present a theoretical analysis comparing and bounding the errors in assessing the fix condition using interventional and counterfactual estimates. We then conduct experiments by systematically varying the SCM’s complexity to demonstrate the cases where IDI’s interventional approach outperforms the counterfactual approach and vice versa. Experiments on both synthetic and PetShop RCD benchmark datasets demonstrate that IDI consistently identifies true root causes more accurately and robustly than nine existing state-of-the-art RCD baselines. Code will be released at https://github.com/nlokeshiisc/IDI_release (opens in new tab).",
      "url": "https://www.microsoft.com/en-us/research/publication/robust-root-cause-diagnosis-using-in-distribution-interventions/"
    },
    {
      "title": "The Impact of Generative AI on Critical Thinking: Self-Reported Reductions in Cognitive Effort and Confidence Effects From a Survey of Knowledge Workers",
      "authors": [
        "Advait Sarkar",
        "Hao-Ping (Hank) Lee",
        "Ian Drosos",
        "Lev Tankelevitch",
        "Nicholas Wilson",
        "Richard Banks",
        "Sean Rintel"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Human-computer interaction",
        "Social sciences"
      ],
      "publication_date": "April 2025",
      "abstract": "The rise of Generative AI (GenAI) in knowledge workflows raises questions about its impact on critical thinking skills and practices. We survey 319 knowledge workers to investigate 1) when and how they perceive the enaction of critical thinking when using GenAI, and 2) when and why GenAI affects their effort to do so. Participants shared 936 first-hand examples of using GenAI in work tasks. Quantitatively, when considering both task- and user-specific factors, a user’s task-specific self-confidence and confidence in GenAI are predictive of whether critical thinking is enacted and the effort of doing so in GenAI-assisted tasks. Specifically, higher confidence in GenAI is associated with less critical thinking, while higher self-confidence is associated with more critical thinking. Qualitatively, GenAI shifts the nature of critical thinking toward information verification, response integration, and task stewardship. Our insights reveal new design challenges and opportunities for developing GenAI tools for knowledge work.",
      "url": "https://www.microsoft.com/en-us/research/publication/the-impact-of-generative-ai-on-critical-thinking-self-reported-reductions-in-cognitive-effort-and-confidence-effects-from-a-survey-of-knowledge-workers/"
    },
    {
      "title": "Early Impacts of M365 Copilot",
      "authors": [
        "Alexia Cambon",
        "Eleanor Dillon",
        "Sida Peng",
        "Sonia Jaffe"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Economics"
      ],
      "publication_date": "April 2025",
      "abstract": "Advances in generative AI have rapidly expanded the potential of computers to perform or assist in a wide array of tasks traditionally performed by humans. We analyze a large, real-world randomized experiment of over 6,000 workers at 56 firms to present some of the earliest evidence on how these technologies are changing the way knowledge workers do their jobs. We find substantial time savings on common core tasks across a wide range of industries and occupations: workers who make use of this technology spent half an hour less reading email each week and completed documents 12% faster. Despite the newness of the technology, nearly 40% of workers who were given access to the tool used it regularly in their work throughout the 6-month study.",
      "url": "https://www.microsoft.com/en-us/research/publication/early-impacts-of-m365-copilot/"
    },
    {
      "title": "Are We On Track? AI-Assisted Active and Passive Goal Reflection During Meetings",
      "authors": [
        "Ava Elizabeth Scott",
        "Lev Tankelevitch",
        "Payod Panda",
        "Rishi Vanukuru",
        "Sean Rintel",
        "Xinyue Chen"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Human-computer interaction",
        "Social sciences"
      ],
      "publication_date": "April 2025",
      "abstract": "Meetings often suffer from a lack of intentionality, such as unclear goals and straying off-topic. Identifying goals and maintaining their clarity throughout a meeting is challenging, as discussions and uncertainties evolve. Yet meeting technologies predominantly fail to support meeting intentionality. AI-assisted reflection is a promising approach. To explore this, we conducted a technology probe study with 15 knowledge workers, integrating their real meeting data into two AI-assisted reflection probes: a passive and active design. Participants identified goal clarification as a foundational aspect of reflection. Goal clarity enabled people to assess when their meetings were off-track and reprioritize accordingly. Passive AI intervention helped participants maintain focus through non-intrusive feedback, while active AI intervention, though effective at triggering immediate reflection and action, risked disrupting the conversation flow. We identify three key design dimensions for AI-assisted reflection systems, and provide insights into design trade-offs, emphasizing the need to adapt intervention intensity and timing, balance democratic input with efficiency, and offer user control to foster intentional, goal-oriented behavior during meetings and beyond. KEYWORDS: videoconferencing, meeting, goal, intentionality, generative AI, probe, active, passive, intervention, interruption\n\nRELATED RESEARCH\n\n\nFormative studies\n\n\n\nMental Models of Meeting Goals: Supporting Intentionality in Meeting Technology\n\n\n\nPrototype studies\n\n\n\nBefore meetings: What Does Success Look Like? Catalyzing Meeting Intentionality with AI-Assisted Prospective Reflection\n\n\nBefore and during meetings: The CoExplorer Technology Probe: A Generative AI-Powered Adaptive Interface to Support Intentionality in Planning and Running Video Meetings\n\n\nDuring meetings: Are We On Track? AI-Assisted Active and Passive Goal Reflection During Meetings – Microsoft Research\n\n\nBetween meetings: Designing Interfaces that Support Temporal Work Across Meetings with Generative AI",
      "url": "https://www.microsoft.com/en-us/research/publication/are-we-on-track-ai-assisted-active-and-passive-goal-reflection-during-meetings/"
    },
    {
      "title": "An Empirical Study of Validating Synthetic Data for Formula Generation",
      "authors": [
        "Aditya Kanade",
        "Anirudh Khatry",
        "Gust Verbruggen",
        "J. Cambronero",
        "Mukul Singh",
        "Sumit Gulwani",
        "Usneek Singh",
        "Vu Le"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Data platforms and analytics",
        "Programming languages and software engineering"
      ],
      "publication_date": "April 2025",
      "abstract": "Large language models (LLMs) can be leveraged to help with writing formulas in spreadsheets, but resources on these formulas are scarce, impacting both the base performance of pre-trained models and limiting the ability to fine-tune them. Given a corpus of formulas, we can use a(nother) model to generate synthetic natural language utterances for fine-tuning. However, it is important to validate whether the NL generated by the LLM is indeed accurate to be beneficial for fine-tuning. In this paper, we provide empirical results on the impact of validating these synthetic training examples with surrogate objectives that evaluate the accuracy of the synthetic annotations. We demonstrate that validation improves performance over raw data across four models (2 open and 2 closed weight). Interestingly, we show that although validation tends to prune more challenging examples, it increases the complexity of problems that models can solve after being fine-tuned on validated data.",
      "url": "https://www.microsoft.com/en-us/research/publication/an-empirical-study-of-validating-synthetic-data-for-formula-generation/"
    },
    {
      "title": "Inference-Time Scaling for Complex Tasks: Where We Stand and What Lies Ahead",
      "authors": [
        "Besmira Nushi",
        "Jingya Chen",
        "John Langford",
        "Lingjiao Chen",
        "Neel Joshi",
        "Safoora Yousefi",
        "Shivam Garg",
        "Vibhav Vineet",
        "Vidhisha Balachandran",
        "Yash Lara",
        "Yue Wu"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "March 2025",
      "abstract": "Inference-time scaling can enhance the reasoning capabilities of large language models (LLMs) on complex problems that benefit from step-by-step problem solving. Although lengthening generated scratchpads has proven effective for mathematical tasks, the broader impact of this approach on other tasks remains less clear. In this work, we investigate the benefits and limitations of scaling methods across nine state-of-the-art models and eight challenging tasks, including math and STEM reasoning, calendar planning, NP-hard problems, navigation, and spatial reasoning. We compare conventional models (e.g., GPT-4o) with models fine-tuned for inference-time scaling (e.g., o1) through evaluation protocols that involve repeated model calls, either independently or sequentially with feedback. These evaluations approximate lower and upper performance bounds and potential for future performance improvements for each model, whether through enhanced training or multi-model inference systems. Our extensive empirical analysis reveals that the advantages of inference-time scaling vary across tasks and diminish as problem complexity increases. In addition, simply using more tokens does not necessarily translate to higher accuracy in these challenging regimes. Results from multiple independent runs with conventional models using perfect verifiers show that, for some tasks, these models can achieve performance close to the average performance of today’s most advanced reasoning models. However, for other tasks, a significant performance gap remains, even in very high scaling regimes. Encouragingly, all models demonstrate significant gains when inference is further scaled with perfect verifiers or strong feedback, suggesting ample potential for future improvements.",
      "url": "https://www.microsoft.com/en-us/research/publication/inference-time-scaling-for-complex-tasks-where-we-stand-and-what-lies-ahead/"
    },
    {
      "title": "Execution-guided within-prompt search for programming-by-example",
      "authors": [
        "Ashish Tiwari",
        "Gust Verbruggen",
        "Mukul Singh",
        "Sumit Gulwani",
        "Vu Le"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "April 2025",
      "abstract": "Large language models (LLMs) can generate code from examples without being\nlimited to a DSL, but they lack search, as sampled programs are independent. In\nthis paper, we use an LLM as a policy that generates lines of code and then join\nthese lines of code to let the LLM implicitly estimate the value of each of these\nlines in its next iteration. We further guide the policy and value estimation by\nexecuting each line and annotating it with its results on the given examples. This\nallows us to search for programs within a single (expanding) prompt until a sound\nprogram is found, by letting the policy reason in both the syntactic (code) and\nsemantic (execution) space. We evaluate within-prompt search on straight-line\nPython code generation using five benchmarks across different domains (strings,\nlists, and arbitrary Python programming problems). We show that the model uses\nthe execution results to guide the search and that within-prompt search performs\nwell at low token budgets. We also analyze how the model behaves as a policy and\nvalue, show that it can parallelize the search, and that it can implicitly backtrack\nover earlier generations",
      "url": "https://www.microsoft.com/en-us/research/publication/execution-guided-within-prompt-search-for-programming-by-example/"
    },
    {
      "title": "Performance Aware LLM Load Balancer for Mixed Workloads",
      "authors": [
        "A. Parayil",
        "Ankur Mallick",
        "Anoop Kulkarni",
        "Chetan Bansal",
        "Esha Choukse",
        "Jue Zhang",
        "Kunal Jain",
        "Rujia Wang",
        "Saravan Rajmohan",
        "Steve Kofsky",
        "Victor Ruehle",
        "Xiaoting Qin",
        "Íñigo Goiri"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "April 2025",
      "abstract": "Large Language Model (LLM) workloads consist of distinct prefill\nand decode phases, each with unique compute and memory requirements that should be considered when routing input queries across\ncluster instances. However, existing load-balancing algorithms treat\nthese workloads as monolithic jobs, ignoring the differences between the two phases. This oversight leads to suboptimal query\ndistribution and increased response latency. In our work, we first\ncharacterize the factors affecting response latency during LLM inference. We show that balancing inference requests across available\nLLM instances can improve end-to-end latency more than simply\noptimizing the instance-level scheduler. Motivated by these findings, we propose a heuristic-guided, reinforcement learning-based\nrouter for data-driven, workload-aware scheduling. Our router distributes queries across LLM instances by using a trainable responselength predictor and a novel formulation for estimating the impact\nof mixing different workloads, achieving over 11% lower end-toend latency than existing methods on mixed public datasets. Our\nframework represents a first step toward a holistic optimization\nframework and serves as a benchmark for deriving optimal load\nbalancing strategies tailored to different reward functions and requirements. Beyond latency, we can extend the proposed framework to optimize for various performance criteria ensuring that\nthe system meets diverse operational objectives.",
      "url": "https://www.microsoft.com/en-us/research/publication/performance-aware-llm-load-balancer-for-mixed-workloads/"
    },
    {
      "title": "HyperDrive: Direct Network Telemetry Storage via Programmable Switches",
      "authors": [
        "Dan R. K. Ports",
        "Jacob Nelson",
        "Lihua Yuan",
        "Peng Cheng",
        "Ran Shu",
        "Wenxue Cheng",
        "Yongqiang Xiong",
        "Zhixiong Niu",
        "Ziyuan Liu"
      ],
      "research_areas": [
        "Programming languages and software engineering"
      ],
      "publication_date": "March 2025",
      "abstract": "In cloud datacenter operations, telemetry and logs are indispensable, enabling essential services such as network diagnostics, auditing, and knowledge discovery. The escalating scale of data centers, coupled with increased bandwidth and finer-grained telemetry, results in an overwhelming volume of data. This proliferation poses significant storage challenges for telemetry systems. In this article, we introduce HyperDrive, an innovative system designed to efficiently store large volumes of telemetry and logs in data centers using programmable switches. This in-network approach effectively mitigates bandwidth bottlenecks commonly associated with traditional endpoint-based methods. To our knowledge, we are the first to use a programmable switch to directly control storage, bypassing the CPU to achieve the best performance. With merely 21% of a switch’s resources, our HyperDrive implementation showcases remarkable scalability and efficiency. Through rigorous evaluation, it has demonstrated linear scaling capabilities, efficiently managing 12 SSDs on a single server with minimal host overhead. In an eight-server testbed, HyperDrive achieved an impressive throughput of approximately 730 Gbps, underscoring its potential to transform data center telemetry and logging practices.",
      "url": "https://www.microsoft.com/en-us/research/publication/hyperdrive-direct-network-telemetry-storage-via-programmable-switches/"
    },
    {
      "title": "Exploring the Experiences of Individuals Who are Blind or Low-Vision Using Object-Recognition Technologies in India",
      "authors": [
        "Cecily Morrison"
      ],
      "research_areas": [
        "Computer vision",
        "Human-computer interaction"
      ],
      "publication_date": "April 2025",
      "abstract": "Assistive technologies, such as smartphone-based object-recognition (OR) apps, provide visual assistance to people who are blind or low vision to enable increased independent participation in society. While previous research has explored the functional accessibility of object-recognition technologies, little attention has been given to their social accessibility, particularly in interdependent sociocultural contexts of the Global South. Through a mixed-methods approach, employing a seven-day diary study followed by one-on one interviews with seven OR app users in India, we explore their experiences in depth. Our findings highlight the nuances of what interdependence looks like in a multicultural, Indian society, as people navigate public and private spheres with a camera-based assistive technology designed for independent, western contexts. We argue for the necessity to design assistive technologies following the interdependence framework that accommodates the social and cultural context of the Global South. Additionally, we propose design guidelines for assistive technologies in community-oriented societies, emphasizing community-centered approaches, cultural alignment, and locally adaptable designs.",
      "url": "https://www.microsoft.com/en-us/research/publication/exploring-the-experiences-of-individuals-who-are-blind-or-low-vision-using-object-recognition-technologies-in-india/"
    },
    {
      "title": "Taxonomizing Representational Harms using Speech Act Theory",
      "authors": [
        "Alex Chouldechova",
        "Alex Dow",
        "Chad Atalla",
        "Dan Vann",
        "Emily Corvi",
        "Emily Sheng",
        "Hanna Wallach",
        "Hannah Washington",
        "Jean Garcia-Gathright",
        "Matthew Vogel",
        "Nick Pangakis",
        "Stefanie Reed"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Social sciences"
      ],
      "publication_date": "March 2025",
      "abstract": "Representational harms are widely recognized among fairness-related harms caused by generative language systems. However, their definitions are commonly under-specified. We present a framework, grounded in speech act theory (Austin, 1962), that conceptualizes representational harms caused by generative language systems as the perlocutionary effects (i.e., real-world impacts) of particular types of illocutionary acts (i.e., system behaviors). Building on this argument and drawing on relevant literature from linguistic anthropology and sociolinguistics, we provide new definitions stereotyping, demeaning, and erasure. We then use our framework to develop a granular taxonomy of illocutionary acts that cause representational harms, going beyond the high-level taxonomies presented in previous work. We also discuss the ways that our framework and taxonomy can support the development of valid measurement instruments. Finally, we demonstrate the utility of our framework and taxonomy via a case study that engages with recent conceptual debates about what constitutes a representational harm and how such harms should be measured.",
      "url": "https://www.microsoft.com/en-us/research/publication/taxonomizing-representational-harms-using-speech-act-theory/"
    },
    {
      "title": "Using Life Cycle Assessment to Drive Innovation for Sustainable Cool Clouds",
      "authors": [
        "Alberto Arribas Herranz",
        "Ashish Raniwala",
        "Bharath Ramakrishnan",
        "Brijesh Warrier",
        "Christian Belady",
        "Husam Alissa",
        "Ioannis Manousakis",
        "Jim Kleewein",
        "Julie Sinistore",
        "Kali Frost",
        "Kari Lio",
        "Kathryn Oseen-Senda",
        "Lauren Johnson",
        "Madeline Frieze",
        "Marcus Fontoura",
        "Mukunth Natarajan",
        "Naval Gupta",
        "Praneet Arshi",
        "Ricardo Bianchini",
        "T. J. DiCaprio",
        "Teresa Nick",
        "Vaidehi Oruganti",
        "VeeAnder Mealing"
      ],
      "research_areas": [
        "Systems and networking"
      ],
      "publication_date": "April 2025",
      "abstract": "Addressing climate change requires accelerating the development of sustainable alternatives to energy- and water-intensive technologies, particularly for rapidly growing infrastructure such as data centres and cloud. Here we present a life cycle assessment study examining the impacts of advanced cooling technologies on cloud infrastructure, from virtual machines to server architecture, data centre buildings and the grid. Life cycle assessment is important for early-stage design decisions, enhancing sustainability outcomes alongside feasibility and cost analysis. We discuss constructing a life cycle assessment for a complex cloud ecosystem (including software, chips, servers and data centre buildings), analysing how different advanced cooling technologies interact with this ecosystem and evaluating each technology from a sustainability perspective to provide adoption guidelines. Life cycle assessment quantifies the benefits of advanced cooling methods, such as cold plates and immersion cooling, in reducing greenhouse gas emissions (15–21%), energy demand (15–20%) and blue water consumption (31–52%) in data centres. This comprehensive approach demonstrates the transformative potential of life cycle assessment in driving sustainable innovation across resource-intensive technologies.",
      "url": "https://www.microsoft.com/en-us/research/publication/using-life-cycle-assessment-to-drive-innovation-for-sustainable-cool-clouds/"
    },
    {
      "title": "RustAssistant: Using LLMs to Fix Compilation Errors in Rust Code",
      "authors": [
        "Akash Lal",
        "Aseem Rastogi",
        "Nikita Mehrotra",
        "Pantazis Deligiannis",
        "Rishi Poddar"
      ],
      "research_areas": [
        "Programming languages and software engineering"
      ],
      "publication_date": "April 2025",
      "abstract": "The Rust programming language, with its safety guarantees, has established itself as a viable choice for low-level systems programming language over the traditional, unsafe alternatives like C/C++. These guarantees come from a strong ownership-based type system, as well as primitive support for features like closures, pattern matching, etc., that make the code more concise and amenable to reasoning. These unique Rust features also pose a steep learning curve for programmers.\nThis paper presents a tool called RustAssistant that leverages the emergent capabilities of Large Language Models (LLMs) to automatically suggest fixes for Rust compilation errors. RustAssistant uses a careful combination of prompting techniques as well as iteration between an LLM and the Rust compiler to deliver high accuracy of fixes. RustAssistant is able to achieve an impressive peak accuracy of roughly 74% on real-world compilation errors in popular open-source Rust repositories. We also contribute a dataset of Rust compilation errors to enable further research.",
      "url": "https://www.microsoft.com/en-us/research/publication/rustassistant-using-llms-to-fix-compilation-errors-in-rust-code/"
    },
    {
      "title": "Toward deep learning sequence–structure co-generation for protein design",
      "authors": [
        "Ava P. Amini",
        "Carles Domingo-Enrich",
        "Chentong Wang",
        "Kevin Kaichuang Yang",
        "Sarah Alamdari"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Medical, health and genomics"
      ],
      "publication_date": "March 2025",
      "abstract": "Deep generative models that learn from the distribution of natural protein sequences and structures may enable the design of new proteins with valuable functions. While the majority of today’s models focus on generating either sequences or structures, emerging co-generation methods promise more accurate and controllable protein design, ideally achieved by modeling both modalities simultaneously. Here we review recent advances in deep generative models for protein design, with a particular focus on sequence-structure co-generation methods. We describe the key methodological and evaluation principles underlying these methods, highlight recent advances from the literature, and discuss opportunities for continued development of sequence-structure co-generation approaches.",
      "url": "https://www.microsoft.com/en-us/research/publication/toward-deep-learning-sequence-structure-co-generation-for-protein-design/"
    },
    {
      "title": "Ultra-high resolution and long-range OFDRs for characterizing and monitoring Hollow-core DNANFs",
      "authors": [
        "B. J. Puttnam",
        "D. J. Richardson",
        "D. T. Neilson",
        "E. Numkam Fokoua",
        "H. Chen",
        "L. Dallachiesa",
        "M. Mazur",
        "N. K. Fontaine",
        "R. Ryf",
        "R. Slavík",
        "S. Bakhtiari Gorajoobi"
      ],
      "research_areas": [
        "Systems and networking"
      ],
      "publication_date": "March 2025",
      "abstract": "We demonstrate distributed characterization of hollow-core DNANFs using two OFDR systems: the first reaches 5-km with sub-mm resolution and measures distributed modal birefringence, whilst the second probes over 100-km with 3-m (25-m) resolution at 10-km (100-km) and >90-dB dynamic range.",
      "url": "https://www.microsoft.com/en-us/research/publication/ultra-high-resolution-and-long-range-ofdrs-for-characterizing-and-monitoring-hollow-core-dnanfs/"
    },
    {
      "title": "Unrepeated HCF Transmission over spans up to 301.7 km",
      "authors": [
        "A. Ali",
        "B. Gholizadeh",
        "B. Guan",
        "B. J Puttnam",
        "C. Wallace",
        "D. J. Richardson",
        "E. Numkam Fokoua",
        "F. Rey",
        "J. Gaudette",
        "J. Hooley",
        "M. Alonso",
        "M. Kamalian-Kopae",
        "M. Tuggle",
        "S. Bakhtiari Gorajoobi",
        "S. Bawn",
        "S. Doran",
        "T. Pearson",
        "Y. Hong",
        "Y. Yin"
      ],
      "research_areas": [
        "Systems and networking"
      ],
      "publication_date": "March 2025",
      "abstract": "We transmit real-time data-signals over HCF span-lengths reaching 301.7 km using a novel high-power (37 dBm) HCF line-system, achieving full C-band system data-rate of 25.6 Tb/s after 200.5 km with negligible fiber transmission penalty, demonstrating the potential of HCFs to expand unamplified links.",
      "url": "https://www.microsoft.com/en-us/research/publication/unrepeated-hcf-transmission-over-spans-up-to-301-7-km/"
    },
    {
      "title": "DEDUCE: Deductive Consistency as a Frame Work to Evaluate LLM Reasoning",
      "authors": [
        "Amit Sharma",
        "Atharva Pandey",
        "Kshitij Dubey",
        "Rahul Sharma"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "March 2025",
      "abstract": "Despite great performance on Olympiad-level reasoning problems, frontier large language models can still struggle on high school math. We study the nature of language models’ (LM) reasoning by analyzing their chain-of-thought traces. To avoid memorization issues, we present a framework that can evaluate reasoning of LMs over novel, perturbed versions of benchmark problems. Formally, we compare LMs to ideal deductive reasoners that given a set of premises, can provide valid conclusions over any number of reasoning hops. To assess reasoning performance beyond final accuracy, we introduce deductive consistency, a metric that evaluates the correctness of system’s reasoning across varying input premise lengths and the number of solution hops. Using this metric, we examine potential explanations for language models’ failures on novel problems. Through experiments on GSM8K and a synthetic dataset, we find that the failure is not primarily due to shifts in language style or the propagation of early errors. Instead, it stems from a fundamental limitation: as the number of reasoning hops increases, language models exhibit a decline in deductive consistency, which was masked by memorization for existing benchmark problems. Our analysis provides a new view to characterize LM reasoning—as computations over a window of input premises and reasoning hops—that can provide unified evaluation across problem domains.",
      "url": "https://www.microsoft.com/en-us/research/publication/deduce-deductive-consistency-as-a-frame-work-to-evaluate-llm-reasoning/"
    },
    {
      "title": "My CXL Pool Obviates Your PCIe Switch",
      "authors": [
        "Antonis Psistakis",
        "Asaf Cidon",
        "Daniel S. Berger",
        "Enrique Saurez",
        "Jacob Nelson",
        "Joshua Fried",
        "P. Zardoshti",
        "Yuhong Zhong"
      ],
      "research_areas": [
        "Programming languages and software engineering"
      ],
      "publication_date": "March 2025",
      "abstract": "Pooling PCIe devices across multiple hosts offers a promising solution to mitigate stranded I/O resources, enhance device utilization, address device failures, and reduce total cost of ownership. The only viable option today are PCIe switches, which decouple PCIe devices from hosts by connecting them through a hardware switch. However, the high cost and limited flexibility of PCIe switches hinder their widespread adoption beyond specialized datacenter use cases. This paper argues that PCIe device pooling can be effectively implemented in software using CXL memory pools. CXL memory pools improve memory utilization and already have positive return on investment. We find that, once CXL pools are in place, they can serve as a building block for pooling any kind of PCIe device. We demonstrate that PCIe devices can directly use CXL memory as I/O buffers without device modifications, which enables routing PCIe traffic through CXL pool memory. This software-based approach is deployable on today’s hardware and is more flexible than hardware PCIe switches. In particular, we explore how disaggregating devices such as NICs can transform datacenter infrastructure.",
      "url": "https://www.microsoft.com/en-us/research/publication/my-cxl-pool-obviates-your-pcie-switch/"
    },
    {
      "title": "SNRAware: Improved Deep Learning MRI Denoising with SNR Unit Training and G-factor Map Augmentation",
      "authors": [
        "A. Campbell-Washburn",
        "Charlotte Manisty",
        "Hui Xue",
        "Iain Pierce",
        "James C. Moon",
        "John Stairs",
        "Joseph Naegele",
        "Michael Hansen",
        "Peter Kellman",
        "Rhodri H. Davies",
        "Sarah M. Hooper",
        "T. Treibel"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Medical, health and genomics"
      ],
      "publication_date": "March 2025",
      "abstract": "To develop and evaluate a new deep learning MR denoising method that leverages quantitative noise distribution information from the reconstruction process to improve denoising performance and generalization. This retrospective study trained 14 different transformer and convolutional models with two backbone architectures on a large dataset of 2,885,236 images from 96,605 cardiac retro-gated cine complex series acquired at 3T. The proposed training scheme, termed SNRAware, leverages knowledge of the MRI reconstruction process to improve denoising performance by simulating large, high quality, and diverse synthetic datasets, and providing quantitative information about the noise distribution to the model. In-distribution testing was performed on a hold-out dataset of 3000 samples with performance measured using PSNR and SSIM, with ablation comparison without the noise augmentation. Out-of-distribution tests were conducted on cardiac real-time cine, first-pass cardiac perfusion, and neuro and spine MRI, all acquired at 1.5T, to test model generalization across imaging sequences, dynamically changing contrast, different anatomies, and field strengths. The best model found in the in-distribution test generalized well to out-of-distribution samples, delivering 6.5x and 2.9x CNR improvement for real-time cine and perfusion imaging, respectively. Further, a model trained with 100% cardiac cine data generalized well to a T1 MPRAGE neuro 3D scan and T2 TSE spine MRI.\n ",
      "url": "https://www.microsoft.com/en-us/research/publication/snraware-improved-deep-learning-mri-denoising-with-snr-unit-training-and-g-factor-map-augmentation/"
    },
    {
      "title": "TUNA: Tuning Unstable and Noisy Cloud Applications",
      "authors": [
        "Brian Kroth",
        "Johannes Freischuetz",
        "Konstantinos Kanellis",
        "Shivaram Venkataraman"
      ],
      "research_areas": [
        "Systems and networking"
      ],
      "publication_date": "March 2025",
      "abstract": "Autotuning plays a pivotal role in optimizing the performance of systems, particularly in large-scale cloud deployments, and has been used to improve the performance of a number of systems including databases, key-value stores, and operating systems. We find that one of the main challenges in performing autotuning in the cloud arises from performance variability or noise in system measurements. We first investigate the extent to which noise slows down autotuning and find that as little as 5% noise can lead to a 2.5x slowdown in converging to the best-performing configuration We also measure the magnitude of noise in cloud computing settings and find that, while some components (CPU, disk) have almost no performance variability there are still sources of significant variability (caches, memory). Additionally, we find that variability leads to autotuning finding unstable configurations, where for some workloads as many as 63.3% of configurations selected as “best” during tuning can degrade by 30% or more when deployed. Using this as motivation, this paper proposes a novel approach to improve the efficiency of autotuning systems by (a) detecting and removing outlier configurations, and (b) using ML-based approaches to provide a more stable true signal of de-noised experiment results to the optimizer. The resulting system, TUNA (Tuning Unstable and Noisy Cloud Applications) enables faster convergence and robust configurations. We find that configurations learned using TUNA perform better and with lower standard deviations during deployment, as compared to traditional sampling methodologies. Tuning PostgreSQL running an enterprise production workload, we find that TUNA can lead to 1.88x lower running time on average with 2.58𝑥 lower standard deviation compared to traditional sampling methodologies.  TUNA will be incorporated into the MLOS (opens in new tab) project and has both artifacts (opens in new tab) and multiple (opens in new tab) datasets (opens in new tab) available.",
      "url": "https://www.microsoft.com/en-us/research/publication/tuna-tuning-unstable-and-noisy-cloud-applications/"
    },
    {
      "title": "debug-gym: A Text-Based Environment for Interactive Debugging",
      "authors": [
        "Alessandro Sordoni",
        "Charbel Feghali",
        "Chinmay Singh",
        "Darya Moldavskaya",
        "Drew MacPhee",
        "Lucas Caccia",
        "Marc-Alexandre Côté",
        "Matheus Pereira",
        "Minseon Kim",
        "Morgane M Moss",
        "Xingdi Yuan"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Programming languages and software engineering"
      ],
      "publication_date": "March 2025",
      "abstract": "Large Language Models (LLMs) are increasingly relied upon for coding tasks, yet in most scenarios it is assumed that all relevant information can be either accessed in context or matches their training data. We posit that LLMs can benefit from the ability to interactively explore a codebase to gather the information relevant to their task. To achieve this, we present a textual environment, namely debug-gym, for developing LLM-based agents in an interactive coding setting. Our environment is lightweight and provides a preset of useful tools, such as a Python debugger (pdb), designed to facilitate an LLM-based agent’s interactive debugging. Beyond coding and debugging tasks, this approach can be generalized to other tasks that would benefit from information-seeking behavior by an LLM agent.",
      "url": "https://www.microsoft.com/en-us/research/publication/debug-gym-a-text-based-environment-for-interactive-debugging/"
    },
    {
      "title": "Overcoming Vocabulary Mismatch: Vocabulary-agnostic Teacher Guided Language Modeling",
      "authors": [
        "Haebin Shin",
        "Lei Ji",
        "Xiao Liu",
        "Yeyun Gong"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "March 2025",
      "abstract": "Using large teacher models to guide the training of smaller student models has become the prevailing paradigm for efficient and effective learning. However, vocabulary mismatches between teacher and student language models pose significant challenges in language modeling, resulting in divergent token sequences and output distributions. To overcome these limitations, we propose Vocabulary-agnostic Teacher Guided Language Modeling (VocAgnoLM), a novel approach that bridges the gap caused by vocabulary mismatch through two key methods: (1) Token-level Lexical Alignment, which aligns token sequences across mismatched vocabularies, and (2) Teacher Guided Loss, which leverages the loss of teacher model to guide effective student training. We demonstrate its effectiveness in language modeling with 1B student model using various 7B teacher models with different vocabularies. Notably, with Qwen2.5-Math-Instruct, a teacher model sharing only about 6% of its vocabulary with TinyLlama, VocAgnoLM achieves a 46% performance improvement compared to naive continual pretraining. Furthermore, we demonstrate that VocAgnoLM consistently benefits from stronger teacher models, providing a robust solution to vocabulary mismatches in language modeling.",
      "url": "https://www.microsoft.com/en-us/research/publication/overcoming-vocabulary-mismatch-vocabulary-agnostic-teacher-guided-language-modeling/"
    },
    {
      "title": "Coach: Exploiting Temporal Patterns for All-Resource Oversubscription in Cloud Platforms",
      "authors": [
        "Abhisek Pan",
        "Benjamin Reidys",
        "Celine Irvene",
        "Chetan Bansal",
        "Daniel S. Berger",
        "Eugene Bak",
        "Haoran Ma",
        "Jian Huang",
        "Kapil Arya",
        "Karel Trueba",
        "Lisa Hsu",
        "Mehmet Iyigun",
        "Pantea Zardoshti",
        "Ricardo Bianchini",
        "Saravan Rajmohan",
        "Stanko Novakovic",
        "Taylor Stark",
        "Íñigo Goiri"
      ],
      "research_areas": [
        "Systems and networking"
      ],
      "publication_date": "March 2025",
      "abstract": "Cloud platforms remain underutilized despite multiple proposals to improve their utilization (e.g., disaggregation, harvesting, and oversubscription). Our characterization of the resource utilization of virtual machines (VMs) in Azure reveals that, while CPU is the main underutilized resource, we need to provide a solution to manage all resources holistically. We also observe that many VMs exhibit complementary temporal patterns, which can be leveraged to improve the oversubscription of underutilized resources.\nBased on these insights, we propose Coach: a system that exploits temporal patterns for all-resource oversubscription in cloud platforms. Coach uses long-term predictions and an efficient VM scheduling policy to exploit temporally complementary patterns. We introduce a new general-purpose VM type, called CoachVM, where we partition each resource allocation into a guaranteed and an oversubscribed portion. Coach monitors the oversubscribed resources to detect contention and mitigate any potential performance degradation. We focus on memory management, which is particularly challenging due to memory’s sensitivity to contention and the overhead required to reassign it between CoachVMs. Our experiments show that Coach enables platforms to host up to ~26% more VMs with minimal performance degradation.",
      "url": "https://www.microsoft.com/en-us/research/publication/coach-exploiting-temporal-patterns-for-all-resource-oversubscription-in-cloud-platforms/"
    },
    {
      "title": "Sandi: A System for Accountability and Applications in Direct Communication",
      "authors": [
        "Betül Durak",
        "Kim Laine",
        "Radames Cruz Moreno",
        "Simon Langowski"
      ],
      "research_areas": [
        "Security, privacy, and cryptography"
      ],
      "publication_date": "March 2025",
      "abstract": "We present a system, Sandi, for creating trust through accountability. Concretely, we focus on online communication scenarios, where the communicating parties do not know each other, yet would benefit from a degree of initial trust. Sandi can be seen as a reputation system that measures bad behavior, with strong integrity protections and resistance to manipulation. Unlike most reputation systems, Sandi is entirely based on “downvotes” and therefore requires strong privacy guarantees to prevent retaliation. It utilizes a ticket-based reporting mechanism to limit who can report. We also prove that Sandi incentivizes good behavior in a well-defined sense.\nSandi is by design unidirectional, so that message senders have Sandi scores and receivers can report them for inappropriate communication, but it is designed to benefit both senders and receivers. Senders benefit, as receivers are more likely to react to communication with the added trust signal. Receivers benefit from seeing senders’ scores, allowing them to make more informed decisions about which senders to trust.\nReceivers do not need registered accounts and neither senders nor receivers need long-term keys. Sandi guarantees score integrity, communication privacy, reporter privacy to protect reporting receivers, and sender unlinkability. Sandi can be implemented on top of any communication system that allows for small binary data transfer.",
      "url": "https://www.microsoft.com/en-us/research/publication/sandi-a-system-for-accountability-and-applications-in-direct-communication/"
    },
    {
      "title": "Probing the Limit of Heat Transfer in Inorganic Crystals with Deep Learning",
      "authors": [
        "Andrew Fowler",
        "Bing Lv",
        "Claudio Zeni",
        "Daniel Zugner",
        "Davide Donadio",
        "Guanzhi Li",
        "Haiguang Liu",
        "Han Yang",
        "Hongxia Hao",
        "Jielan Li",
        "Junfu Tan",
        "Matthew Horton",
        "Mingfa Tang",
        "Qian Wang",
        "Robert Pinsler",
        "Shuizhou Chen",
        "Tao Qin",
        "Tianyidan Xie",
        "Tiemin Liu",
        "Xixian Liu",
        "Yichi Zhou",
        "Yu Zhu",
        "Zekun Chen",
        "Ziheng Lu"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "March 2025",
      "abstract": "Heat transfer is a fundamental property of matter. Research spanning decades has attempted to discover materials with exceptional thermal conductivity, yet the upper limit remains unknown. Using deep learning accelerated crystal structure prediction and first-principles calculation, we systematically explore the thermal conductivity landscape of inorganic crystals. We brute-force over half a million ordered crystalline structures, encompassing an extensive coverage of local energy minima in binary compounds with up to four atoms per primitive cell. We confirm diamond sets the upper bound of thermal conductivity within our search space, very likely also among all stable crystalline solids at ambient conditions. We identify over 20 novel crystals with high thermal conductivity surpassing silicon at room temperature validated by density functional theory. These include a series of metallic compounds, especially MnV, exhibiting high lattice and electronic thermal conductivity simultaneously, a distinctive feature not observed before. The fast deep learning-driven screening method, as well as the large comprehensive thermal conductivity database, pave the way for the discovery and design of next-generation materials with tailored thermal properties.",
      "url": "https://www.microsoft.com/en-us/research/publication/probing-the-limit-of-heat-transfer-in-inorganic-crystals-with-deep-learning/"
    },
    {
      "title": "Re-Imagine: Symbolic Benchmark Synthesis for Reasoning Evaluation",
      "authors": [
        "Aditya Nori",
        "Amit Sharma",
        "Atharva Pandey",
        "Fabian Falck",
        "Javier González",
        "Kshitij Dubey",
        "Rachel Lawrence",
        "Rahul Sharma",
        "Risa Ueno",
        "Xinnuo Xu"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "March 2025",
      "abstract": "Recent Large Language Models (LLMs) have reported high accuracy on reasoning benchmarks. However, it is still unclear whether the observed results arise from true “reasoning” or from statistical recall of the training set. Inspired by the ladder of causation (Pearl, 2009) and its three levels (associations, interventions and counterfactuals), this paper introduces RE-IMAGINE: a framework to characterize a hierarchy of reasoning ability in LLMs, alongside an automated pipeline to generate problem variations at different levels of the hierarchy. By altering problems in an intermediate symbolic representation, RE-IMAGINE generates arbitrarily many problems that are not solvable using memorization alone. Moreover, the framework is general and can work across reasoning domains, including math, code, and logic. We demonstrate our framework on four widely-used benchmarks to evaluate several families of LLMs, and observe reductions in performance when the models are queried with problem variations. These assessments indicate a degree of reliance on statistical recall for past performance, and open the door to further research targeting skills across the reasoning hierarchy.",
      "url": "https://www.microsoft.com/en-us/research/publication/re-imagine-symbolic-benchmark-synthesis-for-reasoning-evaluation/"
    },
    {
      "title": "Search and Society: Reimagining Information Access for Radical Futures",
      "authors": [
        "Bhaskar Mitra"
      ],
      "research_areas": [
        "Search and information retrieval",
        "Social sciences"
      ],
      "publication_date": "March 2025",
      "abstract": "Information retrieval (IR) technologies and research are undergoing transformative changes. It is our perspective that the community should accept this opportunity to re-center our research agendas on societal needs while dismantling the artificial separation between the work on fairness, accountability, transparency, and ethics in IR and the rest of IR research. Instead of adopting a reactionary strategy of trying to mitigate potential social harms from emerging technologies, the community should aim to proactively set the research agenda for the kinds of systems we should build inspired by diverse explicitly-stated sociotechnical imaginaries. The sociotechnical imaginaries that underpin the design and development of information access technologies needs to be explicitly articulated, and we need to develop theories of change in context of these diverse perspectives. Our guiding future imaginaries must be informed by other academic fields, such as democratic theory and critical theory, and should be co-developed with social science scholars, legal scholars, civil rights and social justice activists, and artists, among others. In this perspective paper, we motivate why the community must consider this radical shift in how we do research and what we work on, and sketch a path forward towards this transformation.",
      "url": "https://www.microsoft.com/en-us/research/publication/search-and-society/"
    },
    {
      "title": "DistiLLM-2: A Contrastive Approach Boosts the Distillation of LLMs",
      "authors": [
        "Ilya Zharkov",
        "Jongwoo Ko",
        "Luming Liang",
        "SeYoung Yun",
        "Sungnyun Kim",
        "Tianyi Chen",
        "Tianyu Ding"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "March 2025",
      "abstract": "Despite the success of distillation in large language models (LLMs), most prior work applies identical loss functions to both teacher- and student-generated data. These strategies overlook the synergy between loss formulations and data types, leading to a suboptimal performance boost in student models. To address this, we propose DistiLLM-2, a contrastive approach that simultaneously increases the likelihood of teacher responses and decreases that of student responses by harnessing this synergy. Our extensive experiments show that DistiLLM-2 not only builds high-performing student models across a wide range of tasks, including instruction-following and code generation, but also supports diverse applications, such as preference alignment and vision-language extensions. These findings highlight the potential of a contrastive approach to enhance the efficacy of LLM distillation by effectively aligning teacher and student models across varied data types.",
      "url": "https://www.microsoft.com/en-us/research/publication/distillm-2-a-contrastive-approach-boosts-the-distillation-of-llms/"
    },
    {
      "title": "The Impact of Generative AI Coding Assistants on Developers Who Are Visually Impaired",
      "authors": [
        "Ben Hanrahan",
        "Claudia Flores-Saviaga",
        "Kashif Imteyaz",
        "Saiph Savage",
        "Steven Clarke"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Programming languages and software engineering"
      ],
      "publication_date": "March 2025",
      "abstract": "The rapid adoption of generative AI in software development has impacted the industry, yet its effects on developers with visual impairments remain largely unexplored. To address this gap, we used an Activity Theory framework to examine how developers with visual impairments interact with AI coding assistants. For this purpose, we conducted a study where developers who are visually impaired completed a series of programming tasks using a generative AI coding assistant. We uncovered that, while participants found the AI assistant beneficial and reported significant advantages, they also highlighted accessibility challenges. Specifically, the AI coding assistant often exacerbated existing accessibility barriers and introduced new challenges. For example, it overwhelmed users with an excessive number of suggestions, leading developers who are visually impaired to express a desire for “AI timeouts.” Additionally, the generative AI coding assistant made it more difficult for developers to switch contexts between the AI-generated content and their own code. Despite these challenges, participants were optimistic about the potential of AI coding assistants to transform the coding experience for developers with visual impairments. Our findings emphasize the need to apply activity-centered design principles to generative AI assistants, ensuring they better align with user behaviors and address specific accessibility needs. This approach can enable the assistants to provide more intuitive, inclusive, and effective experiences, while also contributing to the broader goal of enhancing accessibility in software development.",
      "url": "https://www.microsoft.com/en-us/research/publication/the-impact-of-generative-ai-coding-assistants-on-developers-who-are-visually-impaired/"
    },
    {
      "title": "A Unifying Framework for Representation Learning",
      "authors": [
        "Axel Feldmann",
        "John R. Hershey",
        "Mark Hamilton",
        "Shaden Naif Alshammari",
        "William T. Freeman"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "March 2025",
      "abstract": "As the field of representation learning grows, there has been a proliferation of different loss functions to solve different classes of problems. We introduce a single information-theoretic equation that generalizes a large collection of modern loss functions in machine learning. In particular, we introduce a framework that shows that several broad classes of machine learning methods are precisely minimizing an integrated KL divergence between two conditional distributions: the supervisory and learned representations. This viewpoint exposes a hidden information geometry underlying clustering, spectral methods, dimensionality re- duction, contrastive learning, and supervised learning. This framework enables the development of new loss functions by combining successful techniques from across the literature. We not only present a wide array of proofs, connecting over 23 different approaches, but we also leverage these theoretical results to create state-of-the-art unsupervised image classifiers that achieve a +8% improvement over the prior state-of-the-art on unsupervised classification on ImageNet-1K. We also demonstrate that I-Con can be used to derive principled debiasing methods which improve contrastive representation learners.",
      "url": "https://www.microsoft.com/en-us/research/publication/a-unifying-framework-for-representation-learning/"
    },
    {
      "title": "Plan*RAG: Efficient Test-Time Planning for Retrieval Augmented Generation",
      "authors": [
        "Amit Sharma",
        "Arno Solin",
        "Gaurav Sinha",
        "Nagarajan Natarajan",
        "Prakhar Verma",
        "Sukruta Prakash Midigeshi"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "March 2025",
      "abstract": "We introduce Plan*RAG, a novel framework that enables structured multi-hop reasoning in retrieval-augmented generation (RAG) through test-time reasoning plan generation. While existing approaches such as ReAct maintain reasoning chains within the language model’s context window, we observe that this often leads to plan fragmentation and execution failures. Our key insight is that by isolating the reasoning plan as a directed acyclic graph (DAG) outside the LM’s working memory, we can enable (1) systematic exploration of reasoning paths, (2) atomic subqueries enabling precise retrievals and grounding, and (3) efficiency through parallel execution and bounded context window utilization. Moreover, Plan*RAG’s modular design allows it to be integrated with existing RAG methods, thus providing a practical solution to improve current RAG systems. On standard multi-hop reasoning benchmarks, Plan*RAG consistently achieves improvements over recently proposed methods such as RQ-RAG and Self-RAG, while maintaining comparable computational costs.",
      "url": "https://www.microsoft.com/en-us/research/publication/planrag-efficient-test-time-planning-for-retrieval-augmented-generation/"
    },
    {
      "title": "Reasoning Elicitation in Language Models via Counterfactual Feedback",
      "authors": [
        "Aditya Nori",
        "Alihan Hüyük",
        "Jacqueline Maasch",
        "Javier González",
        "Xinnuo Xu"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "March 2025",
      "abstract": "Despite the increasing effectiveness of language models, their reasoning capabilities remain underdeveloped. In particular, causal reasoning through counterfactual question answering is lacking. This work aims to bridge this gap. We first derive novel metrics that balance accuracy in factual and counterfactual questions, capturing a more complete view of the reasoning abilities of language models than traditional factual-only based metrics. Second, we propose several fine-tuning approaches that aim to elicit better reasoning mechanisms, in the sense of the proposed metrics. Finally, we evaluate the performance of the fine-tuned language models in a variety of realistic scenarios. In particular, we investigate to what extent our fine-tuning approaches systemically achieve better generalization with respect to the base models in several problems that require, among others, inductive and deductive reasoning capabilities.",
      "url": "https://www.microsoft.com/en-us/research/publication/reasoning-elicitation-in-language-models-via-counterfactual-feedback/"
    },
    {
      "title": "Teaching Transformers Causal Reasoning through Axiomatic Training",
      "authors": [
        "Abbavaram Gowtham Reddy",
        "Abhinav Kumar",
        "Amit Sharma",
        "Aniket Vashishtha",
        "Kabir Ahuja",
        "Vineeth N. Balasubramanian"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "March 2025",
      "abstract": "For text-based AI systems to interact in the real world, causal reasoning is an essential skill. Since interventional data is costly to generate, we study to what extent an agent can learn causal reasoning from passive data. Specifically, we consider an axiomatic training setup where an agent learns from multiple demonstrations of a causal axiom (or rule), rather than incorporating the axiom as an inductive bias or inferring it from data values. A key question is whether the agent would learn to generalize from the axiom demonstrations to new scenarios. For example, if a transformer model is trained on demonstrations of the causal transitivity axiom over small graphs, would it generalize to applying the transitivity axiom over large graphs? Our results, based on a novel axiomatic training scheme, indicate that such generalization is possible. We consider the task of inferring whether a variable causes another variable, given a causal graph structure. We find that a 67 million parameter transformer model, when trained on linear causal chains (along with some noisy variations) can generalize well to new kinds of graphs, including longer causal chains, causal chains with reversed order, and graphs with branching; even when it is not explicitly trained for such settings. Our model performs at par (or even better) than many larger language models such as GPT-4, Gemini Pro, and Phi-3. Overall, our axiomatic training framework provides a new paradigm of learning causal reasoning from passive data that can be used to learn arbitrary axioms, as long as sufficient demonstrations can be generated.",
      "url": "https://www.microsoft.com/en-us/research/publication/teaching-transformers-causal-reasoning-through-axiomatic-training/"
    },
    {
      "title": "3D Optical Data Storage of Glass for Sustainable Cloud Archival Storage (JSAP)",
      "authors": [
        "Andromachi Chatzieleftheriou",
        "Ant Rowstron",
        "Ariel Gomez Diaz",
        "Austin Donnelly",
        "Benn Thomsen",
        "Burcu Canakci",
        "Charles Whittaker",
        "Christos Gkantsidis",
        "Daniel Cletheroe",
        "David Sweeney",
        "Erika B. Aranas",
        "Freddie Hong",
        "Hugh Williams",
        "Ioan Stefanovici",
        "James Clegg",
        "Masaaki Sakakura",
        "Pashmina Cameron",
        "Patrick Anderson",
        "Richard Black",
        "Rokas Drevinskas",
        "Sergey Legtchenko",
        "Stefan Winzeck",
        "Takashi Lawson",
        "Tim Deegan",
        "Valentin Kapitany"
      ],
      "research_areas": [
        "Systems and networking"
      ],
      "publication_date": "March 2025",
      "abstract": "Three-dimensional data storage technology of glass using ultrashort pulse lasers is expected to be applied for long-term information preservation due to its high data capacity and the high durability of the recording medium. This paper introduces our end-to-end research to achieve sustainable and cost-effective cloud archive storage based on the data storage of glass, especially the development of data writing technology including massive improvement of the write throughput, energy efficiency and data capacity.",
      "url": "https://www.microsoft.com/en-us/research/publication/3d-optical-data-storage-of-glass-for-sustainable-cloud-archival-storage-jsap/"
    },
    {
      "title": "Scalable Universal T-Cell Receptor Embeddings from Adaptive Immune Repertoires",
      "authors": [
        "Elon Portugaly",
        "H. Jabran Zahid",
        "Ilker Demirel",
        "Javier Zazo",
        "Julia Greissl",
        "Lorenzo Pisani",
        "Paidamoyo Chapfuwa"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Medical, health and genomics"
      ],
      "publication_date": "March 2025",
      "abstract": "T cells are a key component of the adaptive immune system, targeting infections, cancers, and allergens with specificity encoded by their T cell receptors (TCRs), and retaining a memory of their targets. High-throughput TCR repertoire sequencing captures a cross-section of TCRs that encode the immune history of any subject, though the data are heterogeneous, high dimensional, sparse, and mostly unlabeled. Sets of TCRs responding to the same antigen, i.e., a protein fragment, co-occur in subjects sharing immune genetics and exposure history. Here, we leverage TCR co-occurrence across a large set of TCR repertoires and employ the GloVe (Pennington et al., 2014) algorithm to derive low-dimensional, dense vector representations (embeddings) of TCRs. We then aggregate these TCR embeddings to generate subject-level embeddings based on observed subject-specific TCR subsets. Further, we leverage random projection theory to improve GloVe’s computational efficiency in terms of memory usage and training time. Extensive experimental results show that TCR embeddings targeting the same pathogen have high cosine similarity, and subject-level embeddings encode both immune genetics and pathogenic exposure history.",
      "url": "https://www.microsoft.com/en-us/research/publication/scalable-universal-t-cell-receptor-embeddings-from-adaptive-immune-repertoires/"
    },
    {
      "title": "Overreliance risk identification and mitigation framework",
      "authors": [
        "Mihaela Vorvoreanu",
        "Ruth Kikin-Gil",
        "Samir Passi"
      ],
      "research_areas": [
        "Human-computer interaction"
      ],
      "publication_date": "March 2025",
      "abstract": "The Overreliance risk identification and mitigation framework guides product teams through 1) a series of questions meant to help them identify and characterize what the risk of overreliance looks like in their particular product or feature; 2) three UX goals to accomplish in order to foster appropriate reliance on genAI, with associated strategies, examples, and UX research evaluation questions.\n \nCite as:\n \n\nPassi, S., Vorvoreanu, M., & Kikin-Gil, R. (2025). Overreliance risk identification and mitigation framework. Microsoft Technical Report MSR-TR-2025-26. https://www.microsoft.com/en-us/research/publication/overreliance-risk-identification-and-mitigation-framework/\n",
      "url": "https://www.microsoft.com/en-us/research/publication/overreliance-risk-identification-and-mitigation-framework/"
    },
    {
      "title": "Overview of the TREC 2024 Tip-of-the-Tongue Track",
      "authors": [
        "Bhaskar Mitra",
        "Evangelos Kanoulas",
        "Fernando Diaz",
        "Jaime Arguello",
        "Samarth Bhargav",
        "To Eun Kim",
        "Yifan He"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Search and information retrieval"
      ],
      "publication_date": "March 2025",
      "abstract": "Tip-of-the-tongue (ToT) known-item retrieval involves re-finding an item for which the searcher does not reliably recall an identifier. ToT information requests (or queries) are verbose and tend to include several complex phenomena, making them especially difficult for existing information retrieval systems. The TREC 2024 ToT track focused on a single ad-hoc retrieval task. Participants were provided with training and development data in the movie domain. Conversely, systems were tested on data that combined three domains: movies, celebrities, and landmarks. This year, 6 groups (including the track coordinators) submitted 18 runs.",
      "url": "https://www.microsoft.com/en-us/research/publication/overview-of-the-trec-2024-tip-of-the-tongue-track/"
    },
    {
      "title": "Anyprefer: An Agentic Framework for Preference Data Synthesis",
      "authors": [
        "Bo Li",
        "Chetan Bansal",
        "Huaxiu Yao",
        "Kaiyuan Zheng",
        "Mohit Bansal",
        "Peng Xia",
        "Shangyu Xing",
        "Tianle Wang",
        "Weitong Zhang",
        "Wenhao Zheng",
        "Xuchao Zhang",
        "Ying Wei",
        "Yiyang Zhou",
        "Zhaorun Chen",
        "Zhaoyang Wang",
        "Zijian Zhang"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "March 2025",
      "abstract": "High-quality preference data is essential for aligning foundation models with human values through preference learning. However, manual annotation of such data is often time-consuming and costly. Recent methods often adopt a self-rewarding approach, where the target model generates and annotates its own preference data, but this can lead to inaccuracies since the reward model shares weights with the target model, thereby amplifying inherent biases. To address these issues, we propose Anyprefer, a framework designed to synthesize high-quality preference data for aligning the target model. Anyprefer frames the data synthesis process as a cooperative two-player Markov Game, where the target model and the judge model collaborate together. Here, a series of external tools are introduced to assist the judge model in accurately rewarding the target model’s responses, mitigating biases in the rewarding process. In addition, a feedback mechanism is introduced to optimize prompts for both models, enhancing collaboration and improving data quality. The synthesized data is compiled into a new preference dataset, Anyprefer-V1, consisting of 58K high-quality preference pairs. Extensive experiments show that Anyprefer significantly improves model alignment performance across four main applications, covering 21 datasets, achieving average improvements of 18.55% in five natural language generation datasets, 3.66% in nine vision-language understanding datasets, 30.05% in three medical image analysis datasets, and 16.00% in four visuo-motor control tasks.",
      "url": "https://www.microsoft.com/en-us/research/publication/anyprefer-an-agentic-framework-for-preference-data-synthesis/"
    },
    {
      "title": "Nature Language Model: Deciphering the Language of Nature for Scientific Discovery",
      "authors": [
        "Chang Liu",
        "Chen Hu",
        "Chuan Cao",
        "Guoqing Liu",
        "Hai-Long Liu",
        "Han Yang",
        "Hongxia Hao",
        "Houtian Zhu",
        "Jia Zhang",
        "Jian-Bo Zhu",
        "Jielan Li",
        "Kaiyuan Gao",
        "Ke-Ming Wu",
        "Krzysztof Maziarz",
        "Liang He",
        "Lijun Wu",
        "Marwin H. S. Segler",
        "Mingqian Ma",
        "Pan Deng",
        "Peggy Dai",
        "Peiran Jin",
        "Qian Wang",
        "Qizhi Pei",
        "Renqian Luo",
        "Shufang Xie",
        "Shuxin Zheng",
        "Tao Qin",
        "Tian Xie",
        "Tiemin Liu",
        "Wei Zhang",
        "Xixian Liu",
        "Yanting Li",
        "Yaosen Min",
        "Yeqi Bai",
        "Yeqing Lu",
        "Yingce Xia",
        "Yu Shi",
        "Yuan Chen",
        "Yuan-Jyue Chen",
        "Yue Wang",
        "Zekun Guo",
        "Zequn Liu",
        "Zhao Yang",
        "Zi-wei Chen",
        "Zi‐Ang Lu",
        "Zun Wang"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "March 2025",
      "abstract": "Foundation models have revolutionized natural language processing and artificial intelligence, significantly enhancing how machines comprehend and generate human languages. Inspired by the success of these foundation models, researchers have developed foundation models for individual scientific domains, including small molecules, materials, proteins, DNA, RNA and even cells. However, these models are typically trained in isolation, lacking the ability to integrate across different scientific domains. Recognizing that entities within these domains can all be represented as sequences, which together form the”language of nature”, we introduce Nature Language Model (NatureLM), a sequence-based science foundation model designed for scientific discovery. Pre-trained with data from multiple scientific domains, NatureLM offers a unified, versatile model that enables various applications including: (i) generating and optimizing small molecules, proteins, RNA, and materials using text instructions; (ii) cross-domain generation/design, such as protein-to-molecule and protein-to-RNA generation; and (iii) top performance across different domains, matching or surpassing state-of-the-art specialist models. NatureLM offers a promising generalist approach for various scientific tasks, including drug discovery (hit generation/optimization, ADMET optimization, synthesis), novel material design, and the development of therapeutic proteins or nucleotides. We have developed NatureLM models in different sizes (1 billion, 8 billion, and 46.7 billion parameters) and observed a clear improvement in performance as the model size increases.",
      "url": "https://www.microsoft.com/en-us/research/publication/nature-language-model-deciphering-the-language-of-nature-for-scientific-discovery/"
    },
    {
      "title": "Fostering appropriate reliance on GenAI: Lessons learned from early research",
      "authors": [
        "Amy Heger",
        "Kathleen Walker",
        "Mihaela Vorvoreanu",
        "Samir Passi",
        "Shipi Dhanorkar"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "March 2025",
      "abstract": "In this report, we summarize lessons learned from our work on fostering appropriate reliance on AI. We derived three UX goals for fostering appropriate reliance on AI from the barriers to appropriate reliance observed in multiple studies. These three UX goals inform the Overreliance Risk Identification and Mitigation Framework (opens in new tab), which guides AI builders through the same process we used to tackle overreliance on AI in various products.\nOverreliance on AI is a complex phenomenon, which we explained in our previous syntheses of research literature. Existing research makes clear that overreliance mitigations can backfire. Therefore, it is essential to test their effectiveness with user research. In this report, we also share tips for user researchers to identify and assess overreliance on AI and the effectiveness of mitigations\n \nCite as: Mihaela Vorvoreanu, Samir Passi, Shipi Dhanorkar, Amy Heger, & Kathleen Walker. 2025. Fostering appropriate reliance on GenAI: Lessons learned from early research. Microsoft Technical Report MSR-TR-2025-4. Microsoft Corporation.",
      "url": "https://www.microsoft.com/en-us/research/publication/fostering-appropriate-reliance-on-genai-lessons-learned-from-early-research/"
    },
    {
      "title": "Xavier: Toward Better Coding Assistance in Authoring Tabular Data Wrangling Scripts",
      "authors": [
        "Di Weng",
        "Haotian Li",
        "Huamin Qu",
        "Qiming Shi",
        "Xiwen Cai",
        "Yanwei Huang",
        "Yingcai Wu",
        "Yunfan Zhou"
      ],
      "research_areas": [
        "Data platforms and analytics",
        "Human-computer interaction"
      ],
      "publication_date": "March 2025",
      "abstract": "Data analysts frequently employ code completion tools in writing custom scripts to tackle complex tabular data wrangling tasks. However, existing tools do not sufficiently link the data contexts such as schemas and values with the code being edited. This not only leads to poor code suggestions, but also frequent interruptions in coding processes as users need additional code to locate and understand relevant data. We introduce Xavier, a tool designed to enhance data wrangling script authoring in computational notebooks. Xavier maintains users’ awareness of data contexts while providing data-aware code suggestions. It automatically highlights the most relevant data based on the user’s code, integrates both code and data contexts for more accurate suggestions, and instantly previews data transformation results for easy verification. To evaluate the effectiveness and usability of Xavier, we conducted a user study with 16 data analysts, showing its potential to streamline data wrangling scripts authoring.",
      "url": "https://www.microsoft.com/en-us/research/publication/xavier-toward-better-coding-assistance-in-authoring-tabular-data-wrangling-scripts/"
    },
    {
      "title": "Compositional Causal Reasoning Evaluation in Language Models",
      "authors": [
        "Aditya Nori",
        "Alihan Hüyük",
        "Jacqueline Maasch",
        "Javier González",
        "Xinnuo Xu"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "March 2025",
      "abstract": "Causal reasoning and compositional reasoning are two core aspirations in AI. Measuring the extent of these behaviors requires principled evaluation methods. We explore a unified perspective that considers both behaviors simultaneously, termed compositional causal reasoning (CCR): the ability to infer how causal measures compose and, equivalently, how causal quantities propagate through graphs. We instantiate a framework for the systematic evaluation of CCR for the average treatment effect and the probability of necessity and sufficiency. As proof of concept, we demonstrate CCR evaluation for language models in the LLama, Phi, and GPT families. On a math word problem, our framework revealed a range of taxonomically distinct error patterns. CCR errors increased with the complexity of causal paths for all models except o1.",
      "url": "https://www.microsoft.com/en-us/research/publication/compositional-causal-reasoning-evaluation-in-language-models/"
    },
    {
      "title": "BRIDGE: Bootstrapping Text to Control Time-Series Generation via Multi-Agent Iterative Optimization and Diffusion Modelling",
      "authors": [
        "Chang Xu",
        "Goran Nenadic",
        "Hao Li",
        "Jiang Bian",
        "R. Batista-Navarro",
        "Ren-He Jiang",
        "Viktor Schlegel",
        "Yu-Hao Huang"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "March 2025",
      "abstract": "Time-series Generation (TSG) is a prominent research area with broad applications in simulations, data augmentation, and counterfactual analysis. While existing methods have shown promise in unconditional single-domain TSG, real-world applications demand for cross-domain approaches capable of controlled generation tailored to domain-specific constraints and instance-level requirements. In this paper, we argue that text can provide semantic insights, domain information and instance-specific temporal patterns, to guide and improve TSG. We introduce “Text-Controlled TSG”, a task focused on generating realistic time series by incorporating textual descriptions. To address data scarcity in this setting, we propose a novel LLM-based Multi-Agent framework that synthesizes diverse, realistic text-to-TS datasets. Furthermore, we introduce BRIDGE, a hybrid text-controlled TSG framework that integrates semantic prototypes with text description for supporting domain-level guidance. This approach achieves state-of-the-art generation fidelity on 11 of 12 datasets, and improves controllability by 12.52% on MSE and 6.34% MAE compared to no text input generation, highlighting its potential for generating tailored time-series data.",
      "url": "https://www.microsoft.com/en-us/research/publication/bridge-bootstrapping-text-to-control-time-series-generation-via-multi-agent-iterative-optimization-and-diffusion-modelling/"
    },
    {
      "title": "Deep learning guided design of protease substrates",
      "authors": [
        "Ava P. Amini",
        "Carmen Martin-Alonso",
        "Kevin Kaichuang Yang",
        "Sangeeta N. Bhatia",
        "Sarah Alamdari",
        "Tahoura S. Samad"
      ],
      "research_areas": [
        "Medical, health and genomics"
      ],
      "publication_date": "March 2025",
      "abstract": "Proteases, a class of enzymes that play critical roles in health and disease, exert their function through the cleavage of peptide bonds. Identifying substrates that are efficiently and selectively cleaved by target proteases is essential for studying protease activity and for harnessing their activity in protease-activated diagnostics and therapeutics. However, the vast design space of possible substrates (c.a. 2010 unique amino acid combinations for a 10-mer peptide) and the limited accessibility of high-throughput activity profiling tools hinder the speed and success of substrate design. We present CleaveNet, an end-to-end AI pipeline for the design of protease substrates. Applied to matrix metalloproteinases, CleaveNet enhances the scale, tunability, and efficiency of substrate design. CleaveNet generates peptide substrates that exhibit sound biophysical properties and capture not only well-established but also novel cleavage motifs. To enable precise control over substrate design, CleaveNet incorporates a conditioning tag that enables generation of peptides guided by a target cleavage profile, enabling targeted design of efficient and selective substrates. CleaveNet-generated substrates were validated experimentally through a large-scale in vitro screen, even in the challenging case of designing highly selective substrates for MMP13. We envision that CleaveNet will accelerate our ability to study and capitalize on protease activity, paving the way for new in silico design tools across enzyme classes.",
      "url": "https://www.microsoft.com/en-us/research/publication/deep-learning-guided-design-of-protease-substrates/"
    },
    {
      "title": "What Makes a Good Diffusion Planner for Decision Making?",
      "authors": [
        "Dongqi Han",
        "Dongsheng Li",
        "Haofei Lu",
        "Yifei Shen"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "March 2025",
      "abstract": "Diffusion models have recently shown significant potential in solving decision-making problems, particularly in generating behavior plans — also known as diffusion planning. While numerous studies have demonstrated the impressive performance of diffusion planning, the mechanisms behind the key components of a good diffusion planner remain unclear and the design choices are highly inconsistent in existing studies. In this work, we address this issue through systematic empirical experiments on diffusion planning in an offline reinforcement learning (RL) setting, providing practical insights into the essential components of diffusion planning. We trained and evaluated over 6,000 diffusion models, identifying the critical components such as guided sampling, network architecture, action generation and planning strategy. We revealed that some design choices opposite to the common practice in previous work in diffusion planning actually lead to better performance, e.g., unconditional sampling with selection can be better than guided sampling and Transformer outperforms U-Net as denoising network. Based on these insights, we suggest a simple yet strong diffusion planning baseline that achieves state-of-the-art results on standard offline RL benchmarks.",
      "url": "https://www.microsoft.com/en-us/research/publication/what-makes-a-good-diffusion-planner-for-decision-making/"
    },
    {
      "title": "Advancing Mobile GUI Agents: A Verifier-Driven Approach to Practical Deployment",
      "authors": [
        "Gaole Dai",
        "Lili Qiu",
        "Mo Li",
        "Rui Tan",
        "Shiqi Jiang",
        "Ting Cao",
        "Yuanchun Li",
        "Yuqing Yang"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Human-computer interaction",
        "Systems and networking"
      ],
      "publication_date": "March 2025",
      "abstract": "We propose V-Droid, a mobile GUI task automation agent. Unlike previous mobile agents that utilize Large Language Models (LLMs) as generators to directly generate actions at each step, V-Droid employs LLMs as verifiers to evaluate candidate actions before making final decisions. To realize this novel paradigm, we introduce a comprehensive framework for constructing verifier-driven mobile agents: the discretized action space construction coupled with the prefilling-only workflow to accelerate the verification process, the pair-wise progress preference training to significantly enhance the verifier’s decision-making capabilities, and the scalable human-agent joint annotation scheme to efficiently collect the necessary data at scale. V-Droid sets a new state-of-the-art task success rate across several public mobile task automation benchmarks: 59.5% on AndroidWorld, 38.3% on AndroidLab, and 49% on MobileAgentBench, surpassing existing agents by 9.5%, 2.1%, and 9%, respectively. Furthermore, V-Droid achieves an impressively low latency of 0.7 seconds per step, making it the first mobile agent capable of delivering near-real-time, effective decision-making capabilities.",
      "url": "https://www.microsoft.com/en-us/research/publication/advancing-mobile-gui-agents-a-verifier-driven-approach-to-practical-deployment/"
    },
    {
      "title": "Convolutional neural network transformer (CNNT) for fluorescence microscopy image denoising with improved generalization and fast adaptation",
      "authors": [
        "Hui Xue"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "March 2025",
      "abstract": "Deep neural networks can improve the quality of fluorescence microscopy images. Previous methods, based on Convolutional Neural Networks (CNNs), require time-consuming training of individual models for each experiment, impairing their applicability and generalization. In this study, we propose a novel imaging-transformer based model, Convolutional Neural Network Transformer (CNNT), that outperforms CNN based networks for image denoising. We train a general CNNT based backbone model from pairwise high-low Signal-to-Noise Ratio (SNR) image volumes, gathered from a single type of fluorescence microscope, an instant Structured Illumination Microscope. Fast adaptation to new microscopes is achieved by fine-tuning the backbone on only 5–10 image volume pairs per new experiment. Results show that the CNNT backbone and fine-tuning scheme significantly reduces training time and improves image quality, outperforming models trained using only CNNs such as 3D-RCAN and Noise2Fast. We show three examples of efficacy of this approach in wide-field, two-photon, and confocal fluorescence microscopy.",
      "url": "https://www.microsoft.com/en-us/research/publication/convolutional-neural-network-transformer-cnnt-for-fluorescence-microscopy-image-denoising-with-improved-generalization-and-fast-adaptation/"
    },
    {
      "title": "POD-Attention: Unlocking Full Prefill-Decode Overlap for Faster LLM Inference",
      "authors": [
        "Aditya K Kamath",
        "Ashish Panwar",
        "Jayashree Mohan",
        "Ramachandran Ramjee",
        "Ramya Prabhu",
        "Simon Peter"
      ],
      "research_areas": [
        "Systems and networking"
      ],
      "publication_date": "March 2025",
      "abstract": "Each request in LLM inference goes through two phases: compute-bound prefill and memory-bandwidth-bound decode. To improve GPU utilization, recent systems use hybrid batching that combines the prefill and decode phases of different requests into the same batch. This approach optimizes linear operations but remains inefficient for attention computation because existing attention kernels specialize execution independently for the prefill and decode phases.\nIn this paper, we present POD-Attention – the first GPU kernel that efficiently computes attention for hybrid batches. POD-Attention aims to maximize the utilization of both compute and memory bandwidth by carefully allocating the GPU’s resources such that prefill and decode operations happen concurrently on the same multiprocessor. POD-Attention speeds up attention computation by up to 59% (mean 28%), enabling higher throughput and lower latency LLM inference compared to the use of independently optimized prefill and decode attention kernels.",
      "url": "https://www.microsoft.com/en-us/research/publication/pod-attention-unlocking-full-prefill-decode-overlap-for-faster-llm-inference/"
    },
    {
      "title": "DynamoLLM: Designing LLM Inference Clusters for Performance and Energy Efficiency",
      "authors": [
        "Chaojie Zhang",
        "Esha Choukse",
        "Josep Torrellas",
        "Jovan Stojkovic",
        "Íñigo Goiri"
      ],
      "research_areas": [
        "Systems and networking"
      ],
      "publication_date": "March 2025",
      "abstract": "The rapid evolution and widespread adoption of generative large language models (LLMs) have made them a pivotal workload in various applications. Today, LLM inference clusters receive a large number of queries with strict Service Level Objectives (SLOs). To achieve the desired performance, these models execute on power-hungry GPUs causing the inference clusters to consume large amount of energy and, consequently, result in excessive carbon emissions. Fortunately, we find that there is a great opportunity to exploit the heterogeneity in inference compute properties and fluctuations in inference workloads, to significantly improve energy-efficiency. However, such a diverse and dynamic environment creates a large search-space where different system configurations (e.g., number of instances, model parallelism, and GPU frequency) translate into different energy-performance trade-offs. To address these challenges, we propose DynamoLLM, the first energy-management framework for LLM inference environments. DynamoLLM automatically and dynamically reconfigures the inference cluster to optimize for energy and cost of LLM serving under the service’s performance SLOs. We show that at a service-level, DynamoLLM conserves 53% energy and 38% operational carbon emissions, and reduces 61% cost to the customer, while meeting the latency SLOs.",
      "url": "https://www.microsoft.com/en-us/research/publication/dynamollm-designing-llm-inference-clusters-for-performance-and-energy-efficiency/"
    },
    {
      "title": "Expected Return Symmetries",
      "authors": [
        "Darius Muglich",
        "Elise van der Pol",
        "Jakob Foerster",
        "Johannes Forkel"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "March 2025",
      "abstract": "Symmetry is an important inductive bias that can improve model robustness and generalization across many deep learning domains. In multi-agent settings, a priori known symmetries have been shown to address a fundamental coordination failure mode known as mutually incompatible symmetry breaking; e.g. in a game where two independent agents can choose to move “left” or “right”, and where a reward of +1 or -1 is received when the agents choose the same action or different actions, respectively. However, the efficient and automatic discovery of environment symmetries, in particular for decentralized partially observable Markov decision processes, remains an open problem. Furthermore, environmental symmetry breaking constitutes only one type of coordination failure, which motivates the search for a more accessible and broader symmetry class. In this paper, we introduce such a broader group of previously unexplored symmetries, which we call expected return symmetries, which contains environment symmetries as a subgroup. We show that agents trained to be compatible under the group of expected return symmetries achieve better zero-shot coordination results than those using environment symmetries. As an additional benefit, our method makes minimal a priori assumptions about the structure of their environment and does not require access to ground truth symmetries.",
      "url": "https://www.microsoft.com/en-us/research/publication/expected-return-symmetries/"
    },
    {
      "title": "KnapsackLB: Enabling Performance-Aware Layer-4 Load Balancing",
      "authors": [
        "Rohan Gandhi",
        "Srinivas Narayana"
      ],
      "research_areas": [
        "Systems and networking"
      ],
      "publication_date": "March 2025",
      "abstract": "The layer-4 load balancer (LB) is one of the key building blocks of online services. In this paper, we empower such LBs to adapt to different and dynamic performance of backend instances (DIPs). Our system, KnapsackLB, is generic (can work with variety of LBs), does not require agents on DIPs, LBs or clients, and scales to large numbers of DIPs. KnapsackLB uses judicious active probes to learn a mapping from LB weights to the response latency of each DIP, and then applies Integer Linear Programming (ILP) to calculate LB weights that optimize latency, using an iterative method to scale the computation to large numbers of DIPs. Using testbed experiments and simulations, we show that KnapsackLB load balances traffic according to DIP performance and cuts average latency by up to 45% compared to existing designs.",
      "url": "https://www.microsoft.com/en-us/research/publication/knapsacklb-enabling-performance-aware-layer-4-load-balancing/"
    },
    {
      "title": "Scaling Laws of Synthetic Data for Language Models",
      "authors": [
        "Dongdong Zhang",
        "Furu Wei",
        "Hany Hassan Awadalla",
        "Li Dong",
        "Mahmoud Khademi",
        "Minhao Cheng",
        "Qingxiu Dong",
        "Weizhu Chen",
        "Xiaolong Huang",
        "Xingxing Zhang",
        "Yi R. Fung",
        "Zeyu Qin",
        "Ziyi Yang"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Human language technologies"
      ],
      "publication_date": "March 2025",
      "abstract": "Large language models (LLMs) achieve strong performance across diverse tasks, largely driven by high-quality web data used in pre-training. However, recent studies indicate this data source is rapidly depleting. Synthetic data emerges as a promising alternative, but it remains unclear whether synthetic datasets exhibit predictable scalability comparable to raw pre-training data. In this work, we systematically investigate the scaling laws of synthetic data by introducing SynthLLM, a scalable framework that transforms pre-training corpora into diverse, high-quality synthetic datasets. Our approach achieves this by automatically extracting and recombining high-level concepts across multiple documents using a graph algorithm. Key findings from our extensive mathematical experiments on SynthLLM include: (1) SynthLLM generates synthetic data that reliably adheres to the rectified scaling law across various model sizes; (2) Performance improvements plateau near 300B tokens; and (3) Larger models approach optimal performance with fewer training tokens. For instance, an 8B model peaks at 1T tokens, while a 3B model requires 4T. Moreover, comparisons with existing synthetic data generation and augmentation methods demonstrate that SynthLLM achieves superior performance and scalability. Our findings highlight synthetic data as a scalable and reliable alternative to organic pre-training corpora, offering a viable path toward continued improvement in model performance.",
      "url": "https://www.microsoft.com/en-us/research/publication/scaling-laws-of-synthetic-data-for-language-models/"
    },
    {
      "title": "DesignDiffusion: High-Quality Text-to-Design Image Generation with Diffusion Models",
      "authors": [
        "Dong Chen",
        "Houqiang Li",
        "Jianmin Bao",
        "Shuyang Gu",
        "Wengang Zhou",
        "Zhendong Wang"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "March 2025",
      "abstract": "In this paper, we present DesignDiffusion, a simple yet effective framework for the novel task of synthesizing design images from textual descriptions. A primary challenge lies in generating accurate and style-consistent textual and visual content. Existing works in a related task of visual text generation often focus on generating text within given specific regions, which limits the creativity of generation models, resulting in style or color inconsistencies between textual and visual elements if applied to design image generation. To address this issue, we propose an end-to-end, one-stage diffusion-based framework that avoids intricate components like position and layout modeling. Specifically, the proposed framework directly synthesizes textual and visual design elements from user prompts. It utilizes a distinctive character embedding derived from the visual text to enhance the input prompt, along with a character localization loss for enhanced supervision during text generation. Furthermore, we employ a self-play Direct Preference Optimization fine-tuning strategy to improve the quality and accuracy of the synthesized visual text. Extensive experiments demonstrate that DesignDiffusion achieves state-of-the-art performance in design image generation.",
      "url": "https://www.microsoft.com/en-us/research/publication/designdiffusion-high-quality-text-to-design-image-generation-with-diffusion-models/"
    },
    {
      "title": "Societal AI: Research Challenges and Opportunities",
      "authors": [
        "Beibei Shi",
        "Haotian Li",
        "Societal AI Team",
        "Xing Xie"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "March 2025",
      "abstract": "Artificial intelligence is reshaping society at an unprecedented scale, influencing key domains such as education, labor, governance, and scientific discovery. As AI models, particularly large language models, become more capable and autonomous, their societal impact raises urgent questions regarding fairness, interpretability, alignment with human values, and regulatory governance. This white paper introduces Societal AI — a multidisciplinary research agenda that examines the intersection of AI and societal well-being. By integrating insights from computer science, psychology, law, sociology, and philosophy, we identify ten critical research questions that address AI’s ethical, technical, and social implications. These challenges include ensuring AI safety and reliability, optimizing human-AI collaboration, evaluating AI’s cognitive and creative potential, and establishing robust regulatory frameworks. Through interdisciplinary collaboration, we aim to develop AI systems that are not only powerful but also aligned with societal values, fostering a future where AI serves as a trusted and beneficial partner to humanity.",
      "url": "https://www.microsoft.com/en-us/research/publication/societal-ai-research-challenges-and-opportunities/"
    },
    {
      "title": "vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention",
      "authors": [
        "Ajay Nayak",
        "Ashish Panwar",
        "Jayashree Mohan",
        "Ramachandran Ramjee",
        "Ramya Prabhu"
      ],
      "research_areas": [
        "Systems and networking"
      ],
      "publication_date": "March 2025",
      "abstract": "Efficient use of GPU memory is essential for high throughput LLM inference. Prior systems reserved memory for the KV-cache ahead-of-time, resulting in wasted capacity due to internal fragmentation. Inspired by OS-based virtual memory systems, vLLM proposed PagedAttention to enable dynamic memory allocation for KV-cache. This approach eliminates fragmentation, enabling high-throughput LLM serving with larger batch sizes. However, to be able to allocate physical memory dynamically, PagedAttention changes the layout of KV-cache from contiguous virtual memory to non-contiguous virtual memory. This change requires attention kernels to be rewritten to support paging, and serving framework to implement a memory manager. Thus, the PagedAttention model leads to software complexity, portability issues, redundancy and inefficiency.\n \nIn this paper, we propose vAttention for dynamic KV-cache memory management. In contrast to PagedAttention, vAttention retains KV-cache in contiguous virtual memory and leverages low-level system support for demand paging, that already exists, to enable on-demand physical memory allocation. Thus, vAttention unburdens the attention kernel developer from having to explicitly support paging and avoids re-implementation of memory management in the serving framework. We show that vAttention enables seamless dynamic memory management for unchanged implementations of various attention kernels. vAttention also generates tokens up to 1.97x faster than vLLM, while processing input prompts up to 3.92x and 1.45x faster than the PagedAttention variants of FlashAttention and FlashInfer.",
      "url": "https://www.microsoft.com/en-us/research/publication/vattention-dynamic-memory-management-for-serving-llms-without-pagedattention/"
    },
    {
      "title": "Global Renewables Watch: A Temporal Dataset of Solar and Wind Energy Derived from Satellite Imagery",
      "authors": [
        "Allen Kim",
        "Andrew Zolli",
        "Anthony Ortiz",
        "Caleb Robinson",
        "James Oakleaf",
        "Joe Kiesecker",
        "Juan M. Lavista Ferres",
        "Rahul Dodhia",
        "Shivaprakash K Nagaraju"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Computer vision",
        "Ecology and environment"
      ],
      "publication_date": "March 2025",
      "abstract": "We present a comprehensive global temporal dataset of commercial solar photovoltaic (PV) farms and onshore wind turbines, derived from high-resolution satellite imagery analyzed quarterly from the fourth quarter of 2017 to the second quarter of 2024. We create this dataset by training deep learning-based segmentation models to identify these renewable energy installations from satellite imagery, then deploy them on over 13 trillion pixels covering the world. For each detected feature, we estimate the construction date and the preceding land use type. This dataset offers crucial insights into progress toward sustainable development goals and serves as a valuable resource for policymakers, researchers, and stakeholders aiming to assess and promote effective strategies for renewable energy deployment. Our final spatial dataset includes 375,197 individual wind turbines and 86,410 solar PV installations. We aggregate our predictions to the country level — estimating total power capacity based on construction date, solar PV area, and number of windmills — and find an r2 value of 0.96 and 0.93 for solar PV and onshore wind respectively compared to IRENA’s most recent 2023 country-level capacity estimates.",
      "url": "https://www.microsoft.com/en-us/research/publication/global-renewables-watch-a-temporal-dataset-of-solar-and-wind-energy-derived-from-satellite-imagery/"
    },
    {
      "title": "Multi-modal Language models in bioacoustics with zero-shot transfer: a case study",
      "authors": [
        "Benjamin Elizalde",
        "Huaming Wang",
        "Juan M. Lavista Ferres",
        "Justin Kitzes",
        "Rahul Dodhia",
        "Soham Deshmukh",
        "Zhongqi Miao"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Ecology and environment"
      ],
      "publication_date": "February 2025",
      "abstract": "Automatically detecting sound events with Artificial Intelligence (AI) has become increasingly popular in the field of bioacoustics, ecoacoustics, and soundscape ecology, particularly for wildlife monitoring and conservation. Conventional methods predominantly employ supervised learning techniques that depend on substantial amounts of manually annotated bioacoustic data. However, manual annotation in bioacoustics is tremendously resource-intensive in terms of both human labor and financial resources, and it requires considerable domain expertise. Moreover, the supervised learning framework limits the application scope to predefined categories within a closed setting. The recent advent of Multi-Modal Language Models has markedly enhanced the versatility and possibilities within the realm of AI applications, as this technique addresses many of the challenges that inhibit the deployment of AI in real-world applications. In this paper, we explore the potential of Multi-Modal Language Models in the context of bioacoustics through a case study. We aim to showcase the potential and limitations of Multi-Modal Language Models in bioacoustic applications. In our case study, we applied an Audio-Language Model–—a type of Multi-Modal Language Model that aligns language with audio/sound recording data—–named CLAP (Contrastive Language–Audio Pretraining) to eight bioacoustic benchmarks covering a wide variety of sounds previously unfamiliar to the model. We demonstrate that CLAP, after simple prompt engineering, can effectively recognize group-level categories such as birds, frogs, and whales across the benchmarks without the need for specific model fine-tuning or additional training, achieving a zero-shot transfer recognition performance comparable to supervised learning baselines. Moreover, we show that CLAP has the potential to perform tasks previously unattainable with supervised bioacoustic approaches, such as estimating relative distances and discovering unknown animal species. On the other hand, we also identify limitations of CLAP, such as the model’s inability to recognize fine-grained species-level categories and the reliance on manually engineered text prompts in real-world applications.",
      "url": "https://www.microsoft.com/en-us/research/publication/multi-modal-language-models-in-bioacoustics-with-zero-shot-transfer-a-case-study/"
    },
    {
      "title": "LLM4Eval@WSDM 2025: Large Language Model for Evaluation in Information Retrieval",
      "authors": [
        "Bhaskar Mitra",
        "Charles L A Clarke",
        "Clemencia Siro",
        "Emine Yilmaz",
        "Guglielmo Faggioli",
        "Hossein A. Rahmani",
        "Mohammad Aliannejadi",
        "Nick Craswell",
        "Paul Thomas"
      ],
      "research_areas": [
        "Search and information retrieval"
      ],
      "publication_date": "March 2025",
      "abstract": "Large language models (LLMs) have demonstrated increasing task-solving abilities not present in smaller models. Utilizing the capabilities and responsibilities of LLMs for automated evaluation (LLM4Eval) has recently attracted considerable attention in multiple research communities. For instance, LLM4Eval models have been studied in the context of automated judgments, natural language generation, and retrieval augmented generation systems. We believe that the information retrieval community can significantly contribute to this growing  research area by designing, implementing, analyzing, and evaluating various aspects of LLMs with applications to LLM4Eval tasks. The main goal of LLM4Eval workshop is to bring together researchers from industry and academia to discuss various aspects of LLMs for evaluation in information retrieval, including automated judgments, retrieval-augmented generation pipeline evaluation, altering human evaluation, robustness, and trustworthiness of LLMs for evaluation in addition to their impact on real-world applications. We also plan to run an automated judgment challenge prior to the workshop, where participants will be asked to generate labels for a given dataset while maximising correlation with human judgments. The format of the workshop is interactive, including roundtable and keynote sessions and tends to avoid the one-sided dialogue of a mini-conference. This is the second iteration of the workshop. The first version was held in conjunction with SIGIR 2024, attracting over 50 participants.",
      "url": "https://www.microsoft.com/en-us/research/publication/llm4evalwsdm-2025-large-language-model-for-evaluation-in-information-retrieval/"
    },
    {
      "title": "General Scales Unlock AI Evaluation with Explanatory and Predictive Power",
      "authors": [
        "Behzad Mehrbakhsh",
        "David Stillwell",
        "Fernando Martínez-Plumed",
        "Jindong Wang",
        "Jiyun Zu",
        "John Burden",
        "Jonathan E. Prunty",
        "José Hernández-Orallo",
        "Katherine M. Collins",
        "Kexin Jiang Chen",
        "Lexin Zhou",
        "Lorenzo Pacchiardi",
        "Lucy Cheke",
        "Luning Sun",
        "Manuel Cebrian",
        "Pablo A. M. Casares",
        "Pablo Sánchez-García",
        "Patrick C. Kyllonen",
        "Peter Henderson",
        "Qinlin Zhao",
        "Seraphina Zhang",
        "Sherry Tongshuang Wu",
        "Xing Xie",
        "Yael Moros-Daval",
        "Yitian Huang",
        "Zongqian Li"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "",
      "abstract": "Ensuring safe and effective use of AI requires understanding and anticipating its performance on novel tasks, from advanced scientific challenges to transformed workplace activities. So far, benchmarking has guided progress in AI, but it has offered limited explanatory and predictive power for general-purpose AI systems, given the low transferability across diverse tasks. In this paper, we introduce general scales for AI evaluation that can explain what common AI benchmarks really measure, extract ability profiles of AI systems, and predict their performance for new task instances, in- and out-of-distribution. Our fully-automated methodology builds on 18 newly-crafted rubrics that place instance demands on general scales that do not saturate. Illustrated for 15 large language models and 63 tasks, high explanatory power is unleashed from inspecting the demand and ability profiles, bringing insights on the sensitivity and specificity exhibited by different benchmarks, and how knowledge, metacognition and reasoning are affected by model size, chain-of-thought and distillation. Surprisingly, high predictive power at the instance level becomes possible using these demand levels, providing superior estimates over black-box baseline predictors based on embeddings or finetuning, especially in out-of-distribution settings (new tasks and new benchmarks). The scales, rubrics, battery, techniques and results presented here represent a major step for AI evaluation, underpinning the reliable deployment of AI in the years ahead.",
      "url": "https://www.microsoft.com/en-us/research/publication/general-scales-unlock-ai-evaluation-with-explanatory-and-predictive-power/"
    },
    {
      "title": "FlashFFTStencil: Bridging Fast Fourier Transforms to Memory-Efficient Stencil Computations on Tensor Core Units",
      "authors": [
        "Donglin Bai",
        "Haozhi Han",
        "Kun Li",
        "Liang Yuan",
        "Mao Yang",
        "Ting Cao",
        "Wei Cui",
        "Yifeng Chen",
        "Yiwei Zhang",
        "Yunquan Zhang"
      ],
      "research_areas": [
        "Mathematics",
        "Systems and networking"
      ],
      "publication_date": "March 2025",
      "abstract": "While Tensor Core Units (TCUs) excel in AI tasks, their application to HPC algorithms like stencil computations faces significant challenges due to sparsity, which leads to underutilization and exacerbates memory-bound limitations. This paper introduces FlashFFTStencil, a memory-efficient stencil computing system designed to bridge FFT to fully-dense stencil computations on TCUs. Aimed at bound shifting, FlashFFTStencil comprises three key techniques: Kernel Tailoring on HBM fuses distinct kernels to enhance parallelism while reducing memory transfer and footprint; Architecture Aligning on SMEMrestructures FFT-based stencil computations into dense matrix multiplications tailored for shared memory architecture; Computation Streamlining on TCU optimizes TCU utilization and thread parallelism by minimizing pipeline stalls and maximizing register reuse. Notably, a distinctive extension is FlashFFTStencil’s ability to enable theoretically unrestricted temporal fusion by FFT. Results show that FlashFFTStencil achieves effective sparsity-free bound shifting, with an average speedup of 2.57x over the state-of-the-art. FlashFFTStencil pioneers a new era in unifying computational patterns within the HPC landscape and bridges them with cutting-edge AI-driven hardware innovations like TCUs.",
      "url": "https://www.microsoft.com/en-us/research/publication/flashfftstencil-bridging-fast-fourier-transforms-to-memory-efficient-stencil-computations-on-tensor-core-units/"
    },
    {
      "title": "Navigating Rifts in Human-LLM Grounding: Study and Benchmark",
      "authors": [
        "Adam Fourney",
        "Eric Horvitz",
        "Gagan Bansal",
        "Hussein Mozannar",
        "Omar Shaikh"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Human language technologies",
        "Human-computer interaction"
      ],
      "publication_date": "March 2025",
      "abstract": "Language models excel at following instructions but often struggle with the collaborative aspects of conversation that humans naturally employ. This limitation in grounding — the process by which conversation participants establish mutual understanding — can lead to outcomes ranging from frustrated users to serious consequences in high-stakes scenarios. To systematically study grounding challenges in human-LLM interactions, we analyze logs from three human-assistant datasets: WildChat, MultiWOZ, and Bing Chat. We develop a taxonomy of grounding acts and build models to annotate and forecast grounding behavior. Our findings reveal significant differences in human-human and human-LLM grounding: LLMs were three times less likely to initiate clarification and sixteen times less likely to provide follow-up requests than humans. Additionally, early grounding failures predicted later interaction breakdowns. Building on these insights, we introduce RIFTS: a benchmark derived from publicly available LLM interaction data containing situations where LLMs fail to initiate grounding. We note that current frontier models perform poorly on RIFTS, highlighting the need to reconsider how we train and prompt LLMs for human interaction. To this end, we develop a preliminary intervention that mitigates grounding failures.",
      "url": "https://www.microsoft.com/en-us/research/publication/navigating-rifts-in-human-llm-grounding-study-and-benchmark/"
    },
    {
      "title": "Model as a Game: On Numerical and Spatial Consistency for Generative Games",
      "authors": [
        "Furu Wei",
        "Jingye Chen",
        "Lei Cui",
        "Li Dong",
        "Qifeng Chen",
        "Tengchao Lv",
        "Yupan Huang",
        "Yuzhong Zhao"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "",
      "abstract": "Recent advances in generative models have significantly impacted game generation. However, despite producing high-quality graphics and adequately receiving player input, existing models often fail to maintain fundamental game properties such as numerical and spatial consistency. Numerical consistency ensures gameplay mechanics correctly reflect score changes and other quantitative elements, while spatial consistency prevents jarring scene transitions, providing seamless player experiences. In this paper, we revisit the paradigm of generative games to explore what truly constitutes a Model as a Game (MaaG) with a well-developed mechanism. We begin with an empirical study on “Traveler”, a 2D game created by an LLM featuring minimalist rules yet challenging generative models in maintaining consistency. Based on the DiT architecture, we design two specialized modules: (1) a numerical module that integrates a LogicNet to determine event triggers, with calculations processed externally as conditions for image generation; and (2) a spatial module that maintains a map of explored areas, retrieving location-specific information during generation and linking new observations to ensure continuity. Experiments across three games demonstrate that our integrated modules significantly enhance performance on consistency metrics compared to baselines, while incurring minimal time overhead during inference.",
      "url": "https://www.microsoft.com/en-us/research/publication/model-as-a-game-on-numerical-and-spatial-consistency-for-generative-games/"
    },
    {
      "title": "DORADD: Deterministic Parallel Execution in the Era of Microsecond-Scale Computing",
      "authors": [
        "Marios Kogias",
        "Matthew J. Parkinson",
        "Musa Unal",
        "Zhengqing Liu"
      ],
      "research_areas": [
        "Programming languages and software engineering",
        "Systems and networking"
      ],
      "publication_date": "March 2025",
      "abstract": "Deterministic parallelism is a key building block for distributed and fault-tolerant systems that offers substantial performance benefits while guaranteeing determinism. By studying existing deterministically parallel systems (DPS), we identify certain design pitfalls, such as batched execution and inefficient runtime synchronization, that preclude them from meeting the demands of μs-scale and high-throughput distributed systems deployed in modern datacenters.\nWe present DORADD, a deterministically parallel runtime with low latency and high throughput, designed for modern datacenter services. DORADD introduces a hybrid scheduling scheme that effectively decouples request dispatching from execution. It employs a single dispatcher to deterministically construct a dynamic dependency graph of incoming requests and worker pools that can independently execute requests in a work-conserving and synchronization-free manner. Furthermore, DORADD overcomes the single-dispatcher throughput bottleneck based on core pipelining.\nWe use DORADD to build an in-memory database and compare it with Caracal, the current state-of-the-art deterministic database, via the YCSB and TPC-C benchmarks. Our evaluation shows up to 2.5× better throughput and more than 150× and 300× better tail latency in non-contended and contended cases, respectively. We also compare DORADD with Caladan, the state-of-the-art non-deterministic remote procedure call (RPC) scheduler, and demonstrate that determinism in DORADD does not incur any performance overhead.",
      "url": "https://www.microsoft.com/en-us/research/publication/doradd-deterministic-parallel-execution-in-the-era-of-microsecond-scale-computing/"
    },
    {
      "title": "Exploring Reduced Feature Sets for American Sign Language Dictionaries",
      "authors": [
        "Aashaka Desai",
        "Alex Lu",
        "Ben Kosa",
        "Danielle Bragg",
        "Richard E. Ladner"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Human-computer interaction"
      ],
      "publication_date": "March 2025",
      "abstract": "There is currently no easy way to look up signs in sign language. Feature-based dictionaries help overcome this challenge by enabling users to look up a sign by inputting descriptive visual features, such as handshape and movement. However, feature-based dictionaries are typically cumbersome, including large numbers of complex features that the user must sort through. In this work, we explore simplifying the set of features used in feature-based American Sign Language (ASL) dictionaries. We present two studies: 1) a simulation study focused on lookup accuracy for various reduced feature sets, and 2) a user study focused on understanding human preferences between feature sets. Our results suggest that it is possible to dramatically reduce the number of features needed to search for signs without significantly impacting the accuracy of search results, and that smaller feature sets can improve the user experience in some cases.",
      "url": "https://www.microsoft.com/en-us/research/publication/exploring-reduced-feature-sets-for-american-sign-language-dictionaries/"
    },
    {
      "title": "Fast, Transparent Filesystem Microkernel Recovery with Ananke",
      "authors": [
        "Andrea C. Arpaci-Dusseau",
        "Jing Liu",
        "Remzi H. Arpaci-Dusseau",
        "Yifan Dai"
      ],
      "research_areas": [
        "Systems and networking"
      ],
      "publication_date": "February 2025",
      "abstract": "We introduce Ananke, a high-performance filesystem microkernel service that provides transparent recovery from unexpected filesystem failures. Ananke does so by leveraging the unique opportunity of the microkernels, running a small amount of recovery code coordinated by the host OS at the moment of a process crash. Ananke can record key pieces of information not usually available during full-system crash recovery, enabling fast and transparent recovery for applications. Through over 30,000 fault-injection experiments, we demonstrate that Ananke achieves lossless recovery; we also show that Ananke recovers quickly, usually in a few hundred milliseconds. Through real application workloads, we show that Ananke delivers high performance in the common case; the extra work needed to detect faults and prepare for recovery incurs minimal overheads.",
      "url": "https://www.microsoft.com/en-us/research/publication/fast-transparent-filesystem-microkernel-recovery-with-ananke/"
    },
    {
      "title": "SatCLIP: Global, General-Purpose Location Embeddings with Satellite Imagery",
      "authors": [
        "Caleb Robinson",
        "Esther Rolf",
        "Konstantin Klemmer",
        "Lester Mackey",
        "M. Russwurm"
      ],
      "research_areas": [
        "Computer vision",
        "Ecology and environment"
      ],
      "publication_date": "February 2025",
      "abstract": "Geographic information is essential for modeling tasks in fields ranging from ecology to epidemiology. However, extracting relevant location characteristics for a given task can be challenging, often requiring expensive data fusion or distillation from massive global imagery datasets. To address this challenge, we introduce Satellite Contrastive Location-Image Pretraining (SatCLIP). This global, general-purpose geographic location encoder learns an implicit representation of locations by matching CNN and ViT inferred visual patterns of openly available satellite imagery with their geographic coordinates. The resulting SatCLIP location encoder efficiently summarizes the characteristics of any given location for convenient use in downstream tasks. In our experiments, we use SatCLIP embeddings to improve prediction performance on nine diverse location-dependent tasks including temperature prediction, animal recognition, and population density estimation. Across tasks, SatCLIP consistently outperforms alternative location encoders and improves geographic generalization by encoding visual similarities of spatially distant environments. These results demonstrate the potential of vision-location models to learn meaningful representations of our planet from the vast, varied, and largely untapped modalities of geospatial data.",
      "url": "https://www.microsoft.com/en-us/research/publication/satclip-global-general-purpose-location-embeddings-with-satellite-imagery-2/"
    },
    {
      "title": "Rapid and accurate prediction of protein homo-oligomer symmetry using Seq2Symm",
      "authors": [
        "Artur Meller",
        "Bonnie Berger",
        "David Baker",
        "Eric Horvitz",
        "Gregory R. Bowman",
        "Ian R. Humphreys",
        "Juan M. Lavista Ferres",
        "Meghana Kshirsagar",
        "Minkyung Baek",
        "Rahul Dodhia",
        "Samuel Sledzieski",
        "Yixi Xu"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Medical, health and genomics"
      ],
      "publication_date": "February 2025",
      "abstract": "The majority of proteins must form higher-order assemblies to perform their biological functions, yet few machine learning models can accurately and rapidly predict the symmetry of assemblies involving multiple copies of the same protein chain. Here, we address this gap by finetuning several classes of protein foundation models, to predict homo-oligomer symmetry. Our best model named Seq2Symm, which utilizes ESM2, outperforms existing template-based and deep learning methods achieving an average AUC-PR of 0.47, 0.44 and 0.49 across homo-oligomer symmetries on three held-out test sets compared to 0.24, 0.24 and 0.25 with template-based search. Seq2Symm uses a single sequence as input and can predict at the rate of ~80,000 proteins/hour. We apply this method to 5 proteomes and ~3.5 million unlabeled protein sequences, showing its promise to be used in conjunction with downstream computationally intensive all-atom structure generation methods such as RoseTTAFold2 and AlphaFold2-multimer. Code, datasets, model are available at: https://github.com/microsoft/seq2symm (opens in new tab).",
      "url": "https://www.microsoft.com/en-us/research/publication/rapid-and-accurate-prediction-of-protein-homo-oligomer-symmetry-using-seq2symm/"
    },
    {
      "title": "Tip of the Tongue Query Elicitation for Simulated Evaluation",
      "authors": [
        "Bhaskar Mitra",
        "Fernando Diaz",
        "Jaime Arguello",
        "To Eun Kim",
        "Yifan He"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Search and information retrieval"
      ],
      "publication_date": "February 2025",
      "abstract": "Tip-of-the-tongue (TOT) search occurs when a user struggles to recall a specific identifier, such as a document title. While common, existing search systems often fail to effectively support TOT scenarios. Research on TOT retrieval is further constrained by the challenge of collecting queries, as current approaches rely heavily on community question-answering (CQA) websites, leading to labor-intensive evaluation and domain bias. To overcome these limitations, we introduce two methods for eliciting TOT queries – leveraging large language models (LLMs) and human participants – to facilitate simulated evaluations of TOT retrieval systems. Our LLM-based TOT user simulator generates synthetic TOT queries at scale, achieving high correlations with how CQA-based TOT queries rank TOT retrieval systems when tested in the Movie domain. Additionally, these synthetic queries exhibit high linguistic similarity to CQA-derived queries. For human-elicited queries, we developed an interface that uses visual stimuli to place participants in a TOT state, enabling the collection of natural queries. In the Movie domain, system rank correlation and linguistic similarity analyses confirm that human-elicited queries are both effective and closely resemble CQA-based queries. These approaches reduce reliance on CQA-based data collection while expanding coverage to underrepresented domains, such as Landmark and Person. LLM-elicited queries for the Movie, Landmark, and Person domains have been released as test queries in the TREC 2024 TOT track, with human-elicited queries scheduled for inclusion in the TREC 2025 TOT track. Additionally, we provide source code for synthetic query generation and the human query collection interface, along with curated visual stimuli used for eliciting TOT queries.",
      "url": "https://www.microsoft.com/en-us/research/publication/tip-of-the-tongue-query-elicitation-for-simulated-evaluation/"
    },
    {
      "title": "Babel: A Scalable Pre-trained Model for Multi-Modal Sensing via Expandable Modality Alignment",
      "authors": [
        "Lili Qiu",
        "Mo Li",
        "Shenghong Dai",
        "Shiqi Jiang",
        "Suman Banerjee",
        "Ting Cao",
        "Yifan Yang"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Computer vision",
        "Systems and networking"
      ],
      "publication_date": "February 2025",
      "abstract": "This paper presents Babel, the expandable modality alignment model, specially designed for multi-modal sensing. While there has been considerable work on multi-modality alignment, they all struggle to effectively incorporate multiple sensing modalities due to the data scarcity constraints. How to utilize multi-modal data with partial pairings in sensing remains an unresolved challenge. Babel tackles this challenge by introducing the concept of expandable modality alignment. The key idea involves transforming the N-modality alignment into a series of binary-modality alignments. Novel techniques are also proposed to further mitigate data scarcity issue and balance the contribution of the newly incorporated modality with the previously established modality alignment during the expandable alignment process. We provide the comprehensive implementation. In the pre-training phase, Babel currently aligns 6 sensing modalities, namely Wi-Fi, mmWave, IMU, LiDAR, video, and depth. For the deployment phase, as a foundation model, any single or combination of aligned modalities could be selected from Babel and applied to downstream tasks. Evaluation demonstrates Babel’s outstanding performance on eight human activity recognition datasets, compared to a broad range of baselines e.g., the SOTA single-modal sensing networks, multi-modal sensing framework, and multi-modal large language models. Babel not only improves the performance of individual modality sensing (12% averaged accuracy improvement), but also effectively fuses multiple available modalities (up to 22% accuracy increase). Case studies also highlight emerging application scenarios empowered by Babel, including cross-modality retrieval (i.e.,sensing imaging), and bridging LLMs for sensing comprehension.",
      "url": "https://www.microsoft.com/en-us/research/publication/advancing-multi-modal-sensing-through-expandable-modality-alignment/"
    },
    {
      "title": "Consequences of training data composition for deep learning models in single-cell biology",
      "authors": [
        "Ajay Nadig",
        "Akshaya Thoutam",
        "Anay Gupta",
        "Andrew W. Navia",
        "Ava P. Amini",
        "Lorin Crawford",
        "Madeline Hughes",
        "Nicolo Fusi",
        "Peter S. Winter",
        "Srivatsan Raghavan"
      ],
      "research_areas": [
        "Medical, health and genomics"
      ],
      "publication_date": "February 2025",
      "abstract": "Foundation models for single-cell transcriptomics have the potential to augment (or replace) purpose-built tools for a variety of common analyses, especially when data are sparse. Recent work with large language models has shown that training data composition greatly shapes performance; however, to date, single-cell foundation models have ignored this aspect, opting instead to train on the largest possible corpus. We systematically investigate the consequences of training dataset composition on the behavior of deep learning models of single-cell transcriptomics, focusing on human hematopoiesis as a tractable model system and including cells from adult and developing tissues, disease states, and perturbation atlases. We find that (1) these models generalize poorly to unseen cell types, (2) adding malignant cells to a healthy cell training corpus does not necessarily improve modeling of unseen malignant cells, and (3) including an embryonic stem cell differentiation atlas during training improves performance on out-of-distribution tasks. Our results emphasize the importance of diverse training data and suggest strategies to optimize future single-cell foundation models.",
      "url": "https://www.microsoft.com/en-us/research/publication/consequences-of-training-data-composition-for-deep-learning-models-in-single-cell-biology/"
    },
    {
      "title": "AMPO: Active Multi-Preference Optimization",
      "authors": [
        "Chetan Bansal",
        "Rahul Madhavan",
        "Saravan Rajmohan",
        "Taneesh Gupta",
        "Xuchao Zhang"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "February 2025",
      "abstract": "Multi-preference optimization enriches language-model alignment beyond pairwise preferences by contrasting entire sets of helpful and undesired responses, thereby enabling richer training signals for large language models. During self-play alignment, these models often produce numerous candidate answers per query, rendering it computationally infeasible to include all responses in the training objective. In this work, we propose $\\textit{Active Multi-Preference Optimization}$ (AMPO), a novel approach that combines on-policy generation, a multi-preference group-contrastive loss, and active subset selection. Specifically, we score and embed large candidate pools of responses and then select a small, yet informative, subset that covers reward extremes and distinct semantic clusters for preference optimization. Our contrastive training scheme is capable of identifying not only the best and worst answers but also subtle, underexplored modes that are crucial for robust alignment. Theoretically, we provide guarantees for expected reward maximization using our active selection method, and empirically, AMPO achieves state-of-the-art results on $\\textit{AlpacaEval}$ using Llama 8B.",
      "url": "https://www.microsoft.com/en-us/research/publication/ampo-active-multi-preference-optimization/"
    },
    {
      "title": "Ironies of Generative AI: Understanding and Mitigating Productivity Loss in Human-AI Interaction.",
      "authors": [
        "Abigail Sellen"
      ],
      "research_areas": [
        "Human-computer interaction"
      ],
      "publication_date": "February 2025",
      "abstract": "Generative AI (GenAI) systems offer opportunities to increase user productivity in many tasks, such as programming and writing. However, while they boost productivity in some studies, many others show that users are working ineffectively with GenAI systems and losing productivity. Despite the apparent novelty of these usability challenges, these ‘ironies of automation’ have been observed for over three decades in Human Factors research on the introduction of automation in domains such as aviation, automated driving, and intelligence. We draw on this extensive research alongside recent GenAI user studies to outline four key reasons for productivity loss with GenAI systems: a shift in users’ roles from production to evaluation, unhelpful restructuring of workflows, interruptions, and a tendency for automation to make easy tasks easier and hard tasks harder. We then suggest how Human Factors research can also inform GenAI system design to mitigate productivity loss by using approaches such as continuous feedback, system personalization, ecological interface design, task stabilization, and clear task allocation. Thus, we ground developments in GenAI system usability in decades of Human Factors research, ensuring that the design of human-AI interactions in this rapidly moving field learns from history instead of repeating it.",
      "url": "https://www.microsoft.com/en-us/research/publication/ironies-of-generative-ai-understanding-and-mitigating-productivity-loss-in-human-ai-interaction-2/"
    },
    {
      "title": "The future of the industrial AI edge is cellular",
      "authors": [
        "Bozidar Radunovic",
        "Xenofon Foukas"
      ],
      "research_areas": [
        "Systems and networking"
      ],
      "publication_date": "February 2025",
      "abstract": "Ensuring reliable and high-bandwidth wireless connectivity and local processing at the edge are crucial enablers for emerging industrial AI applications. In this work, we argue that the recent trends in cellular networking make the technology the ideal connectivity solution for these applications, due to its virtualization and support for open APIs. We foresee the emergence of a converged industrial AI edge encompassing both compute and connectivity, in which application developers leverage the API to implement advanced functionalities. We demonstrate the usefulness of this approach through a case study evaluated on an enterprise-grade 5G testbed deployed in our lab.",
      "url": "https://www.microsoft.com/en-us/research/publication/the-future-of-the-industrial-ai-edge-is-cellular/"
    },
    {
      "title": "Automated Feature Engineering for Single-Trial EEG and Eye-Tracking Classification in Predictive Text Interfaces",
      "authors": [
        "Ard Kastrati",
        "Ivan Tashev",
        "Nemanja Djuric",
        "R. Michael Winters",
        "Yu-Te Wang"
      ],
      "research_areas": [
        "Audio and Acoustics",
        "Human-computer interaction"
      ],
      "publication_date": "February 2025",
      "abstract": "Brain-Computer Interfaces (BCIs) offer a direct connection between the human brain and digital systems, enabling innovative applications. However, realizing the full potential of BCIs remains challenging due to issues like noise, artifacts, and limited data availability. In this study, we develop a multimodal classifier that integrates electroencephalogram (EEG) and eye-tracking (ET) data to decode user responses to predictive text suggestions. Utilizing an automated feature engineering approach, our pipeline efficiently generates and selects relevant features without extensive manual intervention or deep theoretical insights. Applied to a recent BCI case study involving predictive text input, our method achieved higher classification accuracies compared to traditional approaches. Additionally, it revealed novel insights, such as behavioral patterns where participants did not fully read incorrect predictions and the enhanced performance of multimodal classifiers when combining ICA-preprocessed EEG data with ET data. While automated feature engineering is standard in other domains, it is seldom applied in BCI research. Our findings demonstrate that this approach is a valuable tool for data-driven exploration and the development of competitive single-trial classifiers in novel multimodal BCI paradigms, particularly during the initial stages of research with limited data.",
      "url": "https://www.microsoft.com/en-us/research/publication/automated-feature-engineering-for-single-trial-eeg-and-eye-tracking-classification-in-predictive-text-interfaces/"
    },
    {
      "title": "Understanding Screenwriters’ Practices, Attitudes, and Future Expectations in Human-AI Co-Creation",
      "authors": [
        "Haotian Li",
        "Huamin Qu",
        "Minghe Lan",
        "Xiaojuan Ma",
        "Yuying Tang"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "February 2025",
      "abstract": "With the rise of AI technologies and their growing influence in the screenwriting field, understanding the opportunities and concerns related to AI’s role in screenwriting is essential for enhancing human-AI co-creation. Through semi-structured interviews with 23 screenwriters, we explored their creative practices, attitudes, and expectations in collaborating with AI for screenwriting. Based on participants’ responses, we identified the key stages in which they commonly integrated AI, including story structure&plot development, screenplay text, goal&idea generation, and dialogue. Then, we examined how different attitudes toward AI integration influence screenwriters’ practices across various workflow stages and their broader impact on the industry. Additionally, we categorized their expected assistance using four distinct roles of AI: actor, audience, expert, and executor. Our findings provide insights into AI’s impact on screenwriting practices and offer suggestions on how AI can benefit the future of screenwriting.",
      "url": "https://www.microsoft.com/en-us/research/publication/understanding-screenwriters-practices-attitudes-and-future-expectations-in-human-ai-co-creation/"
    },
    {
      "title": "Leveraging public data: Changes in local economic distress and drug overdose deaths at the county level, 2000 – 2019",
      "authors": [
        "Bill Weeks",
        "Juan M. Lavista Ferres",
        "Shaddy K. Saba"
      ],
      "research_areas": [
        "Medical, health and genomics"
      ],
      "publication_date": "February 2025",
      "abstract": "Indices of local economic distress, including local-area unemployment, poverty, low income, and low education, have been linked with lower healthcare quality, higher care costs, and poorer health [1 (opens in new tab)]. There exist vast county-level disparities in economic conditions across the U.S., and counties with greater economic distress demonstrate inequities in health and social determinants of health [2 (opens in new tab)]. Improvements in local economic conditions predict better health including reduced mortality [3 (opens in new tab)] and improved cardiovascular outcomes [4 (opens in new tab)].\nThe early 2000s saw a reduction in life expectancy, driven by an increase in deaths by drug overdose, suicide, and chronic liver disease [5 (opens in new tab)]. These have been termed “deaths of despair;” while overdose deaths during the early 2000s’ opioid epidemic are attributed to complex factors including overprescribing, there are also likely socioeconomic determinants including distressed local economic conditions [5 (opens in new tab)].\nWe leveraged two public data sources [the Centers for Disease Control and Prevention’s Wide-ranging Online Data for Epidemiologic Research (CDC WONDER; wonder.cdc.gov (opens in new tab)) database and the Economic Innovation Group’s Distressed Communities Index (DCI; eig.org/dci-fact-sheet)] to examine whether local economic distress and changes in distress were associated local drug overdose. This approach highlights how public data can be used to explore critical public health questions.",
      "url": "https://www.microsoft.com/en-us/research/publication/leveraging-public-data-changes-in-local-economic-distress-and-drug-overdose-deaths-at-the-county-level-2000-2019/"
    },
    {
      "title": "Towards Foundation Models for Mixed Integer Linear Programming",
      "authors": [
        "Beibin Li",
        "Cathy Wu",
        "Ishai Menache",
        "Janardhan (Jana) Kulkarni",
        "Sirui Li"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "February 2025",
      "abstract": "Mixed Integer Linear Programming (MILP) is essential for modeling complex decision-making problems but faces challenges in computational tractability and requires expert formulation. Current deep learning approaches for MILP focus on specific problem classes and do not generalize to unseen classes. To address this shortcoming, we take a foundation model training approach, where we train a single deep learning model on a diverse set of MILP problems to generalize across problem classes. As existing datasets for MILP lack diversity and volume, we introduce MILP-Evolve, a novel LLM-based evolutionary framework that is capable of generating a large set of diverse MILP classes with an unlimited amount of instances. We study our methodology on three key learning tasks that capture diverse aspects of MILP: (1) integrality gap prediction, (2) learning to branch, and (3) a new task of aligning MILP instances with natural language descriptions. Our empirical results show that models trained on the data generated by MILP-Evolve achieve significant improvements on unseen problems, including MIPLIB benchmarks. Our work highlights the potential of moving towards a foundation model approach for MILP that can generalize to a broad range of MILP applications. We are committed to fully open-sourcing our work to advance further research.",
      "url": "https://www.microsoft.com/en-us/research/publication/towards-foundation-models-for-mixed-integer-linear-programming/"
    },
    {
      "title": "Netherite: Efficient Execution of Serverless Workflows (Extended Journal Version)",
      "authors": [
        "Badrish Chandramouli",
        "Chris Gillum (cgillum)",
        "Christopher S. Meiklejohn",
        "Connor McMahon",
        "David Justo (dajusto)",
        "Konstantinos Kallas",
        "Sebastian Burckhardt",
        "Xiangfeng Zhu"
      ],
      "research_areas": [
        "Data platforms and analytics",
        "Systems and networking"
      ],
      "publication_date": "February 2025",
      "abstract": "Serverless applications are popular because they can provide scalability and load-based billing with minimal developer effort.\nBeyond simple stateless functions (FaaS), modern serverless programming models such as Durable Functions (DF) now\nprovide stateful abstractions, such as workflows, tasks and actors, whose state and progress is implicitly and continuously\npersisted. This automatic resilience greatly simplifies development, but also creates performance challenges, such as a large\nnumber of IOps. To address these challenges, we introduce Netherite, a novel architecture for executing serverless workflows\non an elastic cluster. Netherite groups the numerous application objects into a smaller number of partitions, and pipelines\nthe state persistence of each partition. This improves latency and throughput, as it enables workflow steps to group commit,\neven if causally dependent. Moreover, Netherite leverages FASTER’s hybrid log approach to support larger-than-memory\napplication state, and to enable efficient partition movement between compute hosts. Our evaluation shows that (a) Netherite\nachieves lower latency and higher throughput than the original DF engine, by more than an order of magnitude in some cases,\nand (b) that Netherite has lower latency than some commonly used alternatives, like AWS Step Functions or cloud storage\ntriggers.",
      "url": "https://www.microsoft.com/en-us/research/publication/netherite-efficient-execution-of-serverless-workflows-extended-journal-version/"
    },
    {
      "title": "SWAN: SGD with Normalization and Whitening Enables Stateless LLM Training",
      "authors": [
        "Chao Ma",
        "Edward Meeds",
        "Meyer Scetbon",
        "Wenbo Gong"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "February 2025",
      "abstract": "Adaptive optimizers such as Adam (Kingma&Ba, 2015) have been central to the success of large language models. However, they often require to maintain optimizer states throughout training, which can result in memory requirements several times greater than the model footprint. This overhead imposes constraints on scalability and computational efficiency. Stochastic Gradient Descent (SGD), in contrast, is a stateless optimizer, as it does not track state variables during training.\nConsequently, it achieves optimal memory efficiency. However, its capability in LLM training is limited (Zhao et al., 2024b). In this work, we show that pre-processing SGD in a stateless manner can achieve the same performance as the Adam optimizer for LLM training, while drastically reducing the memory cost. Specifically, we propose to pre-process the instantaneous stochastic gradients using normalization and whitening. We show that normalization stabilizes gradient distributions, and whitening counteracts the local curvature of the loss landscape. This results in SWAN (SGD with Whitening And Normalization), a stochastic optimizer that eliminates the need to store any optimizer states. Empirically, SWAN has the same memory footprint as SGD, achieving $\\approx 50\\%$ reduction on total end-to-end memory compared to Adam. In language modeling tasks, SWAN demonstrates comparable or even better performance than Adam: when pre-training the LLaMA model with 350M and 1.3B parameters, SWAN achieves a 2x speedup by reaching the same evaluation perplexity using half as many tokens.",
      "url": "https://www.microsoft.com/en-us/research/publication/swan-sgd-with-normalization-and-whitening-enables-stateless-llm-training/"
    },
    {
      "title": "Addressing biomedical data challenges and opportunities to inform a large-scale data lifecycle for enhanced data sharing, interoperability, analysis, and collaboration across stakeholders",
      "authors": [
        "Amanda K. Hall",
        "Ashley Conard",
        "Dokyoon Kim",
        "Ilyana Rosenberg",
        "Scott Saponas",
        "Vivek Sriram"
      ],
      "research_areas": [
        "Data platforms and analytics",
        "Human-computer interaction",
        "Medical, health and genomics"
      ],
      "publication_date": "February 2025",
      "abstract": "Biomedical discovery is fraught with challenges stemming from diverse data types and siloed analysis. In this study, we explored common biomedical data tasks and pain points that could be addressed to elevate data quality, enhance sharing, streamline analysis, and foster collaboration across stakeholders. We recruited fifteen professionals from various biomedical roles and industries to participate in sixty-minute semi-structured interviews, which involved an assessment of their challenges, needs, and tasks as well as a brainstorm exercise to validate each professional’s research process. We applied a qualitative analysis of individual interviews using an inductive-deductive thematic coding approach for emerging themes. We identified a common set of challenges related to procuring and validating data, applying new analysis techniques and navigating varied computational environments, distributing results effectively and reproducibly, and managing the flow of data across phases of the data lifecycle. Our findings emphasize the importance of secure data sharing and facilities for collaboration throughout the discovery process. Our identified pain points provide researchers with an opportunity to align workstreams and enhance research data lifecycles to conduct biomedical discovery. We conclude our study with a summary of key actionable recommendations to tackle multiomic data challenges across the stages and phases of biomedical discovery.",
      "url": "https://www.microsoft.com/en-us/research/publication/addressing-biomedical-data-challenges-and-opportunities-to-inform-a-large-scale-data-lifecycle-for-enhanced-data-sharing-interoperability-analysis-and-collaboration-across-stakeholders/"
    },
    {
      "title": "Judging the Judges: A Collection of LLM-Generated Relevance Judgements",
      "authors": [
        "Bhaskar Mitra",
        "Charles L. A. Clarke",
        "Clemencia Siro",
        "Emine Yilmaz",
        "Guglielmo Faggioli",
        "Hossein A. Rahmani",
        "Mohammad Aliannejadi",
        "Nick Craswell",
        "Paul Thomas"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Search and information retrieval"
      ],
      "publication_date": "February 2025",
      "abstract": "Using Large Language Models (LLMs) for relevance assessments offers promising opportunities to improve Information Retrieval (IR), Natural Language Processing (NLP), and related fields. Indeed, LLMs hold the promise of allowing IR experimenters to build evaluation collections with a fraction of the manual human labor currently required. This could help with fresh topics on which there is still limited knowledge and could mitigate the challenges of evaluating ranking systems in low-resource scenarios, where it is challenging to find human annotators. Given the fast-paced recent developments in the domain, many questions concerning LLMs as assessors are yet to be answered. Among the aspects that require further investigation, we can list the impact of various components in a relevance judgment generation pipeline, such as the prompt used or the LLM chosen.\nThis paper benchmarks and reports on the results of a large-scale automatic relevance judgment evaluation, the LLMJudge challenge at SIGIR 2024, where different relevance assessment approaches were proposed. In detail, we release and benchmark 42 LLM-generated labels of the TREC 2023 Deep Learning track relevance judgments produced by eight international teams who participated in the challenge. Given their diverse nature, these automatically generated relevance judgments can help the community not only investigate systematic biases caused by LLMs but also explore the effectiveness of ensemble models, analyze the trade-offs between different models and human assessors, and advance methodologies for improving automated evaluation techniques. The released resource is available at the following link: https://llm4eval.github.io/LLMJudge-benchmark/.",
      "url": "https://www.microsoft.com/en-us/research/publication/judging-the-judges-a-collection-of-llm-generated-relevance-judgements/"
    },
    {
      "title": "Towards Efficient Optimizer Design for LLM via Structured Fisher Approximation with a Low-Rank Extension",
      "authors": [
        "Chao Ma",
        "Edward Meeds",
        "Meyer Scetbon",
        "Wenbo Gong"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "February 2025",
      "abstract": "Designing efficient optimizers for large language models (LLMs) with low-memory requirements and fast convergence is an important and challenging problem. This paper makes a step towards the systematic design of such optimizers through the lens of structured Fisher information matrix (FIM) approximation. We show that many state-of-the-art efficient optimizers can be viewed as solutions to FIM approximation (under the Frobenius norm) with specific structural assumptions. Building on these insights, we propose two design recommendations of practical efficient optimizers for LLMs, involving the careful selection of structural assumptions to balance generality and efficiency, and enhancing memory efficiency of optimizers with general structures through a novel low-rank extension framework. We demonstrate how to use each design approach by deriving new memory-efficient optimizers: Row and Column Scaled SGD (RACS) and Adaptive low-dimensional subspace estimation (Alice). Experiments on LLaMA pre-training (up to 1B parameters) validate the effectiveness, showing faster and better convergence than existing memory-efficient baselines and Adam with little memory overhead. Notably, Alice achieves better than 2x faster convergence over Adam, while RACS delivers strong performance on the 1B model with SGD-like memory.",
      "url": "https://www.microsoft.com/en-us/research/publication/towards-efficient-optimizer-design-for-llm-via-structured-fisher-approximation-with-a-low-rank-extension/"
    },
    {
      "title": "Serving Models, Fast and Slow:Optimizing Heterogeneous LLM Inferencing Workloads at Scale",
      "authors": [
        "A. Parayil",
        "Ankur Mallick",
        "Anoop Kulkarni",
        "Chetan Bansal",
        "Kunal Jain",
        "Renee St. Amant",
        "Rujia Wang",
        "Saravan Rajmohan",
        "Shashwat Jaiswal",
        "Steve Kofsky",
        "Victor Ruehle",
        "Yogesh Simmhan"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Systems and networking"
      ],
      "publication_date": "February 2025",
      "abstract": "Large Language Model (LLM) inference workloads handled by global cloud providers can include both latency-sensitive and insensitive tasks, creating a diverse range of Service Level Agreement (SLA) requirements. Managing these mixed workloads is challenging due to the complexity of the inference stack, which includes multiple LLMs, hardware configurations, and geographic distributions. Current optimization strategies often silo these tasks to ensure that SLAs are met for latency-sensitive tasks, but this leads to significant under-utilization of expensive GPU resources despite the availability of spot and on-demand Virtual Machine (VM) provisioning. We propose SAGESERVE, a comprehensive LLM serving framework that employs adaptive control knobs at varying time scales, ensuring SLA compliance while maximizing the utilization of valuable GPU resources. Short-term optimizations include efficient request routing to data center regions, while long-term strategies involve scaling GPU VMs out/in and redeploying models to existing VMs to align with traffic patterns. These strategies are formulated as an optimization problem for resource allocation and solved using Integer Linear Programming (ILP). We perform empirical and simulation studies based on production workload traces with over 8M requests using four open-source models deployed across three regions. SAGESERVE achieves up to 25% savings in GPU-hours while maintaining tail latency and satisfying all SLOs, and it reduces the scaling overhead compared to baselines by up to 80%, confirming the effectiveness of our proposal. In terms of dollar cost, this can save cloud providers up to $2M over the course of a month.\n",
      "url": "https://www.microsoft.com/en-us/research/publication/serving-models-fast-and-slowoptimizing-heterogeneous-llm-inferencing-workloads-at-scale/"
    },
    {
      "title": "MPO: An Efficient Post-Processing Framework for Mixing Diverse Preference Alignment",
      "authors": [
        "Dongnan Gui",
        "Linjun Zhang",
        "Shuhang Lin",
        "Tianze Wang",
        "Yifan Hu"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Human-computer interaction"
      ],
      "publication_date": "February 2025",
      "abstract": "Reinforcement Learning from Human Feedback (RLHF) has shown promise in aligning large language models (LLMs). Yet its reliance on a singular reward model often overlooks the diversity of human preferences. Recent approaches address this limitation by leveraging multi-dimensional feedback to fine-tune corresponding reward models and train LLMs using reinforcement learning. However, the process is costly and unstable, especially given the competing and heterogeneous nature of human preferences. In this paper, we propose Mixing Preference Optimization (MPO), a post-processing framework for aggregating single-objective policies as an alternative to both multi-objective RLHF (MORLHF) and MaxMin-RLHF. MPO avoids alignment from scratch. Instead, it log-linearly combines existing policies into a unified one with the weight of each policy computed via a batch stochastic mirror descent. Empirical results demonstrate that MPO achieves balanced performance across diverse preferences, outperforming or matching existing models with significantly reduced computational costs.",
      "url": "https://www.microsoft.com/en-us/research/publication/mpo-an-efficient-post-processing-framework-for-mixing-diverse-preference-alignment/"
    },
    {
      "title": "Non-Monotonicity of Branching Rules with Respect to Linear Relaxations",
      "authors": [
        "Marco Molinaro",
        "Prachi Shah",
        "Santanu S. Dey"
      ],
      "research_areas": [
        "Algorithms"
      ],
      "publication_date": "February 2025",
      "abstract": "Modern mixed-integer programming solvers use the branch-and-cut framework, where cutting planes are added to improve the tightness of the linear programming (LP) relaxation, with the expectation that the tighter formulation would produce smaller branch-and-bound trees. In this work, we consider the question of whether adding cuts will always lead to smaller trees for a given fixed branching rule. We formally call such a property of a branching rule monotonicity. We prove that any branching rule which exclusively branches on fractional variables in the LP solution is nonmonotonic. Moreover, we present a family of instances where adding a single cut leads to an exponential increase in the size of full strong branching trees, despite improving the LP bound. Finally, we empirically attempt to estimate the prevalence of nonmonotonicity in practice while using full strong branching. We consider randomly generated multidimensional knapsacks tightened by cover cuts as well as instances from the MIPLIB 2017 benchmark set for the computational experiments. Our main insight from these experiments is that if the gap closed by cuts is small, change in tree size is difficult to predict, and often increases, possibly due to inherent nonmonotonicity. However, when a sufficiently large gap is closed, a significant decrease in tree size may be expected. History: Accepted by Alice E. Smith, Andrea Lodi / Design & Analysis of Algorithms–Discrete. Funding: This work was supported by Air Force Office of Scientific Research [Grant F9550-22-1-0052]. Supplemental Material: The software that supports the findings of this study is available within the paper and its Supplemental Information ( https://pubsonline.informs.org/doi/suppl/10.1287/ijoc.2024.0709 ) as well as from the IJOC GitHub software repository ( https://github.com/INFORMSJoC/2024.0709 ). The complete IJOC Software and Data Repository is available at https://informsjoc.github.io/ .",
      "url": "https://www.microsoft.com/en-us/research/publication/non-monotonicity-of-branching-rules-with-respect-to-linear-relaxations/"
    },
    {
      "title": "The Canary’s Echo: Auditing Privacy Risks of LLM-Generated Synthetic Text",
      "authors": [
        "Lukas Wutschitz",
        "Matthieu Meeus",
        "Reza Shokri",
        "Santiago Zanella-Béguelin",
        "Shruti Tople"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "February 2025",
      "abstract": "How much information about training samples can be gleaned from synthetic data generated by Large Language Models (LLMs)? Overlooking the subtleties of information flow in synthetic data generation pipelines can lead to a false sense of privacy. In this paper, we design membership inference attacks (MIAs) that target data used to fine-tune pre-trained LLMs that are then used to synthesize data, particularly when the adversary does not have access to the fine-tuned model but only to the synthetic data. We show that such data-based MIAs do significantly better than a random guess, meaning that synthetic data leaks information about the training data. Further, we find that canaries crafted to maximize vulnerability to model-based MIAs are sub-optimal for privacy auditing when only synthetic data is released. Such out-of-distribution canaries have limited influence on the model’s output when prompted to generate useful, in-distribution synthetic data, which drastically reduces their vulnerability. To tackle this problem, we leverage the mechanics of auto-regressive models to design canaries with an in-distribution prefix and a high-perplexity suffix that leave detectable traces in synthetic data. This enhances the power of data-based MIAs and provides a better assessment of the privacy risks of releasing synthetic data generated by LLMs.",
      "url": "https://www.microsoft.com/en-us/research/publication/the-canarys-echo-auditing-privacy-risks-of-llm-generated-synthetic-text/"
    },
    {
      "title": "AdaptiveStep: Automatically Dividing Reasoning Step through Model Confidence",
      "authors": [
        "Chaofeng Qu",
        "Chonghan Liu",
        "Chuheng Zhang",
        "Jason Klein Liu",
        "Jiang Bian",
        "Junjie Lu",
        "Li Zhao",
        "Wei Shen",
        "Yuliang Liu",
        "Yunhui Xia",
        "Zefan Cai",
        "Zhaoling Chen",
        "Zhouhan Lin"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "February 2025",
      "abstract": "Current approaches for training Process Reward Models (PRMs) often involve breaking down responses into multiple reasoning steps using rule-based techniques, such as using predefined placeholder tokens or setting the reasoning step’s length into a fixed size. These approaches overlook the fact that specific words do not typically mark true decision points in a text. To address this, we propose AdaptiveStep, a method that divides reasoning steps based on the model’s confidence in predicting the next word. This division method provides more decision-making information at each step, enhancing downstream tasks, such as reward model learning. Moreover, our method does not require manual annotation. We demonstrate its effectiveness through experiments with AdaptiveStep-trained PRMs in mathematical reasoning and code generation tasks. Experimental results indicate that the outcome PRM achieves state-of-the-art Best-of-N performance, surpassing greedy search strategy with token-level value-guided decoding, while also reducing construction costs by over 30% compared to existing open-source PRMs. In addition, we provide a thorough analysis and case study on the PRM’s performance, transferability, and generalization capabilities.",
      "url": "https://www.microsoft.com/en-us/research/publication/adaptivestep-automatically-dividing-reasoning-step-through-model-confidence/"
    },
    {
      "title": "Magma: A Foundation Model for Multimodal AI Agents",
      "authors": [
        "Baolin Peng",
        "Jianfeng Gao",
        "Jianwei Yang",
        "Joel Jang",
        "Lars Liden",
        "Mu Cai",
        "Qianhui Wu",
        "Reuben Tan",
        "Ruijie Zheng",
        "Seonghyeon Ye",
        "Yongyuan Liang",
        "Yu Gu",
        "Yuquan Deng"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Computer vision"
      ],
      "publication_date": "February 2025",
      "abstract": "We present Magma, a foundation model that serves multimodal AI agentic tasks in both the digital and physical worlds. Magma is a significant extension of vision-language (VL) models in that it not only retains the VL understanding ability (verbal intelligence) of the latter, but is also equipped with the ability to plan and act in the visual-spatial world (spatial-temporal intelligence) and complete agentic tasks ranging from UI navigation to robot manipulation. To endow the agentic capabilities, Magma is pretrained on large amounts of heterogeneous datasets spanning from images, videos to robotics data, where the actionable visual objects (e.g., clickable buttons in GUI) in images are labeled by Set-of-Mark (SoM) for action grounding, and the object movements (e.g., the trace of human hands or robotic arms) in videos are labeled by Trace-of-Mark (ToM) for action planning. Extensive experiments show that SoM and ToM reach great synergy and facilitate the acquisition of spatial-temporal intelligence for our Magma model, which is fundamental to a wide range of tasks as shown in Fig.1. In particular, Magma creates new state-of-the-art results on UI navigation and robotic manipulation tasks, outperforming previous models that are specifically tailored to these tasks. On image and video-related multimodal tasks, Magma also compares favorably to popular large multimodal models that are trained on much larger datasets. We make our model and code public for reproducibility at https://microsoft.github.io/Magma (opens in new tab).",
      "url": "https://www.microsoft.com/en-us/research/publication/magma-a-foundation-model-for-multimodal-ai-agents/"
    },
    {
      "title": "Explorer: Scaling Exploration-driven Web Trajectory Synthesis for Multimodal Web Agents",
      "authors": [
        "Ahmed Awadallah",
        "Arindam Mitra",
        "Boyu Gou",
        "Corby Rosset",
        "Spencer Whitehead",
        "Vardaan Pahuja",
        "Yadong Lu",
        "Yu Su"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Computer vision",
        "Human-computer interaction"
      ],
      "publication_date": "February 2025",
      "abstract": "Recent success in large multimodal models (LMMs) has sparked promising applications of agents capable of autonomously completing complex web tasks. While open-source LMM agents have made significant advances in offline evaluation benchmarks, their performance still falls substantially short of human-level capabilities in more realistic online settings. A key bottleneck is the lack of diverse and large-scale trajectory-level datasets across various domains, which are expensive to collect. In this paper, we address this challenge by developing a scalable recipe to synthesize the largest and most diverse trajectory-level dataset to date, containing over 94K successful multimodal web trajectories, spanning 49K unique URLs, 720K screenshots, and 33M web elements. In particular, we leverage extensive web exploration and refinement to obtain diverse task intents. The average cost is 28 cents per successful trajectory, making it affordable to a wide range of users in the community. Leveraging this dataset, we train Explorer, a multimodal web agent, and demonstrate strong performance on both offline and online web agent benchmarks such as Mind2Web-Live, Multimodal-Mind2Web, and MiniWob++. Additionally, our experiments highlight data scaling as a key driver for improving web agent capabilities. We hope this study makes state-of-the-art LMM-based agent research at a larger scale more accessible.",
      "url": "https://www.microsoft.com/en-us/research/publication/explorer-scaling-exploration-driven-web-trajectory-synthesis-for-multimodal-web-agents/"
    },
    {
      "title": "World and Human Action Models towards gameplay ideation",
      "authors": [
        "Abdelhak Lemkhenter",
        "Anssi Kanervisto",
        "Cecily Morrison",
        "Chentian Jiang",
        "Dave Bignell",
        "Gavin Costello",
        "Gunshi Gupta",
        "Katja Hofmann",
        "Linda Yilin Wen",
        "Marko Tot",
        "Martin Grayson",
        "Raluca Georgescu",
        "Ryen W. White",
        "Sam Devlin",
        "Sergio Valcarcel Macua",
        "Shan Zheng Tan",
        "Shu Ishida",
        "Tabish Rashid",
        "Tarun Gupta",
        "Tim Pearce",
        "Udit Arora",
        "Yuhan Cao"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "February 2025",
      "abstract": "Generative artificial intelligence (AI) has the potential to transform creative industries through supporting human creative ideation—the generation of new ideas1–5. However, limitations in model capabilities raise key challenges in integrating these technologies more fully into creative practices. Iterative tweaking and divergent thinking remain key to enabling creativity support using technology6,7, yet these practices are insufficiently supported by state-of-the-art generative AI models. Using game development as a lens, we demonstrate that we can make use of an understanding of user needs to drive the development and evaluation of generative AI models in a way that aligns with these creative practices. Concretely, we introduce a state-of-the-art generative model, the World and Human Action Model (WHAM), and show that it can generate consistent and diverse gameplay sequences and persist user modifications—three capabilities that we identify as being critical for this alignment. In contrast to previous approaches to creativity support tools that required manually defining or extracting structure for relatively narrow domains, generative AI models can learn relevant structure from available data, opening the potential for a much broader range of applications.",
      "url": "https://www.microsoft.com/en-us/research/publication/world-and-human-action-models-towards-gameplay-ideation/"
    },
    {
      "title": "The Canary’s Echo: Auditing Privacy Risks of LLM-Generated Synthetic Text",
      "authors": [
        "Lukas Wutschitz",
        "Matthieu Meeus",
        "Reza Shokri",
        "Santiago Zanella-Béguelin",
        "Shruti Tople"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Security, privacy, and cryptography"
      ],
      "publication_date": "February 2025",
      "abstract": "How much information about training samples can be gleaned from synthetic data generated by Large Language Models (LLMs)? Overlooking the subtleties of information flow in synthetic data generation pipelines can lead to a false sense of privacy. In this paper, we design membership inference attacks (MIAs) that target data used to fine-tune pre-trained LLMs that are then used to synthesize data, particularly when the adversary does not have access to the fine-tuned model but only to the synthetic data. We show that such data-based MIAs do significantly better than a random guess, meaning that synthetic data leaks information about the training data. Further, we find that canaries crafted to maximize vulnerability to model-based MIAs are sub-optimal for privacy auditing when only synthetic data is released. Such out-of-distribution canaries have limited influence on the model’s output when prompted to generate useful, in-distribution synthetic data, which drastically reduces their vulnerability. To tackle this problem, we leverage the mechanics of auto-regressive models to design canaries with an in-distribution prefix and a high-perplexity suffix that leave detectable traces in synthetic data. This enhances the power of data-based MIAs and provides a better assessment of the privacy risks of releasing synthetic data generated by LLMs.",
      "url": "https://www.microsoft.com/en-us/research/publication/how-to-audit-privacy-of-synthetic-data-generated-by-llms/"
    },
    {
      "title": "Roadmap to fault tolerant quantum computation using topological qubit arrays",
      "authors": [
        "Aarthi Sundaram",
        "Adam Paetznick",
        "Adrian C Dumitrascu",
        "Ajuan Cui",
        "Alejandro Alcaraz Ramirez",
        "Alex Bocharov",
        "Alex Webster",
        "Alexander Vaschillo",
        "Amin Barzegar",
        "Andreas Ekefjard",
        "Andres Paz",
        "Andrew Zimmerman",
        "Andrey Antipov",
        "Anirudh Narla",
        "Anna Wulff Christensen",
        "Antonio Rodolph Mei",
        "Bela Bauer",
        "Ben Reichardt",
        "Benjamin J. Chapman",
        "Brad Lackey",
        "Brian Paquelet Wutz",
        "Camille Papon",
        "Chetan Nayak (cnayak)",
        "Christian Mollgaard",
        "Christina Knapp",
        "Chung Kai Yang",
        "Cristina Sfiligoj",
        "D. Govender",
        "David Aasen",
        "David Bohn",
        "David J. van Woerkom",
        "David Razmadze",
        "Dennis Tom",
        "Derek Knee",
        "Diego Olivier Fernandez Pons",
        "Dima Pikulin",
        "Dmitrii Viazmitinov",
        "Dominik Vogel",
        "Emily Toomey",
        "Emma Schmidgall",
        "Emrah Yucelen",
        "Eoin O'Farrell",
        "Eric Stuppard",
        "Esben Bork Hansen",
        "Esteban Martinez",
        "Fabiano Corsetti",
        "Farhad Karimi",
        "Flavio Griggio",
        "Fr'ed'eric Nolet",
        "G. Winkler",
        "Gary Campbell",
        "Geoff Gardner",
        "George Moussa",
        "Gijs de Lange",
        "Gopakumar Mohandas",
        "Grant Leum",
        "Guoji Zheng",
        "H. Suominen",
        "Haris Gavranovic",
        "Henrik Ingerslev",
        "Irene Sanlorenzo",
        "Ivan Sadovskyy",
        "Ivan Urban",
        "J. M. Bello-Rivas",
        "Jake Mattinson",
        "Jamie R. Kuesel",
        "Jan Borovsky",
        "Jan Gukelberger",
        "Jason Lee",
        "Jaspreet Jhoja",
        "Jeffrey Jones",
        "Jeffrey Lai",
        "Jens Hedegaard Nielsen",
        "Jeongwan Haah",
        "Jes'us Herranz Zamorano",
        "Jinnapat Indrapiromkul",
        "John Watson",
        "Jonathan Becker",
        "Jonne Koski",
        "Joseph Weston",
        "Josh Tracy",
        "Jouri Bommer",
        "Juan Carlos Estrada Saldana",
        "Judith Suter",
        "Justin Hogaboam",
        "Justin Zilke",
        "K. Otani",
        "Karl Petersson",
        "Katrine Rasmussen",
        "Ken Reneris",
        "Kevin Van Hoogdalem",
        "Kongyi Li",
        "Kostya Kalashnikov",
        "Krysta Svore",
        "Kyunghoon Lee",
        "L. Bourdet",
        "L. Galletti",
        "Laurens Holgaard",
        "Lauri Sainiemi",
        "Lieuwe Stek",
        "Lovro Ivancevic",
        "Luca Petit",
        "Lucas Casparis",
        "Lukas Avilovas",
        "M. Astafev",
        "Marco Mattila",
        "Marcus P. da Silva",
        "Maren Elisabeth Kloster",
        "Marijn Lucas",
        "Mariusz Andrzejczuk",
        "Marzie Hamdast",
        "Mason Thomas",
        "Mathias Soeken",
        "Matthew D. Turner",
        "Matthew Hastings",
        "Matthias Troyer",
        "Michael Goulding",
        "Michael Manfra",
        "Michelle Turley",
        "Michiel de Moor",
        "Mike Nystrom",
        "Mohana Rajpalke",
        "Morten Hannibal Madsen",
        "Morteza Aghaee",
        "Nash Madulid",
        "Nivetha Thiyagarajah",
        "P. Kostamo",
        "Parsa Bonderson",
        "Patrick Codd",
        "Patrick Sohr",
        "Patrick Strom-Hansen",
        "Paul Cooper",
        "R. Mishmash",
        "Rachpon Kalra",
        "Raghu Gatta",
        "Raj Tholapi",
        "Ray Kallaher",
        "Richard Yu",
        "Robert McNeil",
        "Roland Zeisel",
        "Roman Lutchyn",
        "Roy Riccomini",
        "Ruben Grigoryan",
        "Rui Chao",
        "S. Boddapati",
        "S. Kimes",
        "Saeed Fallahi",
        "Sahar Daraeizadeh",
        "Sam Quinn",
        "Sam Teicher",
        "Samantha Ho",
        "Samuel Boutin",
        "Sarah Jablonski",
        "Sarat Sinha",
        "Satoshi Suzuki",
        "Sebastian Grijalva",
        "Sebastian Heedt",
        "Sergei Gronin",
        "Shivendra Upadhyay",
        "Signe Brynold Markussen",
        "Simon Schaal",
        "Sohail Chatoor",
        "Srivatsa Chakravarthi",
        "Tareq El Dandachi",
        "Thomas Jensen",
        "Thorvald Larsen",
        "Timothy Williamson",
        "Tom Brown",
        "Tom Laeven",
        "Tomas Stankevic",
        "Torsten Karzig",
        "Trevor Morgan",
        "Tyler Lindemann",
        "Umesh Bhaskar",
        "Vadym Kliuchnikov",
        "William Cole",
        "William Hvidtfelt Padkaer Nielsen",
        "Wim van Dam",
        "Yuan Ren",
        "Zhenghan Wang",
        "Zulfi Alam"
      ],
      "research_areas": [
        "Quantum computing"
      ],
      "publication_date": "February 2025",
      "abstract": "We describe a concrete device roadmap towards a fault-tolerant quantum computing architecture based on noise-resilient, topologically protected Majorana-based qubits. Our roadmap encompasses four generations of devices: a single-qubit device that enables a measurement-based qubit benchmarking protocol; a two-qubit device that uses measurement-based braiding to perform single-qubit Clifford operations; an eight-qubit device that can be used to show an improvement of a two-qubit operation when performed on logical qubits rather than directly on physical qubits; and a topological qubit array supporting lattice surgery demonstrations on two logical qubits. Devices that enable this path require a superconductor-semiconductor heterostructure that supports a topological phase, quantum dots and coupling between those quantum dots that can create the appropriate loops for interferometric measurements, and a microwave readout system that can perform fast, low-error single-shot measurements. We describe the key design components of these qubit devices, along with the associated protocols for demonstrations of single-qubit benchmarking, Clifford gate execution, quantum error detection, and quantum error correction, which differ greatly from those in more conventional qubits. Finally, we comment on implications and advantages of this architecture for utility-scale quantum computation.",
      "url": "https://www.microsoft.com/en-us/research/publication/roadmap-to-fault-tolerant-quantum-computation-using-topological-qubit-arrays/"
    },
    {
      "title": "HybriDNA: A Hybrid Transformer-Mamba2 Long-Range DNA Language Model",
      "authors": [
        "Albert Gu",
        "Chuan Cao",
        "Guoqing Liu",
        "Haiguang Liu",
        "Mingqian Ma",
        "Pan Deng",
        "Peiran Jin",
        "Pipi Hu",
        "Renqian Luo",
        "Tao Qin",
        "Tri Dao",
        "Yingce Xia",
        "Yuan-Jyue Chen",
        "Zhao Yang",
        "Zun Wang"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "February 2025",
      "abstract": "Advances in natural language processing and large language models have sparked growing interest in modeling DNA, often referred to as the”language of life”. However, DNA modeling poses unique challenges. First, it requires the ability to process ultra-long DNA sequences while preserving single-nucleotide resolution, as individual nucleotides play a critical role in DNA function. Second, success in this domain requires excelling at both generative and understanding tasks: generative tasks hold potential for therapeutic and industrial applications, while understanding tasks provide crucial insights into biological mechanisms and diseases. To address these challenges, we propose HybriDNA, a decoder-only DNA language model that incorporates a hybrid Transformer-Mamba2 architecture, seamlessly integrating the strengths of attention mechanisms with selective state-space models. This hybrid design enables HybriDNA to efficiently process DNA sequences up to 131kb in length with single-nucleotide resolution. HybriDNA achieves state-of-the-art performance across 33 DNA understanding datasets curated from the BEND, GUE, and LRB benchmarks, and demonstrates exceptional capability in generating synthetic cis-regulatory elements (CREs) with desired properties. Furthermore, we show that HybriDNA adheres to expected scaling laws, with performance improving consistently as the model scales from 300M to 3B and 7B parameters. These findings underscore HybriDNA’s versatility and its potential to advance DNA research and applications, paving the way for innovations in understanding and engineering the”language of life”.",
      "url": "https://www.microsoft.com/en-us/research/publication/hybridna-a-hybrid-transformer-mamba2-long-range-dna-language-model/"
    },
    {
      "title": "RLTHF: Targeted Human Feedback for LLM Alignment",
      "authors": [
        "Bibek Aryal",
        "Eduardo Rodrigues",
        "Emre Kiciman",
        "Jessica Wolk",
        "Leonardo Nunes",
        "Maria Angels de Luis Balaguer",
        "Rafael Padilha",
        "Ranveer Chandra",
        "Roberto Estevao",
        "Shobana Balakrishna",
        "Songwu Lu",
        "Srinagesh Sharma",
        "Tusher Chakraborty",
        "Yifei Xu"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "February 2025",
      "abstract": "Fine-tuning large language models (LLMs) to align with user preferences is challenging due to the high cost of quality human annotations in Reinforcement Learning from Human Feedback (RLHF) and the generalizability limitations of AI Feedback. To address these challenges, we propose RLTHF, a human-AI hybrid framework that combines LLM-based initial alignment with selective human annotations to achieve full-human annotation alignment with minimal effort. RLTHF identifies hard-to-annotate samples mislabeled by LLMs using a reward model’s reward distribution and iteratively enhances alignment by integrating strategic human corrections while leveraging LLM’s correctly labeled samples. Evaluations on HH-RLHF and TL;DR datasets show that RLTHF reaches full-human annotation-level alignment with only 6-7% of the human annotation effort. Furthermore, models trained on RLTHF’s curated datasets for downstream tasks outperform those trained on fully human-annotated datasets, underscoring the effectiveness of RLTHF’s strategic data curation.",
      "url": "https://www.microsoft.com/en-us/research/publication/rlthf-targeted-human-feedback-for-llm-alignment/"
    },
    {
      "title": "Towards Effective Extraction and Evaluation of Factual Claims",
      "authors": [
        "Dasha Metropolitansky",
        "Jonathan Larson"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Human language technologies"
      ],
      "publication_date": "February 2025",
      "abstract": "A common strategy for fact-checking long-form content generated by Large Language Models (LLMs) is extracting simple claims that can be verified independently. Since inaccurate or incomplete claims compromise fact-checking results, ensuring claim quality is critical. However, the lack of a standardized evaluation framework impedes assessment and comparison of claim extraction methods. To address this gap, we propose a framework for evaluating claim extraction in the context of fact-checking along with automated, scalable, and replicable methods for applying this framework, including novel approaches for measuring coverage and decontextualization. We also introduce Claimify, an LLM-based claim extraction method, and demonstrate that it outperforms existing methods under our evaluation framework. A key feature of Claimify is its ability to handle ambiguity and extract claims only when there is high confidence in the correct interpretation of the source text.",
      "url": "https://www.microsoft.com/en-us/research/publication/towards-effective-extraction-and-evaluation-of-factual-claims/"
    },
    {
      "title": "Efficient Cloud Server Deployment Under Demand Uncertainty",
      "authors": [
        "Beibin Li",
        "David Simchi-Levi",
        "Ishai Menache",
        "Jeevan Pathuri",
        "Konstantina Mellou",
        "Rui Peng Liu",
        "Thomas Coffee",
        "Xiao-Yue Gong"
      ],
      "research_areas": [
        "Algorithms",
        "Mathematics"
      ],
      "publication_date": "February 2025",
      "abstract": "Problem definition: Cloud computing is a multibillion-dollar business that draws substantial capital investments from large companies such as Amazon, Microsoft, and Google. Large cloud providers need to accommodate the growing demand for computing resources while avoiding unnecessary overprovisioning of hardware and operational costs. The underlying decision processes are challenging, as they involve long-term hardware and infrastructure investments under future demand uncertainty. In this paper, we introduce the cloud server deployment problem. One important aspect of the problem is that the infrastructure preparation work has to be planned for before server deployments can take place. Furthermore, a combination of temporal constraints has to be considered together with a variety of physical constraints. Methodology/results: We formulate the underlying optimization problem as a two-stage stochastic program. After carefully examining the demand data and on-the-ground deployment operations, we distill two structural properties on deployment throughput constraints and provide tightness results on a convex relaxation of the second stage. Based on that, we develop efficient cutting-plane methods that exploit the special structure of the problem and can accommodate different risk measures. We test our algorithms with real production traces from Microsoft Azure and demonstrate sizeable cost reductions. We show empirically that the algorithms remain optimal even when the two properties are not fully satisfied. Managerial implications: Cloud supply chain operations were largely executed manually due to their complexity and dynamic nature. In this paper, we show that the key decision processes can be systematically optimized. In particular, we demonstrate that accounting for the stochastic nature of demands results in substantial cost reductions in cloud server deployments. Another benefit of our stochastic optimization approach is the ability to seamlessly integrate configurable risk preferences of cloud providers.",
      "url": "https://www.microsoft.com/en-us/research/publication/efficient-cloud-server-deployment-under-demand-uncertainty/"
    },
    {
      "title": "Abstract Operations Research Modeling Using Natural Language Inputs",
      "authors": [
        "Arko Mukherjee",
        "Junxuan Li",
        "Raj Kumar Maity",
        "Ryan Wickman",
        "Sahil Bhatnagar"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "February 2025",
      "abstract": "Operations research (OR) uses mathematical models to enhance decision making, but developing these models requires expert knowledge and can be time-consuming. Automated mathematical programming (AMP) has emerged to simplify this process, but existing systems have limitations. This paper introduces a novel methodology that uses recent advances in a large language model (LLM) to create and edit abstract OR models from non-expert user queries expressed using natural language. This reduces the need for domain expertise and the time to formulate a problem, and an abstract OR model generated can be deployed to a multi-tenant platform to support a class of users with different input data. This paper presents an end-to-end pipeline, named NL2OR, that generates solutions to OR problems from natural language input, and shares experimental results on several important OR problems.",
      "url": "https://www.microsoft.com/en-us/research/publication/abstract-operations-research-modeling-using-natural-language-inputs/"
    },
    {
      "title": "Low-Rank Thinning",
      "authors": [
        "A. M. Carrell",
        "Abhishek Shetty",
        "Albert Gong",
        "Lester Mackey",
        "Raaz Dwivedi"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "February 2025",
      "abstract": "The goal in thinning is to summarize a dataset using a small set of representative points. Remarkably, sub-Gaussian thinning algorithms like Kernel Halving and Compress can match the quality of uniform subsampling while substantially reducing the number of summary points. However, existing guarantees cover only a restricted range of distributions and kernel-based quality measures and suffer from pessimistic dimension dependence. To address these deficiencies, we introduce a new low-rank analysis of sub-Gaussian thinning that applies to any distribution and any kernel, guaranteeing high-quality compression whenever the kernel or data matrix is approximately low-rank. To demonstrate the broad applicability of the techniques, we design practical sub-Gaussian thinning approaches that improve upon the best known guarantees for approximating attention in transformers, accelerating stochastic gradient training through reordering, and distinguishing distributions in near-linear time.",
      "url": "https://www.microsoft.com/en-us/research/publication/low-rank-thinning/"
    },
    {
      "title": "Gradient Multi-Normalization for Stateless and Scalable LLM Training",
      "authors": [
        "Chao Ma",
        "Edward Meeds",
        "Meyer Scetbon",
        "Wenbo Gong"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "February 2025",
      "abstract": "Training large language models (LLMs) typically relies on adaptive optimizers like Adam (Kingma&Ba, 2015) which store additional state information to accelerate convergence but incur significant memory overhead. Recent efforts, such as SWAN (Ma et al., 2024) address this by eliminating the need for optimizer states while achieving performance comparable to Adam via a multi-step preprocessing procedure applied to instantaneous gradients. Motivated by the success of SWAN, we introduce a novel framework for designing stateless optimizers that normalizes stochastic gradients according to multiple norms. To achieve this, we propose a simple alternating scheme to enforce the normalization of gradients w.r.t these norms. We show that our procedure can produce, up to an arbitrary precision, a fixed-point of the problem, and that SWAN is a particular instance of our approach with carefully chosen norms, providing a deeper understanding of its design. However, SWAN’s computationally expensive whitening/orthogonalization step limit its practicality for large LMs. Using our principled perspective, we develop of a more efficient, scalable, and practical stateless optimizer. Our algorithm relaxes the properties of SWAN, significantly reducing its computational cost while retaining its memory efficiency, making it applicable to training large-scale models. Experiments on pre-training LLaMA models with up to 1 billion parameters demonstrate a 3X speedup over Adam with significantly reduced memory requirements, outperforming other memory-efficient baselines.",
      "url": "https://www.microsoft.com/en-us/research/publication/gradient-multi-normalization-for-stateless-and-scalable-llm-training/"
    },
    {
      "title": "Examining False Positives under Inference Scaling for Mathematical Reasoning",
      "authors": [
        "Furu Wei",
        "Liang Wang",
        "Nan Yang",
        "Yu Wang"
      ],
      "research_areas": [
        "Human language technologies"
      ],
      "publication_date": "February 2025",
      "abstract": "Recent advancements in language models have led to significant improvements in mathematical reasoning across various benchmarks. However, most of these benchmarks rely on automatic evaluation methods that only compare final answers using heuristics, without verifying the underlying reasoning steps. This limitation results in false positive solutions, where models may produce correct final answers but with flawed deduction paths. In this paper, we systematically examine the prevalence of false positive solutions in mathematical problem solving for language models. We analyze the characteristics and extent of this issue across different open-source models, datasets of varying difficulty levels, and decoding strategies. Specifically, we explore how false positives influence the inference time scaling behavior of language models. Our experimental results reveal that: (1) false positive solutions persist across different models, datasets, and decoding methods, (2) sampling-based inference time scaling methods do not alleviate the problem, and (3) the pass@N evaluation metric is more susceptible to false positives, suggesting a significantly lower scaling ceiling than what automatic evaluations indicate. Additionally, we analyze specific instances of false positives and discuss potential limitations in self-improvement techniques and synthetic data generation under such conditions.",
      "url": "https://www.microsoft.com/en-us/research/publication/examining-false-positives-under-inference-scaling-for-mathematical-reasoning/"
    },
    {
      "title": "Project Silica: Towards Sustainable Cloud Archival Storage in Glass (Transactions on Storage)",
      "authors": [
        "Aaron Ogus",
        "Adam Smith",
        "Alana Marzoev",
        "Alexander Gaunt",
        "Andromachi Chatzieleftheriou",
        "Ant Rowstron",
        "Ariel Gomez Diaz",
        "Austin Donnelly",
        "Benn Thomsen",
        "Bridgette Cooper",
        "Bruno Magalhaes",
        "Burcu Canakci",
        "Charles Whittaker",
        "Christos Gkantsidis",
        "Daniel Cletheroe",
        "David Lara",
        "David Sweeney",
        "Erika B. Aranas",
        "Fanglin Liu",
        "Freddie Hong",
        "Govert Verkes",
        "Hiske Overweg",
        "Hugh Williams",
        "Ioan Stefanovici",
        "Istvan Haller",
        "James Clegg",
        "Jayashree Mohan",
        "Jonathan Westcott",
        "Luke Weston",
        "Maneesh Sah",
        "Marco Caballero",
        "Marvin McNett",
        "Masaaki Sakakura",
        "Michael Myrah",
        "Nina Schreiner",
        "Omer Sella",
        "Pablo Wilke Berenguer",
        "Pashmina Cameron",
        "Patrick Anderson",
        "Peter Scholtz",
        "Phil Wainman",
        "Raphael Behrendt",
        "Rebekah Clarke",
        "Richard Black",
        "Rokas Drevinskas",
        "Russell Joyce",
        "Sebastian Nowozin",
        "Sergey Legtchenko",
        "Shashidhar Joshi",
        "Stefan Winzeck",
        "Teodora Ilieva",
        "Thales De Carvalho",
        "Thomas Winkler",
        "Tim Deegan",
        "Truong Nguyen",
        "William Kunkel",
        "Youssef Assaf"
      ],
      "research_areas": [
        "Systems and networking"
      ],
      "publication_date": "February 2025",
      "abstract": "Sustainable and cost-effective long-term storage remains an unsolved problem. The most widely used storage technologies today are magnetic (hard disk drives and tape). They use media that degrades over time and has a limited lifetime, which leads to inefficient, wasteful, and costly solutions for long-lived data. This article presents Silica: the first cloud storage system for archival data underpinned by quartz glass, an extremely resilient media that allows data to be left in situ indefinitely. The hardware and software of Silica have been co-designed and co-optimized from the media up to the service level with sustainability as a primary objective. The design follows a cloud-first, data-driven methodology underpinned by principles derived from analyzing the archival workload of a large public cloud service. Silica can support a wide range of archival storage workloads and ushers in a new era of sustainable, cost-effective storage.",
      "url": "https://www.microsoft.com/en-us/research/publication/project-silica-towards-sustainable-cloud-archival-storage-in-glass-transactions-on-storage/"
    },
    {
      "title": "Simplifying DINO via Coding Rate Regularization",
      "authors": [
        "Chandan Singh",
        "Druv Pai",
        "Jianfeng Gao",
        "Jianwei Yang",
        "Jingyuan Zhang",
        "XuDong Wang",
        "Yi Ma",
        "Ziyang Wu"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Computer vision"
      ],
      "publication_date": "February 2025",
      "abstract": "DINO and DINOv2 are two model families being widely used to learn representations from unlabeled imagery data at large scales. Their learned representations often enable state-of-the-art performance for downstream tasks, such as image classification and segmentation. However, they employ many empirically motivated design choices and their training pipelines are highly complex and unstable — many hyperparameters need to be carefully tuned to ensure that the representations do not collapse — which poses considerable difficulty to improving them or adapting them to new domains. In this work, we posit that we can remove most such-motivated idiosyncrasies in the pre-training pipelines, and only need to add an explicit coding rate term in the loss function to avoid collapse of the representations. As a result, we obtain highly simplified variants of the DINO and DINOv2 which we call SimDINO and SimDINOv2, respectively. Remarkably, these simplified models are more robust to different design choices, such as network architecture and hyperparameters, and they learn even higher-quality representations, measured by performance on downstream tasks, offering a Pareto improvement over the corresponding DINO and DINOv2 models. This work highlights the potential of using simplifying design principles to improve the empirical practice of deep learning.",
      "url": "https://www.microsoft.com/en-us/research/publication/simplifying-dino-via-coding-rate-regularization/"
    },
    {
      "title": "On the Query Complexity of Verifier-Assisted Language Generation",
      "authors": [
        "Aashay Mehta",
        "Andrej Risteski",
        "Cyril Zhang",
        "Edoardo Botta",
        "Jordan Ash",
        "Yuchen Li"
      ],
      "research_areas": [
        "Algorithms",
        "Artificial intelligence"
      ],
      "publication_date": "February 2025",
      "abstract": "Recently, a plethora of works have proposed inference-time algorithms (e.g. best-of-n), which incorporate verifiers to assist the generation process. Their quality-efficiency trade-offs have been empirically benchmarked on a variety of constrained generation tasks, but the algorithmic design landscape is still largely poorly understood. In this paper, we develop a mathematical framework for reasoning about constrained generation using a pre-trained language model generator oracle and a process verifier–which can decide whether a prefix can be extended to a string which satisfies the constraints of choice. We show that even in very simple settings, access to a verifier can render an intractable problem (information-theoretically or computationally) to a tractable one. In fact, we show even simple algorithms, like tokenwise rejection sampling, can enjoy significant benefits from access to a verifier. Empirically, we show that a natural modification of tokenwise rejection sampling, in which the sampler is allowed to “backtrack”(i.e., erase the final few generated tokens) has robust and substantive benefits over natural baselines (e.g. (blockwise) rejection sampling, nucleus sampling)–both in terms of computational efficiency, accuracy and diversity.",
      "url": "https://www.microsoft.com/en-us/research/publication/on-the-query-complexity-of-verifier-assisted-language-generation/"
    },
    {
      "title": "LM Agents for Coordinating Multi-User Information Gathering",
      "authors": [
        "Ben Van Durme",
        "Harsh Jhamtani",
        "Jacob Andreas"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Human language technologies"
      ],
      "publication_date": "February 2025",
      "abstract": "This paper introduces PeopleJoin, a benchmark for evaluating LM-mediated collaborative problem solving. Given a user request, PeopleJoin agents must identify teammates who might be able to assist, converse with these teammates to gather information, and finally compile a useful answer or summary for the original user. PeopleJoin comprises two evaluation domains: PeopleJoin-QA, focused on questions about tabular data, and PeopleJoin-DocCreation, focused on document creation tasks. The two domains are adapted from existing NLP benchmarks for database question answering and multi-document summarization; here, however, the information needed to complete these tasks is distributed across synthetic “organizations” of 2–20 users, simulating natural multi-user collaboration scenarios. We implemented several popular LM agent architectures, evaluating their accuracy and efficiency at completing tasks, and highlight new research questions that can be studied using PeopleJoin.",
      "url": "https://www.microsoft.com/en-us/research/publication/lm-agents-for-coordinating-multi-user-information-gathering/"
    },
    {
      "title": "Are Language Models Up to Sequential Optimization Problems? From Evaluation to a Hegelian-Inspired Enhancement",
      "authors": [
        "Soheil Abbasloo"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Systems and networking"
      ],
      "publication_date": "February 2025",
      "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities across numerous fields, presenting an opportunity to revolutionize optimization problem-solving, a crucial, ubiquitous, and complex domain. This paper explores the proficiency of LLMs in handling Sequential Optimization Problems (SOPs). We introduce WorldGen, a dynamic framework for generating unseen SOPs with controllable complexities, to evaluate LLM performance. Our initial observations reveal that while LLMs perform well on simple SOPs, their performance significantly degrades with increased complexity. Motivated by this, we revisit philosophical hypotheses on reasoning to enhance LLM performance. Inspired by the influential framework of Hegelian Dialectics, we propose ACE, demonstrating how the performance of LLMs in SOP contexts can be significantly improved without any retraining or further fine-tuning.",
      "url": "https://www.microsoft.com/en-us/research/publication/are-language-models-up-to-sequential-optimization-problems-from-evaluation-to-a-hegelian-inspired-enhancement/"
    },
    {
      "title": "Minerva: A Programmable Memory Test Benchmark for Language Models",
      "authors": [
        "Menglin Xia",
        "Reza Shokri",
        "Saravan Rajmohan",
        "Victor Ruehle"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "February 2025",
      "abstract": "How effectively can LLM-based AI assistants utilize their memory (context) to perform various tasks? Traditional data benchmarks, which are often manually crafted, suffer from several limitations: they are static, susceptible to overfitting, difficult to interpret, and lack actionable insights–failing to pinpoint the specific capabilities a model lacks when it does not pass a test. In this paper, we present a framework for automatically generating a comprehensive set of tests to evaluate models’ abilities to use their memory effectively. Our framework extends the range of capability tests beyond the commonly explored (passkey, key-value, needle in the haystack) search, a dominant focus in the literature. Specifically, we evaluate models on atomic tasks such as searching, recalling, editing, matching, comparing information in context memory, and performing basic operations when inputs are structured into distinct blocks, simulating real-world data. Additionally, we design composite tests to investigate the models’ ability to maintain state while operating on memory. Our benchmark enables an interpretable, detailed assessment of memory capabilities of LLMs.",
      "url": "https://www.microsoft.com/en-us/research/publication/minerva-a-programmable-memory-test-benchmark-for-language-models/"
    },
    {
      "title": "Recall, Robustness, and Lexicographic Evaluation",
      "authors": [
        "Bhaskar Mitra",
        "Fernando Diaz",
        "Michael D. Ekstrand"
      ],
      "research_areas": [
        "Search and information retrieval"
      ],
      "publication_date": "February 2025",
      "abstract": "Although originally developed to evaluate sets of items, recall is often used to evaluate rankings of items, including those produced by recommender, retrieval, and other machine learning systems. The application of recall without a formal evaluative motivation has led to criticism of recall as a vague or inappropriate measure. In light of this debate, we reflect on the measurement of recall in rankings from a formal perspective. Our analysis is composed of three tenets: recall, robustness, and lexicographic evaluation. First, we formally define `recall-orientation’ as the sensitivity of a metric to a user interested in finding every relevant item. Second, we analyze recall-orientation from the perspective of robustness with respect to possible content consumers and providers, connecting recall to recent conversations about fair ranking. Finally, we extend this conceptual and theoretical treatment of recall by developing a practical preference-based evaluation method based on lexicographic comparison. Through extensive empirical analysis across three recommendation tasks and 17 information retrieval tasks, we establish that our new evaluation method, lexirecall, has convergent validity (i.e., it is correlated with existing recall metrics) and exhibits substantially higher sensitivity in terms of discriminative power and stability in the presence of missing labels. Our conceptual, theoretical, and empirical analysis substantially deepens our understanding of recall and motivates its adoption through connections to robustness and fairness.",
      "url": "https://www.microsoft.com/en-us/research/publication/recall-as-a-measure-of-ranking-robustness/"
    },
    {
      "title": "AI in the Era of GPT: Transforming the Future of Work and Discovery",
      "authors": [
        "Charles K Crawford",
        "Elliot K Fishman",
        "Felipe Lopez-Ramirez",
        "Juan M. Lavista Ferres",
        "Linda C Chu",
        "Steven P Rowe"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Medical, health and genomics"
      ],
      "publication_date": "February 2025",
      "abstract": "In this new era of accelerated discovery and rapid technological advancements, artificial intelligence (AI) stands out as one of the most prominent areas of innovation, transforming how we approach complex problems across different fields. The recognition by the Nobel Prize committee of the recent physics and chemistry laureates, John Hopfield and Geoffrey Hinton, for their role in enabling artificial neural networks is one example that emphasizes the transformative power that AI holds in our time [1]. The capabilities of AI seem to expand as quickly as hardware can provide increased capabilities [2,3]. With such power, AI has inspired a new level of optimism in the scientific community—although many are still skeptical of its potential impact on society [4,5]. That only seems natural, as throughout history, humans have evolved to focus on negative news, adopting a pessimistic outlook as a vital survival mechanism. Indeed, although we are in an era in which the majority of human development metrics—including infant mortality, poverty, and global literacy rates—have been trending in a positive direction for the past several decades, there remains a focus on negative news [6,7].",
      "url": "https://www.microsoft.com/en-us/research/publication/ai-in-the-era-of-gpt-transforming-the-future-of-work-and-discovery/"
    },
    {
      "title": "Boosting GPT Models for Genomics Analysis: Generating Trusted Genetic Variant Annotations and Interpretations through RAG and fine-tuning",
      "authors": [
        "Erdal Cosgun",
        "Shuangjia Lu"
      ],
      "research_areas": [
        "Medical, health and genomics"
      ],
      "publication_date": "February 2025",
      "abstract": "Large language models (LLMs) have acquired a remarkable level of knowledge through their initial training. However, they lack expertise in particular domains such as genomics. Variant annotation data, an important component of genomics, is crucial for interpreting and prioritizing disease-related variants among millions of variants identified by genetic sequencing. In our project, we aimed to improve LLM performance in genomics by adding variant annotation data to LLMs by retrieval-augmented generation (RAG) and fine-tuning techniques. Using RAG, we successfully integrated 190 million highly accurate variant annotations, curated from 5 major annotation datasets and tools, into GPT-4o. This integration empowers users to query specific variants and receive accurate variant annotations and interpretations supported by advanced reasoning and language understanding capabilities of LLMs. Additionally, fine-tuning GPT-4 on variant annotation data also improved model performance in some annotation fields, although the accuracy across more fields remains suboptimal. Our model significantly improved the accessibility and efficiency of the variant interpretation process by leveraging LLM capabilities. Our project also revealed that RAG outperforms fine-tuning in factual knowledge injection in terms of data volume, accuracy, and cost-effectiveness. As a pioneering study for adding genomics knowledge to LLMs, our work paves the way for developing more comprehensive and informative genomics AI systems to support clinical diagnosis and research projects, and it demonstrates the potential of LLMs in specialized domains.",
      "url": "https://www.microsoft.com/en-us/research/publication/boosting-gpt-models-for-genomics-analysis-generating-trusted-genetic-variant-annotations-and-interpretations-through-rag-and-fine-tuning/"
    },
    {
      "title": "Emancipatory Information Retrieval",
      "authors": [
        "Bhaskar Mitra"
      ],
      "research_areas": [
        "Search and information retrieval",
        "Social sciences"
      ],
      "publication_date": "February 2025",
      "abstract": "Our world today is facing a confluence of several mutually reinforcing crises each of which intersects with concerns of social justice and emancipation. This paper is a provocation for the role of computer-mediated information access in our emancipatory struggles. We define emancipatory information retrieval as the study and development of information access methods that challenge various forms of human oppression, and situates its activities within broader collective emancipatory praxis. The term “emancipatory” here signifies the moral concerns of universal humanization of all peoples and the elimination of oppression to create the conditions under which we can collectively flourish. To develop an emancipatory research agenda for IR, in this paper we speculate about the practices that the community can adopt, enumerate some of the projects that the field should undertake, and discuss provocations to spark new ideas and directions for research. We challenge the field of information retrieval (IR) research to embrace humanistic values and commit to universal emancipation and social justice as part of our research.",
      "url": "https://www.microsoft.com/en-us/research/publication/emancipatory-information-retrieval/"
    },
    {
      "title": "Effects of LLM Use and Note-Taking On Reading Comprehension and Memory: A Randomised Experiment in Secondary Schools",
      "authors": [
        "Abigail Sellen"
      ],
      "research_areas": [
        "Programming languages and software engineering"
      ],
      "publication_date": "February 2025",
      "abstract": "The rapid uptake of Generative AI, particularly large language models (LLMs), by students raises urgent questions about their effects on learning. We compared the impact of LLM use to that of traditional note-taking, or a combination of both, on secondary school students’ reading comprehension and retention. We conducted a pre-registered, randomised controlled experiment with within-and between-participant design elements in schools. 405 students aged 14-15 studied two text passages and completed comprehension and retention tests three days later. Quantitative results demonstrated that both note-taking alone and combined with the LLM had significant positive effects on retention and comprehension compared to the LLM alone. Yet, most students preferred using the LLM over note-taking, and perceived it as more helpful. Qualitative results revealed that many students valued LLMs for making complex material more accessible and reducing cognitive load, while they appreciated note-taking for promoting deeper engagement and aiding memory. Additionally, we identified “archetypes” of prompting behaviour, offering insights into the different ways students interacted with the LLM. Overall, our findings suggest that, while note-taking promotes cognitive engagement and long-term comprehension and retention, LLMs may facilitate initial understanding and student interest. The study reveals the continued importance of traditional learning approaches, the benefits of combining AI use with traditional learning over using AI alone, and the AI skills that students need to maximise those benefits.",
      "url": "https://www.microsoft.com/en-us/research/publication/effects-of-llm-use-and-note-taking-on-reading-comprehension-and-memory-a-randomised-experiment-in-secondary-schools/"
    },
    {
      "title": "ε-VAE: Denoising as Visual Decoding",
      "authors": [
        "Boqing Gong",
        "Han Zhang",
        "Hartwig Adam",
        "Long Zhao",
        "Sanghyun Woo",
        "Ting Liu",
        "Xuhui Jia",
        "Yandong Li",
        "Ziyu Wan"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Computer vision",
        "Graphics and multimedia"
      ],
      "publication_date": "February 2025",
      "abstract": "In generative modeling, tokenization simplifies complex data into compact, structured representations, creating a more efficient, learnable space. For high-dimensional visual data, it reduces redundancy and emphasizes key features for high-quality generation. Current visual tokenization methods rely on a traditional autoencoder framework, where the encoder compresses data into latent representations, and the decoder reconstructs the original input. In this work, we offer a new perspective by proposing denoising as decoding, shifting from single-step reconstruction to iterative refinement. Specifically, we replace the decoder with a diffusion process that iteratively refines noise to recover the original image, guided by the latents provided by the encoder. We evaluate our approach by assessing both reconstruction (rFID) and generation quality (FID), comparing it to state-of-the-art autoencoding approaches. By adopting iterative reconstruction through diffusion, our autoencoder, namely \\(\\epsilon\\)-VAE, achieves high reconstruction quality, which in turn enhances downstream generation quality by 22% and provides 2.3\\(\\times\\) inference speedup. We hope this work offers new insights into integrating iterative generation and autoencoding for improved compression and generation.",
      "url": "https://www.microsoft.com/en-us/research/publication/ε-vae-denoising-as-visual-decoding/"
    },
    {
      "title": "From Uncertain to Safe: Conformal Fine-Tuning of Diffusion Models for Safe PDE Control",
      "authors": [
        "Haodong Feng",
        "Long Wei",
        "Peiyan Hu",
        "Rui Wang",
        "Ruiqi Feng",
        "Tailin Wu",
        "Tao Zhang",
        "Wenhao Deng",
        "Xiaowei Qian",
        "Yue Wang",
        "Zhi-Ming Ma"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "February 2025",
      "abstract": "The application of deep learning for partial differential equation (PDE)-constrained control is gaining increasing attention. However, existing methods rarely consider safety requirements crucial in real-world applications. To address this limitation, we propose Safe Diffusion Models for PDE Control (SafeDiffCon), which introduce the uncertainty quantile as model uncertainty quantification to achieve optimal control under safety constraints through both post-training and inference phases. Firstly, our approach post-trains a pre-trained diffusion model to generate control sequences that better satisfy safety constraints while achieving improved control objectives via a reweighted diffusion loss, which incorporates the uncertainty quantile estimated using conformal prediction. Secondly, during inference, the diffusion model dynamically adjusts both its generation process and parameters through iterative guidance and fine-tuning, conditioned on control targets while simultaneously integrating the estimated uncertainty quantile. We evaluate SafeDiffCon on three control tasks: 1D Burgers’ equation, 2D incompressible fluid, and controlled nuclear fusion problem. Results demonstrate that SafeDiffCon is the only method that satisfies all safety constraints, whereas other classical and deep learning baselines fail. Furthermore, while adhering to safety constraints, SafeDiffCon achieves the best control performance. The code can be found at https://github.com/AI4Science-WestlakeU/safediffcon.",
      "url": "https://www.microsoft.com/en-us/research/publication/from-uncertain-to-safe-conformal-fine-tuning-of-diffusion-models-for-safe-pde-control/"
    },
    {
      "title": "PIKE-RAG: sPecIalized KnowledgE and Rationale Augmented Generation",
      "authors": [
        "Jiang Bian",
        "Jingjing Fu",
        "Jinyu Wang",
        "Lei Song",
        "Rui Wang"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Human language technologies",
        "Search and information retrieval"
      ],
      "publication_date": "February 2025",
      "abstract": "Despite notable advancements in Retrieval-Augmented Generation (RAG) systems that expand large language model (LLM) capabilities through external retrieval, these systems often struggle to meet the complex and diverse needs of real-world industrial applications. The reliance on retrieval alone proves insufficient for extracting deep, domain-specific knowledge performing in logical reasoning from specialized corpora. To address this, we introduce sPecIalized KnowledgE and Rationale Augmentation Generation (PIKE-RAG), focusing on extracting, understanding, and applying specialized knowledge, while constructing coherent rationale to incrementally steer LLMs toward accurate responses. Recognizing the diverse challenges of industrial tasks, we introduce a new paradigm that classifies tasks based on their complexity in knowledge extraction and application, allowing for a systematic evaluation of RAG systems’ problem-solving capabilities. This strategic approach offers a roadmap for the phased development and enhancement of RAG systems, tailored to meet the evolving demands of industrial applications. Furthermore, we propose knowledge atomizing and knowledge-aware task decomposition to effectively extract multifaceted knowledge from the data chunks and iteratively construct the rationale based on original query and the accumulated knowledge, respectively, showcasing exceptional performance across various benchmarks.",
      "url": "https://www.microsoft.com/en-us/research/publication/pike-rag-specialized-knowledge-and-rationale-augmented-generation/"
    },
    {
      "title": "When Do LLMs Help With Node Classification? A Comprehensive Analysis",
      "authors": [
        "Caihua Shan",
        "Fangzhou Ge",
        "Hong Cheng",
        "Xiangguo Sun",
        "Xixi Wu",
        "Yifei Shen",
        "Yizhu Jiao"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "February 2025",
      "abstract": "Node classification is a fundamental task in graph analysis, with broad applications across various fields. Recent breakthroughs in Large Language Models (LLMs) have enabled LLM-based approaches for this task. Although many studies demonstrate the impressive performance of LLM-based methods, the lack of clear design guidelines may hinder their practical application. In this work, we aim to establish such guidelines through a fair and systematic comparison of these algorithms. As a first step, we developed LLMNodeBed, a comprehensive codebase and testbed for node classification using LLMs. It includes 10 homophilic datasets, 4 heterophilic datasets, 8 LLM-based algorithms, 8 classic baselines, and 3 learning paradigms. Subsequently, we conducted extensive experiments, training and evaluating over 2,700 models, to determine the key settings (e.g., learning paradigms and homophily) and components (e.g., model size and prompt) that affect performance. Our findings uncover 8 insights, e.g., (1) LLM-based methods can significantly outperform traditional methods in a semi-supervised setting, while the advantage is marginal in a supervised setting; (2) Graph Foundation Models can beat open-source LLMs but still fall short of strong LLMs like GPT-4o in a zero-shot setting. We hope that the release of LLMNodeBed, along with our insights, will facilitate reproducible research and inspire future studies in this field.",
      "url": "https://www.microsoft.com/en-us/research/publication/when-do-llms-help-with-node-classification-a-comprehensive-analysis/"
    },
    {
      "title": "ART: Anonymous Region Transformer for Variable Multi-Layer Transparent Image Generation",
      "authors": [
        "Baining Guo",
        "Dong Chen",
        "Gao Huang",
        "Haoxing Ye",
        "Ji Li",
        "Jianmin Bao",
        "Lijuan Wang",
        "Lin Liang",
        "Ruihong Yin",
        "Sirui Zhang",
        "Xiu Li",
        "Yanbin Wang",
        "Yifan Pu",
        "Yiming Zhao",
        "Yuhui Yuan",
        "Zhicong Tang",
        "Zhouhui Lian"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "February 2025",
      "abstract": "Multi-layer image generation is a fundamental task that enables users to isolate, select, and edit specific image layers, thereby revolutionizing interactions with generative models. In this paper, we introduce the Anonymous Region Transformer (ART), which facilitates the direct generation of variable multi-layer transparent images based on a global text prompt and an anonymous region layout. Inspired by Schema theory suggests that knowledge is organized in frameworks (schemas) that enable people to interpret and learn from new information by linking it to prior knowledge.}, this anonymous region layout allows the generative model to autonomously determine which set of visual tokens should align with which text tokens, which is in contrast to the previously dominant semantic layout for the image generation task. In addition, the layer-wise region crop mechanism, which only selects the visual tokens belonging to each anonymous region, significantly reduces attention computation costs and enables the efficient generation of images with numerous distinct layers (e.g., 50+). When compared to the full attention approach, our method is over 12 times faster and exhibits fewer layer conflicts. Furthermore, we propose a high-quality multi-layer transparent image autoencoder that supports the direct encoding and decoding of the transparency of variable multi-layer images in a joint manner. By enabling precise control and scalable layer generation, ART establishes a new paradigm for interactive content creation.",
      "url": "https://www.microsoft.com/en-us/research/publication/art-anonymous-region-transformer-for-variable-multi-layer-transparent-image-generation/"
    },
    {
      "title": "Enhancing Network Failure Mitigation with Performance-Aware Ranking",
      "authors": [
        "Arvin Ghavidel",
        "Behnaz Arzani",
        "Daniel Berger",
        "Daniel Crankshaw",
        "Kevin Hsieh",
        "Pooria Namyar",
        "Ramesh Govindan",
        "Srikanth Kandula"
      ],
      "research_areas": [
        "Systems and networking"
      ],
      "publication_date": "February 2025",
      "abstract": "Some faults in data center networks require hours to days to repair because they may need reboots, re-imaging, or manual work by technicians. To reduce traffic impact, cloud providers \\textit{mitigate} the effect of faults, for example, by steering traffic to alternate paths. The state-of-art in automatic network mitigations uses simple safety checks and proxy metrics to determine mitigations. SWARM, the approach described in this paper, can pick orders of magnitude better mitigations by estimating end-to-end connection-level performance (CLP) metrics. At its core is a scalable CLP estimator that quickly ranks mitigations with high fidelity and, on failures observed at a large cloud provider, outperforms the state-of-the-art by over 700\\(×\\) in some cases.",
      "url": "https://www.microsoft.com/en-us/research/publication/enhancing-network-failure-mitigation-with-performance-aware-ranking/"
    },
    {
      "title": "Egocentric zone-aware action recognition across environments",
      "authors": [
        "Andrea Bottino",
        "Barbara Caputo",
        "Gabriele Goletto",
        "Giuseppe Averta",
        "M. Planamente",
        "Simone Alberto Peirone"
      ],
      "research_areas": [
        "Computer vision"
      ],
      "publication_date": "February 2025",
      "abstract": "Human activities exhibit a strong correlation between actions and the places where these are performed, such as washing something at a sink. More specifically, in daily living environments we may identify particular locations, hereinafter named activity-centric zones, which may afford a set of homogeneous actions. Their knowledge can serve as a prior to favor vision models to recognize human activities. However, the appearance of these zones is scene-specific, limiting the transferability of this prior information to unfamiliar areas and domains. This problem is particularly relevant in egocentric vision, where the environment takes up most of the image, making it even more difficult to separate the action from the context. In this paper, we discuss the importance of decoupling the domain-specific appearance of activity-centric zones from their universal, domain-agnostic representations, and show how the latter can improve the cross-domain transferability of Egocentric Action Recognition (EAR) models. We validate our solution on the EPIC-Kitchens-100 and Argo1M datasets",
      "url": "https://www.microsoft.com/en-us/research/publication/egocentric-zone-aware-action-recognition-across-environments/"
    },
    {
      "title": "QURE: AI-assisted and Automatically Verified UDF Inlining",
      "authors": [
        "Arnd Christian König",
        "Cong Yan",
        "Jiashen Cao",
        "Shuvendu Lahiri (shuvendu)",
        "Tarique Siddiqui"
      ],
      "research_areas": [
        "Data platforms and analytics",
        "Programming languages and software engineering"
      ],
      "publication_date": "February 2025",
      "abstract": "User-defined functions (UDFs) extend the capabilities of SQL by improving code reusability and encapsulating complex logic, but can hinder the performance due to optimization and execution inefficiencies. Prior approaches attempt to address this by rewriting UDFs into native SQL, which is then inlined into the SQL queries that invoke them. However, these approaches are either limited to simple pattern matching or require the synthesis of complex verification conditions from procedural code, a process that is brittle and difficult to automate. This limits coverage and makes the translation approaches less extensible to unseen procedural constructs. In this work, we present QURE, a framework that (1) leverages large language models (LLMs) to translate UDFs to native SQL, and (2) introduces a novel formal verification method to establish equivalence between the UDF and its translation. QURE uses the semantics of SQL operators to automate the derivation of verification conditions, in turn resulting in broad coverage and high extensibility. We model a large set of imperative constructs, particularly those common in Python and Pandas UDFs, in an intermediate verification language, allowing for the verification of their SQL translation. In our empirical evaluation of Python and Pandas UDFs, equivalence is successfully verified for 88% of UDF-SQL pairs, with LLMs correctly translating 84.8% of these UDFs. The remaining UDFs lack semantically equivalent SQL. Executing the translated UDFs achieves median performance improvements of 23.7x on single-node clusters and 12.5x on 12-node clusters compared to the original UDFs, while also significantly reducing out-of-memory errors.\nPython and Pandas UDFs used for evaluating QURE: QURE Benchmark.zip (opens in new tab)",
      "url": "https://www.microsoft.com/en-us/research/publication/qure/"
    },
    {
      "title": "Efficient and Scalable Density Functional Theory Hamiltonian Prediction through Adaptive Sparsity",
      "authors": [
        "Bin Shao",
        "Chang Liu",
        "Erpai Luo",
        "Han Yang",
        "Jia Zhang",
        "Lin Huang",
        "Xinran wei",
        "Yunyang Li",
        "Zaishuo Xia",
        "Zun Wang"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "February 2025",
      "abstract": "Hamiltonian matrix prediction is pivotal in computational chemistry, serving as the foundation for determining a wide range of molecular properties. While SE(3) equivariant graph neural networks have achieved remarkable success in this domain, their substantial computational cost–driven by high-order tensor product (TP) operations–restricts their scalability to large molecular systems with extensive basis sets. To address this challenge, we introduce SPHNet, an efficient and scalable equivariant network, that incorporates adaptive SParsity into Hamiltonian prediction. SPHNet employs two innovative sparse gates to selectively constrain non-critical interaction combinations, significantly reducing tensor product computations while maintaining accuracy. To optimize the sparse representation, we develop a Three-phase Sparsity Scheduler, ensuring stable convergence and achieving high performance at sparsity rates of up to 70%. Extensive evaluations on QH9 and PubchemQH datasets demonstrate that SPHNet achieves state-of-the-art accuracy while providing up to a 7x speedup over existing models. Beyond Hamiltonian prediction, the proposed sparsification techniques also hold significant potential for improving the efficiency and scalability of other SE(3) equivariant networks, further broadening their applicability and impact. Our code can be found at https://github.com/microsoft/SPHNet (opens in new tab).",
      "url": "https://www.microsoft.com/en-us/research/publication/efficient-and-scalable-density-functional-theory-hamiltonian-prediction-through-adaptive-sparsity/"
    },
    {
      "title": "Triangle: Empowering Incident Triage with Multi-LLM-Agents",
      "authors": [
        "Chaoyun Zhang",
        "Chetan Bansal",
        "Minghua Ma",
        "Murali Chintalapati",
        "Qingwei Lin 林庆维",
        "Rujia Wang",
        "Ruomeng Ding",
        "Saravan Rajmohan",
        "Xiaoyong Feng",
        "Xuchao Zhang",
        "Ze Li",
        "Zhaoyang Yu"
      ],
      "research_areas": [
        "Data platforms and analytics",
        "Systems and networking"
      ],
      "publication_date": "February 2025",
      "abstract": "As cloud service systems grow in scale and complexity, incidents that indicate unplanned interruptions and outages become unavoidable. Rapid and accurate triage of these incidents to the appropriate responsible teams is crucial to maintain service reliability and prevent significant financial losses. However, existing incident triage methods relying on manual operations and predefined rules often struggle with efficiency and accuracy due to the heterogeneity of incident data and the dynamic nature of domain knowledge across multiple teams. To solve these issues, we propose Triangle, an end-to-end incident triage system based on a Multi-LLM-Agent framework. Triangle leverages a semantic distillation mechanism to tackle the issue of semantic heterogeneity in incident data, enhancing the accuracy of incident triage. Additionally, we introduce multi-role agents and a negotiation mechanism to emulate human engineers’ workflows, effectively handling decentralized and dynamic domain knowledge from multiple teams. Furthermore, our system incorporates an automated troubleshooting information collection and mitigation mechanism, reducing the reliance on human labor and enabling fully automated end-to-end incident triage. Extensive experiments conducted on real-world cloud production environment demonstrate that Triangle significantly improves the accuracy of incident triage more than 20% and reduces the time to engage about 3 time units per incident compared to state-of-the-art methods. Triangle has been successfully deployed in a system with tens of millions of users at a leading global technology company.",
      "url": "https://www.microsoft.com/en-us/research/publication/triangle-empowering-incident-triage-with-multi-llm-agents/"
    },
    {
      "title": "MMDT: Decoding the Trustworthiness and Safety of Multimodal Foundation Models",
      "authors": [
        "Alexander Xiong",
        "Andy Zhou",
        "Bo Li",
        "Chejian Xu",
        "Chengquan Guo",
        "Chenhui Zhang",
        "Chulin Xie",
        "Dan Hendrycks",
        "Dawn Song",
        "Francesco Pinto",
        "Jeffrey Ziwei Tan",
        "Jiawei Zhang",
        "Lingzhi Yuan",
        "Mintong Kang",
        "Peiyang Xu",
        "Xuandong Zhao",
        "Yi Zeng",
        "Yu Gai",
        "Yujin Potter",
        "Zhaorun Chen",
        "Zhen Xiang",
        "Zhun Wang",
        "Zhuowen Yuan",
        "Zidi Xiong",
        "Zinan Lin"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Computer vision",
        "Security, privacy, and cryptography"
      ],
      "publication_date": "February 2025",
      "abstract": "Multimodal foundation models (MMFMs) play a crucial role in various applications, including autonomous driving, healthcare, and virtual assistants. However, several studies have revealed vulnerabilities in these models, such as generating unsafe content by text-to-image models. Existing benchmarks on multimodal models either predominantly assess the helpfulness of these models, or only focus on limited perspectives such as fairness and privacy. In this paper, we present the first unified platform, MMDT (Multimodal DecodingTrust), designed to provide a comprehensive safety and trustworthiness evaluation for MMFMs. Our platform assesses models from multiple perspectives, including safety, hallucination, fairness/bias, privacy, adversarial robustness, and out-of-distribution (OOD) generalization. We have designed various evaluation scenarios and red teaming algorithms under different tasks for each perspective to generate challenging data, forming a high-quality benchmark. We evaluate a range of multimodal models using MMDT, and our findings reveal a series of vulnerabilities and areas for improvement across these perspectives. This work introduces the first comprehensive and unique safety and trustworthiness evaluation platform for MMFMs, paving the way for developing safer and more reliable MMFMs and systems.",
      "url": "https://www.microsoft.com/en-us/research/publication/mmdt-decoding-the-trustworthiness-and-safety-of-multimodal-foundation-models/"
    },
    {
      "title": "MEETING DELEGATE: Benchmarking LLMs on Attending Meetings on Our Behalf",
      "authors": [
        "Dongmei Zhang",
        "Jue Zhang",
        "Lingxiang Hu",
        "Qi Zhang",
        "Qingwei Lin 林庆维",
        "Saravan Rajmohan",
        "Shurun Yuan",
        "Xiaoting Qin"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "February 2025",
      "abstract": "In contemporary workplaces, meetings are essential for exchanging ideas and ensuring team alignment but often face challenges such as time consumption, scheduling conflicts, and inefficient participation. Recent advancements in Large Language Models (LLMs) have demonstrated their strong capabilities in natural language generation and reasoning, prompting the question: can LLMs effectively delegate participants in meetings? To explore this, we develop a prototype LLM-powered meeting delegate system and create a comprehensive benchmark using real meeting transcripts. Our evaluation reveals that GPT-4/4o maintain balanced performance between active and cautious engagement strategies. In contrast, Gemini 1.5 Pro tends to be more cautious, while Gemini 1.5 Flash and Llama3-8B/70B display more active tendencies. Overall, about 60\\% of responses address at least one key point from the ground-truth. However, improvements are needed to reduce irrelevant or repetitive content and enhance tolerance for transcription errors commonly found in real-world settings. Additionally, we implement the system in practical settings and collect real-world feedback from demos. Our findings underscore the potential and challenges of utilizing LLMs as meeting delegates, offering valuable insights into their practical application for alleviating the burden of meetings.",
      "url": "https://www.microsoft.com/en-us/research/publication/meeting-delegate-benchmarking-llms-on-attending-meetings-on-our-behalf/"
    },
    {
      "title": "Distilled Decoding 1: One-step Sampling of Image Auto-regressive Models with Flow Matching",
      "authors": [
        "Enshu Liu",
        "Xuefei Ning",
        "Yu Wang",
        "Zinan Lin"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Computer vision"
      ],
      "publication_date": "February 2025",
      "abstract": "Autoregressive (AR) models have achieved state-of-the-art performance in text and image generation but suffer from slow generation due to the token-by-token process. We ask an ambitious question: can a pre-trained AR model be adapted to generate outputs in just one or two steps? If successful, this would significantly advance the development and deployment of AR models. We notice that existing works that try to speed up AR generation by generating multiple tokens at once fundamentally cannot capture the output distribution due to the conditional dependencies between tokens, limiting their effectiveness for few-step generation. To address this, we propose Distilled Decoding (DD), which uses flow matching to create a deterministic mapping from Gaussian distribution to the output distribution of the pre-trained AR model. We then train a network to distill this mapping, enabling few-step generation. DD doesn’t need the training data of the original AR model, making it more practical. We evaluate DD on state-of-the-art image AR models and present promising results on ImageNet-256. For VAR, which requires 10-step generation, DD enables one-step generation (6.3× speed-up), with an acceptable increase in FID from 4.19 to 9.96. For LlamaGen, DD reduces generation from 256 steps to 1, achieving an 217.8× speed-up with a comparable FID increase from 4.11 to 11.35. In both cases, baseline methods completely fail with FID>100. DD also excels on text-to-image generation, reducing the generation from 256 steps to 2 for LlamaGen with minimal FID increase from 25.70 to 28.95. As the first work to demonstrate the possibility of one-step generation for image AR models, DD challenges the prevailing notion that AR models are inherently slow, and opens up new opportunities for efficient AR generation. The project website is at this https URL.",
      "url": "https://www.microsoft.com/en-us/research/publication/distilled-decoding-1-one-step-sampling-of-image-auto-regressive-models-with-flow-matching/"
    },
    {
      "title": "Linear Combination of Saved Checkpoints Makes Consistency and Diffusion Models Better",
      "authors": [
        "Enshu Liu",
        "Guohao Dai",
        "Huazhong Yang",
        "Junyi Zhu",
        "Matthew B. Blaschko",
        "Sergey Yekhanin",
        "Shengen Yan",
        "Shuaiqi Wang",
        "Xuefei Ning",
        "Yu Wang",
        "Zinan Lin"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Computer vision"
      ],
      "publication_date": "February 2025",
      "abstract": "Diffusion Models (DM) and Consistency Models (CM) are two types of popular generative models with good generation quality on various tasks. When training DM and CM, intermediate weight checkpoints are not fully utilized and only the last converged checkpoint is used. In this work, we find that high-quality model weights often lie in a basin which cannot be reached by SGD but can be obtained by proper checkpoint averaging. Based on these observations, we propose LCSC, a simple but effective and efficient method to enhance the performance of DM and CM, by combining checkpoints along the training trajectory with coefficients deduced from evolutionary search. We demonstrate the value of LCSC through two use cases: (a) Reducing training cost. With LCSC, we only need to train DM/CM with fewer number of iterations and/or lower batch sizes to obtain comparable sample quality with the fully trained model. For example, LCSC achieves considerable training speedups for CM (23× on CIFAR-10 and 15× on ImageNet-64). (b) Enhancing pre-trained models. Assuming full training is already done, LCSC can further improve the generation quality or speed of the final converged models. For example, LCSC achieves better performance using 1 number of function evaluation (NFE) than the base model with 2 NFE on consistency distillation, and decreases the NFE of DM from 15 to 9 while maintaining the generation quality on CIFAR-10. Our code is available at this https URL.",
      "url": "https://www.microsoft.com/en-us/research/publication/linear-combination-of-saved-checkpoints-makes-consistency-and-diffusion-models-better/"
    },
    {
      "title": "Differentially Private Synthetic Data via APIs 3: Using Simulators Instead of Foundation Models",
      "authors": [
        "Sergey Yekhanin",
        "Tadas Baltrusaitis",
        "Zinan Lin"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Security, privacy, and cryptography"
      ],
      "publication_date": "February 2025",
      "abstract": "Differentially private (DP) synthetic data, which closely resembles the original private data while maintaining strong privacy guarantees, has become a key tool for unlocking the value of private data without compromising privacy. Recently, Private Evolution (PE) has emerged as a promising method for generating DP synthetic data. Unlike other training-based approaches, PE only requires access to inference APIs from foundation models, enabling it to harness the power of state-of-the-art models. However, a suitable foundation model for a specific private data domain is not always available. In this paper, we discover that the PE framework is sufficiently general to allow inference APIs beyond foundation models. Specifically, we show that simulators — such as computer graphics-based image synthesis tools — can also serve as effective APIs within the PE framework. This insight greatly expands the applicability of PE, enabling the use of a wide variety of domain-specific simulators for DP data synthesis. We explore the potential of this approach, named Sim-PE, in the context of image synthesis. Across three diverse simulators, Sim-PE performs well, improving the downstream classification accuracy of PE by up to 3x and reducing the FID score by up to 80%. We also show that simulators and foundation models can be easily leveraged together within the PE framework to achieve further improvements. The code is open-sourced in the Private Evolution Python library: this https URL.",
      "url": "https://www.microsoft.com/en-us/research/publication/differentially-private-synthetic-data-via-apis-3-using-simulators-instead-of-foundation-models/"
    },
    {
      "title": "Weaving Sound Information to Support Deaf and Hard of Hearing People’s Real-time Sensemaking of Auditory Environments: Co-designing with a DHH User",
      "authors": [
        "Cecily Morrison",
        "Dhruv Jain",
        "Jaylin Herskovitz",
        "Jeremy Zhengqi Huang",
        "Liang-Yuan Wu"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Human-computer interaction"
      ],
      "publication_date": "February 2025",
      "abstract": "Current AI sound awareness systems can provide deaf and hard of hearing people with information about sounds, including discrete sources and transcriptions. However, synthesizing AI outputs based on DHH people’s ever-changing intents to facilitate their sensemaking of complex auditory environments remains a challenge. In this paper, we describe the co-design process of SoundWeaver, a sound awareness system prototype that dynamically weaves AI outputs from different AI models based on users’ intents and presents synthesized information through a heads-up display. Adopting a Research through Design perspective, we created SoundWeaver with one DHH co-designer, adapting it to his different personal contexts and goals (e.g., cooking at home and chatting in the game store). Through this process, we present design implications for the future of “intent-driven” AI systems for sound accessibility.",
      "url": "https://www.microsoft.com/en-us/research/publication/weaving-sound-information-to-support-deaf-and-hard-of-hearing-peoples-real-time-sensemaking-of-auditory-environments-co-designing-with-a-dhh-user/"
    },
    {
      "title": "Inferentially-Private Private Information",
      "authors": [
        "Giulia Fanti",
        "Shuaiqi Wang",
        "Shuran Zheng",
        "Zhiwei Steven Wu",
        "Zinan Lin"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Security, privacy, and cryptography"
      ],
      "publication_date": "February 2025",
      "abstract": "Information disclosure can compromise privacy when revealed information is correlated with private information. We consider the notion of inferential privacy, which measures privacy leakage by bounding the inferential power a Bayesian adversary can gain by observing a released signal. Our goal is to devise an inferentially-private private information structure that maximizes the informativeness of the released signal, following the Blackwell ordering principle, while adhering to inferential privacy constraints. To achieve this, we devise an efficient release mechanism that achieves the inferentially-private Blackwell optimal private information structure for the setting where the private information is binary. Additionally, we propose a programming approach to compute the optimal structure for general cases given the utility function. The design of our mechanisms builds on our geometric characterization of the Blackwell-optimal disclosure mechanisms under privacy constraints, which may be of independent interest.",
      "url": "https://www.microsoft.com/en-us/research/publication/inferentially-private-private-information/"
    },
    {
      "title": "On the Emergence of Thinking in LLMs I: Searching for the Right Intuition",
      "authors": [
        "Baolin Peng",
        "Beibin Li",
        "Guanghao Ye",
        "Huseyin Inan",
        "Janardhan (Jana) Kulkarni",
        "Khiem Duc Pham",
        "Sivakanth Gopi",
        "Xinzhi Zhang"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "February 2025",
      "abstract": "Recent AI advancements, such as OpenAI’s new models, are transforming LLMs into LRMs (Large Reasoning Models) that perform reasoning during inference, taking extra time and compute for higher-quality outputs. We aim to uncover the algorithmic framework for training LRMs. Methods like self-consistency, PRM, and AlphaZero suggest reasoning as guided search. We ask: what is the simplest, most scalable way to enable search in LLMs?\nWe propose a post-training framework called Reinforcement Learning via Self-Play (RLSP). RLSP involves three steps: (1) supervised fine-tuning with human or synthetic demonstrations of the reasoning process, (2) using an exploration reward signal to encourage diverse and efficient reasoning behaviors, and (3) RL training with an outcome verifier to ensure correctness while preventing reward hacking. Our key innovation is to decouple exploration and correctness signals during PPO training, carefully balancing them to improve performance and efficiency.\nEmpirical studies in the math domain show that RLSP improves reasoning. On the Llama-3.1-8B-Instruct model, RLSP can boost performance by 23% in MATH-500 test set; On AIME 2024 math problems, Qwen2.5-32B-Instruct improved by 10% due to RLSP. However, a more important finding of this work is that the models trained using RLSP, even with the simplest exploration reward that encourages the model to take more intermediate steps, showed several emergent behaviors such as backtracking, exploration of ideas, and verification. These findings demonstrate that RLSP framework might be enough to enable emergence of complex reasoning abilities in LLMs when scaled. Lastly, we propose a theory as to why RLSP search strategy is more suitable for LLMs inspired by a remarkable result that says CoT provably increases computational power of LLMs, which grows as the number of steps in CoT.",
      "url": "https://www.microsoft.com/en-us/research/publication/on-the-emergence-of-thinking-in-llms-i-searching-for-the-right-intuition/"
    },
    {
      "title": "Everything Matters in Programmable Packet Scheduling",
      "authors": [
        "Albert Gran Alcoz",
        "Balazs Vass",
        "Behnaz Arzani",
        "Gabor Retvari",
        "Laurent Vanbever",
        "Pooria Namyar"
      ],
      "research_areas": [
        "Systems and networking"
      ],
      "publication_date": "February 2025",
      "abstract": "Operators can deploy any scheduler they desire on existing switches through programmable packet schedulers: they tag packets with ranks (which indicate their priority) and schedule them in the order of these ranks. The ideal programmable scheduler is the Push-In First-Out (PIFO) queue, which schedules packets in a perfectly sorted order by “pushing” packets into any position of the queue based on their ranks. However, it is hard to implement PIFO queues in hardware due to their need to sort packets at line rate (based on their ranks). Recent proposals approximate PIFO behaviors on existing data-planes. While promising, they fail to simultaneously capture both of the necessary behaviors of PIFO queues: their scheduling behavior and admission control. We introduce PACKS, an approximate PIFO scheduler that addresses this problem. PACKS runs on top of a set of priority queues and uses packet-rank information and queue-occupancy levels during enqueue to determine whether to admit each incoming packet and to which queue it should be mapped. We fully implement PACKS in P4 and evaluate it on real workloads. We show that PACKS better-approximates PIFO than state-of-the-art approaches. Specifically, PACKS reduces the rank inversions by up to 7× and 15× with respect to SPPIFO and AIFO, and the number of packet drops by up to 60% compared to SP-PIFO. Under pFabric ranks, PACKS reduces the mean FCT across small flows by up to 33% and 2.6×, compared to SP-PIFO and AIFO. We also show that PACKS runs at line rate on existing hardware (Intel Tofino).",
      "url": "https://www.microsoft.com/en-us/research/publication/everything-matters-in-programmable-packet-scheduling/"
    },
    {
      "title": "ViDiT-Q: Efficient and Accurate Quantization of Diffusion Transformers for Image and Video Generation",
      "authors": [
        "Enshu Liu",
        "Guohao Dai",
        "Huazhong Yang",
        "Shengen Yan",
        "Shiyao Li",
        "Tianchen Zhao",
        "Tongcheng Fang",
        "Wan Rui",
        "Widyadewi Soedarmadji",
        "Xuefei Ning",
        "Zinan Lin"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Computer vision"
      ],
      "publication_date": "February 2025",
      "abstract": "Diffusion transformers (DiTs) have exhibited remarkable performance in visual generation tasks, such as generating realistic images or videos based on textual instructions. However, larger model sizes and multi-frame processing for video generation lead to increased computational and memory costs, posing challenges for practical deployment on edge devices. Post-Training Quantization (PTQ) is an effective method for reducing memory costs and computational complexity. When quantizing diffusion transformers, we find that applying existing diffusion quantization methods designed for U-Net faces challenges in preserving quality. After analyzing the major challenges for quantizing diffusion transformers, we design an improved quantization scheme: “ViDiT-Q”: Video and Image Diffusion Transformer Quantization) to address these issues. Furthermore, we identify highly sensitive layers and timesteps hinder quantization for lower bit-widths. To tackle this, we improve ViDiT-Q with a novel metric-decoupled mixed-precision quantization method (ViDiT-Q-MP). We validate the effectiveness of ViDiT-Q across a variety of text-to-image and video models. While baseline quantization methods fail at W8A8 and produce unreadable content at W4A8, ViDiT-Q achieves lossless W8A8 quantization. ViDiTQ-MP achieves W4A8 with negligible visual quality degradation, resulting in a 2.5x memory optimization and a 1.5x latency speedup.",
      "url": "https://www.microsoft.com/en-us/research/publication/vidit-q-efficient-and-accurate-quantization-of-diffusion-transformers-for-image-and-video-generation/"
    },
    {
      "title": "Masked Generative Nested Transformers with Decode Time Scaling",
      "authors": [
        "Debapriya Tula",
        "Gagan Jain",
        "Pradeep Shenoy",
        "Prateek Jain",
        "Sahil Goyal",
        "Sujoy Paul"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "February 2025",
      "abstract": "Recent advances in visual generation have made significant strides in producing content of exceptional quality. However, most methods suffer from a fundamental problem – a bottleneck of inference computational efficiency. Most of these algorithms involve multiple passes over a transformer model to generate tokens or denoise inputs. However, the model size is kept consistent throughout all iterations, which makes it computationally expensive. In this work, we aim to address this issue primarily through two key ideas – (a) not all parts of the generation process need equal compute, and we design a decode time model scaling schedule to utilize compute effectively, and (b) we can cache and reuse some of the computation. Combining these two ideas leads to using smaller models to process more tokens while large models process fewer tokens. These different-sized models do not increase the parameter size, as they share parameters. We rigorously experiment with ImageNet256$\\times$256 , UCF101, and Kinetics600 to showcase the efficacy of the proposed method for image/video generation and frame prediction. Our experiments show that with almost $3\\times$ less compute than baseline, our model obtains competitive performance.",
      "url": "https://www.microsoft.com/en-us/research/publication/masked-generative-nested-transformers-with-decode-time-scaling/"
    },
    {
      "title": "Logic-RL: Unleashing LLM Reasoning with Rule-Based Reinforcement Learning",
      "authors": [
        "Bryan Dai",
        "Chong Luo",
        "Haoming Luo",
        "Joey Zhou",
        "Kai Qiu",
        "Qingnan Ren",
        "Tian Xie",
        "Yuqian Hong",
        "Zhirong Wu",
        "Zitian Gao"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "",
      "abstract": "Inspired by the success of DeepSeek-R1, we explore the potential of rule-based reinforcement learning (RL) in large reasoning models. To analyze reasoning dynamics, we use synthetic logic puzzles as training data due to their controllable complexity and straightforward answer verification. We make some key technical contributions that lead to effective and stable RL training: a system prompt that emphasizes the thinking and answering process, a stringent format reward function that penalizes outputs for taking shortcuts, and a straightforward training recipe that achieves stable convergence. Our 7B model develops advanced reasoning skills-such as reflection, verification, and summarization-that are absent from the logic corpus. Remarkably, after training on just 5K logic problems, it demonstrates generalization abilities to the challenging math benchmarks AIME and AMC.",
      "url": "https://www.microsoft.com/en-us/research/publication/logic-rl-unleashing-llm-reasoning-with-rule-based-reinforcement-learning/"
    },
    {
      "title": "Towards Efficient Large Multimodal Model Serving",
      "authors": [
        "Alind Khare",
        "Anish Biswas",
        "Chetan Bansal",
        "Esha Choukse",
        "Haiying Shen",
        "Haoran Qiu",
        "Jayashree Mohan",
        "Ramachandran Ramjee",
        "Rodrigo Fonseca",
        "Zeyu Zhang",
        "Zihan Zhao",
        "Íñigo Goiri"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Systems and networking"
      ],
      "publication_date": "February 2025",
      "abstract": "Recent advances in generative AI have led to large multimodal models (LMMs) capable of simultaneously processing inputs of various modalities such as text, images, video, and audio. While these models demonstrate impressive capabilities, efficiently serving them in production environments poses significant challenges due to their complex architectures and heterogeneous resource requirements.\nWe present the first comprehensive systems analysis of two prominent LMM architectures, decoder-only and cross attention, on six representative open-source models. We investigate their multi-stage inference pipelines and resource utilization patterns that lead to unique systems design implications. We also present an in-depth analysis of production LMMinference traces, uncovering unique workload characteristics, including variable, heavy-tailed request distributions, diverse modal combinations, and bursty traffic patterns.\nOur key findings reveal that different LMM inference stages exhibit highly heterogeneous performance characteristics and resource demands, while concurrent requests across modalities lead to significant performance interference. To address these challenges, we propose a decoupled serving architecture that enables independent resource allocation and adaptive scaling for each stage. We further propose optimizations such as stage colocation to maximize throughput and resource utilization while meeting the latency objectives.",
      "url": "https://www.microsoft.com/en-us/research/publication/towards-efficient-large-multimodal-model-serving/"
    },
    {
      "title": "Integrative Decoding: Improve Factuality via Implicit Self-consistency",
      "authors": [
        "Jian Jiao",
        "Kaishuai Xu",
        "Peng Cheng",
        "Qi Chen",
        "Song Wang",
        "Wayne Xiong",
        "Wen Xiao",
        "Wenge Liu",
        "Wenjie Li",
        "Wenjun Hou",
        "Xiao Liang",
        "Yeyun Gong",
        "Yi Cheng",
        "Yuji Zhang"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "January 2025",
      "abstract": "Self-consistency-based approaches, which involve repeatedly sampling multiple outputs and selecting the most consistent one as the final response, prove to be remarkably effective in improving the factual accuracy of large language models. Nonetheless, existing methods usually have strict constraints on the task format, largely limiting their applicability. In this paper, we present Integrative Decoding (ID), to unlock the potential of self-consistency in open-ended generation tasks. ID operates by constructing a set of inputs, each prepended with a previously sampled response, and then processes them concurrently, with the next token being selected by aggregating of all their corresponding predictions at each decoding step. In essence, this simple approach implicitly incorporates self-consistency in the decoding objective. Extensive evaluation shows that ID consistently enhances factuality over a wide range of language models, with substantial improvements on the TruthfulQA (+11.2%), Biographies (+15.4%) and LongFact (+8.5%) benchmarks. The performance gains amplify progressively as the number of sampled responses increases, indicating the potential of ID to scale up with repeated sampling.",
      "url": "https://www.microsoft.com/en-us/research/publication/integrative-decoding-improve-factuality-via-implicit-self-consistency/"
    },
    {
      "title": "LLaVA-Rad MIMIC-CXR Annotations",
      "authors": [
        "Akshay Chaudhari",
        "Chunyuan Li",
        "Cliff Wong",
        "Curtis Langlotz",
        "Fei Wang",
        "Hanwen Xu",
        "Hany Awadalla",
        "Hoifung Poon",
        "Houdong Hu",
        "Jianfeng Gao",
        "Jianwei Yang",
        "Juan Manuel Zambrano Chaves (juanza)",
        "Julia Gong",
        "Mahmoud Khademi",
        "Matthew P Lungren",
        "Mu-Hsin Wei",
        "Muhao Chen",
        "Naoto Usuyama (naotous)",
        "Serena Yeung",
        "Sheng Wang",
        "Sheng Zhang",
        "Shih-Cheng Huang",
        "Tristan Naumann",
        "Yanbo Xu",
        "Yu Gu",
        "Yujia Xie",
        "Ziyi Yang"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Computer vision",
        "Medical, health and genomics"
      ],
      "publication_date": "January 2025",
      "abstract": "LLaVA-Rad MIMIC-CXR features more accurate section extractions from MIMIC-CXR free-text radiology reports. Traditionally, rule-based methods were used to extract sections such as the reason for exam, findings, and impression. However, these approaches often fail due to inconsistencies in report structure and clinical language. In this work, we leverage GPT-4 to extract these sections more reliably, adding 237,073 image-text pairs to the training split and 1,952 pairs to the validation split. This enhancement afforded the development and fine-tuning of LLaVA-Rad, a multimodal large language model (LLM) tailored for radiology applications, achieving improved performance on report generation tasks. This resource is provided to support reproducibility and for the benefit of the research community, enabling further exploration in vision–language modeling. For more details, please refer to the accompanying paper.",
      "url": "https://www.microsoft.com/en-us/research/publication/llava-rad-mimic-cxr-annotations/"
    },
    {
      "title": "Online Rack Placement in Large-Scale Data Centers",
      "authors": [
        "Alexandre Jacquillat",
        "Ishai Menache",
        "Kayla Cummings",
        "Konstantina Mellou",
        "Marco Molinaro",
        "Rob McDonald",
        "Saumil Baxi",
        "Sean Lo"
      ],
      "research_areas": [
        "Algorithms",
        "Mathematics"
      ],
      "publication_date": "January 2025",
      "abstract": "This paper optimizes the configuration of large-scale data centers toward cost-effective, reliable and sustainable cloud supply chains. We formulate an integer optimization model that optimizes the placement of racks of servers within a data center to maximize demand coverage, adhere to space, power and cooling restrictions, and pace resource utilization for future demand. We also define a tractable single-sample online approximation (SSOA) approach to multi-stage stochastic optimization, which approximates unknown parameters with a single realization and re-optimizes decisions dynamically. Theoretical results provide strong performance guarantees of SSOA in the canonical online generalized assignment and online bin packing settings. Computational results using real-world data show that our optimization approach can enhance utilization and reduce power stranding in data centers. Following iterative improvements in collaboration with data center managers, our algorithm has been packaged into a software solution deployed in Microsoft’s data centers worldwide. Deployment data indicate a significant increase in adoption, leading to improved power utilization, multi-million-dollar annual cost savings, and concomitant savings in greenhouse gas emissions. Ultimately, this paper constitutes one of the first large-scale deployments of a decision-making tool in data centers, contributing an interactive decision-making process at the human-machine interface.",
      "url": "https://www.microsoft.com/en-us/research/publication/online-rack-placement-in-large-scale-data-centers/"
    },
    {
      "title": "sciLaMA: A Single-Cell Representation Learning Framework to Leverage Prior Knowledge from Large Language Models",
      "authors": [
        "G. Quon",
        "Hongru Hu",
        "Shuwen Zhang",
        "Venkat S. Malladi",
        "Yongin Choi"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "January 2025",
      "abstract": "Single-cell RNA sequencing (scRNA-seq) enables high-resolution exploration of cellular diversity and gene regulation, yet analyzing such data remains challenging due to technical and methodological limitations. Existing task-specific deep generative models like Variational Auto-Encoder (VAE) and its variants struggle to incorporate external biological knowledge, while transformer-based foundational large Language Models (LLMs or large LaMs) face limitations in computational cost and applicability to tabular gene expression data. Here, we introduce sciLaMA (single-cell interpretable Language Model Adapter), a novel representation learning framework that bridges these gaps by integrating static gene embeddings from multimodal LaMs with scRNA-seq tabular data through a paired-VAE architecture. Our approach generates context-aware representations for both cells and genes and outperforms state-of-the-art methods in key single-cell downstream tasks, including batch effect correction, cell clustering, and cell-state-specific gene marker and module identification, while maintaining computational efficiency. sciLaMA offers a computationally efficient, unified framework for comprehensive single-cell data analysis and biologically interpretable gene module discovery.",
      "url": "https://www.microsoft.com/en-us/research/publication/scilama-a-single-cell-representation-learning-framework-to-leverage-prior-knowledge-from-large-language-models/"
    },
    {
      "title": "Understanding the LLM-ification of CHI: Unpacking the Impact of LLMs at CHI through a Systematic Literature Review",
      "authors": [
        "Danielle Bragg",
        "Emily Tseng",
        "Hope Schroeder",
        "Kynnedy Simone Smith",
        "Rock Yuren Pang",
        "Solon Barocas",
        "Ziang Xiao"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Human-computer interaction"
      ],
      "publication_date": "January 2025",
      "abstract": "Large language models (LLMs) have been positioned to revolutionize HCI, by reshaping not only the interfaces, design patterns, and sociotechnical systems that we study, but also the research practices we use. To-date, however, there has been little understanding of LLMs’ uptake in HCI. We address this gap via a systematic literature review of 153 CHI papers from 2020-24 that engage with LLMs. We taxonomize: (1) domains where LLMs are applied; (2) roles of LLMs in HCI projects; (3) contribution types; and (4) acknowledged limitations and risks. We find LLM work in 10 diverse domains, primarily via empirical and artifact contributions. Authors use LLMs in five distinct roles, including as research tools or simulated users. Still, authors often raise validity and reproducibility concerns, and overwhelmingly study closed models. We outline opportunities to improve HCI research with and on LLMs, and provide guiding questions for researchers to consider the validity and appropriateness of LLM-related work.",
      "url": "https://www.microsoft.com/en-us/research/publication/understanding-the-llm-ification-of-chi-unpacking-the-impact-of-llms-at-chi-through-a-systematic-literature-review/"
    },
    {
      "title": "Self-reflecting Large Language Models: A Hegelian Dialectical Approach",
      "authors": [
        "Can Goksen",
        "K. Koishida",
        "Michael Solodko",
        "Saeed Amizadeh",
        "Sara Abdali"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Human language technologies",
        "Human-computer interaction"
      ],
      "publication_date": "January 2025",
      "abstract": "Investigating NLP through a philosophical lens has recently caught researcher’s eyes as it connects computational methods with classical schools of philosophy. This paper introduces a philosophical approach inspired by the \\textit{Hegelian Dialectic} for LLMs’ \\textit{self-reflection}, utilizing a self-dialectical approach to emulate internal critiques and then synthesize new ideas by resolving the opposing points of view. Moreover, this paper investigates the effect of LLMs’ temperature for generation by establishing a dynamic annealing approach, which promotes the creativity in the early stages and gradually refines it by focusing on the nuances, as well as a fixed-temperature strategy for generation. We assess the effectiveness of our proposed method in generating novel ideas and in improving the reasoning abilities of LLMs during problem-solving. Moreover, we implement a Multi-Agent Majority Voting (MAMV) strategy to assess the validity and novelty of the generated ideas, which proves useful in the absence of domain experts. Our experiments demonstrate promising results in generating ideas and enhancing problem-solving performance.",
      "url": "https://www.microsoft.com/en-us/research/publication/self-reflecting-large-language-models-a-hegelian-dialectical-approach/"
    },
    {
      "title": "Position: Evaluating Generative AI Systems is a Social Science Measurement Challenge",
      "authors": [
        "A. Feder Cooper",
        "Abigail Z. Jacobs",
        "Alex Chouldechova",
        "Alex Dow",
        "Alexandra Olteanu",
        "Angelina Wang",
        "Chad Atalla",
        "Dan Vann",
        "Emily Corvi",
        "Emily Sheng",
        "Hanna Wallach",
        "Hannah Washington",
        "Jean Garcia-Gathright",
        "Jennifer Wortman Vaughan",
        "Matthew Vogel",
        "Meera Desai",
        "Nick Pangakis",
        "Solon Barocas",
        "Stefanie Reed",
        "Su Lin Blodgett"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Social sciences"
      ],
      "publication_date": "January 2025",
      "abstract": "The measurement tasks involved in evaluating generative AI (GenAI) systems are especially difficult, leading to what has been described as”a tangle of sloppy tests [and] apples-to-oranges comparisons” (Roose, 2024). In this position paper, we argue that the ML community would benefit from learning from and drawing on the social sciences when developing and using measurement instruments for evaluating GenAI systems. Specifically, our position is that evaluating GenAI systems is a social science measurement challenge. We present a four-level framework, grounded in measurement theory from the social sciences, for measuring concepts related to the capabilities, behaviors, and impacts of GenAI. This framework has two important implications for designing and evaluating evaluations: First, it can broaden the expertise involved in evaluating GenAI systems by enabling stakeholders with different perspectives to participate in conceptual debates. Second, it brings rigor to both conceptual and operational debates by offering a set of lenses for interrogating the validity of measurement instruments and their resulting measurements.",
      "url": "https://www.microsoft.com/en-us/research/publication/position-evaluating-generative-ai-systems-is-a-social-science-measurement-challenge/"
    },
    {
      "title": "Optimizing Large Language Model Training Using FP4 Quantization",
      "authors": [
        "Baining Guo",
        "Guoshuai Zhao",
        "Peng Cheng",
        "Ruizhe Wang",
        "Xiao Liu",
        "Yeyun Gong",
        "Zhengjun Zha",
        "Ziyue Yang"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "January 2025",
      "abstract": "The growing computational demands of training large language models (LLMs) necessitate more efficient methods. Quantized training presents a promising solution by enabling low-bit arithmetic operations to reduce these costs. While FP8 precision has demonstrated feasibility, leveraging FP4 remains a challenge due to significant quantization errors and limited representational capacity. This work introduces the first FP4 training framework for LLMs, addressing these challenges with two key innovations: a differentiable quantization estimator for precise weight updates and an outlier clamping and compensation strategy to prevent activation collapse. To ensure stability, the framework integrates a mixed-precision training scheme and vector-wise quantization. Experimental results demonstrate that our FP4 framework achieves accuracy comparable to BF16 and FP8, with minimal degradation, scaling effectively to 13B-parameter LLMs trained on up to 100B tokens. With the emergence of next-generation hardware supporting FP4, our framework sets a foundation for efficient ultra-low precision training.",
      "url": "https://www.microsoft.com/en-us/research/publication/optimizing-large-language-model-training-using-fp4-quantization/"
    },
    {
      "title": "Chain-of-Retrieval Augmented Generation",
      "authors": [
        "Furu Wei",
        "Haonan Chen",
        "Liang Wang",
        "Nan Yang",
        "Xiaolong Huang",
        "Zhicheng Dou"
      ],
      "research_areas": [
        "Human language technologies",
        "Search and information retrieval"
      ],
      "publication_date": "January 2025",
      "abstract": "This paper introduces an approach for training o1-like RAG models that retrieve and reason over relevant information step by step before generating the final answer. Conventional RAG methods usually perform a single retrieval step before the generation process, which limits their effectiveness in addressing complex queries due to imperfect retrieval results. In contrast, our proposed method, CoRAG (Chain-of-Retrieval Augmented Generation), allows the model to dynamically reformulate the query based on the evolving state. To train CoRAG effectively, we utilize rejection sampling to automatically generate intermediate retrieval chains, thereby augmenting existing RAG datasets that only provide the correct final answer. At test time, we propose various decoding strategies to scale the model’s test-time compute by controlling the length and number of sampled retrieval chains. Experimental results across multiple benchmarks validate the efficacy of CoRAG, particularly in multi-hop question answering tasks, where we observe more than 10 points improvement in EM score compared to strong baselines. On the KILT benchmark, CoRAG establishes a new state-of-the-art performance across a diverse range of knowledge-intensive tasks. Furthermore, we offer comprehensive analyses to understand the scaling behavior of CoRAG, laying the groundwork for future research aimed at developing factual and grounded foundation models.",
      "url": "https://www.microsoft.com/en-us/research/publication/chain-of-retrieval-augmented-generation/"
    },
    {
      "title": "Test-Time Preference Optimization: On-the-Fly Alignment via Iterative Textual Feedback",
      "authors": [
        "Linjie Li",
        "Xiaoye Qu",
        "Xuyang Hu",
        "Yafu Li",
        "Yu Cheng"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "January 2025",
      "abstract": "Large language models (LLMs) demonstrate impressive performance but lack the flexibility to adapt to human preferences quickly without retraining. In this work, we introduce Test-time Preference Optimization (TPO), a framework that aligns LLM outputs with human preferences during inference, removing the need to update model parameters. Rather than relying on purely numerical rewards, TPO translates reward signals into textual critiques and uses them as textual rewards to iteratively refine its response. Evaluations on benchmarks covering instruction following, preference alignment, safety, and mathematics reveal that TPO progressively improves alignment with human preferences. Notably, after only a few TPO steps, the initially unaligned Llama-3.1-70B-SFT model can surpass the aligned counterpart, Llama-3.1-70B-Instruct. Furthermore, TPO scales efficiently with both the search width and depth during inference. Through case studies, we illustrate how TPO exploits the innate capacity of LLM to interpret and act upon reward signals. Our findings establish TPO as a practical, lightweight alternative for test-time preference optimization, achieving alignment on the fly. Our code is publicly available at https://github.com/yafuly/TPO.",
      "url": "https://www.microsoft.com/en-us/research/publication/test-time-preference-optimization-on-the-fly-alignment-via-iterative-textual-feedback/"
    },
    {
      "title": "Roadmap on Neuromorphic Photonics",
      "authors": [
        "A. Grabulosa",
        "A. Lugnan",
        "A. Lupo",
        "A. Lvovsky",
        "A. McCaughan",
        "A. Rowstron",
        "A. Skalli",
        "A. Tefas",
        "A. Tsakyridis",
        "Alireza Marandi",
        "Antonio Hurtado",
        "Avilash Mukherjee",
        "Aydogan Ozcan",
        "B. Offrein",
        "B. Rahmani",
        "B. Romeira",
        "B. Shastri",
        "B. Tossoun",
        "Bakhrom G. Oripov",
        "Benoît Charbonnier",
        "Birgit Stiller",
        "Bryce Primavera",
        "Burcu Canakci",
        "C. Carmes",
        "Carlos A. R'ios Ocampo",
        "Changming Wu",
        "Chaoran Huang",
        "Christophe Moser",
        "Christos Gkantsidis",
        "Chu Wu",
        "Claudio Conti",
        "D. J. Moss",
        "D. Lenstra",
        "D. Pierangeli",
        "D. Psaltis",
        "Daniel Brunner",
        "Daniel Cletheroe",
        "Daniele Veraldi",
        "Demetrios N. Christodoulides",
        "Douglas J. Kelly",
        "E. A. Vlieg",
        "E. Goi",
        "E. Manuylovich",
        "F. Horst",
        "F. Parmigiani",
        "Fabian Bohm",
        "Fabio Pavanello",
        "Fabrice Raineri",
        "Federico Marchesin",
        "Fei Qiao",
        "Fei Xia",
        "G. Giorgi",
        "G. Li",
        "G. O'Shea",
        "G. Sande",
        "G. Verschaffelt",
        "Gordon Wetzstein",
        "Grace Brennan",
        "Haiou Zhang",
        "Hangbo Yang",
        "Hao Wang",
        "Heiner Kremer",
        "Hitesh Ballani",
        "Ilker Oguz",
        "J. Chiles",
        "J. Chu",
        "J. Hsieh",
        "J. Robertson",
        "Jacob Ewaniuk",
        "James Clegg",
        "James Spall",
        "Jannes Gladrow",
        "Jeffrey M. Shainline",
        "Jiamin Wu",
        "Jianqi Hu",
        "Jongheon Lee",
        "Joshua C. Lederman",
        "Juejun Hu",
        "Jérémie Laydevant",
        "K. Ludge",
        "Kirill Kalinin",
        "L. Chrostowski",
        "L. D. Lauro",
        "L. Jaurigue",
        "L. Pavesi",
        "L. Pickup",
        "L. Puts",
        "Logan G. Wright",
        "Lu Fang",
        "M. Chemnitz",
        "M. Goldmann",
        "M. Khajavikhan",
        "Mahdi Nikdast",
        "Mandar M. Sohoni",
        "Marcello Calvanese Strinati",
        "Matvej Hejda",
        "Mieszko Lis",
        "Miguel C. Soriano",
        "Miltiadis Moralis Pegios",
        "Min Gu",
        "Mo Li",
        "Mohammed A. Al Qadasi",
        "Morteza Kamalian Kopae",
        "Mustafa Yildirim",
        "N. Diamantopoulos",
        "N. Peserico",
        "N. Pleros",
        "Nathan Youngblood",
        "Nir Rotenberg",
        "Niyazi Ulaş Dinç",
        "P. Bienstman",
        "Paul R. Prucnal",
        "Pedro J. Freire",
        "Peter L. McMahon",
        "Qionghai Dai",
        "R. Franchi",
        "R. Morandotti",
        "R. Schwartz",
        "R. Stabile",
        "R. Zambrini",
        "S. Biasi",
        "S. Cheung",
        "S. Gentilini",
        "S. J. B. Yoo",
        "S. Pasricha",
        "S. Shekhar",
        "S. Turitsyn",
        "Satoshi Sunada",
        "Serge Massar",
        "Shanhui Fan",
        "Simon Bilodeau",
        "Sonia M. Buckley",
        "Steffen Schoenhardt",
        "Suyeon Choi",
        "Sylvain Barbay",
        "Sylvain Gigan",
        "T. Vaerenbergh",
        "Tianyu Wang",
        "Volker J. Sorger",
        "W. Yao",
        "Weipeng Zhang",
        "Wim Bogaerts",
        "X. Porte",
        "Xianxin Guo",
        "Xing Lin",
        "Xingyuan Xu",
        "Xinlun Cai",
        "Yitong Chen",
        "Yuhang Li",
        "Zhongjin Lin"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Systems and networking"
      ],
      "publication_date": "January 2025",
      "abstract": "This roadmap consolidates recent advances while exploring emerging applications, reflecting the remarkable diversity of hardware platforms, neuromorphic concepts, and implementation philosophies reported in the field. It emphasizes the critical role of cross-disciplinary collaboration in this rapidly evolving field.",
      "url": "https://www.microsoft.com/en-us/research/publication/roadmap-on-neuromorphic-photonics/"
    },
    {
      "title": "GaussMark: A Practical Approach for Structural Watermarking of Language Models",
      "authors": [
        "Adam Block",
        "Alexander Rakhlin",
        "Ayush Sekhari"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "January 2025",
      "abstract": "Recent advances in Large Language Models (LLMs) have led to significant improvements in natural language processing tasks, but their ability to generate human-quality text raises significant ethical and operational concerns in settings where it is important to recognize whether or not a given text was generated by a human. Thus, recent work has focused on developing techniques for watermarking LLM-generated text, i.e., introducing an almost imperceptible signal that allows a provider equipped with a secret key to determine if given text was generated by their model. Current watermarking techniques are often not practical due to concerns with generation latency, detection time, degradation in text quality, or robustness. Many of these drawbacks come from the focus on token-level watermarking, which ignores the inherent structure of text. In this work, we introduce a new scheme, GaussMark, that is simple and efficient to implement, has formal statistical guarantees on its efficacy, comes at no cost in generation latency, and embeds the watermark into the weights of the model itself, providing a structural watermark. Our approach is based on Gaussian independence testing and is motivated by recent empirical observations that minor additive corruptions to LLM weights can result in models of identical (or even improved) quality. We show that by adding a small amount of Gaussian noise to the weights of a given LLM, we can watermark the model in a way that is statistically detectable by a provider who retains the secret key. We provide formal statistical bounds on the validity and power of our procedure. Through an extensive suite of experiments, we demonstrate that GaussMark is reliable, efficient, and relatively robust to corruptions such as insertions, deletions, substitutions, and roundtrip translations and can be instantiated with essentially no loss in model quality.",
      "url": "https://www.microsoft.com/en-us/research/publication/gaussmark-a-practical-approach-for-structural-watermarking-of-language-models/"
    },
    {
      "title": "FlexiClip: Locality-Preserving Free-Form Character Animation",
      "authors": [
        "Anant Khandelwal"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Graphics and multimedia"
      ],
      "publication_date": "January 2025",
      "abstract": "Animating clipart images with seamless motion while maintaining visual fidelity and temporal coherence presents significant challenges. Existing methods, such as AniClipart, effectively model spatial deformations but often fail to ensure smooth temporal transitions, resulting in artifacts like abrupt motions and geometric distortions. Similarly, text-to-video (T2V) and image-to-video (I2V) models struggle to handle clipart due to the mismatch in statistical properties between natural video and clipart styles. This paper introduces FlexiClip, a novel approach designed to overcome these limitations by addressing the intertwined challenges of temporal consistency and geometric integrity. FlexiClip extends traditional B\\’ezier curve-based trajectory modeling with key innovations: temporal Jacobians to correct motion dynamics incrementally, continuous-time modeling via probability flow ODEs (pfODEs) to mitigate temporal noise, and a flow matching loss inspired by GFlowNet principles to optimize smooth motion transitions. These enhancements ensure coherent animations across complex scenarios involving rapid movements and non-rigid deformations. Extensive experiments validate the effectiveness of FlexiClip in generating animations that are not only smooth and natural but also structurally consistent across diverse clipart types, including humans and animals. By integrating spatial and temporal modeling with pre-trained video diffusion models, FlexiClip sets a new standard for high-quality clipart animation, offering robust performance across a wide range of visual content. Project Page: https://creative-gen.github.io/flexiclip.github.io/",
      "url": "https://www.microsoft.com/en-us/research/publication/flexiclip-locality-preserving-free-form-character-animation/"
    },
    {
      "title": "SynDL: A Large-Scale Synthetic Test Collection for Passage Retrieval",
      "authors": [
        "Bhaskar Mitra",
        "Emine Yilmaz",
        "Hossein A. Rahmani",
        "Nick Craswell",
        "Paul Thomas",
        "Xi Wang"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Search and information retrieval"
      ],
      "publication_date": "January 2025",
      "abstract": "Large-scale test collections play a crucial role in Information Retrieval (IR) research. However, according to the Cranfield paradigm and the research into publicly available datasets, the existing information retrieval research studies are commonly developed on small-scale datasets that rely on human assessors for relevance judgments – a time-intensive and expensive process. Recent studies have shown the strong capability of Large Language Models (LLMs) in producing reliable relevance judgments with human accuracy but at a greatly reduced cost. In this paper, to address the missing large-scale ad-hoc document retrieval dataset, we extend the TREC Deep Learning Track (DL) test collection via additional language model synthetic labels to enable researchers to test and evaluate their search systems at a large scale. Specifically, such a test collection includes more than 1,900 test queries from the previous years of tracks. We compare system evaluation with past human labels from past years and find that our synthetically created large-scale test collection can lead to highly correlated system rankings.",
      "url": "https://www.microsoft.com/en-us/research/publication/syndl-a-large-scale-synthetic-test-collection-for-passage-retrieval/"
    },
    {
      "title": "Lessons From Red Teaming 100 Generative AI Products",
      "authors": [
        "Amanda Minnich",
        "Blake Bullwinkel",
        "Bolor-Erdene Jagdagdorj",
        "Chang Kawaguchi",
        "Daniel Jones",
        "Eugenia Kim",
        "Gary Lopez",
        "Giorgio Severi",
        "Joris de Gruyter",
        "Justin Song",
        "Katherine Pratt",
        "Keegan Hines",
        "Mark Russinovich",
        "Martin Pouliot",
        "Nina Chikanov",
        "Pete Bryan",
        "Raja Sekhar Rao Dheekonda",
        "Ram Shankar Siva Kumar",
        "Richard Lundeen",
        "Roman Lutz",
        "Sam Vaughan",
        "Saphir Qi",
        "Shiven Chawla",
        "Victoria Westerhoff",
        "Whitney Maxwell",
        "Yonatan Zunger"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "January 2025",
      "abstract": "In recent years, AI red teaming has emerged as a practice for probing the safety and security of generative AI systems. Due to the nascency of the field, there are many open questions about how red teaming operations should be conducted. Based on our experience red teaming over 100 generative AI products at Microsoft, we present our internal threat model ontology and eight main lessons we have learned:\n\nUnderstand what the system can do and where it is applied\nYou don’t have to compute gradients to break an AI system\nAI red teaming is not safety benchmarking\nAutomation can help cover more of the risk landscape\nThe human element of AI red teaming is crucial\nResponsible AI harms are pervasive but difficult to measure\nLLMs amplify existing security risks and introduce new ones\nThe work of securing AI systems will never be complete\n\nBy sharing these insights alongside case studies from our operations, we offer practical recommendations aimed at aligning red teaming efforts with real world risks. We also highlight aspects of AI red teaming that we believe are often misunderstood and discuss open questions for the field to consider.",
      "url": "https://www.microsoft.com/en-us/research/publication/lessons-from-red-teaming-100-generative-ai-products/"
    },
    {
      "title": "Hybrid Cost Modeling for Reducing Query Performance Regression in Index Tuning",
      "authors": [
        "Wentao Wu"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Data platforms and analytics"
      ],
      "publication_date": "January 2025",
      "abstract": "Autonomous index tuning (“auto-indexing” for short) has recently started being supported by cloud database service providers. Index tuners rely on query optimizer’s cost estimates to recommend indexes that can minimize the execution cost of an input workload. Such cost estimates can often be erroneous that lead to significant query performance regression. To reduce the chance of regression, existing work primarily uses machine learning (ML) technologies to build prediction models to improve query execution cost estimation using actual query execution telemetry as training data. However, training data collection is typically an expensive process, especially for index tuning due to the significant overhead of creating/dropping indexes. As a result, the amount of training data can be limited in auto-indexing for cloud databases. In this paper, we propose a new approach named “hybrid cost modeling” to address this challenge. The key idea is to limit the ML-based modeling effort to the leaf operators such as table scans, index scans, and index seeks, and then combine the ML-model predicted costs of the leaf operators with optimizer’s estimated costs of the other operators in the query plan. We conduct theoretical study as well as empirical evaluation to demonstrate the efficacy of applying hybrid cost modeling to index tuning, using both industrial benchmarks and real workloads.",
      "url": "https://www.microsoft.com/en-us/research/publication/hybrid-cost-modeling-for-reducing-query-performance-regression-in-index-tuning/"
    },
    {
      "title": "Enhanced Macular Telangiectasia Type 2 Detection: Leveraging Self-Supervised Learning and Ensemble Models",
      "authors": [
        "Aaron Lee",
        "Alyson Muldrew",
        "Bill Weeks",
        "Catherine Jamison",
        "Ferenc B Sallo",
        "Irene Leung",
        "Juan M. Lavista Ferres",
        "Lea Scheppke",
        "Lowy Medical Research Institute",
        "Martin Friedlander",
        "Meghana Kshirsagar",
        "Rahul Dodhia",
        "Roberto Bonelli",
        "Shahrzad Gholami",
        "Tunde Peto",
        "Yue Wu"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Medical, health and genomics"
      ],
      "publication_date": "January 2025",
      "abstract": "Objective or purpose: To investigate an ensemble-based approach utilizing deep learning models for accurate and interpretable detection of Macular Telangiectasia Type 2 (MacTel) on optical coherence tomography (OCT) imaging.\nDesign: Retrospective analysis of OCT scans, model development and assessment.\nSubjects, Participants, and/or Controls: A total of 5200 OCT images from participants in the MacTel Registry conducted by the Lowy Medical Research Institute and from the University of Washington (780 MacTel patients and 1900 non-MacTel patients).\nMethods, Intervention, or Testing: We trained multiple individual MacTel vs. non-MacTel classification models using traditional supervised and self-supervised learning (SSL), and ensembled them using average weighting methods. We investigated diverse methodologies for constructing the ensemble, including varied architectural configurations and learning paradigms of individual models, and manipulating the amount of labeled data accessible for training. Model performance was compared against human expert graders on held-out test set data. Model interpretability was investigated using Grad-CAM visualization and by evaluating interrater agreement.\nMain Outcome Measures: For model performance, area under the receiver operating characteristic curve (AUROC), area under the precision-recall curve (AUPRC), accuracy, sensitivity, and specificity were reported. For interpretability, interrater agreements and Grad-CAM visualization results were evaluated.\nResults: Despite access to only 419 OCT volumes, including 185 MacTel patients within the 10% labeled training dataset, the ensemble model demonstrated a performance level (AUROC 0.972 [95% confidence interval (CI), 0.971-0.973], AUPRC 0.967 [95% CI, 0.965-0.969], accuracy 91.7%, sensitivity 0.905, and specificity 0.925) comparable to the human experts ensemble (AUROC 0.977 [95% CI, 0.975-0.978], AUPRC 0.987 [95% CI, 0.986-0.987], accuracy 96.8%, sensitivity 0.929, and specificity 1) on a test set of 500 patients. The individual models did not achieve the same performance levels when evaluated separately.\nConclusion: Even with limited data, combining SSL with ensemble approaches improved Mactel classification accuracy and interpretation compared to the individual models. SSL captures meaningful representations from unlabeled data, a key benefit in the setting of limited data such as with rare diseases.",
      "url": "https://www.microsoft.com/en-us/research/publication/enhanced-macular-telangiectasia-type-2-detection-leveraging-self-supervised-learning-and-ensemble-models/"
    },
    {
      "title": "JudgeBlender: Ensembling Judgments for Automatic Relevance Assessment",
      "authors": [
        "Bhaskar Mitra",
        "Emine Yilmaz",
        "Hossein A. Rahmani",
        "Nick Craswell"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Search and information retrieval"
      ],
      "publication_date": "January 2025",
      "abstract": "The effective training and evaluation of retrieval systems require a substantial amount of relevance judgments, which are traditionally collected from human assessors — a process that is both costly and time-consuming. Large Language Models (LLMs) have shown promise in generating relevance labels for search tasks, offering a potential alternative to manual assessments. Current approaches often rely on a single LLM, such as GPT-4, which, despite being effective, are expensive and prone to intra-model biases that can favour systems leveraging similar models. In this work, we introduce JudgeBlender, a framework that employs smaller, open-source models to provide relevance judgments by combining evaluations across multiple LLMs (LLMBlender) or multiple prompts (PromptBlender). By leveraging the LLMJudge benchmark [18], we compare JudgeBlender with state-of-the-art methods and the top performers in the LLMJudge challenge. Our results show that JudgeBlender achieves competitive performance, demonstrating that very large models are often unnecessary for reliable relevance assessments.",
      "url": "https://www.microsoft.com/en-us/research/publication/judgeblender-ensembling-judgments-for-automatic-relevance-assessment/"
    },
    {
      "title": "FLAVARS: A Multimodal Foundational Language and Vision Alignment Model for Remote Sensing",
      "authors": [
        "Anthony Ortiz",
        "Caleb Robinson",
        "Isaac Corley",
        "Juan M. Lavista Ferres",
        "Peyman Najafirad",
        "Rahul Dodhia",
        "Simone Fobi Nsutezo"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Computer vision"
      ],
      "publication_date": "January 2025",
      "abstract": "Remote sensing imagery is dense with objects and contextual visual information. There is a recent trend to combine paired satellite images and text captions for pretraining performant encoders for downstream tasks. However, while contrastive image-text methods like CLIP enable vision-language alignment and zero-shot classification ability, vision-only downstream performance tends to degrade compared to image-only pretraining, such as MAE. In this paper, we propose FLAVARS, a pretraining method that combines the best of both contrastive learning and masked modeling, along with geospatial alignment via contrastive location encoding. We find that FLAVARS significantly outperforms a baseline of SkyCLIP for vision-only tasks such as KNN classification and semantic segmentation, +6\\% mIOU on SpaceNet1, while retaining the ability to perform zero-shot classification, unlike MAE pretrained methods.",
      "url": "https://www.microsoft.com/en-us/research/publication/flavars-a-multimodal-foundational-language-and-vision-alignment-model-for-remote-sensing/"
    },
    {
      "title": "A generative model for inorganic materials design",
      "authors": [
        "Aliaksandra Shysheya",
        "Andrew Fowler",
        "Bichlien Nguyen",
        "Chin-Wei Huang",
        "Chunlei Yang",
        "Claudio Zeni",
        "Daniel Zügner",
        "Han Yang",
        "Hannes Schulz",
        "Hongxia Hao",
        "Jake Smith",
        "Jielan Li",
        "Jonathan Crabbé",
        "Lixin Sun",
        "Matthew Horton",
        "Robert Pinsler",
        "Roberto Sordillo",
        "Ryota Tomioka",
        "Sarah Lewis",
        "Shoko Ueda",
        "Tian Xie",
        "Wenjie Li",
        "Xiang Fu",
        "Yichi Zhou",
        "Ziheng Lu",
        "Zilong Wang"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Medical, health and genomics"
      ],
      "publication_date": "January 2025",
      "abstract": "The design of functional materials with desired properties is essential in driving technological advances in areas like energy storage, catalysis, and carbon capture1–3. Generative models provide a new paradigm for materials design by directly generating novel materials given desired property constraints, but current methods have low success rate in proposing stable crystals or can only satisfy a limited set of property constraints 4−11. Here, we present MatterGen, a model that generates stable, diverse inorganic materials across the periodic table and can further be fine-tuned to steer the generation towards a broad range of property constraints. Compared to prior generative models 4,12, structures produced by MatterGen are more than twice as likely to be novel and stable, and more than 10 times closer to the local energy minimum. After fine-tuning, MatterGen successfully generates stable, novel materials with desired chemistry, symmetry, as well as mechanical, electronic and magnetic properties. As a proof of concept, we synthesize one of the generated structures and measure its property value to be within 20 % of our target. We believe that the quality of generated materials and the breadth of MatterGen’s capabilities represent a major advancement towards creating a foundational generative model for materials design.",
      "url": "https://www.microsoft.com/en-us/research/publication/a-generative-model-for-inorganic-materials-design/"
    },
    {
      "title": "FreewayML: An Adaptive and Stable Streaming Learning Framework for Dynamic Data Stream",
      "authors": [
        "Lijie Xu",
        "Mingchao Wu",
        "Wei Wang",
        "Wentao Wu",
        "Wuqiang Shen",
        "Zheheng Liang",
        "Zheng Qin"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Data platforms and analytics"
      ],
      "publication_date": "January 2025",
      "abstract": "Streaming (machine) learning (SML) can capture dynamic changes in real-time data and perform continuous updates. It has been widely applied in real-world scenarios such as network security, financial regulation, and energy supply. However, due to the sensitivity and lightweight nature of SML models, existing work suffers from low robustness, sudden decline, and catastrophic forgetting when facing unexpected data distribution drifts. Previous studies have attempted to enhance the stability\nof SML through methods such as data selection, replay, and constraints. However, these methods are typically designed for specific feature spaces and specific ML algorithms. In this paper, we introduce a shift graph based on the distances between data distributions and define three distinct data shift patterns. For these three patterns, we design three adaptive mechanisms, (a) multi-time granularity models, (b) coherent experience clustering, and (c) historical knowledge reuse, that are triggered by a strategy selector, with the goal of enhancing the accuracy and stability of SML. We implement an adaptive and stable SML framework, FreewayML, on top of PyTorch, which is suitable for most SML models. Experimental results show that FreewayML significantly outperforms existing SML systems in both stability and accuracy, with a comparable throughput and latency.",
      "url": "https://www.microsoft.com/en-us/research/publication/freewayml-an-adaptive-and-stable-streaming-learning-framework-for-dynamic-data-stream/"
    },
    {
      "title": "Exploring Scalable Medical Image Encoders Beyond Text Supervision",
      "authors": [
        "Anton Schwaighofer",
        "Daniel Coelho de Castro",
        "Fernando Pérez-García",
        "Harshita Sharma",
        "Javier Alvarez-Valle",
        "Kenza Bouzid",
        "Matthew P Lungren",
        "Maximilian Ilse",
        "Noel Codella",
        "Ozan Oktay",
        "Sam Bond-Taylor",
        "Shruthi Bannur",
        "Stephanie Hyland",
        "Valentina Salvatelli"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Computer vision",
        "Medical, health and genomics"
      ],
      "publication_date": "January 2025",
      "abstract": "Language-supervised pre-training has proven to be a valuable method for extracting semantically meaningful features from images, serving as a foundational element in multimodal systems within the computer vision and medical imaging domains. However, resulting features are limited by the information contained within the text. This is particularly problematic in medical imaging, where radiologists’ written findings focus on specific observations; a challenge compounded by the scarcity of paired imaging-text data due to concerns over leakage of personal health information. In this work, we fundamentally challenge the prevailing reliance on language supervision for learning general purpose biomedical imaging encoders. We introduce RAD-DINO, a biomedical image encoder pre-trained solely on unimodal biomedical imaging data that obtains similar or greater performance than state-of-the-art biomedical language supervised models on a diverse range of benchmarks. Specifically, the quality of learned representations is evaluated on standard imaging tasks (classification and semantic segmentation), and a vision-language alignment task (text report generation from images). To further demonstrate the drawback of language supervision, we show that features from RAD-DINO correlate with other medical records (e.g., sex or age) better than language-supervised models, which are generally not mentioned in radiology reports. Finally, we conduct a series of ablations determining the factors in RAD-DINO’s performance; notably, we observe that RAD-DINO’s downstream performance scales well with the quantity and diversity of training data, demonstrating that image-only supervision is a scalable approach for training a foundational biomedical image encoder.",
      "url": "https://www.microsoft.com/en-us/research/publication/rad-dino-exploring-scalable-medical-image-encoders-beyond-text-supervision/"
    },
    {
      "title": "rStar-Math: Small LLMs Can Master Math Reasoning with Self-Evolved Deep Thinking",
      "authors": [
        "Fan Yang",
        "Li Lyna Zhang",
        "Mao Yang",
        "Ning Shang",
        "Xinyu Guan",
        "Yi Zhu",
        "Yifei Liu",
        "Youran Sun"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Mathematics"
      ],
      "publication_date": "January 2025",
      "abstract": "We present rStar-Math to demonstrate that small language models (SLMs) can rival or even surpass the math reasoning capability of OpenAI o1, without distillation from superior models. rStar-Math achieves this by exercising”deep thinking”through Monte Carlo Tree Search (MCTS), where a math policy SLM performs test-time search guided by an SLM-based process reward model. rStar-Math introduces three innovations to tackle the challenges in training the two SLMs: (1) a novel code-augmented CoT data sythesis method, which performs extensive MCTS rollouts to generate step-by-step verified reasoning trajectories used to train the policy SLM; (2) a novel process reward model training method that avoids na\\”ive step-level score annotation, yielding a more effective process preference model (PPM); (3) a self-evolution recipe in which the policy SLM and PPM are built from scratch and iteratively evolved to improve reasoning capabilities. Through 4 rounds of self-evolution with millions of synthesized solutions for 747k math problems, rStar-Math boosts SLMs’ math reasoning to state-of-the-art levels. On the MATH benchmark, it improves Qwen2.5-Math-7B from 58.8% to 90.0% and Phi3-mini-3.8B from 41.4% to 86.4%, surpassing o1-preview by +4.5% and +0.9%. On the USA Math Olympiad (AIME), rStar-Math solves an average of 53.3% (8/15) of problems, ranking among the top 20% the brightest high school math students.",
      "url": "https://www.microsoft.com/en-us/research/publication/rstar-math-small-llms-can-master-math-reasoning-with-self-evolved-deep-thinking/"
    },
    {
      "title": "BioAgents: Democratizing Bioinformatics Analysis with Multi-Agent Systems",
      "authors": [
        "Ahmed Alaa",
        "Amanda K. Hall",
        "Daniel Tsirulnikov",
        "David Bamman",
        "Nikita Mehandru",
        "Olesya Melnichenko",
        "Scott Saponas",
        "Venkat S. Malladi",
        "Yulia Dubinina"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Medical, health and genomics"
      ],
      "publication_date": "January 2025",
      "abstract": "Creating end-to-end bioinformatics workflows requires diverse domain expertise, which poses challenges for both junior and senior researchers as it demands a deep understanding of both genomics concepts and computational techniques. While large language models (LLMs) provide some assistance, they often fall short in providing the nuanced guidance needed to execute complex bioinformatics tasks, and require expensive computing resources to achieve high performance. We thus propose a multi-agent system built on small language models, fine-tuned on bioinformatics data, and enhanced with retrieval augmented generation (RAG). Our system, BioAgents, enables local operation and personalization using proprietary data. We observe performance comparable to human experts on conceptual genomics tasks, and suggest next steps to enhance code generation capabilities.",
      "url": "https://www.microsoft.com/en-us/research/publication/bioagents-democratizing-bioinformatics-analysis-with-multi-agent-systems-2/"
    },
    {
      "title": "Benchmarking uncertainty quantification for protein engineering",
      "authors": [
        "Ava P. Amini",
        "Kevin Kaichuang Yang",
        "Kevin P. Greenman"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Medical, health and genomics"
      ],
      "publication_date": "January 2025",
      "abstract": "Machine learning sequence-function models for proteins could enable significant ad vances in protein engineering, especially when paired with state-of-the-art methods to select new sequences for property optimization and/or model improvement. Such methods (Bayesian optimization and active learning) require calibrated estimations of model uncertainty. While studies have benchmarked a variety of deep learning uncertainty quantification (UQ) methods on standard and molecular machine-learning datasets, it is not clear if these results extend to protein datasets. In this work, we implemented a panel of deep learning UQ methods on regression tasks from the Fitness Landscape Inference for Proteins (FLIP) benchmark. We compared results across different degrees of distributional shift using metrics that assess each UQ method’s accuracy, calibration, coverage, width, and rank correlation. Additionally, we compared these metrics using one-hot encoding and pretrained language model representations, and we tested the UQ methods in a retrospective active learning setting. These benchmarks enable us to provide recommendations for more effective design of biological sequences using machine learning.",
      "url": "https://www.microsoft.com/en-us/research/publication/benchmarking-uncertainty-quantification-for-protein-engineering/"
    },
    {
      "title": "Can MLLMs Reason in Multimodality? EMMA: An Enhanced MultiModal ReAsoning Benchmark",
      "authors": [
        "Huichen Will Wang",
        "Jiawei Gu",
        "Lijuan Wang",
        "Linjie Li",
        "Yu Cheng",
        "Yunzhuo Hao",
        "Zhengyuan Yang"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "January 2025",
      "abstract": "The ability to organically reason over and with both text and images is a pillar of human intelligence, yet the ability of Multimodal Large Language Models (MLLMs) to perform such multimodal reasoning remains under-explored. Existing benchmarks often emphasize text-dominant reasoning or rely on shallow visual cues, failing to adequately assess integrated visual and textual reasoning. We introduce EMMA (Enhanced MultiModal reAsoning), a benchmark targeting organic multimodal reasoning across mathematics, physics, chemistry, and coding. EMMA tasks demand advanced cross-modal reasoning that cannot be addressed by reasoning independently in each modality, offering an enhanced test suite for MLLMs’ reasoning capabilities. Our evaluation of state-of-the-art MLLMs on EMMA reveals significant limitations in handling complex multimodal and multi-step reasoning tasks, even with advanced techniques like Chain-of-Thought prompting and test-time compute scaling underperforming. These findings underscore the need for improved multimodal architectures and training paradigms to close the gap between human and model reasoning in multimodality.",
      "url": "https://www.microsoft.com/en-us/research/publication/can-mllms-reason-in-multimodality-emma-an-enhanced-multimodal-reasoning-benchmark/"
    },
    {
      "title": "ReFocus: Visual Editing as a Chain of Thought for Structured Image Understanding",
      "authors": [
        "Cha Zhang",
        "Dan Roth",
        "Dinei Florencio",
        "Jianwei Yang",
        "John Corring",
        "Minqian Liu",
        "Xingyu Fu",
        "Yijuan Lu",
        "Zhengyuan Yang"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "January 2025",
      "abstract": "Structured image understanding, such as interpreting tables and charts, requires strategically refocusing across various structures and texts within an image, forming a reasoning sequence to arrive at the final answer. However, current multimodal large language models (LLMs) lack this multihop selective attention capability. In this work, we introduce ReFocus, a simple yet effective framework that equips multimodal LLMs with the ability to generate”visual thoughts”by performing visual editing on the input image through code, shifting and refining their visual focuses. Specifically, ReFocus enables multimodal LLMs to generate Python codes to call tools and modify the input image, sequentially drawing boxes, highlighting sections, and masking out areas, thereby enhancing the visual reasoning process. We experiment upon a wide range of structured image understanding tasks involving tables and charts. ReFocus largely improves performance on all tasks over GPT-4o without visual editing, yielding an average gain of 11.0% on table tasks and 6.8% on chart tasks. We present an in-depth analysis of the effects of different visual edits, and reasons why ReFocus can improve the performance without introducing additional information. Further, we collect a 14k training set using ReFocus, and prove that such visual chain-of-thought with intermediate information offers a better supervision than standard VQA data, reaching a 8.0% average gain over the same model trained with QA pairs and 2.6% over CoT.",
      "url": "https://www.microsoft.com/en-us/research/publication/refocus-visual-editing-as-a-chain-of-thought-for-structured-image-understanding/"
    },
    {
      "title": "PEACE: Empowering Geologic Map Holistic Understanding with MLLMs",
      "authors": [
        "Furu Wei",
        "Hao Cheng",
        "Haoran Xu",
        "Lei Cui",
        "Qihao Zhao",
        "Scarlett Li",
        "Tengchao Lv",
        "Tianyi Gao",
        "Yang Song",
        "Yangyu Huang",
        "Zhipeng Gui"
      ],
      "research_areas": [
        "Computer vision",
        "Social sciences"
      ],
      "publication_date": "January 2025",
      "abstract": "Geologic map, as a fundamental diagram in geology science, provides critical insights into the structure and composition of Earth’s subsurface and surface. These maps are indispensable in various fields, including disaster detection, resource exploration, and civil engineering. Despite their significance, current Multimodal Large Language Models (MLLMs) often fall short in geologic map understanding. This gap is primarily due to the challenging nature of cartographic generalization, which involves handling high-resolution map, managing multiple associated components, and requiring domain-specific knowledge. To quantify this gap, we construct GeoMap-Bench, the first-ever benchmark for evaluating MLLMs in geologic map understanding, which assesses the full-scale abilities in extracting, referring, grounding, reasoning, and analyzing. To bridge this gap, we introduce GeoMap-Agent, the inaugural agent designed for geologic map understanding, which features three modules: Hierarchical Information Extraction (HIE), Domain Knowledge Injection (DKI), and Prompt-enhanced Question Answering (PEQA). Inspired by the interdisciplinary collaboration among human scientists, an AI expert group acts as consultants, utilizing a diverse tool pool to comprehensively analyze questions. Through comprehensive experiments, GeoMap-Agent achieves an overall score of 0.811 on GeoMap-Bench, significantly outperforming 0.369 of GPT-4o. Our work, emPowering gEologic mAp holistiC undErstanding (PEACE) with MLLMs, paves the way for advanced AI applications in geology, enhancing the efficiency and accuracy of geological investigations.",
      "url": "https://www.microsoft.com/en-us/research/publication/peace-empowering-geologic-map-holistic-understanding-with-mllms/"
    },
    {
      "title": "Imagine while Reasoning in Space: Multimodal Visualization-of-Thought",
      "authors": [
        "Chengzu Li",
        "Furu Wei",
        "Huanyu Zhang",
        "Ivan Vuli'c",
        "Li Dong",
        "Shaoguang Mao",
        "Wenshan Wu",
        "Yan Xia"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "January 2025",
      "abstract": "Chain-of-Thought (CoT) prompting has proven highly effective for enhancing complex reasoning in Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs). Yet, it struggles in complex spatial reasoning tasks. Nonetheless, human cognition extends beyond language alone, enabling the remarkable capability to think in both words and images. Inspired by this mechanism, we propose a new reasoning paradigm, Multimodal Visualization-of-Thought (MVoT). It enables visual thinking in MLLMs by generating image visualizations of their reasoning traces. To ensure high-quality visualization, we introduce token discrepancy loss into autoregressive MLLMs. This innovation significantly improves both visual coherence and fidelity. We validate this approach through several dynamic spatial reasoning tasks. Experimental results reveal that MVoT demonstrates competitive performance across tasks. Moreover, it exhibits robust and reliable improvements in the most challenging scenarios where CoT fails. Ultimately, MVoT establishes new possibilities for complex reasoning tasks where visual thinking can effectively complement verbal reasoning.",
      "url": "https://www.microsoft.com/en-us/research/publication/imagine-while-reasoning-in-space-multimodal-visualization-of-thought/"
    },
    {
      "title": "BioAgents: Democratizing Bioinformatics Analysis with Multi-Agent Systems",
      "authors": [
        "Ahmed Alaa",
        "Amanda K. Hall",
        "Daniel Tsirulnikov",
        "David Bamman",
        "Nikita Mehandru",
        "Olesya Melnichenko",
        "Scott Saponas",
        "V. Malladi",
        "Yulia Dubinina"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Human-computer interaction",
        "Medical, health and genomics"
      ],
      "publication_date": "January 2025",
      "abstract": "Creating end-to-end bioinformatics workflows requires diverse domain expertise, which poses challenges for both junior and senior researchers as it demands a deep understanding of both genomics concepts and computational techniques. While large language models (LLMs) provide some assistance, they often fall short in providing the nuanced guidance needed to execute complex bioinformatics tasks, and require expensive computing resources to achieve high performance. We thus propose a multi-agent system built on small language models, fine-tuned on bioinformatics data, and enhanced with retrieval augmented generation (RAG). Our system, BioAgents, enables local operation and personalization using proprietary data. We observe performance comparable to human experts on conceptual genomics tasks, and suggest next steps to enhance code generation capabilities.",
      "url": "https://www.microsoft.com/en-us/research/publication/bioagents-democratizing-bioinformatics-analysis-with-multi-agent-systems/"
    },
    {
      "title": "ICASSP 2024 Speech Signal Improvement Challenge",
      "authors": [
        "Ando Saabas",
        "Babak Naderi",
        "Nicolae-Cătălin Ristea",
        "Ross Cutler",
        "Sebastian Braun",
        "Solomiya Branets"
      ],
      "research_areas": [
        "Audio and Acoustics"
      ],
      "publication_date": "January 2025",
      "abstract": "The ICASSP 2024 Speech Signal Improvement Challenge aims to advance research in enhancing speech signal quality within communication systems. The speech signal quality can be assessed using the SIG metric from ITU-T P.835 and still remains a top issue in audio communication and conferencing systems. For example, in the ICASSP 2023 Deep Noise Suppression Challenge, the improvement in the background and overall quality is impressive, while the speech signal enhancement was not statistically significant. To improve the speech signal the following speech impairment areas must be addressed: coloration, discontinuity, loudness, reverberation, and noise. To this end, we organized ICASSP 2024 Speech Signal Improvement Challenge, which marks the second signal-focused challenge, built upon the success of the previous ICASSP 2023 Speech Signal Improvement Challenge. A training and test set was provided for the challenge, and the winners were determined using an extended crowdsourced implementation of ITU-T P.804’s listening phase and the word accuracy (WAcc) rate. The results show that significant improvement was made across all measured dimensions of speech quality",
      "url": "https://www.microsoft.com/en-us/research/publication/icassp-2024-speech-signal-improvement-challenge-2/"
    },
    {
      "title": "VLM-driven Behavior Tree for Context-aware Task Planning",
      "authors": [
        "Atsushi Kanehira",
        "Jun Takamatsu",
        "Katsushi Ikeuchi",
        "Kazuhiro Sasabuchi",
        "Naoki Wake"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Computer vision",
        "Human-computer interaction"
      ],
      "publication_date": "January 2025",
      "abstract": "The use of Large Language Models (LLMs) for generating Behavior Trees (BTs) has recently gained attention in the robotics community, yet remains in its early stages of development. In this paper, we propose a novel framework that leverages Vision-Language Models (VLMs) to interactively generate and edit BTs that address visual conditions, enabling context-aware robot operations in visually complex environments. A key feature of our approach lies in the conditional control through self-prompted visual conditions. Specifically, the VLM generates BTs with visual condition nodes, where conditions are expressed as free-form text. Another VLM process integrates the text into its prompt and evaluates the conditions against real-world images during robot execution. We validated our framework in a real-world cafe scenario, demonstrating both its feasibility and limitations.",
      "url": "https://www.microsoft.com/en-us/research/publication/vlm-driven-behavior-tree-for-context-aware-task-planning/"
    },
    {
      "title": "Learning from other Domains to Advance AI Evaluation and Testing: Cybersecurity Standards and Testing — Lessons for AI Safety and Security",
      "authors": [
        "Stewart Baker"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "January 2025",
      "abstract": "There was little need to worry about the security of computer systems until the 1960s. Before that, computers were hulking machines locked in a room that only a few trusted boffins could enter. That all changed when time-sharing debuted, allowing multiple users to use the computer at more or less the same time. That posed the risk that they’d start looking over each other’s shoulders. And that led defense and intelligence customers to wonder how they could protect their classified data from ordinary users.\nSixty years on, we are still trying to answer that question.\nMany experts were sure that the answer was to set security standards and enforce them by testing systems to see whether they met those standards. That is still the closest thing we have to an answer, but it hasn’t been a very good one; it’s at best a partial success. The story of its failures is in some ways the story of politics and policy writ large at the turn of the twenty-first century; as such, it may also tell us a lot about how AI safety and security standards will succeed, and fail.",
      "url": "https://www.microsoft.com/en-us/research/publication/learning-from-other-domains-to-advance-ai-evaluation-and-testing-cybersecurity-standards-and-testing-lessons-for-ai-safety-and-security/"
    },
    {
      "title": "Communication Efficient Secure and Private Multi-Party Deep Learning",
      "authors": [
        "Divya Gupta",
        "Nishanth Chandran",
        "Rahul Sharma",
        "Sankha Das",
        "Satya Lokam",
        "Sayak Ray Chowdhuri"
      ],
      "research_areas": [
        "Security, privacy, and cryptography"
      ],
      "publication_date": "January 2025",
      "abstract": "Distributed training that enables multiple parties to jointly train a model on their respective datasets is a promising approach to address the challenges of large volumes of diverse data for training modern machine learning models. However, this approach immediately raises security and privacy concerns; both about each party wishing to protect its data from other parties during training and preventing leakage of private information from the model after training through various inference attacks. In this paper, we address both these concerns simultaneously by designing efficient Differentially Private, secure Multiparty Computation (DP-MPC) protocols for jointly training a model on data distributed among multiple parties. Our DP-MPC protocol in the two-party setting is 56-794$\\times$ more communication-efficient and 16-182$\\times$ faster than previous such protocols. Conceptually, our work simplifies and improves on previous attempts to combine techniques from secure multiparty computation and differential privacy, especially in the context of ML training.",
      "url": "https://www.microsoft.com/en-us/research/publication/communication-efficient-secure-and-private-multi-party-deep-learning/"
    },
    {
      "title": "Symbolic Automata: Omega-Regularity Modulo Theories",
      "authors": [
        "Ekaterina Zhuchko",
        "Gabriel Ebner",
        "Margus Veanes",
        "Thomas Ball"
      ],
      "research_areas": [
        "Algorithms",
        "Programming languages and software engineering"
      ],
      "publication_date": "January 2025",
      "abstract": "Symbolic automata are finite state automata that support potentially infinite alphabets, such as the set of rational numbers, generally applied to regular expressions and languages over finite words. In symbolic automata (or automata modulo A), an alphabet is represented by an effective Boolean algebra A, supported by a decision procedure for satisfiability. Regular languages over infinite words (so called 𝜔-regular languages) have a rich history paralleling that of regular languages over finite words, with well-known applications to model checking via Büchi automata and temporal logics.\nWe generalize symbolic automata to support 𝜔-regular languages via transition terms and symbolic derivatives, bringing together a variety of classic automata and logics in a unified framework that provides all the necessary ingredients to support symbolic model checking modulo A. In particular, we define: (1) alternating Büchi automata modulo A (ABW⟨A⟩) as well (non-alternating) nondeterministic Büchi automata modulo A (NBW⟨A⟩); (2) an alternation elimination algorithm Æ that incrementally constructs an NBW⟨A⟩ from an ABW⟨A⟩, and can also be used for constructing the product of two NBW⟨A⟩; (3) a definition of linear temporal logic modulo A, LTL⟨A⟩, that generalizes Vardi’s construction of alternating Büchi automata from LTL, using (2) to go from LTL modulo A to NBW⟨A⟩ via ABW⟨A⟩.\nFinally, we present RLTL⟨A⟩, a combination of LTL⟨A⟩ with extended regular expressions modulo A that generalizes the Property Specification Language (PSL). Our combination allows regex complement, that is not supported in PSL but can be supported naturally by using transition terms. We formalize the semantics of RLTL⟨A⟩ using the Lean proof assistant and formally establish correctness of the main derivation theorem.",
      "url": "https://www.microsoft.com/en-us/research/publication/symbolic-automata-omega-regularity-modulo-theories/"
    },
    {
      "title": "Learning from other Domains to Advance AI Evaluation and Testing: Medical Device Testing: Regulatory Requirements, Evolution and Lessons for AI Governance",
      "authors": [
        "Mateo Aboy",
        "Timo Minssen"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Medical, health and genomics"
      ],
      "publication_date": "January 2025",
      "abstract": "The field of medical devices exemplifies the transformative potential of technology in addressing\nhealthcare challenges.\nTesting in the medical device domain is intricately tied to regulatory requirements across\njurisdictions. During the pre-market phase, medical testing establishes baseline safety and\neffectiveness metrics through bench testing, performance standards, and clinical studies. Post market testing ensures that real-world data informs ongoing compliance and safety improvements.\nTesting is indispensable in translating technological innovation into safe and effective medical\ndevices.\nThe regulation of medical devices plays a foundational role in safeguarding public health by\nensuring that devices are safe, effective, and high-quality. In the global marketplace, the United\nStates, the UK Medicines and Healthcare Products Regulatory Agency (MHRA), and the European\nUnion represent some of the most influential regulatory jurisdictions, each with comprehensive\nframeworks: the U.S. Food and Drug Administration (FDA) medical device regulations, the\nMHRA (MDD) and European Union Medical Device Regulation (EU MDR). While there are\ncountry-specific differences with regards to the particular pre-market and post-market review\nprocedures, most developed jurisdictions regulate medical devices similarly to the US or European\nmodels.",
      "url": "https://www.microsoft.com/en-us/research/publication/learning-from-other-domains-to-advance-ai-evaluation-and-testing-medical-device-testing-regulatory-requirements-evolution-and-lessons-for-ai-governance/"
    },
    {
      "title": "Mapping Refugee Camps with AI: A Benchmark Dataset and Baseline Models for Humanitarian Applications",
      "authors": [
        "Amrita Gupta",
        "Anthony Ortiz",
        "Duncan Kebut",
        "Juan M. Lavista Ferres",
        "Rahul Dodhia",
        "Seema Iyer",
        "Simone Fobi Nsutezo"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Computer vision"
      ],
      "publication_date": "January 2025",
      "abstract": "Over 6.6 million people worldwide live in refugee camps, most of which lack comprehensive, up-to-date maps. This hinders effective resource distribution, infrastructure planning, and disaster response in these environments. Automated mapping with aerial imagery offers a promising solution, capturing the detail needed for effective camp management, but it requires datasets that reflect the distinct characteristics of refugee camps. Existing building footprint datasets focus on urban or semi-urban areas leaving refugee camps–characterized by irregular layouts, diverse building sizes, and varied materials–underrepresented and poorly served by current models. This study introduces the KAKUMAAERIAL dataset, an open-source resource for humanitarian mapping. It pairs high-resolution aerial imagery from the Kakuma-Kalobeyei refugee camps in Kenya with annotations for buildings, solar panels, roof materials, and sanitation facilities. The dataset serves as a resource for benchmarking models on tasks crucial to humanitarian aid. Baseline machine learning models achieved strong performance on key tasks: building and solar panel segmentation (IoU of 0.848 and 0.813, respectively), roof material classification (accuracy of 85.6%), and toilet identification (accuracy of 97.8%). By applying these models to broader areas within the camps, the study provides actionable insights into camp infrastructure, including energy access and sanitation availability. This research demonstrates how geospatial technologies and machine learning can enable humanitarian organizations to improve operational efficiency while improving the living conditions and dignity of displaced populations.",
      "url": "https://www.microsoft.com/en-us/research/publication/mapping-refugee-camps-with-ai-a-benchmark-dataset-and-baseline-models-for-humanitarian-applications/"
    },
    {
      "title": "Sublinear Metric Steiner Tree via Improved Bounds for Set Cover",
      "authors": [
        "Ali Vakilian",
        "Jakub Tarnawski",
        "Mohammad Roghani",
        "Sepideh Mahabadi"
      ],
      "research_areas": [
        "Algorithms"
      ],
      "publication_date": "January 2025",
      "abstract": "We study the metric Steiner tree problem in the sublinear query model. In this problem, for a set of n points V in a metric space given to us by means of query access to an n×n matrix w, and a set of terminals T⊆V, the goal is to find the minimum-weight subset of the edges that connects all the terminal vertices.\nRecently, Chen, Khanna and Tan [SODA’23] gave an algorithm that uses Õ(n^(13/7)) queries and outputs a (2−η)-estimate of the metric Steiner tree weight, where η>0 is a universal constant. A key component in their algorithm is a sublinear algorithm for a particular set cover problem where, given a set system (U,F), the goal is to provide a multiplicative-additive estimate for |U|−SC(U,F). Here U is the set of elements, F is the collection of sets, and SC(U,F) denotes the optimal set cover size of (U,F). In particular, their algorithm returns a (1/4,ε⋅|U|)-multiplicative-additive estimate for this set cover problem using Õ(|F|^(7/4)) membership oracle queries (querying whether a set S contains an e), where ε is a fixed constant.\nIn this work, we improve the query complexity of (2−η)-estimating the metric Steiner tree weight to Õ(n^(5/3)) by showing a (1/2,ε⋅|U|)-estimate for the above set cover problem using Õ(|F|^(5/3)) membership queries. To design our set cover algorithm, we estimate the size of a random greedy maximal matching for an auxiliary multigraph that the algorithm constructs implicitly, without access to its adjacency list or matrix.",
      "url": "https://www.microsoft.com/en-us/research/publication/sublinear-metric-steiner-tree-via-improved-bounds-for-set-cover/"
    },
    {
      "title": "EpiCoder: Encompassing Diversity and Complexity in Code Generation",
      "authors": [
        "Haoling Li",
        "Jie Wu",
        "Jinsong Su",
        "Qi Chen",
        "Scarlett Li",
        "Wenxiang Hu",
        "Xiao Liu",
        "Xin Zhang",
        "Yangyu Huang",
        "Yaoxiang Wang",
        "Ying Xin",
        "Yujiu Yang",
        "Zhongxin Guo"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "January 2025",
      "abstract": "Existing methods for code generation use code snippets as seed data, restricting the complexity and diversity of the synthesized data. In this paper, we introduce a novel feature tree-based synthesis framework, which revolves around hierarchical code features derived from high-level abstractions of code. The feature tree is constructed from raw data and refined iteratively to increase the quantity and diversity of the extracted features, which captures and recognizes more complex patterns and relationships within the code. By adjusting the depth and breadth of the sampled subtrees, our framework provides precise control over the complexity of the generated code, enabling functionalities that range from function-level operations to multi-file scenarios. We fine-tuned widely-used base models to obtain EpiCoder series, achieving state-of-the-art performance on multiple benchmarks at both the function and file levels. In particular, empirical evidence indicates that our approach shows significant potential in the synthesizing of repository-level code data. Our code and data are publicly available at https://github.com/microsoft/EpiCoder.",
      "url": "https://www.microsoft.com/en-us/research/publication/epicoder-encompassing-diversity-and-complexity-in-code-generation/"
    },
    {
      "title": "Information Complexity of Mixed-Integer Convex Optimization",
      "authors": [
        "A. Basu",
        "Hongyi Jiang",
        "Marco Molinaro",
        "Phillip A. Kerger"
      ],
      "research_areas": [
        "Algorithms"
      ],
      "publication_date": "January 2025",
      "abstract": "We investigate the information complexity of mixed-integer convex optimization under different types of oracles. We establish new lower bounds for the standard first-order oracle, improving upon the previous best known lower bound. This leaves only a lower order linear term (in the dimension) as the gap between the lower and upper bounds. This is derived as a corollary of a more fundamental “transfer”result that shows how lower bounds on information complexity of continuous convex optimization under different oracles can be transferred to the mixed-integer setting in a black-box manner. Further, we (to the best of our knowledge) initiate the study of, and obtain the first set of results on, information complexity under oracles that only reveal \\emph{partial} first-order information, e.g., where one can only make a binary query over the function value or subgradient at a given point. We give algorithms for (mixed-integer) convex optimization that work under these less informative oracles. We also give lower bounds showing that, for some of these oracles, every algorithm requires more iterations to achieve a target error compared to when complete first-order information is available. That is, these oracles are provably less informative than full first-order oracles for the purpose of optimization.",
      "url": "https://www.microsoft.com/en-us/research/publication/information-complexity-of-mixed-integer-convex-optimization-2/"
    },
    {
      "title": "Learning from other Domains to Advance AI Evaluation and Testing: Testing in Aircraft Design and Manufacturing",
      "authors": [
        "Paul Alp"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "January 2025",
      "abstract": "In aviation design and manufacturing, testing can generally be described as serving the following\npurposes: (i) facilitating development of a product design; (ii) verifying its functionality and\nrobustness; (iii) demonstrating compliance of the design with regulatory standards in order to\nobtain approval from a civil aviation authority such as the U.S. Federal Aviation Administration;\nand (iv) verifying that the as-built product conforms to its design, complies with applicable\nregulations, and is in a condition for safe operation.\nTesting is, therefore, closely linked to showing compliance with regulatory requirements, which\nforms the predicate for obtaining approvals from aviation authorities that allow aircraft to be\noperated. To be sure, some design features do not require testing to show compliance, and\ncompliance showings may also be made based on analysis, historical findings of compliance,\ninspection, or system architecture. But, testing remains a centerpiece of compliance, and the\nregulations to which compliance must be shown fundamentally define the nature of aircraft\nproducts and the processes for creating them.\nFor these reasons, testing with respect to aviation products must be considered in the context of\nthe overarching regulatory framework for standards and how compliance to standards must be\nshown.",
      "url": "https://www.microsoft.com/en-us/research/publication/learning-from-other-domains-to-advance-ai-evaluation-and-testing-testing-in-aircraft-design-and-manufacturing/"
    },
    {
      "title": "Enabling Autonomic Microservice Management through Self-Learning Agents",
      "authors": [
        "Dongmei Zhang",
        "Fangkai Yang",
        "Fenglin Yu",
        "Hongyu Zhang",
        "Jue Zhang",
        "Qi Zhang",
        "Qingwei Lin 林庆维",
        "Saravan Rajmohan",
        "Xiaoting Qin",
        "Yingnong Dang",
        "Zhiyang Zhang"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "January 2025",
      "abstract": "The increasing complexity of modern software systems necessitates robust autonomic self-management capabilities. While Large Language Models (LLMs) demonstrate potential in this domain, they often face challenges in adapting their general knowledge to specific service contexts. To address this limitation, we propose ServiceOdyssey, a self-learning agent system that autonomously manages microservices without requiring prior knowledge of service-specific configurations. By leveraging curriculum learning principles and iterative exploration, ServiceOdyssey progressively develops a deep understanding of operational environments, reducing dependence on human input or static documentation. A prototype built with the Sock Shop microservice demonstrates the potential of this approach for autonomic microservice management.",
      "url": "https://www.microsoft.com/en-us/research/publication/enabling-autonomic-microservice-management-through-self-learning-agents/"
    },
    {
      "title": "Learning from other Domains to Advance AI Evaluation and Testing: The Evolving Use of Bank Stress Tests",
      "authors": [
        "Kathryn Judge"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "January 2025",
      "abstract": "“Stress testing” of banks, or more accurately, banking organizations, is one of the most important regulatory innovations to emerge from the 2008 financial crisis. The stress testing of an individual bank entails assessing how that bank will fare under a given adverse scenario, usually meant to replicate the types of developments that would occur during a deep or prolonged recession, such as a significant rise in unemployment, heightened market volatility and large declines in asset values. Typically, the regulator will provide one or more scenarios, the bank then provides the requisite data and the regulator uses its model to assess how the bank will fare in the face of the scenarios provided. If the regulator determines that the bank would not remain in good enough health, in the sense of remaining well capitalized in the face of the hypothetical adverse scenario, it usually requires the bank to increase its capital by foregoing distributions to shareholders via dividends or share repurchases. A bank’s capital refers to the amount of equity it uses to fund its operations; higher capital increases a bank’s capacity to absorb losses. Banks also engage in bank-run stress tests using their internal risk-management tools as part of their obligations, providing separate insights into the quality of a bank’s risk management regime.\nThe benefits of regulatory stress testing are many, as are the challenges. They vary depending on the conditions under which the stress tests are run. The original regulatory stress tests occurred in early 2009, when the financial system remained mired in the Great Financial Crisis (GFC). This allowed bank regulators to devise an adverse scenario based on the actual and specific hardships the economy was already facing. Bank regulators were willing to subject banks to a rigorous and realistic stress test because Congress had already given the Treasury Department $700 billion in funding that the Treasury Department could use to recapitalize any bank revealed to have deficiencies that it could not remediate by raising additional funds from market-based sources. This was critical to the rigor and credibility of the exercise, as bank regulators are hesitant to undertake any public exercise that might reveal adverse information or otherwise exacerbate financial fragility unless they have the tools to contain the fallout. With the benefit of the temporarily more expansive authority and other wise design choices, the original round of stress tests helped provide credible information about bank health and enhanced the functioning of the financial sector, helping to further pave the road to recovery.",
      "url": "https://www.microsoft.com/en-us/research/publication/learning-from-other-domains-to-advance-ai-evaluation-and-testing-the-evolving-use-of-bank-stress-test/"
    },
    {
      "title": "The New Calculator? Practices, Norms, and Implications of Generative AI in Higher Education",
      "authors": [
        "Abigail Sellen",
        "Auste Simkute",
        "Lev Tankelevitch",
        "Sean Rintel",
        "Viktor Kewenig"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Human-computer interaction",
        "Social sciences"
      ],
      "publication_date": "January 2025",
      "abstract": "Generative AI (GenAI) has introduced myriad opportunities and challenges for higher education. Anticipating this potential transformation requires understanding students’ contextualised practices and norms around GenAI. We conducted semi-structured interviews with 26 students and 11 educators from diverse departments across two universities. Grounded in Strong Structuration Theory, we find diversity in students’ uses and motivations for GenAI. Occurring in the context of unclear university guidelines, institutional fixation on plagiarism, and inconsistent educator communication, students’ practices are informed by unspoken rules around appropriate use, GenAI limitations and reliance strategies, and consideration of agency and skills. Perceived impacts include changes in confidence, and concerns about skill development, relationships with educators, and plagiarism. Both groups envision changes in universities’ attitude to GenAI, responsible use training, assessments, and integration of GenAI into education. We discuss socio-technical implications in terms of current and anticipated changes in the external and internal structures that contextualise students’ GenAI use.",
      "url": "https://www.microsoft.com/en-us/research/publication/the-new-calculator-practices-norms-and-implications-of-generative-ai-in-higher-education/"
    },
    {
      "title": "TimeDP: Learning to Generate Multi-Domain Time Series with Domain Prompts",
      "authors": [
        "Chang Xu",
        "Jiang Bian",
        "Yu-Hao Huang",
        "Yueying Wu"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "",
      "abstract": "Time series generation models are crucial for applications like data augmentation and privacy preservation. Most existing time series generation models are typically designed to generate data from one specified domain. While leveraging data from other domain for better generalization is proved to work in other application areas, this approach remains challenging for time series modeling due to the large divergence in patterns among different real world time series categories. In this paper, we propose a multi-domain time series diffusion model with domain prompts, named TimeDP. In TimeDP, we utilize a time series semantic prototype module which defines time series prototypes to represent time series basis, each prototype vector serving as “word” representing some elementary time series feature. A prototype assignment module is applied to extract the extract domain specific prototype weights, for learning domain prompts as generation condition. During sampling, we extract “domain prompt” with few-shot samples from the target domain and use the domain prompts as condition to generate time series samples. Experiments demonstrate that our method outperforms baselines to provide the state-of-the-art in-domain generation quality and strong unseen domain generation capability.",
      "url": "https://www.microsoft.com/en-us/research/publication/timedp-learning-to-generate-multi-domain-time-series-with-domain-prompts/"
    },
    {
      "title": "Sub-terahertz metamaterial stickers for non-invasive fruit ripeness sensing",
      "authors": [
        "Atsutse Kludze",
        "Ranveer Chandra",
        "Subhajit Karmakar",
        "Yasaman Ghasempour"
      ],
      "research_areas": [
        "Systems and networking",
        "Technology for emerging markets"
      ],
      "publication_date": "January 2025",
      "abstract": "Fruits and vegetables account for around a third of all food loss and waste. Post-harvest, retail and consumer losses and waste could be reduced with better ripeness assessment methods. Here we develop a sub-terahertz metamaterial sticker (called Meta-Sticker) that can be attached to a fruit to provide insights into the edible mesocarp’s ripeness without cutting into the produce. The fruit acts as a complex multilayer substrate to Meta-Sticker and, when excited by sub-terahertz signals, generates two distinct resonances: localized dipole resonance that correlates with the exocarp’s refractive index; and propagating plasmon resonance that penetrates into the mesocarp and resembles the rare phenomenon of ‘extraordinary transmission’. The Meta-Sticker accurately predicted the ripeness of different fruits with a cumulative normalized root mean square error of 0.54% of the produce tested. This study offers a non-invasive, low-cost and biodegradable solution for accurate ripeness assessment with applications in distribution optimization and food waste reduction.",
      "url": "https://www.microsoft.com/en-us/research/publication/sub-terahertz-metamaterial-stickers-for-non-invasive-fruit-ripeness-sensing/"
    },
    {
      "title": "Learning from other Domains to Advance AI Evaluation and Testing: Testing in the Nuclear Industry",
      "authors": [
        "Augusto Debandi",
        "Federico Mezio",
        "Gerónimo Poletto Antonacci",
        "Pablo Cantero",
        "Pablo Ramirez"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "January 2025",
      "abstract": "The growth in global energy demand accelerated in recent years (2023–2024) at rates double the average growth of the past decade (2010-2019). Projections suggest this trend will continue. Combined with the global goal of decarbonization, this positions nuclear energy as a reliable source with significant potential for growth. However, the benefits of the nuclear industry go beyond electricity generation and extend to healthcare, scientific and technological research and development, food security, and environmental protection among many other activities.\nThe nuclear industry is technologically mature and constantly evolving. The main associated risks are related to potential accidents and their consequences, including environmental contamination and radiological harms. Alongside the evolution of the nuclear industry, a consistent regulatory framework has been developed, evolving over time to keep up with new technologies and the operational experience of nuclear facilities. Different regulatory approaches exist in different jurisdictions. While some establish prescriptive regulations (i.e. defining how to do things), others use performance-based regulations (i.e. establishing a limit of risk and leaving the method of compliance to the designer). These approaches have in common that they establish standards aiming to ensure, with a high degree of certainty, that activities are conducted safely. In pursuit of this objective, testing provides an objective means of demonstrating compliance with specific requirements stated in regulatory or industry standards. However, testing is not only performed to ensure safety and regulatory compliance, it also supports and is an essential tool in maintaining the capability and availability of nuclear facilities to provide intended goods and services such as production of electricity, medical radioisotopes, neutron beams, etc.",
      "url": "https://www.microsoft.com/en-us/research/publication/learning-from-other-domains-to-advance-ai-evaluation-and-testing-testing-in-the-nuclear-industry/"
    },
    {
      "title": "Learning from other Domains to Advance AI Evaluation and Testing: The History and Evolution of Testing in Pharmaceutical Regulation",
      "authors": [
        "Daniel Benamouzig",
        "Daniel Carpenter"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Medical, health and genomics"
      ],
      "publication_date": "January 2025",
      "abstract": "Modern pharmaceuticals are inherently dual and uncertain in nature. Like any other technology, pharmaceuticals carry treatment value and the threat of harm – both of which are unknown – and are regulated accordingly. While they promise relief and even curing of serious diseases, they also carry risks even when they are effective. Pharmaceutical regulation is specifically grounded in this duality and uncertainty, aiming to learn about their benefits and risks and then weigh the benefit-risk balance. Modern pharmaceutical treatment co-evolved with systematic testing regimes, and the rise of the modern biotech industry is inseparable from that of the testing and regulatory regimes that have been established to govern its products. Modern pharmaceutical regulation is best understood as a system of approval regulation, that is, experimental minima combined with potential state veto of research and development (where veto is premised upon subset of experimental results).\nThe link between experimentation and state gatekeeping power is as pronounced in the pharmaceutical realm as in any other domain of economic and social activity. The fact that a new or modified molecule must receive regulatory authorization in order to be sold (and hence generate revenue) underpins a vast system of global experiment. Approval regulation as a regulatory-experimental system evolved due to scientific innovation, public and economic demand for information about products, and political pressure forged through critical historical episodes.",
      "url": "https://www.microsoft.com/en-us/research/publication/learning-from-other-domains-to-advance-ai-evaluation-and-testing-the-history-and-evolution-of-testing-in-pharmaceutical-regulation/"
    },
    {
      "title": "TorchGeo: Deep Learning With Geospatial Data",
      "authors": [
        "Adam J. Stewart",
        "Anthony Ortiz",
        "Arindam Banerjee",
        "Caleb Robinson",
        "Isaac A. Corley",
        "Juan M. Lavista Ferres"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Computer vision"
      ],
      "publication_date": "January 2025",
      "abstract": "Remotely sensed geospatial data are critical for applications including precision agriculture, urban planning, disaster monitoring and response, and climate change research, among others. Deep learning methods are particularly promising for modeling many remote sensing tasks given the success of deep neural networks in similar computer vision tasks and the sheer volume of remotely sensed imagery available. However, the variance in data collection methods and handling of geospatial metadata make the application of deep learning methodology to remotely sensed data nontrivial. For example, satellite imagery often includes additional spectral bands beyond red, green, and blue and must be joined to other geospatial data sources that may have differing coordinate systems, bounds, and resolutions. To help realize the potential of deep learning for remote sensing applications, we introduce TorchGeo, a Python library for integrating geospatial data into the PyTorch deep learning ecosystem. TorchGeo provides data loaders for a variety of benchmark datasets, composable datasets for uncurated geospatial data sources, samplers for geospatial data, and transforms that work with multispectral imagery. TorchGeo is also the first library to provide pre-trained models for multispectral satellite imagery (e.g., models that use all bands from the Sentinel-2 satellites), allowing for advances in transfer learning on downstream remote sensing tasks with limited labeled data. We use TorchGeo to create reproducible benchmark results on existing datasets and benchmark our proposed method for preprocessing geospatial imagery on the fly. TorchGeo is open source and available on GitHub: https://github.com/microsoft/torchgeo.",
      "url": "https://www.microsoft.com/en-us/research/publication/torchgeo-deep-learning-with-geospatial-data-2/"
    },
    {
      "title": "Learning from other Domains to Advance AI Evaluation and Testing: Governance of Genome Edition in Human Therapeutics and Agricultural Applications",
      "authors": [
        "Alta Charo"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Medical, health and genomics"
      ],
      "publication_date": "January 2025",
      "abstract": "Genome editing in its current form burst upon the scientific scene most vividly with the 2012 publication\nof a landmark paper by Jennifer Doudna, Emmanuelle Charpentier and colleagues. It was quickly apparent that this approach to genetic alteration i.e. controlled change to DNA sequences, which is more precise than earlier methods due to the use of a programmable, bacterial enzyme called Cas9, had tremendous potential across a wide variety of applications in multiple species, ranging from industrial processes to agricultural innovations to drug and gene therapy development.\nResponses to proposed uses of genome editing have been influenced by how genome editing is itself\nframed as a practice, i.e., what genome editing is considered to be. Some saw its use in agriculture, for\nexample, as continuous with pre-modern breeding techniques, which themselves exploited naturally\noccurring genetic variation in selective breeding for improved traits. Others focused on discontinuities,\nseeing genome editing as a modern biotechnology, which raises questions about whose interests are\nserved by its use and the broader impacts of such technology on society. However genome editing was\ndescribed, the need for attention to attendant ethical issues and appropriate governance immediately\nbecame apparent, particularly in the case of contentious uses, such as heritable changes to the human\ngenome.",
      "url": "https://www.microsoft.com/en-us/research/publication/learning-from-other-domains-to-advance-ai-evaluation-and-testing-governance-of-genome-edition-in-human-therapeutics-and-agricultural-applications/"
    },
    {
      "title": "Sociotechnical Implications of Generative Artificial Intelligence for Information Access",
      "authors": [
        "Bhaskar Mitra",
        "Henriette Cramer",
        "Olya Gurevich"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Search and information retrieval"
      ],
      "publication_date": "",
      "abstract": "Robust access to trustworthy information is a critical need for society with implications for knowledge production, public health education, and promoting informed citizenry in democratic societies. Generative AI technologies may enable new ways to access information and improve effectiveness of existing information retrieval systems but we are only starting to understand and grapple with their long-term social implications. In this chapter, we present an overview of some of the systemic consequences and risks of employing generative AI in the context of information access. We also provide recommendations for evaluation and mitigation, and discuss challenges for future research.",
      "url": "https://www.microsoft.com/en-us/research/publication/sociotechnical-implications-of-generative-artificial-intelligence-for-information-access/"
    },
    {
      "title": "Learning from other Domains to Advance AI Evaluation and Testing: The regulatory landscape of nanoscience and nanotechnology, and applications to future AI regulation",
      "authors": [
        "J. Dionne"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "January 2025",
      "abstract": "The fields of nanoscience and nanotechnology investigate and manipulate matter at the nanoscale\n(generally defined as having critical dimensions less than 1000nm). At these scales, materials exhibit\ndistinct physical, chemical, and biological properties compared to their bulk counterparts due to their\nincreased surface area, enhanced reactivity, and potential for quantum effects. Nanomaterials include\nnanoparticles, two-dimensional materials (such as graphene, hexagonal boron nitride, transition-metal\ndichalcogenides), and their heterostructures, and are pivotal to the development of advanced technologies across numerous industries. These materials have the potential to revolutionize a wide range of sectors, from computing, communications, and catalysis, to medicine and manufacturing, owing to their increased strength, improved conductivity, and/or the ability to interact with biological systems in novel ways. The Nanotechnology Research and Development Act recently celebrated its twentieth anniversary (March 2004-March 2024). As stated in former President Bill Clinton’s letter during that celebration, “in the last quarter century, nanotechnology research has contributed to scientific breakthroughs that have changed the way we live and work—from computer chips, to electric vehicle batteries, to COVID-19 vaccines.\nWhile nanoscience is a nascent field – in many ways even younger than the AI field – the applications of\nnanotechnology are vast and increasingly impactful. In pharmaceuticals, nanomaterials are being used to\ndevelop drug delivery systems that target specific cells or tissues with precision, minimizing side effects\nand enhancing therapeutic efficacy. In chemical manufacturing, nanomaterials serve as catalysts, speeding up reactions and improving energy efficiency. In energy storage, nanomaterials increase the speed of charging, extend the cyclability, and can increase the device capacity and energy density. In automation and robotics, nanomaterials enhance the performance of components such as actuators, sensors, and motors, offering improved strength, flexibility, and responsiveness at the nanoscale. In additive manufacturing, nanomaterials enable the creation of lighter, stronger, and more durable parts, allowing for greater precision and the production of complex, custom-designed structures with enhanced mechanical properties. Nanotechnology is also transforming transportation and construction with stronger, lighter materials, as well as self-healing structures that can extend the lifespan of materials. In the electronics industry, nanomaterials are fundamental to the development of smaller, faster, and more powerful semiconductor chips, enabling the continued miniaturization of devices. Finally, consumer products, such as clothing, cosmetics, and food packaging, also benefit from nanotechnology, with innovations that improve strength and durability, stain resistance, and functionality.",
      "url": "https://www.microsoft.com/en-us/research/publication/learning-from-other-domains-to-advance-ai-evaluation-and-testing-the-regulatory-landscape-of-nanoscience-and-nanotechnology-and-applications-to-future-ai-regulation/"
    },
    {
      "title": "A Geometric Perspective on the Injective Norm of Sums of Random Tensors",
      "authors": [
        "Afonso S. Bandeira",
        "Haotian Jiang",
        "Kevin Lucca",
        "Sivakanth Gopi",
        "Thomas Rothvoss"
      ],
      "research_areas": [
        "Mathematics"
      ],
      "publication_date": "January 2025",
      "abstract": "Matrix concentration inequalities, intimately connected to the Non-Commutative Khintchine inequality, have been an important tool in both applied and pure mathematics. We study tensor versions of these inequalities, and establish non-asymptotic inequalities for the ℓp injective norm of random tensors with correlated entries. In certain regimes of p and the tensor order, our tensor concentration inequalities are nearly optimal in their dimension dependencies. We illustrate our result with applications to problems including structured models of random tensors and matrices, tensor PCA, and connections to lower bounds in coding theory.\nOur techniques are based on covering number estimates as opposed to operator theoretic tools, which also provide a geometric proof of a weaker version of the Non-Commutative Khintchine inequality, motivated by a question of Talagrand.",
      "url": "https://www.microsoft.com/en-us/research/publication/a-geometric-perspective-on-the-injective-norm-of-sums-of-random-tensors/"
    },
    {
      "title": "DiscQuant: A Quantization Method for Neural Networks Inspired by Discrepancy Theory",
      "authors": [
        "Arturs Backurs",
        "Janardhan (Jana) Kulkarni",
        "Jerry Chee",
        "Li Zhang",
        "Rainie Heck",
        "Sivakanth Gopi",
        "Thomas Rothvoss"
      ],
      "research_areas": [
        "Algorithms",
        "Artificial intelligence",
        "Mathematics"
      ],
      "publication_date": "January 2025",
      "abstract": "Quantizing the weights of a neural network has two steps: (1) Finding a good low bit-complexity representation for weights (which we call the quantization grid) and (2) Rounding the original weights to values in the quantization grid. In this paper, we study the problem of rounding optimally given any quantization grid. The simplest and most commonly used way to round is Round-to-Nearest (RTN). By rounding in a data-dependent way instead, one can improve the quality of the quantized model significantly.\nWe study the rounding problem from the lens of discrepancy theory, which studies how well we can round a continuous solution to a discrete solution without affecting solution quality too much. We prove that given m=poly(1/ϵ) samples from the data distribution, we can round all but O(m) model weights such that the expected approximation error of the quantized model on the true data distribution is ≤ϵ as long as the space of gradients of the original model is approximately low rank (which we empirically validate).\nOur proof, which is algorithmic, inspired a simple and practical rounding algorithm called DiscQuant. In our experiments, we demonstrate that DiscQuant significantly improves over the prior state-of-the-art rounding method called GPTQ and the baseline RTN over a range of benchmarks on Phi3mini-3.8B and Llama3.1-8B. For example, rounding Phi3mini-3.8B to a fixed quantization grid with 3.25 bits per parameter using DiscQuant gets 64\\% accuracy on the GSM8k dataset, whereas GPTQ achieves 54\\% and RTN achieves 31\\% (the original model achieves 84\\%).",
      "url": "https://www.microsoft.com/en-us/research/publication/discquant-a-quantization-method-for-neural-networks-inspired-by-discrepancy-theory/"
    },
    {
      "title": "Rethinking Aleatoric and Epistemic Uncertainty",
      "authors": [
        "Adam Foster",
        "Eleanor Trollope",
        "Freddie Bickford-Smith",
        "Jannik Kossen",
        "Mark van der Wilk",
        "Tom Rainforth"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "December 2024",
      "abstract": "The ideas of aleatoric and epistemic uncertainty are widely used to reason about the probabilistic predictions of machine-learning models. We identify incoherence in existing discussions of these ideas and suggest this stems from the aleatoric-epistemic view being insufficiently expressive to capture all of the distinct quantities that researchers are interested in. To explain and address this we derive a simple delineation of different model-based uncertainties and the data-generating processes associated with training and evaluation. Using this in place of the aleatoric-epistemic view could produce clearer discourse as the field moves forward.",
      "url": "https://www.microsoft.com/en-us/research/publication/rethinking-aleatoric-and-epistemic-uncertainty/"
    },
    {
      "title": "RE#: High Performance Derivative-Based Regex Matching with Intersection, Complement, and Restricted Lookarounds",
      "authors": [
        "Ian Erik Varatalu",
        "Juhan Ernits",
        "Margus Veanes"
      ],
      "research_areas": [
        "Algorithms",
        "Programming languages and software engineering"
      ],
      "publication_date": "January 2025",
      "abstract": "We present a tool and theory RE# for regular expression matching that is built on symbolic derivatives, does not use backtracking, and, in addition to the classical operators, also supports complement, intersection and restricted lookarounds.\nWe develop the theory formally and show that the main matching algorithm has input-linear complexity both in theory as well as experimentally.\nWe apply thorough evaluation on popular benchmarks that show that RE# is over 71% faster than the next fastest regex engine in Rust on the baseline, and outperforms all state-of-the-art engines on extensions of the benchmarks often by several orders of magnitude.",
      "url": "https://www.microsoft.com/en-us/research/publication/re-high-performance-derivative-based-regex-matching-with-intersection-complement-and-restricted-lookarounds/"
    },
    {
      "title": "Improving Rural Healthcare by Creating Academic- and Nonacademic-Rural Hospital Partnerships Based on Community Health Needs Assessments and Technological Needs",
      "authors": [
        "Bill Weeks",
        "James Weinstein",
        "Juan M. Lavista Ferres",
        "Suhas Babu"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Medical, health and genomics"
      ],
      "publication_date": "January 2025",
      "abstract": "Over the past 14 years, 149 rural hospitals (comprising 6% of all rural hospitals) have closed or no longer provide inpatient services because of financial distress, staff shortages, and resource constraints.1 In addition, more than 20% of rural hospitals are currently at risk of closing.2 Nearly every state has rural hospitals at risk,3 and many rural facilities are cutting services to stay afloat.4 These closures and service restrictions reduce access to essential healthcare for underserved communities, many of which already face higher health risks and geographic isolation. Rural hospitals are critical for local economies and provide vital emergency care, making their closures a significant public health and economic concern.5 To survive, rural clinics and hospitals need to remain relevant to their service populations, demonstrate value, and remain technologically current. While the White House has recently launched an admirable initiative to support cybersecurity in rural hospitals,6 ecosystem transformation could better address the broader challenges needed to create sustainable partnerships between rural and urban healthcare systems.7 Without significant changes to reimbursement policies, financial pressures on rural hospitals will persist. Although critical access hospitals have a unique Medicare payment system designed to support their survival,8 these financial challenges will continue to hinder their ability to attract and retain an adequate workforce, invest in new technologies, and serve their communities.",
      "url": "https://www.microsoft.com/en-us/research/publication/improving-rural-healthcare-by-creating-academic-andnonacademic-rural-hospital-partnerships-based-on-community-health-needs-assessments-and-technological-needs/"
    },
    {
      "title": "TimeRAF: Retrieval-Augmented Foundation model for Zero-shot Time Series Forecasting",
      "authors": [
        "Chang Xu",
        "Huanyu Zhang",
        "Jiang Bian",
        "Liang Wang",
        "Tien-Ping Tan",
        "Yi-Fan Zhang",
        "Zhang Zhang"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "December 2024",
      "abstract": "Time series forecasting plays a crucial role in data mining, driving rapid advancements across numerous industries. With the emergence of large models, time series foundation models (TSFMs) have exhibited remarkable generalization capabilities, such as zero-shot learning, through large-scale pre-training. Meanwhile, Retrieval-Augmented Generation (RAG) methods have been widely employed to enhance the performance of foundation models on unseen data, allowing models to access to external knowledge. In this paper, we introduce TimeRAF, a Retrieval-Augmented Forecasting model that enhance zero-shot time series forecasting through retrieval-augmented techniques. We develop customized time series knowledge bases that are tailored to the specific forecasting tasks. TimeRAF employs an end-to-end learnable retriever to extract valuable information from the knowledge base. Additionally, we propose Channel Prompting for knowledge integration, which effectively extracts relevant information from the retrieved knowledge along the channel dimension. Extensive experiments demonstrate the effectiveness of our model, showing significant improvement across various domains and datasets.",
      "url": "https://www.microsoft.com/en-us/research/publication/timeraf-retrieval-augmented-foundation-model-for-zero-shot-time-series-forecasting/"
    },
    {
      "title": "How Generative AI Improves Supply Chain Management",
      "authors": [
        "David Simchi-Levi",
        "Ishai Menache",
        "Jeevan Pathuri",
        "Tom Linton"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "January 2025",
      "abstract": "Companies face a variety of complex challenges in designing and optimizing their supply chains. Increasing their resilience, reducing costs, and improving the quality of their planning are just a few of them. Over the past few decades, advances in information technologies have allowed firms to move from decision-making on the basis of intuition and experience to more automated and data-driven methods. As a result, businesses have seen efficiency gains, substantial cost reductions, and improved customer service.",
      "url": "https://www.microsoft.com/en-us/research/publication/how-generative-ai-improves-supply-chain-management/"
    },
    {
      "title": "PGRID: Power Grid Reconstruction in Informal Developments Using High-Resolution Aerial Imagery",
      "authors": [
        "Amrita Gupta",
        "Anthony Ortiz",
        "Duncan Kebut",
        "Juan M. Lavista Ferres",
        "Luana Marotti",
        "Rahul Dodhia",
        "Seema Iyer",
        "Simone Fobi Nsutezo"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Computer vision"
      ],
      "publication_date": "January 2025",
      "abstract": "As of 2023, a record 117 million people have been displaced worldwide, more than double the number from a decade ago [22]. Of these, 32 million are refugees under the UNHCR mandate, with 8.7 million residing in refugee camps. A critical issue faced by these populations is the lack of access to electricity, with 80% of the 8.7 million refugees and displaced persons in camps globally relying on traditional biomass for cooking and lacking reliable power for essential tasks such as cooking and charging phones. Often, the burden of collecting firewood falls on women and children, who frequently travel up to 20 kilometers into dangerous areas, increasing their vulnerability.[7]\nElectricity access could significantly alleviate these challenges, but a major obstacle is the lack of accurate power grid infrastructure maps, particularly in resource-constrained environments like refugee camps, needed for energy access planning. Existing power grid maps are often outdated, incomplete, or dependent on costly, complex technologies, limiting their practicality. To address this issue, PGRID is a novel application-based approach, which utilizes high-resolution aerial imagery to detect electrical poles and segment electrical lines, creating precise power grid maps. PGRID was tested in the Turkana region of Kenya, specifically the Kakuma and Kalobeyei Camps, covering 84 km2 and housing over 200,000 residents.\nOur findings show that PGRID delivers high-fidelity power grid maps especially in unplanned settlements, with F1-scores of 0.71 and 0.82 for pole detection and line segmentation, respectively. This study highlights a practical application for leveraging open data and limited labels to improve power grid mapping in unplanned settlements, where the growing number of displaced persons urgently need sustainable energy infrastructure solutions.",
      "url": "https://www.microsoft.com/en-us/research/publication/pgrid-power-grid-reconstruction-in-informal-developments-using-high-resolution-aerial-imagery/"
    },
    {
      "title": "Evaluating the role of pre-training dataset size and diversity on single-cell foundation model performance",
      "authors": [
        "Akshaya Thoutam",
        "Alan DenAdel",
        "Anay Gupta",
        "Andrew W. Navia",
        "Ava P. Amini",
        "Lorin Crawford",
        "Madeline Hughes",
        "Nicolo Fusi",
        "Peter S. Winter",
        "Srivatsan Raghavan"
      ],
      "research_areas": [
        "Medical, health and genomics"
      ],
      "publication_date": "December 2024",
      "abstract": "The success of transformer-based foundation models on natural language and images has motivated their use in single-cell biology. Single-cell foundation models have been trained on increasingly larger transcriptomic datasets, scaling from initial studies with 1 million cells to newer atlases with over 100 million cells. This study investigates the role of pre-training dataset size and diversity on the performance of single-cell foundation models on both zero-shot and fine-tuned tasks. Using a large corpus of 22.2 million cells, we pre-train a total of 375 models which we evaluate by conducting 3,750 experiments. Our results show that current methods tend to plateau in performance with pre-training datasets that are only a fraction of the size.",
      "url": "https://www.microsoft.com/en-us/research/publication/evaluating-the-role-of-pre-training-dataset-size-and-diversity-on-single-cell-foundation-model-performance/"
    },
    {
      "title": "Bootstrap Your Own Context Length",
      "authors": [
        "Furu Wei",
        "Liang Wang",
        "Nan Yang",
        "Xiaolong Huang",
        "Xingxing Zhang"
      ],
      "research_areas": [
        "Human language technologies"
      ],
      "publication_date": "December 2024",
      "abstract": "We introduce a bootstrapping approach to train long-context language models by exploiting their short-context capabilities only. Our method utilizes a simple agent workflow to synthesize diverse long-context instruction tuning data, thereby eliminating the necessity for manual data collection and annotation. The proposed data synthesis workflow requires only a short-context language model, a text retriever, and a document collection, all of which are readily accessible within the open-source ecosystem. Subsequently, language models are fine-tuned using the synthesized data to extend their context lengths. In this manner, we effectively transfer the short-context capabilities of language models to long-context scenarios through a bootstrapping process. We conduct experiments with the open-source Llama-3 family of models and demonstrate that our method can successfully extend the context length to up to 1M tokens, achieving superior performance across various benchmarks.",
      "url": "https://www.microsoft.com/en-us/research/publication/bootstrap-your-own-context-length/"
    },
    {
      "title": "When Can Proxies Improve the Sample Complexity of Preference Learning?",
      "authors": [
        "Alexander D'Amour",
        "Daniel Augusto de Souza",
        "Matt J. Kusner",
        "Mengyue Yang",
        "Pasquale Minervini",
        "Yuchen Zhu",
        "Zhengyan Shi"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "December 2024",
      "abstract": "We address the problem of reward hacking, where maximising a proxy reward does not necessarily increase the true reward. This is a key concern for Large Language Models (LLMs), as they are often fine-tuned on human preferences that may not accurately reflect a true objective. Existing work uses various tricks such as regularisation, tweaks to the reward model, and reward hacking detectors, to limit the influence that such proxy preferences have on a model. Luckily, in many contexts such as medicine, education, and law, a sparse amount of expert data is often available. In these cases, it is often unclear whether the addition of proxy data can improve policy learning. We outline a set of sufficient conditions on proxy feedback that, if satisfied, indicate that proxy data can provably improve the sample complexity of learning the ground truth policy. These conditions can inform the data collection process for specific tasks. The result implies a parameterisation for LLMs that achieves this improved sample complexity. We detail how one can adapt existing architectures to yield this improved sample complexity.",
      "url": "https://www.microsoft.com/en-us/research/publication/when-can-proxies-improve-the-sample-complexity-of-preference-learning/"
    },
    {
      "title": "AI-Enhanced Sensemaking: Exploring the Design of a Generative AI-Based Assistant to Support Genetic Professionals",
      "authors": [
        "Aleksandra Sarcevic",
        "Amanda K. Hall",
        "Angela Mastrianni",
        "Ashley Conard",
        "C. Austin-Tse",
        "Heidi L. Rehm",
        "Hope Twede",
        "Jeremiah (Miah) Wander",
        "Scott Saponas"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Human-computer interaction",
        "Medical, health and genomics"
      ],
      "publication_date": "December 2024",
      "abstract": "Generative AI has the potential to transform knowledge work, but further research is needed to understand how knowledge workers envision using and interacting with generative AI. We investigate the development of generative AI tools to support domain experts in knowledge work, examining task delegation and the design of human-AI interactions. Our research focused on designing a generative AI assistant to aid genetic professionals in analyzing whole genome sequences (WGS) and other clinical data for rare disease diagnosis. Through interviews with 17 genetics professionals, we identified current challenges in WGS analysis. We then conducted co-design sessions with six genetics professionals to determine tasks that could be supported by an AI assistant and considerations for designing interactions with the AI assistant. From our findings, we identified sensemaking as both a current challenge in WGS analysis and a process that could be supported by AI. We contribute an understanding of how domain experts envision interacting with generative AI in their knowledge work, a detailed empirical study of WGS analysis, and three design considerations for using generative AI to support domain experts in sensemaking during knowledge work.",
      "url": "https://www.microsoft.com/en-us/research/publication/ai-enhanced-sensemaking-exploring-the-design-of-a-generative-ai-based-assistant-to-support-genetic-professionals/"
    },
    {
      "title": "Inverse Design of Vitrimeric Polymers by Molecular Dynamics and Generative Modeling",
      "authors": [
        "Aniruddh Vashisth",
        "Bichlien Nguyen",
        "Jake Smith",
        "Prakash Thakolkaran",
        "Shuxin Zheng",
        "Siddhant Kumar",
        "Yiwen Zheng",
        "Ziheng Lu"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Ecology and environment"
      ],
      "publication_date": "December 2024",
      "abstract": "Vitrimer is a new class of sustainable polymers with the ability of self-healing through rearrangement of dynamic covalent adaptive networks. However, a limited choice of constituent molecules restricts their property space, prohibiting full realization of their potential applications. Through a combination of molecular dynamics (MD) simulations and machine learning (ML), particularly a novel graph variational autoencoder (VAE) model, we establish a method for generating novel vitrimers and guide their inverse design based on desired glass transition temperature (Tg). We build the first vitrimer dataset of one million and calculate Tg on 8,424 of them by high-throughput MD simulations calibrated by a Gaussian process model. The proposed VAE employs dual graph encoders and a latent dimension overlapping scheme which allows for individual representation of multi-component vitrimers. By constructing a continuous latent space containing necessary information of vitrimers, we demonstrate high accuracy and efficiency of our framework in discovering novel vitrimers with desirable Tg beyond the training regime. The proposed vitrimers with reasonable synthesizability cover a wide range of Tg and broaden the potential widespread usage of vitrimeric materials.",
      "url": "https://www.microsoft.com/en-us/research/publication/inverse-design-of-vitrimeric-polymers-by-molecular-dynamics-and-generative-modeling/"
    },
    {
      "title": "From Models to Microtheories: Distilling a Model’s Topical Knowledge for Grounded Question Answering",
      "authors": [
        "Alexander Sabol",
        "Ben Van Durme",
        "Bhavana Dalvi",
        "Nathaniel Weir",
        "Orion Weller",
        "Oyvind Tafjord",
        "P. Jansen",
        "Peter Clark",
        "Sam Hornstein"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "December 2024",
      "abstract": "Recent reasoning methods (e.g., chain-of-thought, entailment reasoning) help users understand how language models (LMs) answer a single question, but they do little to reveal the LM’s overall understanding, or “theory,” about the question’s topic, making it still hard to trust the model. Our goal is to materialize such theories – here called microtheories (a linguistic analog of logical microtheories) – as a set of sentences encapsulating an LM’s core knowledge about a topic. These statements systematically work together to entail answers to a set of questions to both engender trust and improve performance. Our approach is to first populate a knowledge store with (model-generated) sentences that entail answers to training questions and then distill those down to a core microtheory that is concise, general, and non-redundant. We show that, when added to a general corpus (e.g., Wikipedia), microtheories can supply critical, topical information not necessarily present in the corpus, improving both a model’s ability to ground its answers to verifiable knowledge (i.e., show how answers are systematically entailed by documents in the corpus, fully grounding up to +8% more answers), and the accuracy of those grounded answers (up to +8% absolute). We also show that, in a human evaluation in the medical domain, our distilled microtheories contain a significantly higher concentration of topically critical facts than the non-distilled knowledge store. Finally, we show we can quantify the coverage of a microtheory for a topic (characterized by a dataset) using a notion of $p$-relevance. Together, these suggest that microtheories are an efficient distillation of an LM’s topic-relevant knowledge, that they can usefully augment existing corpora, and can provide both performance gains and an interpretable, verifiable window into the model’s knowledge of a topic.",
      "url": "https://www.microsoft.com/en-us/research/publication/from-models-to-microtheories-distilling-a-models-topical-knowledge-for-grounded-question-answering/"
    },
    {
      "title": "Trace is the Next AutoDiff: Generative Optimization with Rich Feedback, Execution Traces, and LLMs",
      "authors": [
        "Adith Swaminathan",
        "Allen Nie",
        "Ching-An Cheng"
      ],
      "research_areas": [
        "Algorithms",
        "Artificial intelligence"
      ],
      "publication_date": "December 2024",
      "abstract": "We study a class of optimization problems motivated by automating the design and update of AI systems like coding assistants, robots, and copilots. AutoDiff frameworks, like PyTorch, enable efficient end-to-end optimization of differentiable systems. However, general computational workflows can be non-differentiable and involve rich feedback (e.g. console output or user’s responses), heterogeneous parameters (e.g. prompts, codes), and intricate objectives (beyond maximizing a score). We investigate end-to-end generative optimization — using generative models such as LLMs within the optimizer for automatic updating of general computational workflows. We discover that workflow execution traces are akin to back-propagated gradients in AutoDiff and can provide key information to interpret feedback for efficient optimization. Formally, we frame a new mathematical setup, Optimization with Trace Oracle (OPTO). In OPTO, an optimizer receives an execution trace along with feedback on the computed output and updates parameters iteratively. We provide a Python library, Trace, that efficiently converts a workflow optimization problem into an OPTO instance using PyTorch-like syntax. Using Trace, we develop a general LLM-based generative optimizer called OptoPrime. In empirical studies, we find that OptoPrime is capable of first-order numerical optimization, prompt optimization, hyper-parameter tuning, robot controller design, code debugging, etc., and is often competitive with specialized optimizers for each domain. We envision Trace as an open research platform for devising novel generative optimizers and developing the next generation of interactive learning agents. Website: this https URL.\n",
      "url": "https://www.microsoft.com/en-us/research/publication/trace-is-the-new-autodiff-unlocking-efficient-optimization-of-computational-workflows/"
    },
    {
      "title": "Holographic Storage for the Cloud: advances and challenges",
      "authors": [
        "Alan Sanders",
        "Ant Rowstron",
        "Benn Thomsen",
        "Douglas J. Kelly",
        "Dushyanth Narayanan",
        "Giorgio Maltese",
        "Grace Brennan",
        "Greg O'Shea",
        "Guilherme Ilunga",
        "Jannes Gladrow",
        "Jiaqi Chu",
        "Joowon Lim",
        "Mengyang Yang",
        "Michael Rudow",
        "Nathanael Cheriere",
        "Pashmina Cameron",
        "Pedro F. da Costa",
        "Sarah Lewis",
        "Soujanya Ponnapalli",
        "Theano Stavrinos",
        "Tony Mason",
        "Xingbo Wu"
      ],
      "research_areas": [
        "Hardware and devices",
        "Systems and networking"
      ],
      "publication_date": "December 2024",
      "abstract": "Holographic Storage is an old idea that has always promised high density and fast random access, but has never been commercially competitive with Hard Disk Drives (HDDs) and Solid State Devices (SSDs). In Project HSD at Microsoft Research we asked the question: “Does holographic storage finally make sense for cloud storage?” This paper describes our journey towards answering this question. We achieved 1.8x higher density than the previous state of the art, using commodity components available today and leveraging machine learning to compensate for the noise and distortions introduced by commodity components. This uncovered two new challenges which are the focus of this paper: achieving high end-to-end energy efficiency without sacrificing capacity, and spatial multiplexing without mechanical movement. Improving end-to-end energy efficiency requires joint optimization across low-level media parameters and higher-level system parameters that govern background maintenance operations such as read refresh and garbage collection. We developed new physics models of the media; analytic and simulation models of the media access and background media maintenance; and workload-driven optimization to find optimal parameter combinations. These techniques resulted in a 14x improvement over the previous approach for typical workloads without sacrificing capacity. We also designed the first scalable and mechanical movement free spatial multiplexing system for holographic storage. Despite these advances, we conclude that currently holographic storage is still far from the combination of density, capacity scaling, and energy efficiency needed to compete with the incumbent technologies. We need fundamental advances in the physical media that improve energy efficiency by another 1–2 orders of magnitude without reducing data density. Further advances in optics are also required to achieve spatial multiplexing that is simultaneously scalable, low-loss, and high-density.",
      "url": "https://www.microsoft.com/en-us/research/publication/holographic-storage-for-the-cloud-advances-and-challenges/"
    },
    {
      "title": "VidTok: A Versatile and Open-Source Video Tokenizer",
      "authors": [
        "Anni Tang",
        "Jiang Bian",
        "Junliang Guo",
        "Li Song",
        "Tianyu He",
        "Xinle Cheng"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Computer vision"
      ],
      "publication_date": "December 2024",
      "abstract": "Encoding video content into compact latent tokens has become a fundamental step in video generation and understanding, driven by the need to address the inherent redundancy in pixel-level representations. Consequently, there is a growing demand for high-performance, open-source video tokenizers as video-centric research gains prominence. We introduce VidTok, a versatile video tokenizer that delivers state-of-the-art performance in both continuous and discrete tokenizations. VidTok incorporates several key advancements over existing approaches: 1) model architecture such as convolutional layers and up/downsampling modules; 2) to address the training instability and codebook collapse commonly associated with conventional Vector Quantization (VQ), we integrate Finite Scalar Quantization (FSQ) into discrete video tokenization; 3) improved training strategies, including a two-stage training process and the use of reduced frame rates. By integrating these advancements, VidTok achieves substantial improvements over existing methods, demonstrating superior performance across multiple metrics, including PSNR, SSIM, LPIPS, and FVD, under standardized evaluation settings.",
      "url": "https://www.microsoft.com/en-us/research/publication/vidtok-a-versatile-and-open-source-video-tokenizer/"
    },
    {
      "title": "Distribution Shifts at Scale: Out-of-distribution Detection in Earth Observation",
      "authors": [
        "Burak Ekim",
        "Caleb Robinson",
        "Gilles Quentin Hacheme",
        "Girmaw Abebe Tadesse",
        "Juan M. Lavista Ferres",
        "Michael Schmitt",
        "Rahul Dodhia"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Ecology and environment"
      ],
      "publication_date": "December 2024",
      "abstract": "Training robust deep learning models is critical in Earth Observation, where globally deployed models often face distribution shifts that degrade performance, especially in low-data regions. Out-of-distribution (OOD) detection addresses this challenge by identifying inputs that differ from in-distribution (ID) data. However, existing methods either assume access to OOD data or compromise primary task performance, making them unsuitable for real-world deployment. We propose TARDIS, a post-hoc OOD detection method for scalable geospatial deployments. The core novelty lies in generating surrogate labels by integrating information from ID data and unknown distributions, enabling OOD detection at scale. Our method takes a pre-trained model, ID data, and WILD samples, disentangling the latter into surrogate ID and surrogate OOD labels based on internal activations, and fits a binary classifier as an OOD detector. We validate TARDIS on EuroSAT and xBD datasets, across 17 experimental setups covering covariate and semantic shifts, showing that it performs close to the theoretical upper bound in assigning surrogate ID and OOD samples in 13 cases. To demonstrate scalability, we deploy TARDIS on the Fields of the World dataset, offering actionable insights into pre-trained model behavior for large-scale deployments.",
      "url": "https://www.microsoft.com/en-us/research/publication/distribution-shifts-at-scale-out-of-distribution-detection-in-earth-observation/"
    },
    {
      "title": "Deeper evaluation of a single-cell foundation model",
      "authors": [
        "Alejandro Buendia",
        "Ava P. Amini",
        "David Sontag",
        "Gad Getz",
        "Nalini M. Singh",
        "Rebecca Boiarsky"
      ],
      "research_areas": [
        "Medical, health and genomics"
      ],
      "publication_date": "December 2024",
      "abstract": "Large-scale foundation models, which are pre-trained on massive, unlabelled datasets and subsequently fine-tuned on specific tasks, have recently achieved unparalleled success on a wide array of applications, including in healthcare and biology (opens in new tab). The success of these models has showcased the power of leveraging generalizable features and contextual understanding to improve a model’s performance. Single-cell bidirectional encoder representations from transformers (scBERT) by Yang et al.7 (opens in new tab) is one of several recently developed foundation models to learn representations of single-cell RNA-sequencing data. Yang et al. pre-trained their model on 1.12 million cells to impute masked gene-expression values and characterize the performance of their model on a fine-tuning task to annotate cell types. We reproduce their results, and provide additional baselines and ablation studies (that is, remove components of the model’s architecture or training process) to develop a deeper understanding of their results and the potential benefits and limitations of single-cell foundation models.",
      "url": "https://www.microsoft.com/en-us/research/publication/deeper-evaluation-of-a-single-cell-foundation-model/"
    },
    {
      "title": "Phi-4 Technical Report",
      "authors": [
        "Adil Salim",
        "Anh Nguyen",
        "Caio CT Mendes",
        "Cyril Zhang",
        "Dingli Yu",
        "Eric Price",
        "Gustavo de Rosa",
        "Harkirat Behl",
        "James R. Lee",
        "Jyoti Aneja",
        "Marah I Abdin",
        "Michael Harrison",
        "Mojan Javaheripi",
        "Olli Saarikivi",
        "Piero Kauffmann",
        "Rachel Ward",
        "Ronen Eldan",
        "Russell J. Hewett",
        "Shital Shah",
        "Suriya Gunasekar",
        "Sébastien Bubeck",
        "Weishung Liu",
        "Xin Wang",
        "Yi Zhang",
        "Yin Tat Lee",
        "Yuanzhi  Li",
        "Yue Wu"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "December 2024",
      "abstract": "We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality. Unlike most language models, where pre-training is based primarily on organic data sources such as web content or code, phi-4 strategically incorporates synthetic data throughout the training process. While previous models in the Phi family largely distill the capabilities of a teacher model (specifically GPT-4), phi-4 substantially surpasses its teacher model on STEM-focused QA capabilities, giving evidence that our data-generation and post-training techniques go beyond distillation. Despite minimal changes to the phi-3 architecture, phi-4 achieves strong performance relative to its size– especially on reasoning-focused benchmarks– due to improved data, training curriculum, and innovations in the post-training scheme.",
      "url": "https://www.microsoft.com/en-us/research/publication/phi-4-technical-report/"
    },
    {
      "title": "InvDiff: Invariant Guidance for Bias Mitigation in Diffusion Models",
      "authors": [
        "Chang Xu",
        "Chenxi Bai",
        "Jiang Bian",
        "Le Wu",
        "Min Hou",
        "Yu-Hao Huang",
        "Yueying Wu"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "December 2024",
      "abstract": "As one of the most successful generative models, diffusion models have demonstrated remarkable efficacy in synthesizing high-quality images. These models learn the underlying high-dimensional data distribution in an unsupervised manner. Despite their success, diffusion models are highly data-driven and prone to inheriting the imbalances and biases present in real-world data. Some studies have attempted to address these issues by designing text prompts for known biases or using bias labels to construct unbiased data. While these methods have shown improved results, real-world scenarios often contain various unknown biases, and obtaining bias labels is particularly challenging. In this paper, we emphasize the necessity of mitigating bias in pre-trained diffusion models without relying on auxiliary bias annotations. To tackle this problem, we propose a framework, InvDiff, which aims to learn invariant semantic information for diffusion guidance. Specifically, we propose identifying underlying biases in the training data and designing a novel debiasing training objective. Then, we employ a lightweight trainable module that automatically preserves invariant semantic information and uses it to guide the diffusion model’s sampling process toward unbiased outcomes simultaneously. Notably, we only need to learn a small number of parameters in the lightweight learnable module without altering the pre-trained diffusion model. Furthermore, we provide a theoretical guarantee that the implementation of InvDiff is equivalent to reducing the error upper bound of generalization. Extensive experimental results on three publicly available benchmarks demonstrate that InvDiff effectively reduces biases while maintaining the quality of image generation. Our code is available at https://github.com/Hundredl/InvDiff.",
      "url": "https://www.microsoft.com/en-us/research/publication/invdiff-invariant-guidance-for-bias-mitigation-in-diffusion-models/"
    },
    {
      "title": "Modality-Driven Design for Multi-Step Dexterous Manipulation: Insights from Neuroscience",
      "authors": [
        "Atsushi Kanehira",
        "Daichi Saito",
        "Hideki Koike",
        "Jun Takamatsu",
        "Katsushi Ikeuchi",
        "Kazuhiro Sasabuchi",
        "Naoki Wake"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Computer vision",
        "Human-computer interaction"
      ],
      "publication_date": "December 2024",
      "abstract": "Multi-step dexterous manipulation is a fundamental skill in household scenarios, yet remains an underexplored area in robotics. This paper proposes a modular approach, where each step of the manipulation process is addressed with dedicated policies based on effective modality input, rather than relying on a single end-to-end model. To demonstrate this, a dexterous robotic hand performs a manipulation task involving picking up and rotating a box. Guided by insights from neuroscience, the task is decomposed into three sub-skills, 1)reaching, 2)grasping and lifting, and 3)in-hand rotation, based on the dominant sensory modalities employed in the human brain. Each sub-skill is addressed using distinct methods from a practical perspective: a classical controller, a Vision-Language-Action model, and a reinforcement learning policy with force feedback, respectively. We tested the pipeline on a real robot to demonstrate the feasibility of our approach. The key contribution of this study lies in presenting a neuroscience-inspired, modality-driven methodology for multi-step dexterous manipulation.",
      "url": "https://www.microsoft.com/en-us/research/publication/modality-driven-design-for-multi-step-dexterous-manipulation-insights-from-neuroscience/"
    },
    {
      "title": "SCBench: A KV Cache-Centric Analysis of Long-Context Methods",
      "authors": [
        "Amir H. Abdi",
        "Chengruidong Zhang",
        "Dongsheng Li",
        "Huiqiang Jiang",
        "Jianfeng Gao",
        "Lili Qiu",
        "Qianhui Wu",
        "Surin Ahn",
        "Xufang Luo",
        "Yucheng Li",
        "Yuqing Yang"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Human language technologies"
      ],
      "publication_date": "December 2024",
      "abstract": "Long-context LLMs have enabled numerous downstream applications but also introduced significant challenges related to computational and memory efficiency. To address these challenges, optimizations for long-context inference have been developed, centered around the KV cache. However, existing benchmarks often evaluate in single-request, neglecting the full lifecycle of the KV cache in real-world use. This oversight is particularly critical, as KV cache reuse has become widely adopted in LLMs inference frameworks, such as vLLM and SGLang, as well as by LLM providers, including OpenAI, Microsoft, Google, and Anthropic. To address this gap, we introduce SCBench(SharedContextBench), a comprehensive benchmark for evaluating long-context methods from a KV cachecentric perspective: 1) KV cache generation, 2) KV cache compression, 3) KV cache retrieval, 4) KV cache loading. Specifically, SCBench uses test examples with shared context, ranging 12 tasks with two shared context modes, covering four categories of long-context capabilities: string retrieval, semantic retrieval, global information, and multi-task. With it, we provide an extensive KV cache-centric analysis of eight categories long-context solutions, including Gated Linear RNNs, Mamba-Attention hybrids, and efficient methods such as sparse attention, KV cache dropping, quantization, retrieval, loading, and prompt compression. The evaluation is conducted on 8 long-context LLMs. Our findings show that sub-O(n) memory methods suffer in multi-turn scenarios, while sparse encoding with O(n) memory and sub-O(n^2) pre-filling computation perform robustly. Dynamic sparsity yields more expressive KV caches than static patterns, and layer-level sparsity in hybrid architectures reduces memory usage with strong performance. Additionally, we identify attention distribution shift issues in long-generation scenarios. https://aka.ms/SCBench",
      "url": "https://www.microsoft.com/en-us/research/publication/scbench-a-kv-cache-centric-analysis-of-long-context-methods/"
    },
    {
      "title": "Sims: An Interactive Tool for Geospatial Matching and Clustering",
      "authors": [
        "Akram Zaytar",
        "Caleb Robinson",
        "Eduardo G Bendito",
        "Gilles Quentin Hacheme",
        "Girmaw Abebe Tadesse",
        "Juan M. Lavista Ferres",
        "Medha Devare",
        "Meklit Chernet",
        "Rahul Dodhia"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Computer vision",
        "Ecology and environment"
      ],
      "publication_date": "December 2024",
      "abstract": "Acquiring, processing, and visualizing geospatial data requires significant computing resources, especially for large spatio-temporal domains. This challenge hinders the rapid discovery of predictive features, which is essential for advancing geospatial modeling. To address this, we developed Similarity Search (Sims), a no-code web tool that allows users to visualize, compare, cluster, and perform similarity search over defined regions of interest using Google Earth Engine as a backend. Sims is designed to complement existing modeling tools by focusing on feature exploration rather than model creation. We demonstrate the utility of Sims through a case study analyzing simulated maize yield data in Rwanda, where we evaluate how different combinations of soil, weather, and agronomic features affect the clustering of yield response zones. Sims is open source and available at https://github.com/microsoft/Sims",
      "url": "https://www.microsoft.com/en-us/research/publication/sims-an-interactive-tool-for-geospatial-matching-and-clustering/"
    },
    {
      "title": "Attribute Structuring Improves LLM-Based Evaluation of Clinical Text Summaries",
      "authors": [
        "Chandan Singh",
        "Hoifung Poon",
        "Jianfeng Gao",
        "Sheng Zhang",
        "Tristan Naumann",
        "Yiqing Xie",
        "Zelalem Gero"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Medical, health and genomics"
      ],
      "publication_date": "December 2024",
      "abstract": "Summarizing clinical text is crucial in health decision-support and clinical research. Large language models (LLMs) have shown the potential to generate accurate clinical text summaries, but still struggle with issues regarding grounding and evaluation, especially in safety-critical domains such as health. Holistically evaluating text summaries is challenging because they may contain unsubstantiated information. Here, we explore a general mitigation framework using Attribute Structuring (AS), which structures the summary evaluation process. It decomposes the evaluation process into a grounded procedure that uses an LLM for relatively simple structuring and scoring tasks, rather than the full task of holistic summary evaluation. Experiments show that AS consistently improves the correspondence between human annotations and automated metrics in clinical text summarization. Additionally, AS yields interpretations in the form of a short text span corresponding to each output, which enables efficient human auditing, paving the way towards trustworthy evaluation of clinical information in resource-constrained scenarios. We release our code, prompts, and an open-source benchmark at https://github.com/microsoft/attribute-structuring (opens in new tab).",
      "url": "https://www.microsoft.com/en-us/research/publication/attribute-structuring-improves-llm-based-evaluation-of-clinical-text-summaries/"
    },
    {
      "title": "MAIRA-Seg: Enhancing Radiology Report Generation with Segmentation-Aware Multimodal Large Language Models",
      "authors": [
        "Anton Schwaighofer",
        "Daniel Coelho de Castro",
        "Fabian Falck",
        "Fernando Pérez-García",
        "Hannah Richardson (nee Murfet)",
        "Harshita Sharma",
        "Javier Alvarez-Valle",
        "Kenza Bouzid",
        "Maria Teodora Wetscherek",
        "Maximilian Ilse",
        "Mercy Ranjit",
        "Sam Bond-Taylor",
        "Shaury  Srivastav",
        "Shruthi Bannur",
        "Stephanie Hyland",
        "Valentina Salvatelli"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Medical, health and genomics"
      ],
      "publication_date": "December 2024",
      "abstract": "There is growing interest in applying AI to radiology report generation, particularly for chest X-rays (CXRs). This paper investigates whether incorporating pixel-level information through segmentation masks can improve fine-grained image interpretation of multimodal large language models (MLLMs) for radiology report generation. We introduce MAIRA-Seg, a segmentation-aware MLLM framework designed to utilize semantic segmentation masks alongside CXRs for generating radiology reports. We train expert segmentation models to obtain mask pseudolabels for radiology-specific structures in CXRs. Subsequently, building on the architectures of MAIRA, a CXR-specialised model for report generation, we integrate a trainable segmentation tokens extractor that leverages these mask pseudolabels, and employ mask-aware prompting to generate draft radiology reports. Our experiments on the publicly available MIMIC-CXR dataset show that MAIRA-Seg outperforms non-segmentation baselines. We also investigate set-of-marks prompting with MAIRA and find that MAIRA-Seg consistently demonstrates comparable or superior performance. The results confirm that using segmentation masks enhances the nuanced reasoning of MLLMs, potentially contributing to better clinical outcomes.",
      "url": "https://www.microsoft.com/en-us/research/publication/maira-seg-enhancing-radiology-report-generation-with-segmentation-aware-multimodal-large-language-models/"
    },
    {
      "title": "Is A Picture Worth A Thousand Words? Delving Into Spatial Reasoning for Vision Language Models",
      "authors": [
        "Neel Joshi",
        "Vibhav Vineet",
        "Xin Wang"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "December 2024",
      "abstract": "Large language models (LLMs) and vision-language models (VLMs) have demonstrated remarkable performance across a wide range of tasks and domains. Despite this promise, spatial understanding and reasoning—a fundamental component of human cognition—remains under-explored. We develop novel benchmarks that cover diverse aspects of spatial reasoning such as relationship understanding, navigation, and counting. We conduct a comprehensive evaluation of competitive language and vision-language models. Our findings reveal several counter-intuitive insights that have been overlooked in the literature: (1) Spatial reasoning poses significant challenges where competitive models can fall behind random guessing; (2) Despite additional visual input, VLMs often under-perform compared to their LLM counterparts; (3) When both textual and visual information is available, multi-modal language models become less reliant on visual information if sufficient textual clues are provided. Additionally, we demonstrate that leveraging redundancy between vision and text can significantly enhance model performance. We hope our study will inform the development of multimodal models so they can have improved spatial intelligence and further close the gap with human intelligence.",
      "url": "https://www.microsoft.com/en-us/research/publication/is-a-picture-worth-a-thousand-words-delving-into-spatial-reasoning-for-vision-language-models/"
    },
    {
      "title": "ElasTST: Towards Robust Varied-Horizon Forecasting with Elastic Time-Series Transformer",
      "authors": [
        "Jiang Bian",
        "Shun Zheng",
        "Xumeng Wen"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "December 2024",
      "abstract": "Numerous industrial sectors necessitate models capable of providing robust forecasts across various horizons. Despite the recent strides in crafting specific architectures for time-series forecasting and developing pre-trained universal models, a comprehensive examination of their capability in accommodating varied-horizon forecasting during inference is still lacking. This paper bridges this gap through the design and evaluation of the Elastic Time-Series Transformer (ElasTST). The ElasTST model incorporates a non-autoregressive design with placeholders and structured self-attention masks, warranting future outputs that are invariant to adjustments in inference horizons. A tunable version of rotary position embedding is also integrated into ElasTST to capture time-series-specific periods and enhance adaptability to different horizons. Additionally, ElasTST employs a multi-scale patch design, effectively integrating both fine-grained and coarse-grained information. Through comprehensive experiments and comparisons with state-of-the-art time-series architectures and contemporary foundation models, we demonstrate the efficacy of ElasTST’s unique design elements. Our findings position ElasTST as a robust solution for the practical necessity of varied-horizon forecasting.",
      "url": "https://www.microsoft.com/en-us/research/publication/elastst-towards-robust-varied-horizon-forecasting-with-elastic-time-series-transformer/"
    },
    {
      "title": "GPMS: Enabling Indoor GNSS Positioning using Passive Metasurfaces",
      "authors": [
        "Guangtao Xue",
        "Hao Pan",
        "Jiting Liu",
        "Ju Ren",
        "Lili Qiu",
        "Linghui Zhong",
        "Ruichun Ma",
        "Yezhou Wang",
        "Yi-Chao Chen"
      ],
      "research_areas": [
        "Systems and networking"
      ],
      "publication_date": "December 2024",
      "abstract": "Global Navigation Satellite System (GNSS) is extensively utilized for outdoor positioning and navigation. However, achieving high-precision indoor positioning is challenging due to the significant attenuation of GNSS signals indoors. To address this issue, we propose an innovative indoor GNSS positioning system called GPMS, which uses passive metasurface technology to redirect GNSS signals from outdoors into indoor spaces. These passive metasurfaces are strategically optimized for indoor coverage by steering and scattering the GNSS signals across a wide range of incident angles. We further develop a novel localization algorithm that can determine which metasurface the signal goes through and localize the user using the set of metasurfaces as anchor points. A distinct advantage of our localization algorithm is that it can be implemented on existing mobile devices without any hardware modifications. We implement the prototype of GPMS, and deploy six metasurfaces in two indoor environments, a 10×50 m2 office floor and a 15×20 m2 lecture room, to evaluate system performance. In terms of coverage, our GPMS increases the C/N0 from 9.1 dB-Hz to 23.2 dB-Hz and increases the number of visible satellites from 3.6 to 21.5 in the office floor. In terms of indoor positioning accuracy, our proposed system decreases the absolute positioning error from 30.6 m to 3.2 m in the office floor, and from 11.2 m to 2.7 m in the lecture room, demonstrating the feasibility and benefits of metasurface-assisted GNSS for indoor positioning.\n\n\n",
      "url": "https://www.microsoft.com/en-us/research/publication/gpms-enabling-indoor-gnss-positioning-using-passive-metasurfaces/"
    },
    {
      "title": "Saving Private WAN: Using Internet Paths to Offload WAN Traffic in Conferencing Services",
      "authors": [
        "Bhaskar Kataria",
        "Chakri Kotipalli",
        "Debopam Bhattacherjee",
        "Irena Atov",
        "Ken Sueda",
        "Kevin Hinton",
        "Palak LNU",
        "Rahul Bothra",
        "Rohan Gandhi",
        "Rui Liang",
        "Somesh Chaturmohta",
        "Sriraam Ramakrishnan",
        "Venkat Padmanabhan",
        "Xin He"
      ],
      "research_areas": [
        "Systems and networking"
      ],
      "publication_date": "December 2024",
      "abstract": "Large-scale video conferencing services incur significant network cost while serving surging global demands. Our work systematically explores the opportunity to offload a fraction of this traffic to the Internet, a cheaper routing option offered already by cloud providers, from WAN without drop in application performance. First, with a large-scale latency measurement study with 3.5 million data points per day spanning 241𝐾 source cities and 21 data centers across the globe, we demonstrate that Internet paths perform comparable to or better than the private WAN for parts of the world (e.g., Europe and North America). Next, we present Titan, a live (12+ months) production system that carefully moves a fraction of the conferencing traffic to the Internet using the above observation. Finally, we propose Titan-Next– a research prototype that jointly assigns the conferencing server and routing option (Internet or WAN) for individual calls. With 5 weeks of production data, we show Titan-Next reduces the sum of peak bandwidth on WAN links that defines the operational network cost by up to 61% compared to state-of the-art baselines.",
      "url": "https://www.microsoft.com/en-us/research/publication/saving-private-wan-using-internet-paths-to-offload-wan-traffic-in-conferencing-services/"
    },
    {
      "title": "Chimera: Accurate retrosynthesis prediction by ensembling models with diverse inductive biases",
      "authors": [
        "Aleksei Kornev",
        "Guoqing Liu",
        "Holger Hoefling",
        "Hubert Misztela",
        "Krzysztof Maziarz",
        "Marwin Segler",
        "Mike Fortunato",
        "Piotr Gaiński",
        "Rishi Gupta"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "December 2024",
      "abstract": "Planning and conducting chemical syntheses remains a major bottleneck in the discovery of functional small molecules, and prevents fully leveraging generative AI for molecular inverse design. While early work has shown that ML-based retrosynthesis models can predict reasonable routes, their low accuracy for less frequent, yet important reactions has been pointed out. As multi-step search algorithms are limited to reactions suggested by the underlying model, the applicability of those tools is inherently constrained by the accuracy of retrosynthesis prediction. Inspired by how chemists use different strategies to ideate reactions, we propose Chimera: a framework for building highly accurate reaction models that combine predictions from diverse sources with complementary inductive biases using a learning-based ensembling strategy. We instantiate the framework with two newly developed models, which already by themselves achieve state of the art in their categories. Through experiments across several orders of magnitude in data scale and time-splits, we show Chimera outperforms all major models by a large margin, owing both to the good individual performance of its constituents, but also to the scalability of our ensembling strategy. Moreover, we find that PhD-level organic chemists prefer predictions from Chimera over baselines in terms of quality. Finally, we transfer the largest-scale checkpoint to an internal dataset from a major pharmaceutical company, showing robust generalization under distribution shift. With the new dimension that our framework unlocks, we anticipate further acceleration in the development of even more accurate models.",
      "url": "https://www.microsoft.com/en-us/research/publication/chimera-accurate-retrosynthesis-prediction-by-ensembling-models-with-diverse-inductive-biases/"
    },
    {
      "title": "GASP: Gaussian Avatars with Synthetic Priors",
      "authors": [
        "Ben Lundell",
        "Charlie Hewitt",
        "Jack Saunders",
        "Marek Kowalski (HE/HIM)",
        "Nicholas Gyde",
        "Prof. Darren Cosker",
        "Tadas Baltrusaitis",
        "Vinay Namboodiri",
        "Virginia Estellers",
        "Yanan Jian",
        "Yiye Chen"
      ],
      "research_areas": [
        "Computer vision",
        "Graphics and multimedia"
      ],
      "publication_date": "December 2024",
      "abstract": "Gaussian Splatting has changed the game for real-time photo-realistic rendering. One of the most popular applications of Gaussian Splatting is to create animatable avatars, known as Gaussian Avatars. Recent works have pushed the boundaries of quality and rendering efficiency but suffer from two main limitations. Either they require expensive multi-camera rigs to produce avatars with free-view rendering, or they can be trained with a single camera but only rendered at high quality from this fixed viewpoint. An ideal model would be trained using a short monocular video or image from available hardware, such as a webcam, and rendered from any view. To this end, we propose GASP: Gaussian Avatars with Synthetic Priors. To overcome the limitations of existing datasets, we exploit the pixel-perfect nature of synthetic data to train a Gaussian Avatar prior. By fitting this prior model to a single photo or video and fine-tuning it, we get a high-quality Gaussian Avatar, which supports 360 degree rendering. Our prior is only required for fitting, not inference, enabling real-time application. Through our method, we obtain high-quality, animatable Avatars from limited data which can be animated and rendered at 70fps on commercial hardware. See our project page (https://aka.ms/GASP (opens in new tab)) for results.",
      "url": "https://www.microsoft.com/en-us/research/publication/gasp/"
    },
    {
      "title": "Scalable emulation of protein equilibrium ensembles with generative deep learning",
      "authors": [
        "Andrew Campbell",
        "Andrew Y. K. Foong",
        "Arne Schneuing",
        "Bastiaan S. Veeling",
        "Cecilia Clementi",
        "Federico Barbero",
        "Frank Noé",
        "Hannes Schulz",
        "I. Zaporozhets",
        "Jason Yim",
        "Jigyasa Nigam",
        "Jose Jimenez-Luna",
        "Marten Lienen",
        "Michael Gastegger",
        "Osama Abdin",
        "Sarah Lewis",
        "Shuxin Zheng",
        "Soojung Yang",
        "Tim Hempel",
        "Usman Munir",
        "Victor Garcia Satorras",
        "Vincent Stimper",
        "Yaoyi Chen",
        "Yu Shi",
        "Yu Xie"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "December 2024",
      "abstract": "Following the sequence and structure revolutions, predicting the dynamical mechanisms of proteins that implement biological function remains an outstanding scientific challenge. Several experimental techniques and molecular dynamics (MD) simulations can, in principle, determine conformational states, binding configurations and their probabilities, but suffer from low throughput. Here we develop a Biomolecular Emulator (BioEmu), a generative deep learning system that can generate thousands of statistically independent samples from the protein structure ensemble per hour on a single graphical processing unit. By leveraging novel training methods and vast data of protein structures, over 200 milliseconds of MD simulation, and experimental protein stabilities, BioEmu’s protein ensembles represent equilibrium in a range of challenging and practically relevant metrics. Qualitatively, BioEmu samples many functionally relevant conformational changes, ranging from formation of cryptic pockets, over unfolding of specific protein regions, to large-scale domain rearrangements. Quantitatively, BioEmu samples protein conformations with relative free energy errors around 1 kcal/mol, as validated against millisecond-timescale MD simulation and experimentally-measured protein stabilities. By simultaneously emulating structural ensembles and thermodynamic properties, BioEmu reveals mechanistic insights, such as the causes for fold destabilization of mutants, and can efficiently provide experimentally-testable hypotheses.",
      "url": "https://www.microsoft.com/en-us/research/publication/scalable-emulation-of-protein-equilibrium-ensembles-with-generative-deep-learning/"
    },
    {
      "title": "Machine Unlearning Doesn’t Do What You Think: Lessons for Generative AI Policy, Research, and Practice",
      "authors": [
        "A. Feder Cooper",
        "Abigail Z. Jacobs",
        "Alexandra Chouldechova",
        "Amy Cyphert",
        "Andreas Terzis",
        "Christopher A. Choquette-Choo",
        "Christopher De Sa",
        "Daniel E. Ho",
        "David Bau",
        "Eleni Triantafillou",
        "Fernando Delgado",
        "Hanna Wallach",
        "Ilia Shumailov",
        "James Grimmelmann",
        "Jamie Hayes",
        "Jennifer Wortman Vaughan",
        "Katherine Lee",
        "Katja Filippova",
        "Ken Ziyu Liu",
        "M. Brundage",
        "Mark A. Lemley",
        "Matthew Jagielski",
        "Miranda Bogen",
        "Nicolas Papernot",
        "Nicole Mitchell",
        "Niloofar Mireshghallah",
        "Percy Liang",
        "Peter Kairouz",
        "Sanmi Koyejo",
        "Seth Neel",
        "Solon Barocas",
        "Vitaly Shmatikov",
        "Yangsibo Huang",
        "Yejin Choi",
        "danah boyd"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "December 2024",
      "abstract": "We articulate fundamental mismatches between technical methods for machine unlearning in Generative AI, and documented aspirations for broader impact that these methods could have for law and policy. These aspirations are both numerous and varied, motivated by issues that pertain to privacy, copyright, safety, and more. For example, unlearning is often invoked as a solution for removing the effects of targeted information from a generative-AI model’s parameters, e.g., a particular individual’s personal data or in-copyright expression of Spiderman that was included in the model’s training data. Unlearning is also proposed as a way to prevent a model from generating targeted types of information in its outputs, e.g., generations that closely resemble a particular individual’s data or reflect the concept of “Spiderman.” Both of these goals–the targeted removal of information from a model and the targeted suppression of information from a model’s outputs–present various technical and substantive challenges. We provide a framework for thinking rigorously about these challenges, which enables us to be clear about why unlearning is not a general-purpose solution for circumscribing generative-AI model behavior in service of broader positive impact. We aim for conceptual clarity and to encourage more thoughtful communication among machine learning (ML), law, and policy experts who seek to develop and apply technical methods for compliance with policy objectives.",
      "url": "https://www.microsoft.com/en-us/research/publication/machine-unlearning-doesnt-do-what-you-think-lessons-for-generative-ai-policy-research-and-practice/"
    },
    {
      "title": "TurboAttention: Efficient Attention Approximation For High Throughputs LLMs",
      "authors": [
        "Hao Kang",
        "James Hensman",
        "Saravan Rajmohan",
        "Srikant Bharadwaj",
        "Tushar Krishna",
        "Victor Ruehle"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "December 2024",
      "abstract": "Large language model (LLM) inference demands significant amount of computation and memory, especially in the key attention mechanism. While techniques, such as quantization and acceleration algorithms, like FlashAttention, have improved efficiency of the overall inference, they address different aspects of the problem: quantization focuses on weight-activation operations, while FlashAttention improves execution but requires high-precision formats. Recent Key-value (KV) cache quantization reduces memory bandwidth but still needs floating-point dequantization for attention operation. We present TurboAttention, a comprehensive approach to enable quantized execution of attention that simultaneously addresses both memory and computational efficiency. Our solution introduces two key innovations: FlashQ, a headwise attention quantization technique that enables both compression of KV cache and quantized execution of activation-activation multiplication, and Sparsity-based Softmax Approximation (SAS), which eliminates the need for dequantization to FP32 during exponentiation operation in attention. Experimental results demonstrate that TurboAttention achieves 1.2-1.8x speedup in attention, reduces the KV cache size by over 4.4x, and enables up to 2.37x maximum throughput over the FP16 baseline while outperforming state-of-the-art quantization and compression techniques across various datasets and models.",
      "url": "https://www.microsoft.com/en-us/research/publication/turboattention-efficient-attention-approximation-for-high-throughputs-llms/"
    },
    {
      "title": "MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention",
      "authors": [
        "Amir H. Abdi",
        "Chengruidong Zhang",
        "Chin-Yew Lin",
        "Dongsheng Li",
        "Huiqiang Jiang",
        "Lili Qiu",
        "Qianhui Wu",
        "Surin Ahn",
        "Xufang Luo",
        "Yucheng Li",
        "Yuqing Yang",
        "Zhenhua Han"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Human language technologies",
        "Systems and networking"
      ],
      "publication_date": "December 2024",
      "abstract": "The computational challenges of Large Language Model (LLM) inference remain a significant barrier to their widespread deployment, especially as prompt lengths continue to increase. Due to the quadratic complexity of the attention computation, it takes 30 minutes for an 8B LLM to process a prompt of 1M tokens (i.e., the pre-filling stage) on a single A100 GPU. Existing methods for speeding up prefilling often fail to maintain acceptable accuracy or efficiency when applied to long-context LLMs. To address this gap, we introduce MInference (Milliontokens Inference), a sparse calculation method designed to accelerate pre-filling of long-sequence processing. Specifically, we identify three unique patterns in long-context attention matrices-the A-shape, Vertical-Slash, and Block-Sparsethat can be leveraged for efficient sparse computation on GPUs. We determine the optimal pattern for each attention head offline and dynamically build sparse indices based on the assigned pattern during inference. With the pattern and sparse indices, we perform efficient sparse attention calculations via our optimized GPU kernels to significantly reduce the latency in the pre-filling stage of long-context LLMs. Our proposed technique can be directly applied to existing LLMs without any modifications to the pre-training setup or additional fine-tuning. By evaluating on a wide range of downstream tasks, including InfiniteBench, RULER, PG-19, and Needle In A Haystack, and models including LLaMA-3-1M, GLM4-1M, Yi-200K, Phi-3-128K, and Qwen2-128K, we demonstrate that MInference effectively reduces inference latency by up to 10x for pre-filling on an A100, while maintaining accuracy. Our code is available at https://aka.ms/MInference (opens in new tab).",
      "url": "https://www.microsoft.com/en-us/research/publication/minference-1-0-accelerating-pre-filling-for-long-context-llms-via-dynamic-sparse-attention/"
    },
    {
      "title": "LLMs can be Fooled into Labelling a Document as Relevant (Best café near me; this paper is perfectly relevant)",
      "authors": [
        "Falk Scholer",
        "Mark Sanderson",
        "Marwah Alaofi",
        "Paul Thomas"
      ],
      "research_areas": [
        "Search and information retrieval"
      ],
      "publication_date": "December 2024",
      "abstract": "Large language models (LLMs) are increasingly being used to assess the relevance of information objects. This work reports on experiments to study the labelling of short texts for relevance, using multiple open-source and proprietary LLMs. While the overall agreement of some LLMs with human judgements is comparable to human-to-human agreement measured in previous research, LLMs are more likely to label passages as relevant compared to human judges, indicating that LLM labels denoting non-relevance are more reliable than those indicating relevance.\nThis observation prompts us to further examine cases where human judges and LLMs disagree, particularly when the human judge labels the passage as non-relevant and the LLM labels it as relevant. Results show a tendency for many LLMs to label passages that include the original query terms as relevant. We therefore conduct experiments to inject query words into random and irrelevant passages, not unlike the way we inserted “Best café near me” into this paper. The results demonstrate that LLMs are highly influenced by the presence of query words in the passages under assessment, even if the wider passage has no relevance to the query. This tendency of LLMs to be fooled by the mere presence of query words demonstrates a weakness in our current measures of LLM labelling: relying on overall agreement misses important patterns of failures. There is a real risk of bias in LLM-generated relevance labels and, therefore, a risk of bias in rankers trained on those labels.\nAdditionally, we investigate the effects of deliberately manipulating LLMs by instructing them to label passages as relevant, similar to the instruction “this paper is perfectly relevant” inserted above. We find that such manipulation influences the performance of some LLMs, highlighting the critical need to consider potential vulnerabilities when deploying LLMs in real-world applications.",
      "url": "https://www.microsoft.com/en-us/research/publication/llms-can-be-fooled-into-labelling-a-document-as-relevant-best-cafe-near-me-this-paper-is-perfectly-relevant/"
    },
    {
      "title": "Computational Design of Dense Servers for Immersion Cooling",
      "authors": [
        "Adriana Schulz",
        "Alvin Lebeck",
        "Daniel S. Berger",
        "Fiodar Kazhamiaka",
        "Milin Kodnongbua",
        "Ricardo Bianchini",
        "Rodrigo Fonseca",
        "Vikram Iyer"
      ],
      "research_areas": [
        "Algorithms",
        "Graphics and multimedia",
        "Hardware and devices"
      ],
      "publication_date": "December 2024",
      "abstract": "The growing demands for computational power in cloud computing have led to a significant increase in the deployment of high-performance servers. The growing power consumption of servers and the heat they produce is on track to outpace the capacity of conventional air cooling systems, necessitating more efficient cooling solutions such as liquid immersion cooling. The superior heat exchange capabilities of immersion cooling both eliminates the need for bulky heat sinks, fans, and air flow channels while also unlocking the potential go beyond conventional 2D blade servers to three-dimensional designs. In this work, we present a computational framework to explore designs of servers in three-dimensional space, specifically targeting the maximization of server density within immersion cooling tanks. Our tool is designed to handle a variety of physical and electrical server design constraints. We demonstrate our optimized designs can reduce server volume by 25–52% compared to traditional flat server designs. This increased density reduces land usage as well as the amount of liquid used for immersion, with significant reduction in the carbon emissions embodied in datacenter buildings. We further create physical prototypes to simulate dense server designs and perform real-world experiments in an immersion cooling tank demonstrating they operate at safe temperatures. This approach marks a critical step forward in sustainable and efficient datacenter management.",
      "url": "https://www.microsoft.com/en-us/research/publication/computational-design-of-dense-servers-for-immersion-cooling/"
    },
    {
      "title": "Autoformalize Mathematical Statements by Symbolic Equivalence and Semantic Consistency",
      "authors": [
        "Fan Yang",
        "Xian Zhang",
        "Xiaoxing Ma",
        "Xinming Wei",
        "Yifan Wu",
        "Zenan Li",
        "Zhaoyu Li"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "December 2024",
      "abstract": "Autoformalization, the task of automatically translating natural language descriptions into a formal language, poses a significant challenge across various domains, especially in mathematics. Recent advancements in large language models (LLMs) have unveiled their promising capabilities to formalize even competition-level math problems. However, we observe a considerable discrepancy between pass@1 and pass@k accuracies in LLM-generated formalizations. To address this gap, we introduce a novel framework that scores and selects the best result from k autoformalization candidates based on two complementary self-consistency methods: symbolic equivalence and semantic consistency. Elaborately, symbolic equivalence identifies the logical homogeneity among autoformalization candidates using automated theorem provers, and semantic consistency evaluates the preservation of the original meaning by informalizing the candidates and computing the similarity between the embeddings of the original and informalized texts. Our extensive experiments on the MATH and miniF2F datasets demonstrate that our approach significantly enhances autoformalization accuracy, achieving up to 0.22-1.35x relative improvements across various LLMs and baseline methods.",
      "url": "https://www.microsoft.com/en-us/research/publication/autoformalize-mathematical-statements-by-symbolic-equivalence-and-semantic-consistency/"
    },
    {
      "title": "MMLU-CF: A Contamination-free Multi-task Language Understanding Benchmark",
      "authors": [
        "Furu Wei",
        "Lei Cui",
        "Qihao Zhao",
        "Qinzheng Sun",
        "Qiufeng Yin",
        "Scarlett Li",
        "Shaoguang Mao",
        "Tengchao Lv",
        "Xin Zhang",
        "Yangyu Huang",
        "Ying Xin"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "December 2024",
      "abstract": "Multiple-choice question (MCQ) datasets like Massive Multitask Language Understanding (MMLU) are widely used to evaluate the commonsense, understanding, and problem-solving abilities of large language models (LLMs). However, the open-source nature of these benchmarks and the broad sources of training data for LLMs have inevitably led to benchmark contamination, resulting in unreliable evaluation results. To alleviate this issue, we propose a contamination-free and more challenging MCQ benchmark called MMLU-CF. This benchmark reassesses LLMs’ understanding of world knowledge by averting both unintentional and malicious data leakage. To avoid unintentional data leakage, we source data from a broader domain and design three decontamination rules. To prevent malicious data leakage, we divide the benchmark into validation and test sets with similar difficulty and subject distributions. The test set remains closed-source to ensure reliable results, while the validation set is publicly available to promote transparency and facilitate independent verification. Our evaluation of mainstream LLMs reveals that the powerful GPT-4o achieves merely a 5-shot score of 73.4% and a 0-shot score of 71.9% on the test set, which indicates the effectiveness of our approach in creating a more rigorous and contamination-free evaluation standard. The GitHub repository is available at this https URL and the dataset refers to this https URL",
      "url": "https://www.microsoft.com/en-us/research/publication/mmlu-cf-a-contamination-free-multi-task-language-understanding-benchmark/"
    },
    {
      "title": "REDSTONE: Curating General, Code, Math, and  QA Data for Large Language Models",
      "authors": [
        "Furu Wei",
        "Lei Cui",
        "Li Dong",
        "Mao Yang",
        "Qinzheng Sun",
        "Qiufeng Yin",
        "Scarlett Li",
        "Shaohan Huang",
        "Shuming Ma",
        "Tengchao Lv",
        "Wenhui Wang",
        "Xingxing Zhang",
        "Yangyu Huang",
        "Yaoyao Chang",
        "Ying Xin",
        "Yupan Huang"
      ],
      "research_areas": [
        "Data platforms and analytics"
      ],
      "publication_date": "December 2024",
      "abstract": "Pre-training Large Language Models (LLMs) on high-quality, meticulously curated datasets is widely recognized as critical for enhancing their performance and generalization capabilities. This study explores the untapped potential of Common Crawl as a comprehensive and flexible resource for pre-training LLMs, addressing both general-purpose language understanding and specialized domain knowledge. We introduce REDSTONE, an innovative and scalable pipeline engineered to extract and process data from Common Crawl, facilitating the creation of extensive and varied pre-training datasets. Unlike traditional datasets, which often require expensive curation and domain-specific expertise, REDSTONE leverages the breadth of Common Crawl to deliver datasets tailored to a wide array of domains. In this work, we exemplify its capability by constructing pre-training datasets across multiple fields, including general language understanding, code, mathematics, and question-answering tasks. The flexibility of REDSTONE allows for easy adaptation to other specialized domains, significantly lowering the barrier to creating valuable domain-specific datasets. Our findings demonstrate that Common Crawl, when harnessed through effective pipelines like REDSTONE, can serve as a rich, renewable source of pre-training data, unlocking new avenues for domain adaptation and knowledge discovery in LLMs. This work also underscores the importance of innovative data acquisition strategies and highlights the role of web-scale data as a powerful resource in the continued evolution of LLMs. RedStone code and data samples will be publicly available at https://aka.ms/redstone.",
      "url": "https://www.microsoft.com/en-us/research/publication/redstone-curating-general-code-math-and-qa-data-for-large-language-models/"
    },
    {
      "title": "Radial Spike and Slab Bayesian Neural Networks for Sparse Data in Ransomware Attacks",
      "authors": [
        "Itai Grady",
        "Jack W. Stokes",
        "Jurijs Nazarovs",
        "Justin Carroll",
        "Melissa Turcotte"
      ],
      "research_areas": [
        "Security, privacy, and cryptography"
      ],
      "publication_date": "December 2024",
      "abstract": "Ransomware attacks are increasing at an alarming rate, leading to large financial losses, unrecoverable encrypted data, data leakage, and privacy concerns. The prompt detection of ransomware attacks is required to minimize further damage, particularly during the encryption stage. However, the frequency and structure of the observed ransomware attack data makes this task difficult to accomplish in practice. The data corresponding to ransomware attacks represents temporal, high-dimensional sparse signals, with limited records and very imbalanced classes. While traditional deep learning models have been able to achieve state-of-the-art results in a wide variety of domains, Bayesian Neural Networks, which are a class of probabilistic models, are better suited to the issues of the ransomware data. These models combine ideas from Bayesian statistics with the rich expressive power of neural networks. In this paper, we propose the Radial Spike and Slab Bayesian Neural Network, which is a new type of Bayesian Neural network that includes a new form of the approximate posterior distribution. The model scales well to large architectures and recovers the sparse structure of target functions. We provide a theoretical justification for using this type of distribution, as well as a computationally efficient method to perform variational inference. We demonstrate the performance of our model on a real dataset of ransomware attacks and show improvement over a large number of baselines, including state-of-the-art models such as Neural ODEs (ordinary differential equations). In addition, we propose to represent low-level events as MITRE ATT&CK tactics, techniques, and procedures (TTPs) which allows the model to better generalize to unseen ransomware attacks.",
      "url": "https://www.microsoft.com/en-us/research/publication/radial-spike-and-slab-bayesian-neural-networks-for-sparse-data-in-ransomware-attacks/"
    },
    {
      "title": "UniGraspTransformer: Simplified Policy Distillation for Scalable Dexterous Robotic Grasping",
      "authors": [
        "Baining Guo",
        "Chang Xu",
        "Fangyun Wei",
        "Jiaolong Yang",
        "Lei Zhou",
        "Lin Luo",
        "Wenbo Wang",
        "Xi Chen",
        "Xiaohan Yi",
        "Yan Lu",
        "Yaobo Liang",
        "Yizhong Zhang"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Computer vision",
        "Human-computer interaction"
      ],
      "publication_date": "December 2024",
      "abstract": "We introduce UniGraspTransformer, a universal Transformer-based network for dexterous robotic grasping that simplifies training while enhancing scalability and performance. Unlike prior methods such as UniDexGrasp++, which require complex, multi-step training pipelines, UniGraspTransformer follows a streamlined process: first, dedicated policy networks are trained for individual objects using reinforcement learning to generate successful grasp trajectories; then, these trajectories are distilled into a single, universal network. Our approach enables UniGraspTransformer to scale effectively, incorporating up to 12 self-attention blocks for handling thousands of objects with diverse poses. Additionally, it generalizes well to both idealized and real-world inputs, evaluated in state-based and vision-based settings. Notably, UniGraspTransformer generates a broader range of grasping poses for objects in various shapes and orientations, resulting in more diverse grasp strategies. Experimental results demonstrate significant improvements over state-of-the-art, UniDexGrasp++, across various object categories, achieving success rate gains of 3.5%, 7.7%, and 10.1% on seen objects, unseen objects within seen categories, and completely unseen objects, respectively, in the vision-based setting. Project page: https://dexhand.github.io/UniGraspTransformer (opens in new tab).",
      "url": "https://www.microsoft.com/en-us/research/publication/unigrasptransformer-simplified-policy-distillation-for-scalable-dexterous-robotic-grasping/"
    },
    {
      "title": "Look Ma, no markers: holistic performance capture without the hassle",
      "authors": [
        "Charlie Hewitt",
        "Fatemeh Sadat Saleh",
        "Julien Valentin",
        "Lohit Petikam",
        "Louis Florentin",
        "Prof. Darren Cosker",
        "Sadegh Aliakbarian",
        "Shideh Rezaeifar",
        "Tadas Baltrusaitis",
        "Tom Cashman",
        "Zafiirah Hosenie"
      ],
      "research_areas": [
        "Computer vision",
        "Graphics and multimedia"
      ],
      "publication_date": "December 2024",
      "abstract": "We tackle the problem of highly-accurate, holistic performance capture for the face, body and hands simultaneously. Motion-capture technologies used in film and game production typically focus only on face, body or hand capture independently, involve complex and expensive hardware and a high degree of manual intervention from skilled operators. While machine-learning-based approaches exist to overcome these problems, they usually only support a single camera, often operate on a single part of the body, do not produce precise world-space results, and rarely generalize outside specific contexts. In this work, we introduce the first technique for marker-free, high-quality reconstruction of the complete human body, including eyes and tongue, without requiring any calibration, manual intervention or custom hardware. Our approach produces stable world-space results from arbitrary camera rigs as well as supporting varied capture environments and clothing. We achieve this through a hybrid approach that leverages machine learning models trained exclusively on synthetic data and powerful parametric models of human shape and motion. We evaluate our method on a number of body, face and hand reconstruction benchmarks and demonstrate state-of-the-art results that generalize on diverse datasets.",
      "url": "https://www.microsoft.com/en-us/research/publication/synthmocap/"
    },
    {
      "title": "Compositional Generalization Across Distributional Shifts with Sparse Tree Operations",
      "authors": [
        "Jianfeng Gao",
        "Paul Smolensky",
        "Roland Fernandez"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "December 2024",
      "abstract": "Neural networks continue to struggle with compositional generalization, and this issue is exacerbated by a lack of massive pre-training. One successful approach for developing neural systems which exhibit human-like compositional generalization is hybrid neurosymbolic techniques. However, these techniques run into the core issues that plague symbolic approaches to AI: scalability and flexibility. The reason for this failure is that at their core, hybrid neurosymbolic models perform symbolic computation and relegate the scalable and flexible neural computation to parameterizing a symbolic system. We investigate a unified neurosymbolic system where transformations in the network can be interpreted simultaneously as both symbolic and neural computation. We extend a unified neurosymbolic architecture called the Differentiable Tree Machine in two central ways. First, we significantly increase the model’s efficiency through the use of sparse vector representations of symbolic structures. Second, we enable its application beyond the restricted set of tree2tree problems to the more general class of seq2seq problems. The improved model retains its prior generalization capabilities and, since there is a fully neural path through the network, avoids the pitfalls of other neurosymbolic techniques that elevate symbolic computation over neural computation.",
      "url": "https://www.microsoft.com/en-us/research/publication/compositional-generalization-across-distributional-shifts-with-sparse-tree-operations/"
    },
    {
      "title": "A Shared Standard for Valid Measurement of Generative AI Systems’ Capabilities, Risks, and Impacts",
      "authors": [
        "A. Feder Cooper",
        "Alex Chouldechova",
        "Alex Dow",
        "Chad Atalla",
        "Dan Vann",
        "Emily Corvi",
        "Emily Sheng",
        "Hanna Wallach",
        "Hannah Washington",
        "Jean Garcia-Gathright",
        "Matthew Vogel",
        "Nick Pangakis",
        "Solon Barocas",
        "Stefanie Reed"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Social sciences"
      ],
      "publication_date": "December 2024",
      "abstract": "The valid measurement of generative AI (GenAI) systems’ capabilities, risks, and impacts forms the bedrock of our ability to evaluate these systems. We introduce a shared standard for valid measurement that helps place many of the disparate-seeming evaluation practices in use today on a common footing. Our framework, grounded in measurement theory from the social sciences, extends the work of Adcock & Collier (2001) in which the authors formalized valid measurement of concepts in political science via three processes: systematizing background concepts, operationalizing systematized concepts via annotation procedures, and applying those procedures to instances. We argue that valid measurement of GenAI systems’ capabilities, risks, and impacts, further requires systematizing, operationalizing, and applying not only the entailed concepts, but also the contexts of interest and the metrics used. This involves both descriptive reasoning about particular instances and inferential reasoning about underlying populations, which is the purview of statistics. By placing many disparate-seeming GenAI evaluation practices on a common footing, our framework enables individual evaluations to be better understood, interrogated for reliability and validity, and meaningfully compared. This is an important step in advancing GenAI evaluation practices toward more formalized and theoretically grounded processes — i.e., toward a science of GenAI evaluations.",
      "url": "https://www.microsoft.com/en-us/research/publication/a-shared-standard-for-valid-measurement-of-generative-ai-systems-capabilities-risks-and-impacts/"
    },
    {
      "title": "The USC Annenberg Relevance Report 2025",
      "authors": [
        "Advait Sarkar",
        "Gonzalo Ramos",
        "Leon Reicherts",
        "Pratik Ghosh",
        "Richard Banks"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Human-computer interaction",
        "Social sciences"
      ],
      "publication_date": "December 2024",
      "abstract": "The annual Relevance Report from the USC Center for Public Relations identifies emerging issues and forecasts topics and trends impacting society, business, and communication in the coming year. The book features contributions from PR industry leaders, and USC academics and graduate students.\nThe 2025 Relevance Report highlights how AI is reshaping the PR industry, driving innovation in storytelling, efficiency, and data-driven insights. Microsoft has contributed to this year’s report in a number of ways, including an article by the Tools for Thought team entitled “AI as ‘Tool for Thought'”.",
      "url": "https://www.microsoft.com/en-us/research/publication/the-usc-annenberg-relevance-report-2025/"
    },
    {
      "title": "Challenges in Human-Agent Communication",
      "authors": [
        "Adam Fourney",
        "Daniel S. Weld",
        "Eric Horvitz",
        "Gagan Bansal",
        "Hussein Mozannar",
        "Jennifer Wortman Vaughan",
        "Saleema Amershi",
        "Victor Dibia"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Human-computer interaction"
      ],
      "publication_date": "December 2024",
      "abstract": "Remarkable advancements in modern generative foundation models have enabled the development of sophisticated and highly capable autonomous agents that can observe their environment, invoke tools, and communicate with other agents to solve problems. Although such agents can communicate with users through natural language, their complexity and wide-ranging failure modes present novel challenges for human-AI interaction. Building on prior research and informed by a communication grounding perspective, we contribute to the study of \\emph{human-agent communication} by identifying and analyzing twelve key communication challenges that these systems pose. These include challenges in conveying information from the agent to the user, challenges in enabling the user to convey information to the agent, and overarching challenges that need to be considered across all human-agent communication. We illustrate each challenge through concrete examples and identify open directions of research. Our findings provide insights into critical gaps in human-agent communication research and serve as an urgent call for new design patterns, principles, and guidelines to support transparency and control in these systems.",
      "url": "https://www.microsoft.com/en-us/research/publication/human-agent-interaction-challenges/"
    },
    {
      "title": "Neuro-Symbolic Data Generation for Math Reasoning",
      "authors": [
        "Chun Cao",
        "Fan Yang",
        "Xian Zhang",
        "Xiaoxing Ma",
        "Yu-Feng Li",
        "Yuan Yao",
        "Zenan Li",
        "Zhi Zhou"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "December 2024",
      "abstract": "A critical question about Large Language Models (LLMs) is whether their apparent deficiency in mathematical reasoning is inherent, or merely a result of insufficient exposure to high-quality mathematical data. To explore this, we developed an automated method for generating high-quality, supervised mathematical datasets. The method carefully mutates existing math problems, ensuring both diversity and validity of the newly generated problems. This is achieved by a neuro-symbolic data generation framework combining the intuitive informalization strengths of LLMs, and the precise symbolic reasoning of math solvers along with projected Markov chain Monte Carlo sampling in the highly-irregular symbolic space. Empirical experiments demonstrate the high quality of data generated by the proposed method, and that the LLMs, specifically LLaMA-2 and Mistral, when realigned with the generated data, surpass their state-of-the-art counterparts.",
      "url": "https://www.microsoft.com/en-us/research/publication/neuro-symbolic-data-generation-for-math-reasoning/"
    },
    {
      "title": "Towards Editing Time Series",
      "authors": [
        "Baoyu Jing",
        "Dongsheng Li",
        "Jingrui He",
        "Kan Ren",
        "Shuqi Gu",
        "Tianyu Chen",
        "Zhiyu Yang"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "December 2024",
      "abstract": "Synthesizing time series data is pivotal in modern society, aiding effective decision making and ensuring privacy preservation in various scenarios. Time series are associated with various attributes, including trends, seasonality, and external information such as location. Recent research has predominantly focused on random unconditional synthesis or conditional synthesis. Nonetheless, these paradigms generate time series from scratch and are incapable of manipulating existing time series samples. This paper introduces a novel task, called Time Series Editing (TSE), to synthesize time series by manipulating existing time series. The objective is to modify the given time series according to the specified attributes while preserving other properties unchanged. This task is not trivial due to the inadequacy of data coverage and the intricate relationships between time series and their attributes. To address these issues, we introduce a novel diffusion model, called TEdit. The proposed TEdit is trained using a novel bootstrap learning algorithm that effectively enhances the coverage of the original data. It is also equipped with an innovative multi-resolution modeling and generation paradigm to capture the complex relationships between time series and their attributes. Experimental results demonstrate the efficacy of TEdit for editing specified attributes upon the existing time series data. The project page is at https://seqml.github.io/tse (opens in new tab).\n\n",
      "url": "https://www.microsoft.com/en-us/research/publication/towards-editing-time-series/"
    },
    {
      "title": "Eyelid Fold Consistency in Facial Modeling",
      "authors": [
        "Charlie Hewitt",
        "Fatemeh Sadat Saleh",
        "Lohit Petikam",
        "Tadas Baltrusaitis"
      ],
      "research_areas": [
        "Computer vision",
        "Graphics and multimedia"
      ],
      "publication_date": "December 2024",
      "abstract": "Eyelid shape is integral to identity and likeness in human facial modeling. Human eyelids are diverse in appearance with varied skin fold and epicanthal fold morphology between individuals. Existing parametric face models express eyelid shape variation to an extent, but do not preserve sufficient likeness across a diverse range of individuals. We propose a new definition of eyelid fold consistency and implement geometric processing techniques to model diverse eyelid shapes in a unified topology. Using this method we reprocess data used to train a parametric face model and demonstrate significant improvements in face-related machine learning tasks.",
      "url": "https://www.microsoft.com/en-us/research/publication/eyelid-fold-consistency-in-facial-modeling/"
    },
    {
      "title": "Query-Efficient Correlation Clustering with Noisy Oracle",
      "authors": [
        "Atsushi Miyauchi",
        "Francesco Bonchi",
        "Wei Chen",
        "Yuko Kuroki"
      ],
      "research_areas": [
        "Algorithms",
        "Artificial intelligence"
      ],
      "publication_date": "December 2024",
      "abstract": "We study a general clustering setting in which we have n elements to be clustered, and we aim to perform as few queries as possible to an oracle that returns a noisy sample of the weighted similarity between two elements. Our setting encompasses many application domains in which the similarity function is costly to compute and inherently noisy. We introduce two novel formulations of online learning problems rooted in the paradigm of Pure Exploration in Combinatorial Multi-Armed Bandits (PE-CMAB): fixed confidence and fixed budget settings. For both settings, we design algorithms that combine a sampling strategy with a classic approximation algorithm for correlation clustering and study their theoretical guarantees. Our results are the first examples of polynomial-time algorithms that work for the case of PE-CMAB in which the underlying offline optimization problem is NP-hard.",
      "url": "https://www.microsoft.com/en-us/research/publication/query-efficient-correlation-clustering-with-noisy-oracle/"
    },
    {
      "title": "Mechanism Design for LLM Fine-tuning with Multiple Reward Models",
      "authors": [
        "Haoran Sun",
        "Siwei Wang",
        "Wei Chen",
        "Xiaotie Deng",
        "Yurong Chen"
      ],
      "research_areas": [
        "Algorithms",
        "Artificial intelligence",
        "Economics"
      ],
      "publication_date": "December 2024",
      "abstract": "Recent research on fine-tuning large language models (LLMs) through the aggregation of multiple preferences has attracted considerable attention. However, the existing literature predominantly focuses on the empirical performance of aggregation algorithms while neglecting the underlying motivation for agents to misreport their preferences. In this paper, we formalize this as a multi-parameter mechanism design problem, where an LLM provider designs training and payment rules to achieve specific objectives and promote the truthful reporting of preferences. Firstly, we claim the necessity of a payment scheme by demonstrating that without payments, truth-telling is a strictly dominated strategy under a wide range of training rules. Then, we introduce the affine maximizer payment scheme for the social welfare maximizing training rules, which ensures both dominant-strategy\nincentive compatibility (DSIC) and individual rationality (IR). Furthermore, we prove that under mild conditions, any other payment rule that implements these training rules in DSIC can be converted to the affine maximizer payment by adding a factor irrelevant to the agents’ reports. We also show that this mechanism satisfies approximate DSIC when the input of the mechanism is a biased version of the reported preferences, showcasing its robustness in real-world applications.",
      "url": "https://www.microsoft.com/en-us/research/publication/mechanism-design-for-llm-fine-tuning-with-multiple-reward-models/"
    },
    {
      "title": "ALPINE: Unveiling the Planning Capability of Autoregressive Learning in Language Models",
      "authors": [
        "Haoran Sun",
        "Shang-Hua Teng",
        "Shi Feng",
        "Siwei Wang",
        "Wei Chen",
        "Yifei Shen"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "December 2024",
      "abstract": "In this paper, we present the findings of our Project ALPINE which stands for “Autoregressive Learning for Planning In NEtworks.” Project ALPINE initiates a theoretical investigation into the development of planning capabilities in Transformer-based language models through their autoregressive learning mechanisms, aiming to identify any potential limitations in their planning abilities. We abstract planning as a network path-finding task where the objective is to generate a valid path from a specified source node to a designated target node. In terms of expressiveness, we show that the Transformer is capable of executing path-finding by embedding the adjacency and reachability matrices within its weights. Our theoretical analysis of the gradient-based learning dynamic of the Transformer reveals that the Transformer is capable of learning both the adjacency matrix and a limited form of the reachability matrix. These theoretical insights are then validated through experiments, which demonstrate that the Transformer indeed learns the adjacency matrix and an incomplete reachability matrix, which aligns with the predictions made in our theoretical analysis. Additionally, when applying our methodology to a real-world planning benchmark, called Blocksworld, our observations remain consistent. Our theoretical and empirical analyses further unveil a potential limitation of Transformer in path-finding: it cannot identify reachability relationships through transitivity, and thus would fail when path concatenation is needed to generate a path. In summary, our findings shed new light on how the internal mechanisms of autoregressive learning enable planning in networks. This study may contribute to our understanding of the general planning capabilities in other related domains.",
      "url": "https://www.microsoft.com/en-us/research/publication/alpine-unveiling-the-planning-capability-of-autoregressive-learning-in-language-models/"
    },
    {
      "title": "Food Choice Mimicry on a Large University Campus",
      "authors": [
        "A. Chiolero",
        "Emre Kiciman",
        "Eric Horvitz",
        "Kristina Gligorić",
        "R. West",
        "Ryen W. White"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Social sciences"
      ],
      "publication_date": "December 2024",
      "abstract": "Social influence is a strong determinant of food consumption, which in turn influences the environment and health. Purchasing mimicry, a phenomenon where a person copies another person’s purchases, has been identified as the key governing mechanism. Although consistent observations have been made on the role of purchasing mimicry in driving similarities in food consumption, much less is known about the precise prevalence, the affected subpopulations, and the food types most strongly associated with mimicry effects. Here, we study social influence on food choice through carefully designed causal analyses, leveraging the sequential nature of shop queues on a large university campus. In particular, we consider a large number of adjacent purchases where a focal user immediately follows another user (“partner”) in the checkout queue and both make a purchase. Across food additions purchased during lunchtime together with a meal, we find that the focal user is significantly more likely to purchase the food item when the partner buys the item, vs. when the partner does not, increasing the purchasing probability by 14% in absolute terms, or by 83% in relative terms. The effect is observed across all food types, but largest for condiments. Furthermore, purchasing mimicry is present across age, gender, and status subpopulations, but strongest for students and the youngest. We elucidate the behavioral mechanism of purchasing mimicry, and derive direct implications for interventions improving dietary behaviors on campus, such as facilitating preordering to reduce detrimental interactions.",
      "url": "https://www.microsoft.com/en-us/research/publication/food-choice-mimicry-on-a-large-university-campus/"
    },
    {
      "title": "Local vs. Global: Local Land-Use and Land-Cover Models Deliver Higher Quality Maps",
      "authors": [
        "Caleb Robinson",
        "Charles Mwangi",
        "Esther Maina",
        "Gilles Quentin Hacheme",
        "Girmaw Abebe Tadesse",
        "Hamed Alemohammad",
        "Joshua Nyakundi",
        "Juan M. Lavista Ferres",
        "Luana Marotti",
        "Rahul Dodhia"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Computer vision",
        "Ecology and environment"
      ],
      "publication_date": "December 2024",
      "abstract": "In 2023, 58.0% of the African population experienced moderate to severe food insecurity, with 21.6% facing severe food insecurity. Land-use and land-cover maps provide crucial insights for addressing food insecurity by improving agricultural efforts, including mapping and monitoring crop types and estimating yield. The development of global land-cover maps has been facilitated by the increasing availability of earth observation data and advancements in geospatial machine learning. However, these global maps exhibit lower accuracy and inconsistencies in Africa, partly due to the lack of representative training data. To address this issue, we propose a data-centric framework with a teacher-student model setup, which uses diverse data sources of satellite images and label examples to produce local land-cover maps. Our method trains a high-resolution teacher model on images with a resolution of 0.331 m/pixel and a low-resolution student model on publicly available images with a resolution of 10 m/pixel. The student model also utilizes the teacher model’s output as its weak label examples through knowledge transfer. We evaluated our framework using Murang’a county in Kenya, renowned for its agricultural productivity, as a use case. Our local models achieved higher quality maps, with improvements of 0.14 in the F1 score and 0.21 in Intersection-over-Union, compared to the best global model. Our evaluation also revealed inconsistencies in existing global maps, with a maximum agreement rate of 0.30 among themselves. Our work provides valuable guidance to decision-makers for driving informed decisions to enhance food security.",
      "url": "https://www.microsoft.com/en-us/research/publication/local-vs-global-local-land-use-and-land-cover-models-deliver-higher-quality-maps/"
    },
    {
      "title": "HYSYNTH: Context-Free LLM Approximation for Guiding Program Synthesis",
      "authors": [
        "Emmanuel Anaya Gonzalez",
        "Nadia Polikarpova",
        "Saketh Ram Kasibatla",
        "Shraddha Barke",
        "Taylor Berg-Kirkpatrick"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Programming languages and software engineering"
      ],
      "publication_date": "December 2024",
      "abstract": "Many structured prediction and reasoning tasks can be framed as program synthesis problems, where the goal is to generate a program in a domain-specific language (DSL) that transforms input data into the desired output. Unfortunately, purely neural approaches, such as large language models (LLMs), often fail to produce fully correct programs in unfamiliar DSLs, while purely symbolic methods based on combinatorial search scale poorly to complex problems. Motivated by these limitations, we introduce a hybrid approach, where LLM completions for a given task are used to learn a task-specific, context-free surrogate model, which is then used to guide program synthesis. We evaluate this hybrid approach on three domains, and show that it outperforms both unguided search and direct sampling from LLMs, as well as existing program synthesizers.",
      "url": "https://www.microsoft.com/en-us/research/publication/hysynth-context-free-llm-approximation-for-guiding-program-synthesis/"
    },
    {
      "title": "Convergence to Equilibrium of No-regret Dynamics in Congestion Games",
      "authors": [
        "Baoxiang Wang",
        "Ioannis Panageas",
        "Jing Dong",
        "Jingyu Wu",
        "Leello Dadi",
        "Luca Viano",
        "Siwei Wang",
        "Stratis Skoulakis",
        "Volkan Cevher",
        "Wei Chen"
      ],
      "research_areas": [
        "Algorithms",
        "Artificial intelligence",
        "Economics"
      ],
      "publication_date": "December 2024",
      "abstract": "The congestion game is a powerful model that encompasses a range of engineering systems such as traffic networks and resource allocation. It describes the behavior of a group of agents who share a common set of \\(F\\) facilities and take actions as subsets of \\(k\\) facilities. In this work, we study the online formulation of congestion games, where agents participate in the game repeatedly and observe feedback with randomness. We note that this paper is the result of the merging of [24] arXiv:2306.13673 and [19] arXiv:2401.09628. In [24], we propose CongestEXP, a decentralized algorithm that is based on the classic exponential weights method. By maintaining weights on the facility level, the regret bound of CongestEXP avoids the exponential dependence on the size of possible facility sets, i.e., \\(\\binom{F}{k} \\approx F^k\\), and scales only linearly with \\(F\\). Specifically, we show that CongestEXP attains a regret upper bound of \\(O(kF\\sqrt{T})\\) for every individual player, where \\(T\\) is the time horizon. If a strict Nash equilibrium exists, we show that CongestEXP can converge to the strict Nash policy almost exponentially fast in \\(O(F exp(−t^{1−α}))\\), where \\(t\\) is the number of iterations and \\(\\alpha \\in (1/2, 1)\\). In [19], we present an online learning algorithm in the bandit feedback model that, once adopted by all agents of a congestion game, results in game-dynamics that converge to an ǫ-approximate Nash Equilibrium in a polynomial number of rounds with respect to \\(1/\\epsilon\\), the number of players and the number of available resources. The proposed algorithm also guarantees sublinear regret to any agent adopting it. As a result, our work answers an open question from [17] and extends the recent results of [37] to the bandit feedback model.",
      "url": "https://www.microsoft.com/en-us/research/publication/convergence-to-equilibrium-of-no-regret-dynamics-in-congestion-games/"
    },
    {
      "title": "GPSense: Passive Sensing with Pervasive GPS Signals",
      "authors": [
        "Huixin Dong",
        "Jie Xiong",
        "Lili Qiu",
        "Minhao Cui",
        "Ning Wang",
        "Wei Wang"
      ],
      "research_areas": [
        "Systems and networking"
      ],
      "publication_date": "December 2024",
      "abstract": "Wireless sensing is gaining increasing attention from both academia and industry. Various wireless signals, such as Wi-Fi, UWB, and acoustic signals, have been leveraged for sensing. While promising in many aspects, two critical limitations still exist: a) limited sensing coverage; and b) the requirement for dedicated sensing signals, which may interfere with the original function of the wireless technology. To address these issues, we propose to utilize GPS signals for sensing, as GPS signals are already pervasive and emitted from satellites 24/7 at pre-allocated frequency bands, causing no interference. To make GPS sensing possible, we reconstruct signals with amplitude and phase information which is critical for sensing using the raw measurements reported by commercial GPS receiver module. We also develop sensing models to tailor the unique properties of GPS signals such as extremely long transmission distance. Finally, we introduce the concept of distributed sensing and design signal processing methods to fuse signals from multiple satellites to improve sensing performance. With all these designs, we prototype the first GPS wireless sensing system on commercial GPS receiver modules. Comprehensive experiments demonstrate that the proposed system can realize meaningful sensing applications such as human activity sensing, passive trajectory tracking, and respiration monitoring.\n\n\n",
      "url": "https://www.microsoft.com/en-us/research/publication/gpsense-passive-sensing-with-pervasive-gps-signals/"
    },
    {
      "title": "Personalized progression modelling and prediction in Parkinson’s disease with a novel multi-modal graph approach",
      "authors": [
        "Caihua Shan",
        "Chencheng Zhang",
        "Dongqi Han",
        "Dongsheng Li",
        "Jie Lian",
        "Lili Qiu",
        "V. Vardhanabhuti",
        "Xufang Luo"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "December 2024",
      "abstract": "Parkinson’s disease (PD) is a complex neurological disorder characterized by dopaminergic neuron degeneration, leading to diverse motor and non-motor impairments. This variability complicates accurate progression modelling and early-stage prediction. Traditional classification methods based on clinical symptoms are often limited by disease heterogeneity. This study introduces an graph-based interpretable personalized progression method, utilizing data from the Parkinson’s Progression Markers Initiative (PPMI) and Stroke Parkinson’s Disease Biomarker Program (PDBP). Our approach integrates multimodal inter-individual and intra-individual data, including clinical assessments, MRI, and genetic information to make multi-dimension predictions. Validated using the PDBP dataset from 12 to 36 months, our AdaMedGraph method demonstrated strong performance, achieving AUC values of 0.748 and 0.714 for the 12-month Hoehn and Yahr Scale and Movement Disorder Society-Sponsored Revision of the Unified Parkinson’s Disease Rating Scale (MDS-UPDRS) III on the PPMI test set. Ablation analysis reveals the importance of baseline clinical assessment predictors. This novel framework improves personalized care and offers insights into unique disease trajectories in PD patients.",
      "url": "https://www.microsoft.com/en-us/research/publication/personalized-progression-modelling-and-prediction-in-parkinsons-disease-with-a-novel-multi-modal-graph-approach/"
    },
    {
      "title": "CTC-GMM: CTC guided modality matching for fast and accurate streaming speech translation",
      "authors": [
        "Jinyu Li",
        "Matt Post",
        "Ruchao Fan",
        "Rui Zhao"
      ],
      "research_areas": [
        "Human language technologies"
      ],
      "publication_date": "December 2024",
      "abstract": "Models for streaming speech translation (ST) can achieve high accuracy and low latency if they’re developed with vast amounts of paired audio in the source language and written text in the target language. Yet, these text labels for the target language are often pseudo labels due to the prohibitive cost of manual ST data labeling. In this paper, we introduce a methodology named Connectionist Temporal Classification guided modality matching (CTC-GMM) that enhances the streaming ST model by leveraging extensive machine translation (MT) text data. This technique employs CTC to compress the speech sequence into a compact embedding sequence that matches the corresponding text sequence, allowing us to utilize matched {source-target} language text pairs from the MT corpora to refine the streaming ST model further. Our evaluations with FLEURS and Covost2 show that the CTC-GMM approach can increase translation accuracy relatively by 13.9% and 6.4% respectively, while also boosting decoding speed by 59.7% on GPU.",
      "url": "https://www.microsoft.com/en-us/research/publication/ctc-gmm-ctc-guided-modality-matching-for-fast-and-accurate-streaming-speech-translation/"
    },
    {
      "title": "Microsoft New Future of Work Report 2024",
      "authors": [
        "Abigail Sellen",
        "Adam Troy",
        "Advait Sarkar",
        "Alex Chouldechova",
        "Alex Farach",
        "Alexandra Olteanu",
        "Alexia Cambon",
        "Arjun Radhakrishna",
        "Asta Roseway",
        "Ben Zorn",
        "Brent Hecht",
        "Daniel G. Goldstein",
        "Dave Brown",
        "Dhruv Joshi",
        "Ed Cutrell",
        "Emre Kiciman",
        "Gonzalo Ramos",
        "Gustavo Soares",
        "Hanna Wallach",
        "Hugo Romat",
        "Ian Drosos",
        "Jack Williams (johnwilliams)",
        "Jacki O'Neill",
        "Jaime Teevan",
        "Jake Hofman",
        "Javier Hernandez",
        "Jenna Butler",
        "Jennifer Wortman Vaughan",
        "Jina Suh",
        "John Tang",
        "Justin Edwards",
        "Kalika Bali",
        "Ken Hinckley",
        "Kori Inkpen",
        "Krishna Madhavan",
        "Laylah Bulman",
        "Leon Reicherts",
        "Lev Tankelevitch",
        "Longqi Yang",
        "Martez Mott",
        "Mercy Muchai",
        "Michael Bentley",
        "Mihaela Vorvoreanu",
        "Millicent Ochieng",
        "Najeeb Abdulhamid",
        "Nancy Baym",
        "Nathalie Henry Riche",
        "Nicolai Marquardt",
        "Nicole Immorlica",
        "Rebecca Janssen",
        "Samuel Maina",
        "Shamsi Iqbal",
        "Siân Lindley",
        "Stephanie Nyairo",
        "Su Lin Blodgett",
        "Sumit Gulwani",
        "Sunayana Sitaram",
        "Vu Le"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Data platforms and analytics",
        "Economics",
        "Human-computer interaction",
        "Programming languages and software engineering",
        "Social sciences"
      ],
      "publication_date": "December 2024",
      "abstract": "As Microsoft approaches its 50th anniversary, the landscape of work continues to evolve at an unprecedented pace. The past year has marked a pivotal shift, moving from predictions and controlled lab studies to the real-world implementation and impact of new technologies. These advancements, built on decades of research and development, are beginning to yield tangible results, offering a clearer view of how generative AI is reshaping the way work gets done.\nWe began the New Future of Work Report series in 2021, during the height of the global shift to remote work. That inaugural report synthesized research to help reimagine work as traditional models were upended. The 2022 report focused on hybrid work, exploring how intentional co-location could complement remote practices. By 2023, the focus turned to integrating large language models (LLMs) into workflows, examining their potential to transform productivity and collaboration.\nThis fourth edition, released on the eve of Microsoft’s 50th anniversary, builds on the foundation of the previous reports but is distinct in its emphasis on lessons learned from real-world applications. Over the past year, organizations have begun deploying generative AI at scale, revealing both its promise and its challenges. As such, this report centers on actionable insights derived from Microsoft’s research and beyond, offering insight into how these tools are shaping work in practice.\nThis is a moment for researchers and practitioners alike to move beyond observation and actively contribute to designing a better future of work. As generative AI matures and its integration deepens, this report aims to serve as both a guide and a catalyst for progress, helping our colleagues worldwide navigate and shape the ongoing transformation.\n \nEditors: Jenna Butler (Principal Applied Research Scientist), Mihaela Vorvoreanu (Director, UX Research and Education), Rebecca Janßen (Applied Research Assistant), Abigail Sellen (Distinguished Scientist and Lab Director), Nicole Immorlica (Senior Principal Research Manager), Brent Hecht (Partner Director of Applied Science), Jaime Teevan (Chief Scientist and Technical Fellow)",
      "url": "https://www.microsoft.com/en-us/research/publication/microsoft-new-future-of-work-report-2024/"
    },
    {
      "title": "Can Graph Learning Improve Task Planning?",
      "authors": [
        "Bohang Zhang",
        "Caihua Shan",
        "Dongsheng Li",
        "Hong Cheng",
        "Jiarui Feng",
        "Kaitao Song",
        "Siwei Wang",
        "Wei Chen",
        "Xixi Wu",
        "Yifei Shen",
        "Yun Xiong"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "December 2024",
      "abstract": "Task planning is emerging as an important research topic alongside the development of large language models (LLMs). It aims to break down complex user requests into solvable sub-tasks, thereby fulfilling the original requests. In this context, the sub-tasks can be naturally viewed as a graph, where the nodes represent the sub-tasks, and the edges denote the dependencies among them. Consequently, task planning is a decision-making problem that involves selecting a connected path or subgraph within the corresponding graph and invoking it. In this paper, we explore graph learning-based methods for task planning, a direction that is orthogonal to the prevalent focus on prompt design. Our interest in graph learning stems from a theoretical discovery: the biases of attention and auto-regressive loss impede LLMs’ ability to effectively navigate decision-making on graphs, which is adeptly addressed by graph neural networks (GNNs). This theoretical insight led us to integrate GNNs with LLMs to enhance overall performance. Extensive experiments demonstrate that GNN-based methods surpass existing solutions even without training, and minimal training can further enhance their performance. Additionally, our approach complements prompt engineering and fine-tuning techniques, with performance further enhanced by improved prompts or a fine-tuned model.",
      "url": "https://www.microsoft.com/en-us/research/publication/can-graph-learning-improve-task-planning/"
    },
    {
      "title": "Investigating neural audio codecs for speech language model-based speech generation",
      "authors": [
        "Canrun Li",
        "Chung-Hsien Tsai",
        "Dongmei Wang",
        "Jiaqi Li",
        "Jinyu Li",
        "Junkun Chen",
        "Long Zhou",
        "Michael Zeng",
        "Midia Yousefi",
        "Sheng Zhao",
        "Shujie Liu",
        "Xiaofei Wang",
        "Yanqing Liu",
        "Yao Qian",
        "Zhen Xiao",
        "Zhizheng Wu"
      ],
      "research_areas": [
        "Human language technologies"
      ],
      "publication_date": "December 2024",
      "abstract": "Neural audio codec tokens serve as the fundamental building blocks for speech language model (SLM)-based speech generation. However, there is no systematic understanding on how the codec system affects the speech generation performance of the SLM. In this work, we examine codec tokens within SLM framework for speech generation to provide insights for effective codec design. We retrain existing high-performing neural codec models on the same data set and loss functions to compare their performance in a uniform setting. We integrate codec tokens into two SLM systems: masked-based parallel speech generation system and an auto-regressive (AR) plus non-auto-regressive (NAR) model-based system. Our findings indicate that better speech reconstruction in codec systems does not guarantee improved speech generation in SLM. A high-quality codec decoder is crucial for natural speech production in SLM, while speech intelligibility depends more on quantization mechanism.",
      "url": "https://www.microsoft.com/en-us/research/publication/investigating-neural-audio-codecs-for-speech-language-model-based-speech-generation/"
    },
    {
      "title": "MicroSurf: Guiding Energy Distribution inside Microwave Oven with Metasurfaces",
      "authors": [
        "Hao Pan",
        "Lili Qiu",
        "Longyuan Ge",
        "Swarun Kumar",
        "Yi-Chao Chen",
        "Yiwen Song"
      ],
      "research_areas": [
        "Systems and networking"
      ],
      "publication_date": "December 2024",
      "abstract": "Microwave ovens have become an essential cooking appliance owing to their convenience and efficiency. However, microwave ovens suffer from uneven distribution of energy, which causes prolonged delays, unpleasant cooking experiences, and even safety concerns. Despite significant research efforts, current solutions remain inadequate. In this paper, we first conduct measurement studies to understand the energy distribution for 10 microwave ovens and show their energy distribution in both 2D and 3D is very skewed, with notably lower energy levels at the center of the microwave cavity, where food is commonly placed. To tackle this challenge, we propose a novel methodology to enhance the performance of microwave ovens. Our approach begins with the development of a measurement driven model of a microwave oven. We construct a detailed 3D model in the High Frequency Structure Simulator (HFSS) and use real temperature measurements from a microwave to derive critical parameters relevant to the appliance’s functionality (e.g., operating frequency, waveguide specifications). We then develop a novel approach that optimizes the design and placement of a low-cost passive metasurface for a given heating objective. Using extensive experiments, we demonstrate the efficacy of our approach across diverse food, optimization objectives, and microwave ovens.\n\n\n",
      "url": "https://www.microsoft.com/en-us/research/publication/microsurf-guiding-energy-distribution-inside-microwave-oven-with-metasurfaces/"
    },
    {
      "title": "Laugh Now Cry Later: Controlling Time-Varying Emotional States of Flow-Matching-Based Zero-Shot Text-to-Speech",
      "authors": [
        "Canrun Li",
        "Chung-Hsien Tsai",
        "Daniel Tompkins",
        "Haibin Wu",
        "Jinyu Li",
        "Manthan Thakker",
        "Naoyuki Kanda",
        "Sefik Emre Eskimez",
        "Sheng Zhao",
        "Xiaofei Wang",
        "Zhen Xiao"
      ],
      "research_areas": [
        "Human language technologies"
      ],
      "publication_date": "December 2024",
      "abstract": "People change their tones of voice, often accompanied by nonverbal vocalizations (NVs) such as laughter and cries, to convey rich emotions. However, most text-to-speech (TTS) systems lack the capability to generate speech with rich emotions, including NVs. This paper introduces EmoCtrl-TTS, an emotion-controllable zero-shot TTS that can generate highly emotional speech with NVs for any speaker. EmoCtrl-TTS leverages arousal and valence values, as well as laughter embeddings, to condition the flow-matching-based zero-shot TTS. To achieve high-quality emotional speech generation, EmoCtrl-TTS is trained using more than 27,000 hours of expressive data curated based on pseudo-labeling. Comprehensive evaluations demonstrate that EmoCtrl-TTS excels in mimicking the emotions of audio prompts in speech-to-speech translation scenarios. We also show that EmoCtrl-TTS can capture emotion changes, express strong emotions, and generate various NVs in zero-shot TTS.",
      "url": "https://www.microsoft.com/en-us/research/publication/laugh-now-cry-later-controlling-time-varying-emotional-states-of-flow-matching-based-zero-shot-text-to-speech/"
    },
    {
      "title": "Efficacy of a conversational chatbot for cigarette smoking cessation: Protocol of the QuitBot full-scale randomized controlled trial",
      "authors": [
        "Brianna M Sullivan",
        "Jonathan B Bricker",
        "Juan M. Lavista Ferres",
        "Kristin E Mull",
        "Margarita Santiago-Torres"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Human language technologies",
        "Medical, health and genomics"
      ],
      "publication_date": "December 2024",
      "abstract": "Globally, cigarette smoking results in over 8 million premature annual deaths. Addressing this issue requires high-impact, cost-effective population-level interventions for smoking cessation. Conversational chatbots offer a potential solution given the recent advancements in machine learning and large language models. Chatbots can deliver supportive, empathetic behaviors, personalized responses, and timely advice tailored to users’ needs that is engaging through therapeutic conversations aimed at creating lasting social-emotional connections. Despite their promise, little is known about the efficacy and underlying mechanisms of chatbots for cigarette smoking cessation. We developed QuitBot, a quit smoking program of two to three-minute conversations covering topics ranging from motivations to quit, setting a quit date, choosing cessation medications, coping with triggers, maintaining abstinence, and recovering from a relapse. QuitBot employs conversational interactions, powered by an expert-curated large language model, allowing users to ask questions and receive personalized guidance on quitting smoking. Here, we report the design and execution of a randomized clinical trial comparing QuitBot (n = 760) against Smokefree TXT (SFT) text messaging program (n = 760), with a 12-month follow-up period. Both interventions include 42-days of content on motivations to quit, skills to cope with triggers, and relapse prevention. The key distinction between QuitBot and SFT is that QuitBot has communication and engagement features. This study aims to determine: whether QuitBot yields higher quit rates than SFT; and whether therapeutic alliance processes and engagement are mechanisms underlying cessation outcomes. Additionally, we will explore whether baseline factors including trust, social support, and demographics, moderate the efficacy of QuitBot.",
      "url": "https://www.microsoft.com/en-us/research/publication/efficacy-of-a-conversational-chatbot-for-cigarette-smoking-cessation-protocol-of-the-quitbot-full-scale-randomized-controlled-trial/"
    },
    {
      "title": "REDUCIO! Generating 1024⨉1024 Video within 16 Seconds using Extremely Compressed Motion Latents",
      "authors": [
        "Chong Luo",
        "Jianmin Bao",
        "Kai Qiu",
        "Qi Dai",
        "Rui Tian",
        "Yifan Yang",
        "Yu-Gang Jiang",
        "Zuxuan Wu"
      ],
      "research_areas": [
        "Computer vision"
      ],
      "publication_date": "November 2024",
      "abstract": "Commercial video generation models have exhibited realistic, high-fidelity results but are still restricted to limited access. One crucial obstacle for large-scale applications is the expensive training and inference cost. In this paper, we argue that videos contain much more redundant information than images, thus can be encoded by very few motion latents based on a content image. Towards this goal, we design an image-conditioned VAE to encode a video to an extremely compressed motion latent space. This magic Reducio charm enables 64x reduction of latents compared to a common 2D VAE, without sacrificing the quality. Training diffusion models on such a compact representation easily allows for generating 1K resolution videos. We then adopt a two-stage video generation paradigm, which performs text-to-image and text-image-to-video sequentially. Extensive experiments show that our Reducio-DiT achieves strong performance in evaluation, though trained with limited GPU resources. More importantly, our method significantly boost the efficiency of video LDMs both in training and inference. We train Reducio-DiT in around 3.2K training hours in total and generate a 16-frame 1024*1024 video clip within 15.5 seconds on a single A100 GPU.",
      "url": "https://www.microsoft.com/en-us/research/publication/reducio-generating-1024⨉1024-video-within-16-seconds-using-extremely-compressed-motion-latents/"
    },
    {
      "title": "Balance Reward and Safety Optimization for Safe Reinforcement Learning: A Perspective of Gradient Manipulation",
      "authors": [
        "Hong Cheng, Hang Dong, Bo Qiao, Si Qin",
        "Qingwei Lin 林庆维",
        "Shangding Gu"
      ],
      "research_areas": [
        "Algorithms",
        "Artificial intelligence",
        "Data platforms and analytics",
        "Programming languages and software engineering",
        "Systems and networking"
      ],
      "publication_date": "December 2024",
      "abstract": "To come soon.",
      "url": "https://www.microsoft.com/en-us/research/publication/balance-reward-and-safety-optimization-for-safe-reinforcement-learning-a-perspective-of-gradient-manipulation/"
    },
    {
      "title": "Ensuring Fair LLM Serving Amid Diverse Applications",
      "authors": [
        "A. Parayil",
        "Ankur Mallick",
        "Anoop Kulkarni",
        "Chetan Bansal",
        "Haiying Shen",
        "Kunal Jain",
        "Pankhuri Choudhary",
        "Redwan Ibne Seraj Khan",
        "Renee St. Amant",
        "Rujia Wang",
        "Saravan Rajmohan",
        "Steve Kofsky",
        "Victor Ruehle",
        "Yue Cheng"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Systems and networking"
      ],
      "publication_date": "November 2024",
      "abstract": "In a multi-tenant large language model (LLM) serving platform hosting diverse applications, some users may submit an excessive number of requests, causing the service to become unavailable to other users and creating unfairness. Existing fairness approaches do not account for variations in token lengths across applications and multiple LLM calls, making them unsuitable for such platforms. To address the fairness challenge, this paper analyzes millions of requests from thousands of users on MS CoPilot, a real-world multi-tenant LLM platform hosted by Microsoft. Our analysis confirms the inadequacy of existing methods and guides the development of FairServe, a system that ensures fair LLM access across diverse applications. FairServe proposes application-characteristic aware request throttling coupled with a weighted service counter based scheduling technique to curb abusive behavior and ensure fairness. Our experimental results on real-world traces demonstrate FairServe’s superior performance compared to the state-of-the-art method in ensuring fairness. We are actively working on deploying our system in production, expecting to benefit millions of customers world-wide.",
      "url": "https://www.microsoft.com/en-us/research/publication/ensuring-fair-llm-serving-amid-diverse-applications/"
    },
    {
      "title": "CogACT: A Foundational Vision-Language-Action Model for Synergizing Cognition and Action in Robotic Manipulation",
      "authors": [
        "Baining Guo",
        "Bei Liu",
        "Dong Chen",
        "Fangyun Wei",
        "Jianlong Fu",
        "Jianmin Bao",
        "Jiaolong Yang",
        "Lin Luo",
        "Mozheng Liao",
        "Qixiu Li",
        "Sicheng Xu",
        "Xi Chen",
        "Xiaofan Wang",
        "Yaobo Liang",
        "Yizhong Zhang",
        "Yu Deng",
        "Yuanchun Shi",
        "Zeyu Wang"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Computer vision",
        "Human-computer interaction"
      ],
      "publication_date": "November 2024",
      "abstract": "The advancement of large Vision-Language-Action (VLA) models has significantly improved robotic manipulation in terms of language-guided task execution and generalization to unseen scenarios. While existing VLAs adapted from pretrained large Vision-Language-Models (VLM) have demonstrated promising generalizability, their task performance is still unsatisfactory as indicated by the low tasks success rates in different environments. In this paper, we present a new advanced VLA architecture derived from VLM. Unlike previous works that directly repurpose VLM for action prediction by simple action quantization, we propose a componentized VLA architecture that has a specialized action module conditioned on VLM output. We systematically study the design of the action module and demonstrates the strong performance enhancement with diffusion action transformers for action sequence modeling, as well as their favorable scaling behaviors. We also conduct comprehensive experiments and ablation studies to evaluate the efficacy of our models with varied designs. The evaluation on 5 robot embodiments in simulation and real work shows that our model not only significantly surpasses existing VLAs in task performance and but also exhibits remarkable adaptation to new robots and generalization to unseen objects and backgrounds. It exceeds the average success rates of OpenVLA which has similar model size (7B) with ours by over 35% in simulated evaluation and 55% in real robot experiments. It also outperforms the large RT-2-X model (55B) by 18% absolute success rates in simulation. Code and models can be found on our project page (https://cogact.github.io/ (opens in new tab)).",
      "url": "https://www.microsoft.com/en-us/research/publication/cogact-a-foundational-vision-language-action-model-for-synergizing-cognition-and-action-in-robotic-manipulation/"
    },
    {
      "title": "Enabling Adoption of Regenerative Agriculture through Soil Carbon Copilots",
      "authors": [
        "Emre Kiciman",
        "Jessica Wolk",
        "Margaret Capetz",
        "Peder Olsen",
        "Rafael Padilha",
        "Ranveer Chandra",
        "Swati Sharma"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "November 2024",
      "abstract": "Mitigating climate change requires transforming agriculture to minimize environ mental impact and build climate resilience. Regenerative agricultural practices enhance soil organic carbon (SOC) levels, thus improving soil health and sequestering carbon. A challenge to increasing regenerative agriculture practices is cheaply measuring SOC over time and understanding how SOC is affected by regenerative agricultural practices and other environmental factors and farm management practices. To address this challenge, we introduce an AI-driven Soil Organic Carbon Copilot that automates the ingestion of complex multi-resolution, multi-modal data to provide large-scale insights into soil health and regenerative practices. Our data includes extreme weather event data (e.g., drought and wildfire incidents), farm management data (e.g., cropland information and tillage predictions), and SOC predictions. We find that integrating public data and specialized models enables large-scale, localized analysis for sustainable agriculture. In comparisons of agricultural practices across California counties, we find evidence that diverse agricultural activity may mitigate the negative effects of tillage; and that while extreme weather conditions heavily affect SOC, composting may mitigate SOC loss. Finally, implementing role-specific personas empowers agronomists, farm consultants, policymakers, and other stakeholders to implement evidence-based strategies that promote sustainable agriculture and build climate resilience.",
      "url": "https://www.microsoft.com/en-us/research/publication/enabling-adoption-of-regenerative-agriculture-through-soil-carbon-copilots/"
    },
    {
      "title": "Hairmony: Fairness-aware hairstyle classification",
      "authors": [
        "Antonio Criminisi",
        "Charlie Hewitt",
        "Chyna McRae",
        "Givi Meishvili",
        "James Clemoes",
        "Marta Wilczkowiak (SHE/HER)",
        "Martin de La Gorce",
        "Nina Jablonski",
        "Tadas Baltrusaitis",
        "Tibor Takacs",
        "Xiao-Xian",
        "Zafiirah Hosenie"
      ],
      "research_areas": [
        "Computer vision"
      ],
      "publication_date": "December 2024",
      "abstract": "We present a method for prediction of a person’s hairstyle from a single image. Despite growing use cases in user digitization and enrollment for virtual experiences, available methods are limited, particularly in the range of hairstyles they can capture. Human hair is extremely diverse and lacks any universally accepted description or categorization, making this a challenging task. Most current methods rely on parametric models of hair at a strand level. These approaches, while very promising, are not yet able to represent short, frizzy, coily hair and gathered hairstyles. We instead choose a classification approach which can represent the diversity of hairstyles required for a truly robust and inclusive system. Previous classification approaches have been restricted by poorly labeled data that lacks diversity, imposing constraints on the usefulness of any resulting enrollment system. We use only synthetic data to train our models. This allows for explicit control of diversity of hairstyle attributes, hair colors, facial appearance, poses, environments and other parameters. It also produces noise-free ground-truth labels. We introduce a novel hairstyle taxonomy developed in collaboration with a diverse group of domain experts which we use to balance our training data, supervise our model, and directly measure fairness. We annotate our synthetic training data and a real evaluation dataset using this taxonomy and release both to enable comparison of future hairstyle prediction approaches. We employ an architecture based on a pre-trained feature extraction network in order to improve generalization of our method to real data and predict taxonomy attributes as an auxiliary task to improve accuracy. Results show our method to be significantly more robust for challenging hairstyles than recent parametric approaches. Evaluation with taxonomy-based metrics also demonstrates the fairness of our method across diverse hairstyles.",
      "url": "https://www.microsoft.com/en-us/research/publication/hairmony/"
    },
    {
      "title": "Language-to-Code Translation with a Single Labeled Example",
      "authors": [
        "Ben Van Durme",
        "Hao Fang",
        "Harsh Jhamtani",
        "Jacob Andreas",
        "Jason Eisner",
        "Kaj Bostrom",
        "Patrick Xia",
        "Richard Shin",
        "Sam Thomson"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Human language technologies"
      ],
      "publication_date": "November 2024",
      "abstract": "Tools for translating natural language into code promise natural, open-ended interaction with databases, web APIs, and other software systems. However, this promise is complicated by the diversity and continual development of these systems, each with its own interface and distinct set of features. Building a new language-to-code translator, even starting with a large language model (LM), typically requires annotating a large set of natural language commands with their associated programs. In this paper, we describe ICIP (In-Context Inverse Programming), a method for bootstrapping a language-to-code system using mostly (or entirely) unlabeled programs written using a potentially unfamiliar (but human-readable) library or API. ICIP uses a pre-trained LM to assign candidate natural language descriptions to these programs, then iteratively refines the descriptions to ensure global consistency. Across nine different application domains from the Overnight and Spider benchmarks and text-davinci-003 and CodeLlama-7b-Instruct models, ICIP outperforms a number of prompting baselines. Indeed, in a “nearly unsupervised” setting with only a single annotated program and 100 unlabeled examples, it achieves up to 85% of the performance of a fully supervised system.",
      "url": "https://www.microsoft.com/en-us/research/publication/language-to-code-translation-with-a-single-labeled-example/"
    },
    {
      "title": "Interpreting Multi-band Galaxy Observations with Large Language Model-Based Agents",
      "authors": [
        "Nan Duan",
        "Song Huang",
        "Yaobo Liang",
        "Yuan-Sen Ting",
        "Zechang Sun",
        "Zheng Cai"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "December 2024",
      "abstract": "Astronomical research traditionally relies on extensive domain knowledge to interpret observations and narrow down hypotheses. We demonstrate that this process can be emulated using large language model-based agents to accelerate research workflows. We propose mephisto, a multi-agent collaboration framework that mimics human reasoning to interpret multi-band galaxy observations. mephisto interacts with the CIGALE codebase, which includes spectral energy distribution (SED) models to explain observations. In this open-world setting, mephisto learns from its self-play experience, performs tree search, and accumulates knowledge in a dynamically updated base. As a proof of concept, we apply mephisto to the latest data from the James Webb Space Telescope. mephisto attains near-human proficiency in reasoning about galaxies’ physical scenarios, even when dealing with a recently discovered population of “Little Red Dot” galaxies. This represents the first demonstration of agentic research in astronomy, advancing towards end-to-end research via LLM agents and potentially expediting astronomical discoveries.",
      "url": "https://www.microsoft.com/en-us/research/publication/interpreting-multi-band-galaxy-observations-with-large-language-model-based-agents/"
    },
    {
      "title": "Gaps Between Research and Practice When Measuring Representational Harms Caused by LLM-Based Systems",
      "authors": [
        "Alex Chouldechova",
        "Alexandra Olteanu",
        "Emily Sheng",
        "Emma Harvey",
        "Hanna Wallach",
        "Jean Garcia-Gathright",
        "Su Lin Blodgett"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Social sciences"
      ],
      "publication_date": "November 2024",
      "abstract": "To facilitate the measurement of representational harms caused by large language model (LLM)-based systems, the NLP research community has produced and made publicly available numerous measurement instruments, including tools, datasets, metrics, benchmarks, annotation instructions, and other techniques. However, the research community lacks clarity about whether and to what extent these instruments meet the needs of practitioners tasked with developing and deploying LLM-based systems in the real world, and how these instruments could be improved. Via a series of semi-structured interviews with practitioners in a variety of roles in different organizations, we identify four types of challenges that prevent practitioners from effectively using publicly available instruments for measuring representational harms caused by LLM-based systems: (1) challenges related to using publicly available measurement instruments; (2) challenges related to doing measurement in practice; (3) challenges arising from measurement tasks involving LLM-based systems; and (4) challenges specific to measuring representational harms. Our goal is to advance the development of instruments for measuring representational harms that are well-suited to practitioner needs, thus better facilitating the responsible development and deployment of LLM-based systems.",
      "url": "https://www.microsoft.com/en-us/research/publication/gaps-between-research-and-practice-when-measuring-representational-harms-caused-by-llm-based-systems/"
    },
    {
      "title": "A Foundation Model for the Earth System",
      "authors": [
        "Alex Archibald",
        "Ana Lucic",
        "Anna Allen",
        "Anna Vaughan",
        "Chun-Chieh Wu",
        "Cristian Bodnar",
        "Elizabeth Heider",
        "Haiyu Dong",
        "Jayesh Gupta",
        "Johannes Brandstetter",
        "Jonathan Weyn",
        "Kit Thambiratnam",
        "Maik Riechert",
        "Max Welling",
        "Megan Stanley",
        "Paris Perdikaris",
        "Patrick Garvan",
        "Richard Turner",
        "Wessel Bruinsma"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "November 2024",
      "abstract": "Reliable forecasts of the Earth system are crucial for human progress and safety from natural disasters. Artificial intelligence offers substantial potential to improve prediction accuracy and computational efficiency in this field, however this remains underexplored in many domains. Here we introduce Aurora, a large-scale foundation model for the Earth system trained on over a million hours of diverse data. Aurora outperforms operational forecasts for air quality, ocean waves, tropical cyclone tracks, and high-resolution weather forecasting at orders of magnitude smaller computational expense than dedicated existing systems. With the ability to fine-tune Aurora to diverse application domains at only modest computational cost, Aurora represents significant progress in making actionable Earth system predictions accessible to anyone.",
      "url": "https://www.microsoft.com/en-us/research/publication/aurora-a-foundation-model-for-the-earth-system/"
    },
    {
      "title": "Building AI Agents for Autonomous Clouds: Challenges and Design Principles",
      "authors": [
        "Chetan Bansal",
        "Gagan Somashekar",
        "Jonathan Mace",
        "Manisha M Shetty",
        "Minghua Ma",
        "Pedro Las-Casas",
        "Saravan Rajmohan",
        "Shachee Mishra Gupta",
        "Suman Nath",
        "Xuchao Zhang",
        "Yinfang Chen",
        "Yogesh L. Simmhan"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Programming languages and software engineering",
        "Systems and networking"
      ],
      "publication_date": "November 2024",
      "abstract": "The rapid growth in the use of Large Language Models (LLMs) and AI Agents as part of software development and deployment is revolutionizing the information technology landscape. While code generation receives significant attention, a higher-impact application lies in using AI agents for operational resilience of cloud services, which currently require significant human effort and domain knowledge. There is a growing interest in AI for IT Operations (AIOps) which aims to automate complex operational tasks, like fault localization and root cause analysis, thereby reducing human intervention and customer impact. However, achieving the vision of autonomous and self-healing clouds through AIOps is hampered by the lack of standardized frameworks for building, evaluating, and improving AIOps agents. This vision paper lays the groundwork for such a framework by first framing the requirements and then discussing design decisions that satisfy them. We also propose AIOpsLab, a prototype implementation leveraging agent-cloud-interface that orchestrates an application, injects real-time faults using chaos engineering, and interfaces with an agent to localize and resolve the faults. We report promising results and lay the groundwork to build a modular and robust framework for building, evaluating, and improving agents for autonomous clouds.",
      "url": "https://www.microsoft.com/en-us/research/publication/building-ai-agents-for-autonomous-clouds-challenges-and-design-principles/"
    },
    {
      "title": "Rethinking the Switch Architecture for Stateful In-network Computing",
      "authors": [
        "Alberto Lerner",
        "Davide Zoni",
        "Gianni Antichi",
        "Paolo Costa"
      ],
      "research_areas": [
        "Systems and networking"
      ],
      "publication_date": "November 2024",
      "abstract": "Programmable switches are a disruptive technology that has seen increasing adoption in the past decade. Since their inception, however, there has been tension regarding how to design these switches. Classic programmable switches operate at line rate but impose significant limitations on the expressiveness of their programming models. In contrast, alternative designs relax the strict line rate requirement but are more easily programmable. The common belief is that a switch’s performance and its programmability are at odds.\nIn this paper, we argue that the tension is elsewhere. Many applications use the network to coordinate sets of flows known as coflows, while current switches are designed to be individual flow directors. We believe that this conceptual gap—the need to handle coflows rather than independent flows—is what prevents us from creating expressive and fast switch designs at once. We introduce a new device we call an Application-Defined Coflow Processor (ADCP) and discuss how it starts to bridge this gap.",
      "url": "https://www.microsoft.com/en-us/research/publication/rethinking-the-switch-architecture-for-stateful-in-network-computing/"
    },
    {
      "title": "Ontologically Faithful Generation of Non-Player Character Dialogues",
      "authors": [
        "Ben Van Durme",
        "Harsh Jhamtani",
        "Kellie Hill",
        "Nathaniel Weir",
        "Randolph D'Amore",
        "Ryan Thomas"
      ],
      "research_areas": [
        "Human language technologies"
      ],
      "publication_date": "November 2024",
      "abstract": "We introduce a language generation task grounded in a popular video game environment. KNUDGE (KNowledge Constrained User-NPC Dialogue GEneration) involves generating dialogue trees conditioned on an ontology captured in natural language passages providing quest and entity specifications. KNUDGE is constructed from side quest dialogues drawn directly from game data of Obsidian Entertainment’s The Outer Worlds, leading to real-world complexities in generation: (1) dialogues are branching trees as opposed to linear chains of utterances; (2) utterances must remain faithful to the game lore–character personas, backstories, and entity relationships; and (3) a dialogue must accurately reveal new quest-related details to the human player. We report results for supervised and in-context learning techniques, finding there is significant room for future work on creating realistic game-quality dialogues.\n",
      "url": "https://www.microsoft.com/en-us/research/publication/ontologically-faithful-generation-of-non-player-character-dialogues/"
    },
    {
      "title": "Triple Peak Day: Work Rhythms of Software Developers in Hybrid Work",
      "authors": [
        "Brian Houck",
        "Daniel McDuff",
        "Gonzalo Ramos",
        "Javier Hernandez",
        "Jina Suh",
        "Judith Amores",
        "Kael Rowan",
        "Mary Czerwinski",
        "Shamsi Iqbal",
        "Vedant Das Swain"
      ],
      "research_areas": [
        "Human-computer interaction",
        "Technology for emerging markets"
      ],
      "publication_date": "November 2024",
      "abstract": "The future of work is rapidly changing, with remote and hybrid settings blurring the boundaries between professional and personal life. To understand how work rhythms vary across different work settings, we conducted a month-long study of 65 software developers, collecting anonymized computer activity data as well as daily ratings for perceived stress, productivity, and work setting. In addition to confirming the double-peak pattern of activity at 10:00 am and 2:00 pm observed in prior research, we observed a significant third peak around 9:00 pm. This third peak was associated with higher perceived productivity during remote days but increased stress during onsite and hybrid days, highlighting a nuanced interplay between work demands and work settings. Additionally, we found strong correlations between computer activity, productivity, and stress, including an inverted U-shaped relationship where productivity peaked at around six hours of computer activity before declining on more active days. These findings provide new insights into evolving work rhythms and highlight the impact of different work settings on productivity and stress.\n",
      "url": "https://www.microsoft.com/en-us/research/publication/triple-peak-day-work-rhythms-of-software-developers-in-hybrid-work/"
    },
    {
      "title": "Artificial Intelligence–Based Copilots to Generate Causal Evidence",
      "authors": [
        "Ahmed Alaa",
        "Chris Holmes",
        "Emre Kiciman",
        "Mark van der Laan",
        "Maya Petersen"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Medical, health and genomics"
      ],
      "publication_date": "November 2024",
      "abstract": "While there is growing consensus that real-world data should play a larger role in generating causal evidence for health care, it is less clear whether and how AI can help. Current approaches to AI-driven analysis of health data are ill-equipped to account for the many threats to causal validity. However, the current human-reliant pipeline for causal analysis also falls short: analyses are complex, require multidisciplinary expertise, and are slow, labor-intensive and error-prone. Here, we speculate how a “human-in-the-loop” AI-based system could help relieve bottlenecks to high-quality causal analyses. We describe how an AI-based causal copilot, leveraging the formal inferential structure of the causal road map, could guide and support researchers through a structured process of translating a causal question into a hypothetical experiment; translating contextual knowledge into transparent and well-justified assumptions; designing, testing, and benchmarking a corresponding statistical analysis plan and code (including integration of machine learning on multimodal data); and supporting causal interpretation of results. Such a system could augment the speed and quality with which researchers conduct causal analyses with real-world data, improve transparency and verification of analyses and assumptions, and ultimately serve as a basis for point-of-care personalized decision support.",
      "url": "https://www.microsoft.com/en-us/research/publication/artificial-intelligence-based-copilots-to-generate-causal-evidence/"
    },
    {
      "title": "CosMAC: Constellation-Aware Medium Access and Scheuduling for IoT Satellites",
      "authors": [
        "Deepak Vasisth",
        "Jayanth Shenoy",
        "Om Chabra",
        "Ranveer Chandra",
        "Suraj Jog",
        "Tusher Chakraborty"
      ],
      "research_areas": [
        "Systems and networking"
      ],
      "publication_date": "November 2024",
      "abstract": "Pico-satellite (picosat) constellations aim to become the defacto connectivity solution for Internet of Things (IoT) devices. These constellations rely on a large number of small picosats and offer global plug-and-play connectivity at low data rates, without the need for Earth-based gateways. As picosat constellations scale, they run into new bottlenecks due to their traditional medium access designs optimized for single (or few) satellite operation. We present CosMAC – a new constellation-aware medium access and scheduling system for picosat networks. CosMAC includes a new overlap-aware medium access approach for uplink from IoT to picosats and a new network layer that schedules downlink traffic from satellites. We empirically evaluate CosMAC using measurements from three picosats and large scale trace-driven simulations for a 173 picosat network supporting 100k devices. Our results demonstrate that CosMAC can improve the over-all network throughput by 6.5× over prior state-of-the-art satellite medium access schemes.",
      "url": "https://www.microsoft.com/en-us/research/publication/cosmac-constellation-aware-medium-access-and-scheuduling-for-iot-satellites/"
    },
    {
      "title": "Dimensions of Generative AI Evaluation Design",
      "authors": [
        "Alex Chouldechova",
        "Alex Dow",
        "Chad Atalla",
        "Hanna Wallach",
        "Jennifer Wortman Vaughan",
        "Solon Barocas"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Social sciences"
      ],
      "publication_date": "November 2024",
      "abstract": "There are few principles or guidelines to ensure evaluations of generative AI (GenAI) models and systems are effective. To help address this gap, we propose a set of general dimensions that capture critical choices involved in GenAI evaluation design. These dimensions include the evaluation setting, the task type, the input source, the interaction style, the duration, the metric type, and the scoring method. By situating GenAI evaluations within these dimensions, we aim to guide decision-making during GenAI evaluation design and provide a structure for comparing different evaluations. We illustrate the utility of the proposed set of general dimensions using two examples: a hypothetical evaluation of the fairness of a GenAI system and three real-world GenAI evaluations of biological threats.",
      "url": "https://www.microsoft.com/en-us/research/publication/dimensions-of-generative-ai-evaluation-design/"
    },
    {
      "title": "SpotLight: Accurate, Explainable and Efficient  Anomaly Detection for Open RAN",
      "authors": [
        "Bozidar Radunovic",
        "Chuanhao Sun",
        "Mahesh K. Marina",
        "Molham Khoja",
        "Ujjwal Pawar",
        "Xenofon Foukas"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Data platforms and analytics",
        "Systems and networking"
      ],
      "publication_date": "November 2024",
      "abstract": "The Open RAN architecture, with disaggregated and virtualized RAN functions communicating over standardized interfaces, promises a diversified and multi-vendor RAN ecosystem. However, these same features contribute to increased operational complexity, making it highly challenging to troubleshoot RAN related performance issues and failures. Tackling this challenge requires a dependable, explainable anomaly detection method that Open RAN is currently lacking. To address this problem, we introduce SpotLight, a tailored system architecture with a distributed deep generative modeling based method running across the edge and cloud. SpotLight takes in a diverse, fine grained stream of metrics from the RAN and the platform, to continually detect and localize anomalies. It introduces a novel multi-stage generative model to detect potential anomalies at the edge using a light-weight algorithm, followed by anomaly confirmation and an explainability phase at the cloud, that helps identify the minimal set of KPIs that caused the anomaly. We evaluate SpotLight using the metrics collected from an enterprise-scale 5G Open RAN deployment in an indoor office building. Our results show that compared to a range of baseline methods, SpotLight yields significant gains in accuracy (13% higher F1 score), explainability (2.3 − 4× reduction in the number of reported KPIs) and efficiency (4 − 7× bandwidth reduction).",
      "url": "https://www.microsoft.com/en-us/research/publication/spotlight-accurate-explainable-and-efficient-anomaly-detection-for-open-ran/"
    },
    {
      "title": "RadPhi-3: Small Language Models for Radiology",
      "authors": [
        "Mercy Ranjit",
        "Shaury Srivastav",
        "Tanuja Ganu"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Medical, health and genomics"
      ],
      "publication_date": "November 2024",
      "abstract": "LLM based copilot assistants are useful in everyday tasks. There is a proliferation in the exploration of AI assistant use cases to support radiology workflows in a reliable manner. In this work, we present RadPhi-3, a Small Language Model instruction tuned from Phi-3-mini-4k-instruct with 3.8B parameters to assist with various tasks in radiology workflows. While impression summary generation has been the primary task which has been explored in prior works w.r.t radiology reports of Chest X-rays, we also explore other useful tasks like change summary generation comparing the current radiology report and its prior report, section extraction from radiology reports, tagging the reports with various pathologies and tubes, lines or devices present in them etc. In-addition, instruction tuning RadPhi-3 involved learning from a credible knowledge source used by radiologists, Radiopaedia.org. RadPhi-3 can be used both to give reliable answers for radiology related queries as well as perform useful tasks related to radiology reports. RadPhi-3 achieves SOTA results on the RaLEs radiology report generation benchmark.",
      "url": "https://www.microsoft.com/en-us/research/publication/radphi-3-small-language-models-for-radiology/"
    },
    {
      "title": "Towards Measuring and Modeling “Culture” in LLMs: A Survey",
      "authors": [
        "Alham Fikri Aji",
        "Ashutosh Dwivedi",
        "Ashutosh Modi",
        "Jacki O'Neill",
        "M. Choudhury",
        "Muhammad Farid Adilazuarda",
        "Pradhyumna Lavania",
        "Sagnik Mukherjee",
        "Siddhant Singh"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Human-computer interaction"
      ],
      "publication_date": "November 2024",
      "abstract": "We present a survey of more than 90 recent papers that aim to study cultural representation and inclusion in large language models (LLMs). We observe that none of the studies explicitly define”culture, which is a complex, multifaceted concept; instead, they probe the models on some specially designed datasets which represent certain aspects of”culture”. We call these aspects the proxies of culture, and organize them across two dimensions of demographic and semantic proxies. We also categorize the probing methods employed. Our analysis indicates that only certain aspects of “culture,” such as values and objectives, have been studied, leaving several other interesting and important facets, especially the multitude of semantic domains (Thompson et al., 2020) and aboutness (Hershcovich et al., 2022), unexplored. Two other crucial gaps are the lack of robustness of probing techniques and situated studies on the impact of cultural mis- and under-representation in LLM-based applications.",
      "url": "https://www.microsoft.com/en-us/research/publication/towards-measuring-and-modeling-culture-in-llms-a-survey/"
    },
    {
      "title": "Dukawalla: Voice Interfaces for Small Businesses in Africa",
      "authors": [
        "Elizabeth Ankrah",
        "Jacki O'Neill",
        "Kagonya Awori",
        "Mark Kariuki",
        "Mercy Muchai",
        "Millicent Ochieng",
        "Stephanie Nyairo"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Human-computer interaction"
      ],
      "publication_date": "November 2024",
      "abstract": "Small and medium-sized businesses (SMBs) often struggle with data-driven decision-making due to a lack of advanced analytics tools, especially in African countries where they make up majority of the workforce. Though many tools exist they are not designed to fit into the ways of working of SMB workers who are mobile-first, have limited time to learn new workflows, and for whom social and business are tightly coupled. To address this, the Dukawalla prototype was created. This intelligent assistant bridges the gap between raw business data and actionable insights by leveraging voice interaction and the power of generative AI. Dukawalla provides an intuitive way for business owners to interact with their data, aiding in informed decision-making. This paper examines Dukawalla’s deployment across SMBs in Nairobi, focusing on their experiences using this voice-based assistant to streamline data collection and provide business insights.",
      "url": "https://www.microsoft.com/en-us/research/publication/dukawalla-voice-interfaces-for-small-businesses-in-africa/"
    },
    {
      "title": "Learning to Retrieve Iteratively for In-Context Learning",
      "authors": [
        "Ben Van Durme",
        "Harsh Jhamtani",
        "Jason Eisner",
        "Patrick Xia",
        "Richard Shin",
        "Tongfei Chen",
        "Yunmo Chen"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Human language technologies"
      ],
      "publication_date": "November 2024",
      "abstract": "We introduce iterative retrieval, a novel framework that empowers retrievers to make iterative decisions through policy optimization. Finding an optimal portfolio of retrieved items is a combinatorial optimization problem, generally considered NP-hard. This approach provides a learned approximation to such a solution, meeting specific task requirements under a given family of large language models (LLMs). We propose a training procedure based on reinforcement learning, incorporating feedback from LLMs. We instantiate an iterative retriever for composing in-context learning (ICL) exemplars and apply it to various semantic parsing tasks that demand synthesized programs as outputs. By adding only 4M additional parameters for state encoding, we convert an off-the-shelf dense retriever into a stateful iterative retriever, outperforming previous methods in selecting ICL exemplars on semantic parsing datasets such as CalFlow, TreeDST, and MTOP. Additionally, the trained iterative retriever generalizes across different inference LLMs beyond the one used during training.",
      "url": "https://www.microsoft.com/en-us/research/publication/learning-to-retrieve-iteratively-for-in-context-learning/"
    },
    {
      "title": "SilvanForge: A Schedule-Guided Retargetable Compiler for Decision Tree Inference",
      "authors": [
        "Ashwin Prasad",
        "Kaushik Rajan",
        "R Govindarajan",
        "Sampath Rajendra",
        "Uday Bondhugula"
      ],
      "research_areas": [
        "Data platforms and analytics",
        "Programming languages and software engineering",
        "Systems and networking"
      ],
      "publication_date": "November 2024",
      "abstract": "The proliferation of machine learning together with the rapid evolution of the hardware ecosystem has led to a surge in the demand for model inference on a variety of hardware. Decision tree based models are the most popular models on tabular data. This paper is motivated by the problems encountered when targeting inference of these models to run at peak performance on CPU and GPU targets. Existing solutions are neither portable nor achieve the best possible performance for the specific hardware they target.\nThis paper describes SilvanForge, a schedule-guided, retargetable compiler for decision tree based models that searches over several optimization choices and automatically generates high-performance inference routines for CPUs and GPUs. SilvanForge has two core components. The first is a scheduling language that encapsulates the optimization space, and techniques to efficiently explore this space. The second is an optimizing retargetable compiler that can generate code for any specified schedule. SilvanForge’s ability to use different data layouts, loop structures and caching strategies enables it to achieve portable performance across a range of targets.\nSilvanForge generated code is an order of magnitude faster than XGBoost and about 2-5x faster on average than RAPIDS FIL and Tahoe over several batch sizes. While these systems only target NVIDIA GPUs, SilvanForge achieves competent performance on AMD GPUs as well.",
      "url": "https://www.microsoft.com/en-us/research/publication/silvanforge-a-schedule-guided-retargetable-compiler-for-decision-tree-inference/"
    },
    {
      "title": "Symbolic Prompt Program Search: A Structure-Aware Approach to Efficient Compile-Time Prompt Optimization",
      "authors": [
        "Jennifer Neville",
        "Tobias Schnabel"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "November 2024",
      "abstract": "In many modern LLM applications, such as retrieval augmented generation, prompts have become programs themselves. In these settings, prompt programs are repeatedly called with different user queries or data instances. A big practical challenge is optimizing such prompt programs. Recent work has mostly focused on either simple prompt programs or assumed that the general structure of a prompt program is fixed. We introduce SAMMO, a framework to perform symbolic prompt program search for compile-time optimizations of prompt programs. SAMMO represents prompt programs on a symbolic level which allows for a rich set of transformations that can be searched over during optimization. We show that SAMMO generalizes previous methods and improves the performance of complex prompts on (1) instruction tuning, (2) RAG pipeline tuning, and (3) prompt compression, across several different LLMs. We make all code available open-source at https://github.com/microsoft/sammo .",
      "url": "https://www.microsoft.com/en-us/research/publication/symbolic-prompt-program-search-a-structure-aware-approach-to-efficient-compile-time-prompt-optimization/"
    },
    {
      "title": "FlexNN: Efficient and Adaptive DNN Inference on Memory-Constrained Edge Devices",
      "authors": [
        "Ting Cao",
        "Xiangyu Li",
        "Yuanchun Li",
        "Yuanzhe Li",
        "Yunxin Liu"
      ],
      "research_areas": [
        "Systems and networking"
      ],
      "publication_date": "November 2024",
      "abstract": "Due to the popularity of deep neural networks (DNNs) and considerations over network overhead, data privacy, and inference latency, there is a growing interest in deploying DNNs to edge devices in recent years. However, the limited memory becomes a major bottleneck for on-device DNN deployment, making it crucial to reduce the memory footprint of DNN. The mainstream model customization solutions require intensive deployment efforts and may lead to severe accuracy degradation, and existing deep learning (DL) frameworks don’t take memory as a priority. Besides, recent works to enhance the memory management scheme cannot be directly applied because of several challenges, including the unbalanced memory footprint across layers, the inevitable overhead of memory management, and the memory budget dynamicity. To tackle these challenges, we introduce FlexNN, an efficient and adaptive memory management framework for DNN inference on memory-constrained devices. FlexNN uses a slicing-loading-computing joint planning approach, to achieve optimal memory utilization and minimal memory management overhead. We implemented FlexNN atop NCNN, and conducted comprehensive evaluations with common model architectures on various devices. The results have shown that our approach is able to adapt to different memory constraints with optimal latency-memory trade-offs. For example, FlexNN can reduce the memory consumption by 93.81% with only a 3.64% increase in latency, as compared with the original NCNN on smartphones.",
      "url": "https://www.microsoft.com/en-us/research/publication/flexnn-efficient-and-adaptive-dnn-inference-on-memory-constrained-edge-devices/"
    },
    {
      "title": "Pearl: Personalizing Large Language Model Writing Assistants with Generation-Calibrated Retrievers",
      "authors": [
        "Bahar Sarrafzadeh",
        "Emmanuel Barajas Gonzalez",
        "Jennifer Neville",
        "Longqi Yang",
        "Mengting Wan",
        "Sheshera Mysore",
        "Steve Menezes",
        "Tara Safavi",
        "Tina Baghaee",
        "Zhuoran Lu"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Human language technologies"
      ],
      "publication_date": "November 2024",
      "abstract": "Powerful large language models have facilitated the development of writing assistants that promise to significantly improve the quality and efficiency of composition and communication. However, a barrier to effective assistance is the lack of personalization in LLM outputs to the author’s communication style, specialized knowledge, and values. In this paper, we address this challenge by proposing Pearl, a LLM writing assistant personalized with a retriever that is trained to be generation-calibrated for personalization. Generation calibration ensures that our retriever selects historic user authored documents to augment an LLM prompt such that they are likely to help an LLM generation better adhere to a users’ preferences. We propose two key novelties for training such a retriever: (1) A training data selection method that identifies user requests likely to benefit from personalization and documents that provide that benefit; and (2) A scale-calibrating KL-divergence objective that ensures that our retriever scores remain proportional to the downstream generation quality from using the document for personalized generation. In a series of holistic evaluations, we demonstrate the effectiveness of Pearl in generating long-form texts on multiple social media datasets. Finally, we demonstrate how a generation-calibrated retriever can double as a performance predictor – detecting low quality retrieval, and improving potentially under-performing outputs via revision with LLMs.",
      "url": "https://www.microsoft.com/en-us/research/publication/pearl-personalizing-large-language-model-writing-assistants-with-generation-calibrated-retrievers/"
    },
    {
      "title": "Evaluating Generative AI Systems is a Social Science Measurement Challenge",
      "authors": [
        "A. Feder Cooper",
        "Abigail Z. Jacobs",
        "Alex Chouldechova",
        "Alex Dow",
        "Alexandra Olteanu",
        "Angelina Wang",
        "Chad Atalla",
        "Dan Vann",
        "Emily Corvi",
        "Emily Sheng",
        "Hanna Wallach",
        "Hannah Washington",
        "Jean Garcia-Gathright",
        "Jennifer Wortman Vaughan",
        "Matthew Vogel",
        "Meera Desai",
        "Nick Pangakis",
        "Solon Barocas",
        "Stefanie Reed",
        "Su Lin Blodgett"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Social sciences"
      ],
      "publication_date": "November 2024",
      "abstract": "Across academia, industry, and government, there is an increasing awareness that the measurement tasks involved in evaluating generative AI (GenAI) systems are especially difficult. We argue that these measurement tasks are highly reminiscent of measurement tasks found throughout the social sciences. With this in mind, we present a framework, grounded in measurement theory from the social sciences, for measuring concepts related to the capabilities, impacts, opportunities, and risks of GenAI systems. The framework distinguishes between four levels: the background concept, the systematized concept, the measurement instrument(s), and the instance-level measurements themselves. This four-level approach differs from the way measurement is typically done in ML, where researchers and practitioners appear to jump straight from background concepts to measurement instruments, with little to no explicit systematization in between. As well as surfacing assumptions, thereby making it easier to understand exactly what the resulting measurements do and do not mean, this framework has two important implications for evaluating evaluations: First, it can enable stakeholders from different worlds to participate in conceptual debates, broadening the expertise involved in evaluating GenAI systems. Second, it brings rigor to operational debates by offering a set of lenses for interrogating the validity of measurement instruments and their resulting measurements.",
      "url": "https://www.microsoft.com/en-us/research/publication/evaluating-generative-ai-systems-is-a-social-science-measurement-challenge/"
    },
    {
      "title": "LORASC: Expressive and Generalizable Low-rank Adaptation for Large Models via Slow Cascaded Learning",
      "authors": [
        "Fangyun Wei",
        "Lili Qiu",
        "Yifan Yang",
        "Yifei Shen",
        "Yuqing Yang"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "November 2024",
      "abstract": "Efficient fine-tuning plays a fundamental role in modern large models, with low-rank adaptation emerging as a particularly promising approach. However, the existing variants of LoRA are hampered by limited expressiveness, a tendency to overfit, and sensitivity to hyperparameter settings. This paper presents LoRA Slow Cascade Learning (LoRASC), an innovative technique designed to enhance LoRA’s expressiveness and generalization capabilities while preserving its training efficiency. Our approach augments expressiveness through a cascaded learning strategy that enables a mixture-of-low-rank adaptation, thereby increasing the model’s ability to capture complex patterns. Additionally, we introduce a slow-fast update mechanism and cascading noisy tuning to bolster generalization. The extensive experiments on various language and vision datasets, as well as robustness benchmarks, demonstrate that the proposed method not only significantly outperforms existing baselines, but also mitigates overfitting, enhances model stability, and improves OOD robustness.",
      "url": "https://www.microsoft.com/en-us/research/publication/lorasc-expressive-and-generalizable-low-rank-adaptation-for-large-models-via-slow-cascaded-learning/"
    },
    {
      "title": "Hearable devices with sound bubbles",
      "authors": [
        "Malek Itani",
        "Sefik Emre Eskimez",
        "Shyamnath Gollakota",
        "Takuya Yoshioka",
        "Tuochao Chen"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Audio and Acoustics"
      ],
      "publication_date": "November 2024",
      "abstract": "The human auditory system has a limited ability to perceive distance and distinguish speakers in crowded settings. A headset technology that can create a sound bubble in which all speakers within the bubble are audible but speakers and noise outside the bubble are suppressed could augment human hearing. However, developing such technology is challenging. Here, we report an intelligent headset system capable of creating sound bubbles. The system is based on real-time neural networks that use acoustic data from up to six microphones integrated into noise-cancelling headsets and are run on the device, processing 8 ms audio chunks in 6.36 ms on an embedded central processing unit. Our neural networks can generate sound bubbles with programmable radii between 1 m and 2 m, and with output signals that reduce the intensity of sounds outside the bubble by 49 dB. With previously unseen environments and wearers, our system can focus on up to two speakers within the bubble, with one to two interfering speakers and noise outside the bubble.",
      "url": "https://www.microsoft.com/en-us/research/publication/hearable-devices-with-sound-bubbles/"
    },
    {
      "title": "An evaluation of online information acquisition in US news deserts",
      "authors": [
        "Alonso Guevara Fernández",
        "Jacob N. Shapiro",
        "Kevin T. Greene",
        "Nathan Evans",
        "Nilima Pisharody"
      ],
      "research_areas": [
        "Social sciences"
      ],
      "publication_date": "November 2024",
      "abstract": "A growing concern is that as local newspapers disappear, communities lose trusted gatekeepers and develop information voids, creating openings for misinformation to thrive. Previous work has not evaluated whether residents of news deserts have developed different information acquisition habits. We fill this gap by directly comparing information consumption and referral patterns inside and outside of news deserts in a novel dataset of engagement with online media by millions of users on the Edge browser. We find little evidence that those in news deserts consume more low-quality sites or are more likely to be referred to low-quality sites from search engines or social media. We find some evidence that those in news deserts do consume more national news than locations with local media outlets. These results contribute to our understanding of how the loss of local newspapers has impacted online information acquisition.",
      "url": "https://www.microsoft.com/en-us/research/publication/an-evaluation-of-online-information-acquisition-in-us-news-deserts/"
    },
    {
      "title": "SpreadsheetLLM: Encoding Spreadsheets for Large Language Models",
      "authors": [
        "Dongmei Zhang",
        "Haoyu Dong",
        "Jianbo Zhao",
        "José Cambronero",
        "Junyu Xiong",
        "Mengyu Zhou",
        "Shi Han",
        "Yeye He",
        "Yun Lin",
        "Yuzhang Tian"
      ],
      "research_areas": [
        "Data platforms and analytics"
      ],
      "publication_date": "November 2024",
      "abstract": "Spreadsheets are characterized by their extensive two-dimensional grids, flexible layouts, and varied formatting options, which pose significant challenges for large language models (LLMs). In response, we introduce SheetEncoder, pioneering an efficient encoding method designed to unleash and optimize LLMs’ powerful understanding and reasoning capability on spreadsheets. Initially, we propose a vanilla serialization approach that incorporates cell addresses, values, and formats. However, this approach was limited by LLMs’ token constraints, making it impractical for most applications. To tackle this challenge, three innovative modules are proposed to compress spreadsheets effectively: structural-anchor-based compression, inverse index translation, and data-format-aware aggregation. It significantly improves performance in spreadsheet table detection task, outperforming the vanilla approach by 25.6\\% in GPT4’s in-context learning setting. Moreover, fine-tuned LLM with SheetEncoder has an average compression ratio of 25×, but achieves a state-of-the-art 78.9\\% F1 score, surpassing the best existing models by 12.3\\%, demonstrating that SheetEncoder greatly boosts LLMs’s performance on spreadsheet data.",
      "url": "https://www.microsoft.com/en-us/research/publication/encoding-spreadsheets-for-large-language-models/"
    },
    {
      "title": "(De)Noise: Moderating the Inconsistency Between Human Decision-Makers",
      "authors": [
        "Jennifer Wortman Vaughan",
        "Junaid Ali",
        "Krishna P. Gummadi",
        "Nina Grgić-Hlača"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Human-computer interaction"
      ],
      "publication_date": "November 2024",
      "abstract": "Prior research in psychology has found that people’s decisions are often inconsistent. An individual’s decisions vary across time, and decisions vary even more across people. Inconsistencies have been identified not only in subjective matters, like matters of taste, but also in settings one might expect to be more objective, such as sentencing, job performance evaluations, or real estate appraisals. In our study, we explore whether algorithmic decision aids can be used to moderate the degree of inconsistency in human decision-making in the context of real estate appraisal. In a large-scale human-subject experiment, we study how different forms of algorithmic assistance influence the way that people review and update their estimates of real estate prices. We find that both (i) asking respondents to review their estimates in a series of algorithmically chosen pairwise comparisons and (ii) providing respondents with traditional machine advice are effective strategies for influencing human responses. Compared to simply reviewing initial estimates one by one, the aforementioned strategies lead to (i) a higher propensity to update initial estimates, (ii) a higher accuracy of post-review estimates, and (iii) a higher degree of consistency between the post-review estimates of different respondents. While these effects are more pronounced with traditional machine advice, the approach of reviewing algorithmically chosen pairs can be implemented in a wider range of settings, since it does not require access to ground truth data.",
      "url": "https://www.microsoft.com/en-us/research/publication/denoise-moderating-the-inconsistency-between-human-decision-makers/"
    },
    {
      "title": "General framework for online-to-nonconvex conversion: Schedule-free SGD is also effective for nonconvex optimization",
      "authors": [
        "Ashok Cutkosky",
        "Gagik Magakyan",
        "Kwangjun Ahn"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "November 2024",
      "abstract": "This work investigates the effectiveness of schedule-free methods, developed by A. Defazio et al. (NeurIPS 2024), in nonconvex optimization settings, inspired by their remarkable empirical success in training neural networks. Specifically, we show that schedule-free SGD achieves optimal iteration complexity for nonsmooth, nonconvex optimization problems. Our proof begins with the development of a general framework for online-to-nonconvex conversion, which converts a given online learning algorithm into an optimization algorithm for nonconvex losses. Our general framework not only recovers existing conversions but also leads to two novel conversion schemes. Notably, one of these new conversions corresponds directly to schedule-free SGD, allowing us to establish its optimality. Additionally, our analysis provides valuable insights into the parameter choices for schedule-free SGD, addressing a theoretical gap that the convex theory cannot explain.",
      "url": "https://www.microsoft.com/en-us/research/publication/general-framework-for-online-to-nonconvex-conversion-schedule-free-sgd-is-also-effective-for-nonconvex-optimization/"
    },
    {
      "title": "ASL STEM Wiki: Dataset and Benchmark for Interpreting STEM Articles",
      "authors": [
        "Alex Lu",
        "Chinmay Singh",
        "Cyril Zhang",
        "Danielle Bragg",
        "Fyodor O. Minakov",
        "Hal Daumé",
        "Kayo Yin",
        "Vanessa Milan"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Human language technologies"
      ],
      "publication_date": "November 2024",
      "abstract": "Deaf and hard-of-hearing (DHH) students face significant barriers in accessing science, technology, engineering, and mathematics (STEM) education, notably due to the scarcity of STEM resources in signed languages. To help address this, we introduce ASL STEM Wiki: a parallel corpus of 254 Wikipedia articles on STEM topics in English, interpreted into over 300 hours of American Sign Language (ASL). ASL STEM Wiki is the first continuous signing dataset focused on STEM, facilitating the development of AI resources for STEM education in ASL.We identify several use cases of ASL STEM Wiki with human-centered applications. For example, because this dataset highlights the frequent use of fingerspelling for technical concepts, which inhibits DHH students’ ability to learn,we develop models to identify fingerspelled words—which can later be used to query for appropriate ASL signs to suggest to interpreters.",
      "url": "https://www.microsoft.com/en-us/research/publication/asl-stem-wiki-dataset-and-benchmark-for-interpreting-stem-articles/"
    },
    {
      "title": "Generative Adapter: Contextualizing Language Models in Parameters with A Single Forward Pass",
      "authors": [
        "Ben Van Durme",
        "Hao Cheng",
        "Hao Fang",
        "Jianfeng Gao",
        "Luke Zettlemoyer",
        "Patrick Xia",
        "Tong Chen",
        "Xiaodong Liu"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "November 2024",
      "abstract": "Large language models (LMs) are typically adapted to improve performance on new contexts (\\eg text prompts that define new tasks or domains) through fine-tuning or prompting. However, there is an accuracy compute tradeoff — fine-tuning incurs significant training cost and prompting increases inference overhead. We introduce $GenerativeAdapter$, an effective and efficient adaptation method that directly maps new contexts to low-rank LM adapters, thereby significantly reducing inference overhead with no need for finetuning. The adapter generator is trained via self-supervised learning, and can be used to adapt a single frozen LM for any new task simply by mapping the associated task or domain context to a new adapter. We apply $GenerativeAdapter$ to two pretrained LMs (Mistral-7B-Instruct and Llama2-7B-Chat) and evaluate the adapted models in three adaption scenarios: knowledge acquisition from documents, learning from demonstrations, and personalization for users. In StreamingQA, our approach is effective in injecting knowledge into the LM’s parameters, achieving a 63.5% improvement in F1 score over the model with supervised fine-tuning (from $19.5$ to $31.5$) for contexts as long as 32K tokens. In the MetaICL in-context learning evaluation, our method achieves an average accuracy of $44.9$ across 26 tasks, outperforming the base model. On MSC, our method proves to be highly competitive in memorizing user information from conversations with a 4x reduction in computation and memory costs compared to prompting with full conversation history. Together, these results suggest that $GenerativeAdapter$ should allow for general adaption to a wide range of different contexts.",
      "url": "https://www.microsoft.com/en-us/research/publication/generative-adapter-contextualizing-language-models-in-parameters-with-a-single-forward-pass/"
    },
    {
      "title": "Hybridge: Bridging Spatiality for Inclusive and Equitable Hybrid Meetings",
      "authors": [
        "Abigail Sellen",
        "Andrew D. Wilson",
        "Becky Spittle",
        "Bill Buxton",
        "John Tang",
        "Kori Inkpen",
        "Lev Tankelevitch",
        "Pat Sweeney",
        "Payod Panda",
        "Qianqian Qi",
        "Sasa Junuzovic",
        "Sean Rintel"
      ],
      "research_areas": [
        "Human-computer interaction"
      ],
      "publication_date": "November 2024",
      "abstract": "Hybrid meetings limit inclusion for remote participants. The Hybridge experimental system provides different interfaces for remote and room endpoints, focusing on improving inclusion via shared spatiality and remote agency. In-room participants see remotes on displays around a table, and remotes see video integrated into a digital twin. Remotes can choose where to appear and from where they view the room. We tested Hybridge in a within-subjects study of group survival tasks. An in-person condition was followed by a counterbalanced order of hybrid traditional videoconferencing (“Gallery”) and Hybridge. We found that co-presence and agency differences between in-room and remotes were alleviated in Hybridge but remained in Gallery. Physical presence for remotes was higher in Hybridge than Gallery. Conversation flow was better in Hybridge than Gallery, but ease of awareness was not different. We argue that asymmetry should be embraced when designing hybrid meeting systems, with inclusivity achieved by tailoring features for the needs of different endpoints.",
      "url": "https://www.microsoft.com/en-us/research/publication/hybridge-bridging-spatiality-for-inclusive-and-equitable-hybrid-meetings/"
    },
    {
      "title": "BitNet a4.8: 4-bit Activations for 1-bit LLMs",
      "authors": [
        "Furu Wei",
        "Hongyu Wang",
        "Shuming Ma"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "November 2024",
      "abstract": "Recent research on the 1-bit Large Language Models (LLMs), such as BitNet b1.58, presents a promising direction for reducing the inference cost of LLMs while maintaining their performance. In this work, we introduce BitNet a4.8, enabling 4-bit activations for 1-bit LLMs. BitNet a4.8 employs a hybrid quantization and sparsification strategy to mitigate the quantization errors introduced by the outlier channels. Specifically, we utilize 4-bit activations for inputs to the attention and feed-forward network layers, while sparsifying intermediate states followed with 8-bit quantization. Extensive experiments demonstrate that BitNet a4.8 achieves performance comparable to BitNet b1.58 with equivalent training costs, while being faster in inference with enabling 4-bit (INT4/FP4) kernels. Additionally, BitNet a4.8 activates only 55% of parameters and supports 3-bit KV cache, further enhancing the efficiency of large-scale LLM deployment and inference.",
      "url": "https://www.microsoft.com/en-us/research/publication/bitnet-a4-8-4-bit-activations-for-1-bit-llms/"
    },
    {
      "title": "Global Structure-from-Motion Revisited",
      "authors": [
        "Dániel Baráth",
        "Johannes L. Schönberger",
        "Linfei Pan",
        "Marc Pollefeys"
      ],
      "research_areas": [
        "Computer vision"
      ],
      "publication_date": "November 2024",
      "abstract": "Recovering 3D structure and camera motion from images has been a long-standing focus of computer vision research and is known as Structure-from-Motion (SfM). Solutions to this problem are categorized into incremental and global approaches. Until now, the most popular systems follow the incremental paradigm due to its superior accuracy and robustness, while global approaches are drastically more scalable and efficient.  With this work, we revisit the problem of global SfM and propose GLOMAP as a new general-purpose system that outperforms the state of the art in global SfM. In terms of accuracy and robustness, we achieve results on-par or superior to COLMAP, the most widely used incremental SfM, while being orders of magnitude faster. We share our system as an open-source implementation at https://github.com/colmap/glomap (opens in new tab).",
      "url": "https://www.microsoft.com/en-us/research/publication/global-structure-from-motion-revisited/"
    },
    {
      "title": "Counter-Empirical Attacking based on Adversarial Reinforcement Learning for Time-Relevant Scoring System",
      "authors": [
        "Bo Qiao",
        "Hang Dong",
        "Hong Cheng",
        "Qingwei Lin 林庆维",
        "Si Qin",
        "Xiangguo Sun"
      ],
      "research_areas": [
        "Algorithms",
        "Artificial intelligence",
        "Data platforms and analytics",
        "Programming languages and software engineering",
        "Systems and networking"
      ],
      "publication_date": "November 2024",
      "abstract": "Scoring systems are commonly seen for platforms in the era of big data. From credit scoring systems in financial services to membership scores in E-commerce shopping platforms, platform managers use such systems to guide users towards the encouraged activity pattern, and manage resources more effectively and more efficiently thereby. To establish such scoring systems, several”empirical criteria”are firstly determined, followed by dedicated top-down design for each factor of the score, which usually requires enormous effort to adjust and tune the scoring function in the new application scenario. What’s worse, many fresh projects usually have no ground-truth or any experience to evaluate a reasonable scoring system, making the designing even harder. To reduce the effort of manual adjustment of the scoring function in every new scoring system, we innovatively study the scoring system from the preset empirical criteria without any ground truth, and propose a novel framework to improve the system from scratch. In this paper, we propose a”counter-empirical attacking”mechanism that can generate”attacking”behavior traces and try to break the empirical rules of the scoring system. Then an adversarial”enhancer”is applied to evaluate the scoring system and find the improvement strategy. By training the adversarial learning problem, a proper scoring function can be learned to be robust to the attacking activity traces that are trying to violate the empirical criteria. Extensive experiments have been conducted on two scoring systems including a shared computing resource platform and a financial credit system. The experimental results have validated the effectiveness of our proposed framework.",
      "url": "https://www.microsoft.com/en-us/research/publication/counter-empirical-attacking-based-on-adversarial-reinforcement-learning-for-time-relevant-scoring-system/"
    },
    {
      "title": "Unearthing Semantic Checks for Cloud Infrastructure-as-Code Programs",
      "authors": [
        "Ang Chen",
        "Patrick Kon",
        "Ryan Beckett",
        "Yiming Qiu"
      ],
      "research_areas": [
        "Programming languages and software engineering",
        "Systems and networking"
      ],
      "publication_date": "November 2024",
      "abstract": "Cloud infrastructures are increasingly managed by Infrastructure-as-Code (IaC) frameworks (e.g., Terraform). IaC frameworks enable cloud users to configure their resources in a declarative manner, without having to directly work with low-level cloud API calls. However, with today’s IaC tooling, IaC programs that pass the compilation phase may still incur errors at deployment time, resulting in significant disruption. We observe that this stems from a fundamental semantic gap between IaC-level programs and cloud-level requirements—even a syntactically correct IaC program may violate cloud-level expectations. To bridge this gap, we develop Zodiac, a tool that can unearth IaC-level semantic checks on cloud-level requirements. It provides an automated pipeline to mine these checks from online IaC repositories and validate them using deployment-based testing. We have applied Zodiac to Terraform resources offered by Microsoft Azure—a leading IaC framework and a leading cloud vendor—where it found 500+ semantic checks where violation would produce deployment failures. With these checks, we have identified 200+ buggy Terraform projects and helped fix errors within official Azure provider usage examples.",
      "url": "https://www.microsoft.com/en-us/research/publication/unearthing-semantic-checks-for-cloud-infrastructure-as-code-programs/"
    },
    {
      "title": "Protein generation with evolutionary diffusion: sequence is all you need",
      "authors": [
        "Alan M. Moses",
        "Alex Lu",
        "Ava P. Amini",
        "Kevin Kaichuang Yang",
        "Neil Tenenholtz",
        "Nicolo Fusi",
        "Nitya Thakkar",
        "Rianne van den Berg",
        "Robert Strome",
        "Sarah Alamdari"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Medical, health and genomics"
      ],
      "publication_date": "November 2024",
      "abstract": "Deep generative models are increasingly powerful tools for the in silico design of novel proteins. Recently, a family of generative models called diffusion models has demonstrated the ability to generate biologically plausible proteins that are dissimilar to any actual proteins seen in nature, enabling unprecedented capability and control in de novo protein design. However, current state-of-the-art diffusion models generate protein structures, which limits the scope of their training data and restricts generations to a small and biased subset of protein design space. Here, we introduce a general-purpose diffusion framework, EvoDiff, that combines evolutionary-scale data with the distinct conditioning capabilities of diffusion models for controllable protein generation in sequence space. EvoDiff generates high-fidelity, diverse, and structurally-plausible proteins that cover natural sequence and functional space. We show experimentally that EvoDiff generations express, fold, and exhibit expected secondary structure elements. Critically, EvoDiff can generate proteins inaccessible to structure-based models, such as those with disordered regions, while maintaining the ability to design scaffolds for functional structural motifs. We validate the universality of our sequence-based formulation by experimentally characterizing intrinsically-disordered mitochondrial targeting signals, metal-binding proteins, and protein binders designed using EvoDiff. We envision that EvoDiff will expand capabilities in protein engineering beyond the structure-function paradigm toward programmable, sequence-first design.",
      "url": "https://www.microsoft.com/en-us/research/publication/protein-generation-with-evolutionary-diffusion-sequence-is-all-you-need/"
    },
    {
      "title": "Ab initio characterization of protein molecular dynamics with AI2BMD",
      "authors": [
        "Bin Shao",
        "Chaoran Cheng",
        "Haiguang Liu",
        "He Zhang",
        "Jiawei Meng",
        "Mingyu Li",
        "Ran Bi",
        "Shaoning Li",
        "Tie-Yan Liu",
        "Tong Wang",
        "Xiangzhen Shen",
        "Xinheng He",
        "Yatao Li",
        "Yusong Wang",
        "Zun Wang"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Medical, health and genomics"
      ],
      "publication_date": "November 2024",
      "abstract": "Biomolecular dynamics simulation is a fundamental technology for life sciences research, and its usefulness depends on its accuracy and efficiency. Classical molecular dynamics simulation is fast but lacks chemical accuracy. Quantum chemistry methods such as density functional theory can reach chemical accuracy but cannot scale to support large biomolecules. Here we introduce an artificial intelligence-based ab initio biomolecular dynamics system (AI2BMD) that can efficiently simulate full-atom large biomolecules with ab initio accuracy. AI2BMD uses a protein fragmentation scheme and a machine learning force field to achieve generalizable ab initio accuracy for energy and force calculations for various proteins comprising more than 10,000 atoms. Compared to density functional theory, it reduces the computational time by several orders of magnitude. With several hundred nanoseconds of dynamics simulations, AI2BMD demonstrated its ability to efficiently explore the conformational space of peptides and proteins, deriving accurate 3J couplings that match nuclear magnetic resonance experiments, and showing protein folding and unfolding processes. Furthermore, AI2BMD enables precise free-energy calculations for protein folding, and the estimated thermodynamic properties are well aligned with experiments. AI2BMD could potentially complement wet-lab experiments, detect the dynamic processes of bioactivities and enable biomedical research that is impossible to conduct at present.",
      "url": "https://www.microsoft.com/en-us/research/publication/ab-initio-characterization-of-protein-molecular-dynamics-with-ai2bmd/"
    },
    {
      "title": "Verus: A Practical Foundation for Systems Verification",
      "authors": [
        "Andrea Lattuada",
        "Bryan Parno",
        "Chanhee Cho",
        "Chris Hawblitzel",
        "Hayley LeBlanc",
        "Jay Bosamiya",
        "Jay Lorch",
        "Jon Howell",
        "Matthias Brun",
        "Oded Padon",
        "Pranav Srinivasan",
        "Reto Achermann",
        "Tej Chajed",
        "Travis Hance"
      ],
      "research_areas": [
        "Programming languages and software engineering",
        "Systems and networking"
      ],
      "publication_date": "November 2024",
      "abstract": "Formal verification is a promising approach to eliminate bugs at compile time, before they ship. Indeed, our community has verified a wide variety of system software. However, much of this success has required heroic developer effort, relied on bespoke logics for individual domains, or sacrificed expressiveness for powerful proof automation.\nBuilding on prior work on Verus, we aim to enable faster, cheaper verification of rich properties for realistic systems. We do so by integrating and optimizing the best choices from prior systems, tuning our design to overcome barriers encountered in those systems, and introducing novel techniques.\nWe evaluate Verus’s effectiveness with a wide variety of case-study systems, including distributed systems, an OS page table, a library for NUMA-aware concurrent data structure replication, a crash-safe storage system, and a concurrent memory allocator, together comprising 6.1K lines of implementation and 31K lines of proof. Verus verifies code 3–61x faster and with less effort than the state of the art.\nOur results suggest that Verus offers a platform for exploring the next frontiers in system-verification research. Because Verus builds on Rust, Verus is also positioned for wider use in production by developers who have already adopted Rust in the pursuit of more robust systems.",
      "url": "https://www.microsoft.com/en-us/research/publication/verus-a-practical-foundation-for-systems-verification/"
    },
    {
      "title": "A Bayesian Approach to Data Point Selection",
      "authors": [
        "Brais Martinez",
        "Minyoung Kim",
        "Royson Lee",
        "Timothy M. Hospedales",
        "Xinnuo Xu"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "November 2024",
      "abstract": "Data point selection (DPS) is becoming a critical topic in deep learning due to the ease of acquiring uncurated training data compared to the difficulty of obtaining curated or processed data. Existing approaches to DPS are predominantly based on a bi-level optimisation (BLO) formulation, which is demanding in terms of memory and computation, and exhibits some theoretical defects regarding minibatches. Thus, we propose a novel Bayesian approach to DPS. We view the DPS problem as posterior inference in a novel Bayesian model where the posterior distributions of the instance-wise weights and the main neural network parameters are inferred under a reasonable prior and likelihood model. We employ stochastic gradient Langevin MCMC sampling to learn the main network and instance-wise weights jointly, ensuring convergence even with minibatches. Our update equation is comparable to the widely used SGD and much more efficient than existing BLO-based methods. Through controlled experiments in both the vision and language domains, we present the proof-of-concept. Additionally, we demonstrate that our method scales effectively to large language models and facilitates automated per-task optimization for instruction fine-tuning datasets.",
      "url": "https://www.microsoft.com/en-us/research/publication/a-bayesian-approach-to-data-point-selection/"
    },
    {
      "title": "From Medprompt to o1: Exploration of Run-Time Strategies for Medical Challenge Problems and Beyond",
      "authors": [
        "Eric Horvitz",
        "Harsha Nori",
        "Naoto Usuyama",
        "Nicholas King",
        "S. McKinney",
        "Sheng Zhang",
        "Xavier Fernandes"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Medical, health and genomics"
      ],
      "publication_date": "November 2024",
      "abstract": "Run-time steering strategies like Medprompt are valuable for guiding large language models (LLMs) to top performance on challenging tasks. Medprompt demonstrates that a general LLM can be focused to deliver state-of-the-art performance on specialized domains like medicine by using a prompt to elicit a run-time strategy involving chain of thought reasoning and ensembling. OpenAI’s o1-preview model represents a new paradigm, where a model is designed to do run-time reasoning before generating final responses. We seek to understand the behavior of o1-preview on a diverse set of medical challenge problem benchmarks. Following on the Medprompt study with GPT-4, we systematically evaluate the o1-preview model across various medical benchmarks. Notably, even without prompting techniques, o1-preview largely outperforms the GPT-4 series with Medprompt. We further systematically study the efficacy of classic prompt engineering strategies, as represented by Medprompt, within the new paradigm of reasoning models. We found that few-shot prompting hinders o1’s performance, suggesting that in-context learning may no longer be an effective steering approach for reasoning-native models. While ensembling remains viable, it is resource-intensive and requires careful cost-performance optimization. Our cost and accuracy analysis across run-time strategies reveals a Pareto frontier, with GPT-4o representing a more affordable option and o1-preview achieving state-of-the-art performance at higher cost. Although o1-preview offers top performance, GPT-4o with steering strategies like Medprompt retains value in specific contexts. Moreover, we note that the o1-preview model has reached near-saturation on many existing medical benchmarks, underscoring the need for new, challenging benchmarks. We close with reflections on general directions for inference-time computation with LLMs.",
      "url": "https://www.microsoft.com/en-us/research/publication/from-medprompt-to-o1-exploration-of-run-time-strategies-for-medical-challenge-problems-and-beyond/"
    },
    {
      "title": "LLM2CLIP: Powerful Language Model Unlocks Richer Visual Representation",
      "authors": [
        "Aoqi Wu",
        "Chong Luo",
        "Dongdong Chen",
        "Liang Hu",
        "Lili Qiu",
        "Qi Dai",
        "Weiquan Huang",
        "Xiyang Dai",
        "Xufang Luo",
        "Yifan Yang",
        "Yuqing Yang"
      ],
      "research_areas": [
        "Computer vision"
      ],
      "publication_date": "November 2024",
      "abstract": "CLIP is one of the most important multimodal foundational models today. What powers CLIP’s capabilities? The rich supervision signals provided by natural language, the carrier of human knowledge, shape a powerful cross-modal representation space. However, with the rapid advancements in large language models LLMs like GPT-4 and LLaMA, the boundaries of language comprehension and generation are continually being pushed. This raises an intriguing question: can the capabilities of LLMs be harnessed to further improve multimodal representation learning? The potential benefits of incorporating LLMs into CLIP are clear. LLMs’ strong textual understanding can fundamentally improve CLIP’s ability to handle image captions, drastically enhancing its ability to process long and complex texts, a well-known limitation of vanilla CLIP. Moreover, LLMs are trained on a vast corpus of text, possessing open-world knowledge. This allows them to expand on caption information during training, increasing the efficiency of the learning process. In this paper, we propose LLM2CLIP, a novel approach that embraces the power of LLMs to unlock CLIP’s potential. By fine-tuning the LLM in the caption space with contrastive learning, we extract its textual capabilities into the output embeddings, significantly improving the output layer’s textual discriminability. We then design an efficient training process where the fine-tuned LLM acts as a powerful teacher for CLIP’s visual encoder. Thanks to the LLM’s presence, we can now incorporate longer and more complex captions without being restricted by vanilla CLIP’s text encoder’s context window and ability limitations. Our experiments demonstrate that this approach brings substantial improvements in cross-modal tasks.",
      "url": "https://www.microsoft.com/en-us/research/publication/llm2clip-powerful-language-model-unlock-richer-visual-representation/"
    },
    {
      "title": "Whispering Wearables: Multimodal Approach to Silent Speech Recognition with Head-Worn Devices",
      "authors": [
        "Ivan Tashev",
        "R. Michael Winters",
        "Tanmay Srivastava",
        "Teresa LaScala",
        "Thomas M. Gable",
        "Yu-Te Wang"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Audio and Acoustics",
        "Hardware and devices",
        "Human-computer interaction"
      ],
      "publication_date": "November 2024",
      "abstract": "Silent speech recognition has emerged as a promising approach for Thomas M. Gable Microsoft Corporation United States thomas.gable@microsoft.com Ivan J. Tashev Microsoft Research Labs, Microsoft Corporation United States ivantash@microsoft.com Silent speech recognition; Accessibility; EXG, and IMU sensing enabling hands-free and discreet interaction with head-worn de vices. In this paper, we present QuietSync, a multimodal system that combines inertial measurement unit (IMU) and contact electrode (ExG) signals to achieve accurate silent speech recognition using of-the-shelf devices. QuietSync utilizes an IMU attached to the lower part of the headphones near the ear and strategically places ExG electrodes on the headphones, glasses (nose and behind the ear), and face (for VR applications) to capture subtle movements and muscle activity associated with silent speech production. We con ducted a user study with 9 participants and successfully recognized 12 commands with an accuracy of 94.2%. Our system leverages the complementary nature of IMU and ExG signals to enhance the robustness and reliability of silent speech recognition. The IMU captures subtle movements of the jaw and facial muscles, while the ExG electrodes detect low-amplitude surface muscle activity associated with speech production. We show that our system is not affected by the length and speech mannerisms of the commands, and can be fine-tuned for users of varied native languages with only 5 samples. Our findings demonstrate the feasibility of using of-the-shelf head-worn devices to enable silent speech recognition, opening up new possibilities for seamless and discreet interaction with devices such as VR/AR headsets and earables. To the best of our knowledge, QuietSync is the first system to enable silent speech interaction for multiple form factors.",
      "url": "https://www.microsoft.com/en-us/research/publication/quietsync-integrating-multimodal-signals-for-accurate-silent-speech-interaction-with-head-worn-devices/"
    },
    {
      "title": "If At First You Don’t Succeed, Try, Try, Again…? Insights and LLM-informed Tooling for Detecting Retry Bugs in Software Systems",
      "authors": [
        "Bogdan Alexandru Stoica",
        "Cyrus Zhou",
        "Jonathan Mace",
        "Madan Musuvathi",
        "Shan Lu",
        "Suman Nath",
        "Utsav Sethi",
        "Yiming Su"
      ],
      "research_areas": [
        "Systems and networking"
      ],
      "publication_date": "November 2024",
      "abstract": "Retry – the re-execution of a task on failure – is a common mechanism to enable resilient software systems. Yet, despite its commonality and long history, retry remains difficult to implement and test in modern systems. Guided by our study of real-world retry issues, we propose a novel suite of static and dynamic techniques to detect retry problems in software systems. In particular, we find that the ad-hoc nature of retry implementation in software systems poses challenges for traditional program analysis but can be well handled by Large Language Models; we also find that careful repurposing existing unit tests can, along with fault injection, expose various types of retry problems.",
      "url": "https://www.microsoft.com/en-us/research/publication/if-at-first-you-dont-succeed-try-try-again-insights-and-llm-informed-tooling-for-detecting-retry-bugs-in-software-systems/"
    },
    {
      "title": "Efficient Reproduction of Fault-Induced Failures in Distributed Systems with Feedback-Driven Fault Injection",
      "authors": [
        "Haoze Wu",
        "Jia Pan",
        "Peng Huang",
        "Suman Nath",
        "Tanakorn Leesatapornwongsa"
      ],
      "research_areas": [
        "Systems and networking"
      ],
      "publication_date": "November 2024",
      "abstract": "Debugging a failure usually requires reproducing it first. This can be hard for failures in production distributed systems, where bugs are exposed only by some unusual faulty events. While fault injection testing becomes popular, existing solutions are designed for bug finding. They are ineffective and inefficient to reproduce a specific failure during debugging.\nWe explore a new type of fault injection technique for quickly reproducing a given fault-induced production failure in distributed systems. We present a tool, Anduril, that uses static causal analysis and a novel feedback-driven algorithm to quickly search the enormous fault space for the root-cause fault and timing. We evaluate Anduril on 22 real-world complex fault-induced failures from five large-scale distributed systems. Anduril reproduced all failures by identifying and injecting the root-cause faults at the right time, in a median of 8 minutes.",
      "url": "https://www.microsoft.com/en-us/research/publication/feedback-driven-fault-injection-efficiently-reproducing-fault-induced-failures/"
    },
    {
      "title": "DroidSpeak: KV Cache Sharing for Efficient Multi-LLM Serving",
      "authors": [
        "Esha Choukse",
        "Hanchen Li",
        "Jiayi Yao",
        "Junchen Jiang",
        "Kuntai Du",
        "Madan Musuvathi",
        "Shan Lu",
        "Yihua Cheng",
        "Yuhan Liu",
        "Yuyang Huang",
        "Zhuohan Gu"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Systems and networking"
      ],
      "publication_date": "November 2024",
      "abstract": "Large Language Models (LLMs) are increasingly employed in complex workflows, where different LLMs and fine-tuned variants collaboratively address complex tasks. However, these systems face significant inefficiencies due to redundant context processing of the shared context. We propose DroidSpeak, a framework that optimizes context sharing between fine-tuned LLMs derived from the same foundational model. DroidSpeak identifies critical layers in the KV cache and selectively recomputes them, enabling effective reuse of intermediate data while maintaining high accuracy. Our approach balances computational efficiency and task fidelity, significantly reducing inference latency and throughput bottlenecks. Experiments on diverse datasets and model pairs demonstrate that DroidSpeak achieves up to 3x higher throughputs and 2.6x faster prefill times with negligible accuracy loss compared to full recomputation.",
      "url": "https://www.microsoft.com/en-us/research/publication/droidspeak-kv-cache-sharing-for-efficient-multi-llm-serving/"
    },
    {
      "title": "Blended Length Genome Sequencing (blend-seq): Combining Short Reads with Low-Coverage Long Reads to Maximize Variant Discovery",
      "authors": [
        "Eric Banks",
        "Fabio Cunial",
        "Megan Shand",
        "Niall Lennon",
        "Ricky Magner",
        "Ron Paulsen",
        "Scott Saponas",
        "Sumit Basu"
      ],
      "research_areas": [
        "Medical, health and genomics"
      ],
      "publication_date": "November 2024",
      "abstract": "We introduce blend-seq, a method for combining data from traditional short-read sequencing pipelines with low-coverage long reads, with the goal of substantially improving variant discovery for single samples without the full cost of high-coverage long reads. We demonstrate that with only 4x long read coverage augmenting 30x short reads, we can improve SNP discovery across the genome and achieve precision and recall beyond what is possible with short reads, even at very high coverage (60x). For genotype-agnostic discovery of structural variants, we see a threefold improvement in recall while maintaining precision by using the low-coverage long reads on their own. For the more specialized scenario of genotype-aware structural variant calling, we show how combining the long and short reads in a graph-based approach results in greater performance than either technology on its own. The observed gains highlight the complementary nature of short and long read technologies: long reads help with SNP discovery by better mapping to difficult regions, and they provide better performance with long insertions and deletions (structural variants) by virtue of their length, while the larger number of short-read layers help with genotyping structural variants discovered by long reads. In this way, blend-seq offers many of the benefits of long-read pipelines without incurring the cost of high-coverage long reads.",
      "url": "https://www.microsoft.com/en-us/research/publication/blended-length-genome-sequencing-blend-seq-combining-short-reads-with-low-coverage-long-reads-to-maximize-variant-discovery/"
    },
    {
      "title": "Scaling Laws for Pre-training Agents and World Models",
      "authors": [
        "Dave Bignell",
        "Katja Hofmann",
        "Raluca Stevenson",
        "Sam Devlin",
        "Tabish Rashid",
        "Tim Pearce"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "November 2024",
      "abstract": "The performance of embodied agents has been shown to improve by increasing model parameters, dataset size, and compute. This has been demonstrated in domains from robotics to video games, when generative learning objectives on offline datasets (pre-training) are used to model an agent’s behavior (imitation learning) or their environment (world modeling). This paper characterizes the role of scale in these tasks more precisely. Going beyond the simple intuition that `bigger is better’, we show that the same types of power laws found in language modeling (e.g. between loss and optimal model size), also arise in world modeling and imitation learning. However, the coefficients of these laws are heavily influenced by the tokenizer, task \\&architecture — this has important implications on the optimal sizing of models and data.",
      "url": "https://www.microsoft.com/en-us/research/publication/scaling-laws-for-pre-training-agents-and-world-models/"
    },
    {
      "title": "Optimizing GPU Data Center Power",
      "authors": [
        "Ashish Jain",
        "Brijesh Warrier",
        "Esha Choukse",
        "Frank Helms",
        "Indrani Paul",
        "Mehdi Saidi",
        "Nithish Mahalingam",
        "Paul Van der Arend",
        "Rajit Seahra",
        "Rashad Oreifej",
        "Ricardo Bianchini",
        "Srilatha Manne",
        "Sriram Sundaram",
        "Tawfik Rahal-Arabi"
      ],
      "research_areas": [
        "Systems and networking"
      ],
      "publication_date": "November 2024",
      "abstract": "GPUs are used in products from ultra-low power mobile devices to high performance machine learning accelerators in data centers. Across the products, power and power delivery have become top limiters to performance and are key considerations in the early stages of product definition and design. In particular, the power and power delivery problem has been significantly exacerbated with the recent trends in the growth of AI workloads. In this joint AMD and Microsoft paper, we present some of the power optimizations used in latest generation of AMD GPUs including the recently announced AMD Instinct™ MI300 GPU. To this end, we cover power and power delivery optimization techniques spanning the product life cycle from architecture, physical design, validation, test, manufacturing and conclude with a data center scale view of the challenges ahead to power optimize the GPUs for the data centers of the future.",
      "url": "https://www.microsoft.com/en-us/research/publication/optimizing-gpu-data-center-power/"
    },
    {
      "title": "Quantifying reliance on external information over parametric knowledge during Retrieval Augmented Generation (RAG) using mechanistic analysis",
      "authors": [
        "Ehsan Aghazadeh",
        "Hitesh Wadhwa",
        "Rahul Seetharaman",
        "Reshmi Ghosh",
        "Samyadeep Basu",
        "Shreyas Chaudhari",
        "Somyaa Aggarwal",
        "Soundararajan Srinivasan",
        "Wenlong Zhao"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Human language technologies",
        "Search and information retrieval"
      ],
      "publication_date": "November 2024",
      "abstract": "Retrieval Augmented Generation (RAG) is a widely used approach for leveraging external context in several natural language applications such as question answering and information retrieval. Yet, the exact nature in which a Language Model (LM) leverages this nonparametric memory or retrieved context isn’t clearly understood. This paper mechanistically examines the RAG pipeline to highlight that LMs demonstrate a “shortcut” effect and have a strong bias towards utilizing the retrieved context to answer questions, while relying minimally on model priors. We propose (a) Causal Mediation Analysis; for proving that parametric memory is minimally utilized when answering a question and (b) Attention Contributions and Knockouts for showing the last token residual stream do not get enriched from the subject token in the question, but gets enriched from tokens of RAG-context. We find this pronounced “shortcut” behaviour to be true across both LLMs (e.g.,LlaMa) and SLMs (e.g., Phi)",
      "url": "https://www.microsoft.com/en-us/research/publication/quantifying-reliance-on-external-information-over-parametric-knowledge-during-retrieval-augmented-generation-rag-using-mechanistic-analysis/"
    },
    {
      "title": "GenXD: Generating Any 3D and 4D Scenes",
      "authors": [
        "Chung-Ching Lin",
        "Gim Hee Lee",
        "Jianfeng Wang",
        "K. Lin",
        "Lijuan Wang",
        "Linjie Li",
        "Yuyang Zhao",
        "Zhengyuan Yang",
        "Zhiwen Yan"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Computer vision"
      ],
      "publication_date": "November 2024",
      "abstract": "Recent developments in 2D visual generation have been remarkably successful. However, 3D and 4D generation remain challenging in real-world applications due to the lack of large-scale 4D data and effective model design. In this paper, we propose to jointly investigate general 3D and 4D generation by leveraging camera and object movements commonly observed in daily life. Due to the lack of real-world 4D data in the community, we first propose a data curation pipeline to obtain camera poses and object motion strength from videos. Based on this pipeline, we introduce a large-scale real-world 4D scene dataset: CamVid-30K. By leveraging all the 3D and 4D data, we develop our framework, GenXD, which allows us to produce any 3D or 4D scene. We propose multiview-temporal modules, which disentangle camera and object movements, to seamlessly learn from both 3D and 4D data. Additionally, GenXD employs masked latent conditions to support a variety of conditioning views. GenXD can generate videos that follow the camera trajectory as well as consistent 3D views that can be lifted into 3D representations. We perform extensive evaluations across various real-world and synthetic datasets, demonstrating GenXD’s effectiveness and versatility compared to previous methods in 3D and 4D generation.",
      "url": "https://www.microsoft.com/en-us/research/publication/genxd-generating-any-3d-and-4d-scenes/"
    },
    {
      "title": "Memory Allocation under Hardware Compression",
      "authors": [
        "Ali R. Butt",
        "Chandler Jearls",
        "David Bears",
        "Esha Choukse",
        "Gagandeep Panwar",
        "Kirk Cameron",
        "Muhammad Laghari",
        "Raghavendra Srinivas",
        "Xun Jian",
        "Yuqing Liu"
      ],
      "research_areas": [
        "Systems and networking"
      ],
      "publication_date": "November 2024",
      "abstract": "As the scaling of DRAM density slows physically, a promising solution is to scale it up logically via hardware memory compression, which enhances CPU’s memory controller (MC) to squeeze more data into DRAM. Hardware-compressed memory decouples OS-managed physical memory from actual memory (i.e., DRAM); the MC spends a dynamically varying amount of DRAM on each physical page, depending on how compressible are its values.\nThe newly-decoupled actual memory effectively forms a new layer of memory beyond the traditional layers of virtual, pseudo- physical, and physical memory. We note unlike these traditional memory layers, each with its own specialized allocation interface (e.g., malloc/mmap for virtual memory, page tables+MMU for physical memory), this new layer of memory introduced by hardware memory compression still awaits its own unique memory allocation interface; its absence makes the allocation of actual memory imprecise and, sometimes, even impossible.\nImprecisely allocating less actual memory, and/or unable to allocate more, can harm performance. Even imprecisely allocating more actual memory to some jobs can be harmful as it can lead to allocating less to other jobs in highly-consolidated/utilized memory systems, where compression is useful.\nTo restore precise memory allocation, we design a new memory allocation specialized for this new layer of memory by architecting a new MMU-like component to add to the memory controller and tackling the corresponding design challenges. In our full-system FPGA evaluations, jobs perform stably when colocated with jobs of different sizes (e.g., with only 1%-2% average performance variation, down from 19%-89% under the prior art).",
      "url": "https://www.microsoft.com/en-us/research/publication/memory-allocation-under-hardware-compression/"
    },
    {
      "title": "End-to-End Performance Analysis of Learning-enabled Systems",
      "authors": [
        "Behnaz Arzani",
        "Michael Schapira",
        "Pooria Namyar",
        "Ramesh Govindan",
        "Ryan Beckett",
        "Santiago Segarra",
        "Siva Kesava Reddy Kakarla"
      ],
      "research_areas": [
        "Systems and networking"
      ],
      "publication_date": "November 2024",
      "abstract": "We propose a performance analysis tool for learning-enabled systems that allows operators to uncover potential performance issues before deploying DNNs in their systems. The tools that exist for this purpose require operators to faithfully model all components (a white-box approach) or do inefficient black-box local search. We propose a gray-box alternative, which eliminates the need to precisely model all the system’s components. Our approach is faster and finds substantially worse scenarios compared to prior work. We show that a state-of-the-art learning-enabled traffic engineering pipeline can underperform the optimal by 6× — a much higher number compared to what the authors found.",
      "url": "https://www.microsoft.com/en-us/research/publication/end-to-end-performance-analysis-of-learning-enabled-systems/"
    },
    {
      "title": "Continuous Analysis: Evolution of Software Engineering and Reproducibility for Science",
      "authors": [
        "Maria Yazykova",
        "Olesya Melnichenko",
        "Venkat S. Malladi",
        "Yulia Dubinina"
      ],
      "research_areas": [
        "Medical, health and genomics",
        "Programming languages and software engineering"
      ],
      "publication_date": "November 2024",
      "abstract": "Reproducibility in research remains hindered by complex systems involving data, models, tools, and algorithms. Studies highlight a reproducibility crisis due to a lack of standardized reporting, code and data sharing, and rigorous evaluation. This paper introduces the concept of Continuous Analysis to address the reproducibility challenges in scientific research, extending the DevOps lifecycle. Continuous Analysis proposes solutions through version control, analysis orchestration, and feedback mechanisms, enhancing the reliability of scientific results. By adopting CA, the scientific community can ensure the validity and generalizability of research outcomes, fostering transparency and collaboration and ultimately advancing the field.",
      "url": "https://www.microsoft.com/en-us/research/publication/continuous-analysis-evolution-of-software-engineering-and-reproducibility-for-science/"
    },
    {
      "title": "ProvCam: A Camera Module with Self-Contained TCB for Producing Verifiable Videos",
      "authors": [
        "Ardalan Amiri Sani",
        "Gene Tsudik",
        "Mingyi Chen",
        "Sharad Agarwal",
        "Yuxin (Myles) Liu",
        "Zhihao Yao"
      ],
      "research_areas": [
        "Security, privacy, and cryptography",
        "Systems and networking"
      ],
      "publication_date": "November 2024",
      "abstract": "Our perception of reality is under constant threat from ever-improving video manipulation techniques, including deep-fakes and generative AI. Therefore, proving authenticity of videos is increasingly important, especially in legal and news contexts. However, it is very challenging to prove it based on post-factum video content analysis.\nIn this work, we take a preventative stance and construct ProvCam, a novel camera module that generates a cryptographic proof of video authenticity. Our solution greatly reduces the size of Trusted Computing Base (TCB) to include the module itself. Moreover, it mitigates tampering during the numerous processing steps between video capture by the camera sensor and generation of the digital video output. To confirm its practicality, we present a complete prototype of ProvCam on a Xilinx FPGA evaluation board. As experiments show, ProvCam incurs a negligible performance overhead (latency and throughput) and small energy consumption overhead when recording a video. It imposes a moderate hardware cost but is relatively small compared to other major components such as SoC. Moreover, it does not change the existing camera software stack and thus can be easily integrated with various camera-bearing devices, such as smartphones.",
      "url": "https://www.microsoft.com/en-us/research/publication/provcam-a-camera-module-with-self-contained-tcb-for-producing-verifiable-videos/"
    },
    {
      "title": "Deoxys: A Causal Inference Engine for Unhealthy Node Mitigation in Large-scale Cloud Infrastructure",
      "authors": [
        "Binit R. Mishra",
        "Chaoyun Zhang",
        "Dongmei Zhang",
        "Minghua Ma",
        "Murali Chintalapati",
        "Qingwei Lin 林庆维",
        "Randolph Yao",
        "Shekhar Agrawal",
        "Si Qin",
        "Tri Tran",
        "Ze Li"
      ],
      "research_areas": [
        "Data platforms and analytics",
        "Systems and networking"
      ],
      "publication_date": "November 2024",
      "abstract": "The presence of unhealthy nodes in cloud infrastructure signals the potential failure of machines, which can significantly impact the availability and reliability of cloud services, resulting in negative customer experiences. Effectively addressing unhealthy node mitigation is therefore vital for sustaining cloud system performance. This paper introduces Deoxys, a causal inference engine tailored to recommending mitigation actions for unhealthy node in cloud systems to minimize virtual machine downtime and interruptions during unhealthy events. It employs double machine learning combined with causal forest to produce precise and reliable mitigation recommendations based solely on limited observational data collected from the historical unhealthy events. To enhance the causal inference model, Deoxys further incorporates a policy fallback mechanism based on model uncertainty and action overriding mechanisms to (i) improve the reliability of the system, and (ii) strike a good tradeoff between downtime reduction and resource utilization, thereby enhancing the overall system performance.\n\nAfter deploying Deoxys in a large-scale cloud infrastructure at Microsoft, our observations demonstrate that Deoxys significantly reduces average VM downtime by 53% compared to a legacy policy, while leading to 49.5% lower VM interruption rate. This substantial improvement enhances the reliability and stability of cloud platforms, resulting in a seamless customer experience.",
      "url": "https://www.microsoft.com/en-us/research/publication/deoxys-a-causal-inference-engine-for-unhealthy-node-mitigation-in-large-scale-cloud-infrastructure/"
    },
    {
      "title": "A Study on Context Length and Efficient Transformers for Biomedical Image Analysis",
      "authors": [
        "Hui Xue",
        "Sarah Hooper"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Medical, health and genomics"
      ],
      "publication_date": "November 2024",
      "abstract": "Biomedical images are often high-resolution and multi-dimensional, presenting computational challenges for deep neural networks. These computational challenges are compounded when training transformers due to the self-attention operator, which scales quadratically with context length. Recent works have proposed alternatives to self-attention that scale more favorably with context length, alleviating these computational difficulties and potentially enabling more efficient application of transformers to large biomedical images. However, a systematic evaluation on this topic is lacking. In this study, we investigate the impact of context length on biomedical image analysis and we evaluate the performance of recently proposed substitutes to self-attention. We first curate a suite of biomedical imaging datasets, including 2D and 3D data for segmentation, denoising, and classification tasks. We then analyze the impact of context length on network performance using the Vision Transformer and Swin Transformer. Our findings reveal a strong relationship between context length and performance, particularly for pixel-level prediction tasks. Finally, we show that recent attention-free models demonstrate significant improvements in efficiency while maintaining comparable performance to self-attention-based models, though we highlight where gaps remain.",
      "url": "https://www.microsoft.com/en-us/research/publication/a-study-on-context-length-and-efficient-transformers-for-biomedical-image-analysis/"
    },
    {
      "title": "Working with Generative AI: We need more African Voices, Real African Voices",
      "authors": [
        "Jacki O'Neill",
        "Najeeb G. Abdulhamid",
        "Stephanie Nyairo"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Human-computer interaction",
        "Social sciences"
      ],
      "publication_date": "November 2024",
      "abstract": "Generative AI has taken the world by storm, appearing to be more usable than previous generations of AI. We describe the findings of a qualitative study of Small and Medium Businesses in Kenya and Nigeria who were using generative AI tools in their everyday work. We found that AI tools were used to support both mundane and creative work and provided both organisational and individual benefits. Participants adopted a number of methods to navigate the strengths and weaknesses of different tools and comparing the output of multiple tools was common. Additionally, our findings suggest that whilst to some extent rhetorics around the democratisation of AI might hold true, these tools did not well support or represent African languages, identities or locales and were understood by participants to embody Western biases. We propose that regional bias should be explicitly called out to encourage researchers to focus on these concerns.\n \n \n ",
      "url": "https://www.microsoft.com/en-us/research/publication/working-with-generative-ai-we-need-more-african-voices-real-african-voices/"
    },
    {
      "title": "VPTQ: Extreme Low-bit Vector Post-Training Quantization for Large Language Models",
      "authors": [
        "Cheng Li",
        "Jicheng Wen",
        "Li Lyna Zhang",
        "Mao Yang",
        "Shengyu Ye",
        "Ting Cao",
        "Yang Wang",
        "Yifei Liu"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Systems and networking"
      ],
      "publication_date": "November 2024",
      "abstract": "Scaling model size significantly challenges the deployment and inference of Large Language Models (LLMs). Due to the redundancy in LLM weights, recent research has focused on pushing weight-only quantization to extremely low-bit (even down to 2 bits). It reduces memory requirements, optimizes storage costs, and decreases memory bandwidth needs during inference. However, due to numerical representation limitations, traditional scalar-based weight quantization struggles to achieve such extreme low-bit. Recent research on Vector Quantization (VQ) for LLMs has demonstrated the potential for extremely low-bit model quantization by compressing vectors into indices using lookup tables.\nIn this paper, we introduce Vector Post-Training Quantization (VPTQ) for extremely low-bit quantization of LLMs. We use Second-Order Optimization to formulate the LLM VQ problem and guide our quantization algorithm design by solving the optimization. We further refine the weights using Channel-Independent Second-Order Optimization for a granular VQ. In addition, by decomposing the optimization problem, we propose a brief and effective codebook initialization algorithm. We also extend VPTQ to support residual and outlier quantization, which enhances model accuracy and further compresses the model. Our experimental results show that VPTQ reduces model quantization perplexity by 0.01–0.34 on LLaMA-2, 0.38–0.68 on Mistral-7B, 4.41–7.34 on LLaMA-3 over SOTA at 2-bit, with an average accuracy improvement of 0.79–1.5% on LLaMA-2, 1% on Mistral-7B, 11–22% on LLaMA-3 on QA tasks on average. We only utilize 10.4–18.6% of the quantization algorithm execution time, resulting in a 1.6–1.8× increase in inference throughput compared to SOTA.\n ",
      "url": "https://www.microsoft.com/en-us/research/publication/vptq-extreme-low-bit-vector-post-training-quantization-for-large-language-models/"
    },
    {
      "title": "Mosaic: Harnessing Micro-architectural Resources of Servers in Serverless Environments",
      "authors": [
        "Enrique Saurez",
        "Esha Choukse",
        "Josep Torrellas",
        "Jovan Stojkovic",
        "Íñigo Goiri"
      ],
      "research_areas": [
        "Systems and networking"
      ],
      "publication_date": "November 2024",
      "abstract": "With serverless computing, users develop scalable applications using lightweight functions as building blocks, while cloud providers own most of the stack, allowing for better resource optimizations. However, due to the frequent context switches within function invocations and a high degree of core oversubscription, functions frequently lose their microarchitectural state causing a performance degradation. At the same time, hardware structures in modern cores are dimensionedfor the broader computational needs of applications, rendering them oversized for many serverless functions.\nBased on these insights, we propose Mosaic, a hardware-software co-design composed of (1) MosaicCPU, a processor architecture that can efficiently run both serverless workloads and traditional monolithic applications, and (2) MosaicScheduler, a serverless software stack that maximizes the architecture benefits. MosaicCPU slices the oversized micro-architectural resources into smaller chunks and dedicates tiles of such chunks to functions. The processor maintains the function’s state across context switches and concurrently for multiple functions across different tiles, improving their performance. Furthermore, it operates the currently inactive tiles in low-power mode, thereby reducing energy consumption. To maximize efficiency, MosaicScheduler introduces predictive right-sizing of the per-function tiles alongside smart scheduling based on the status of the tile contexts. Mosaic improves the throughput by 225% while using 22% less power than the conventional server-class processors.",
      "url": "https://www.microsoft.com/en-us/research/publication/mosaic-harnessing-micro-architectural-resources-of-servers-in-serverless-environments/"
    },
    {
      "title": "CoCoST: Automatic Complex Code Generation with Online Searching and Correctness Testing",
      "authors": [
        "Dongmei Zhang",
        "Jiaru Zou",
        "Mengyu Zhou",
        "Shi Han",
        "Xinyi He",
        "Yun Lin",
        "Zejian Yuan"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Programming languages and software engineering"
      ],
      "publication_date": "November 2024",
      "abstract": "Large Language Models have revolutionized code generation ability by converting natural language descriptions into executable code. However, generating complex code within real-world scenarios remains challenging due to intricate structures, subtle bugs, understanding of advanced data types, and lack of supplementary contents. To address these challenges, we introduce the CoCoST framework, which enhances complex code generation by online searching for more information with planned queries and correctness testing for code refinement. Moreover, CoCoST serializes the complex inputs and outputs to improve comprehension and generates test cases to ensure the adaptability for real-world applications. CoCoST is validated through rigorous experiments on the DS-1000 and ClassEval datasets. Experimental results show that CoCoST substantially improves the quality of complex code generation, highlighting its potential to enhance the practicality of LLMs in generating complex code.",
      "url": "https://www.microsoft.com/en-us/research/publication/cocost-automatic-complex-code-generation-with-online-searching-and-correctness-testing/"
    },
    {
      "title": "Rhythm of Work: Mixed-methods Characterization of Information Workers Scheduling Preferences and Practices",
      "authors": [
        "Bahar Sarrafzadeh",
        "Lillio Mok",
        "Lu Sun",
        "Shilad Sen"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Data platforms and analytics",
        "Social sciences"
      ],
      "publication_date": "November 2024",
      "abstract": "As processes around hybrid work, spatially distant collaborations, and work-life boundaries grow increasingly complex, managing workers’ schedules for synchronous meetings has become a critical aspect of building successful global teams. However, gaps remain in our understanding of workers’ scheduling preferences and practices, which we aim to fill in this large-scale, mixed-methods study of individuals’ calendars in a multinational organization. Using interviews with eight participants, survey data from 165 respondents, and telemetry data from millions of meetings scheduled by 211 thousand workers, we characterize scheduling preferences, practices, and their relationship with each other and organizational factors. We find that temporal preferences can be broadly classified as either cyclical, such as suitability of certain days, or relational, such as dispersed meetings, at various time scales. Furthermore, our results suggest that these preferences are disconnected from actual practice–albeit with several notable exceptions–and that individual differences are associated with factors like meeting load, time-zones, importance of meetings to job function, and job titles. We discuss key themes for our findings, along with the implications for calendar and scheduling systems and socio-technical systems more broadly.",
      "url": "https://www.microsoft.com/en-us/research/publication/rhythm-of-work-mixed-methods-characterization-of-information-workers-scheduling-preferences-and-practices/"
    },
    {
      "title": "Sharingan: Extract user action sequence from desktop recordings",
      "authors": [
        "Dongmei Zhang",
        "Jue Zhang",
        "Kehong Yuan",
        "Lu Han",
        "Qi Zhang",
        "Qingwei Lin 林庆维",
        "Saravan Rajmohan",
        "Xiaoting Qin",
        "Yanting Chen",
        "Yi Ren"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "November 2024",
      "abstract": "Video recordings of user activities, particularly desktop recordings, offer a rich source of data for understanding user behaviors and automating processes. However, despite advancements in Vision-Language Models (VLMs) and their increasing use in video analysis, extracting user actions from desktop recordings remains an underexplored area. This paper addresses this gap by proposing two novel VLM-based methods for user action extraction: the Direct Frame-Based Approach (DF), which inputs sampled frames directly into VLMs, and the Differential Frame-Based Approach (DiffF), which incorporates explicit frame differences detected via computer vision techniques. We evaluate these methods using a basic self-curated dataset and an advanced benchmark adapted from prior work. Our results show that the DF approach achieves an accuracy of 70% to 80% in identifying user actions, with the extracted action sequences being re-playable though Robotic Process Automation. We find that while VLMs show potential, incorporating explicit UI changes can degrade performance, making the DF approach more reliable. This work represents the first application of VLMs for extracting user action sequences from desktop recordings, contributing new methods, benchmarks, and insights for future research.",
      "url": "https://www.microsoft.com/en-us/research/publication/sharingan-extract-user-action-sequence-from-desktop-recordings/"
    },
    {
      "title": "TAP4LLM: Table Provider on Sampling, Augmenting, and Packing Semi-structured Data for Large Language Model Reasoning",
      "authors": [
        "Dongmei Zhang",
        "Jiaru Zou",
        "Lun Du",
        "Mengyu Zhou",
        "Shi Han",
        "Xinyi He",
        "Yuan Sui"
      ],
      "research_areas": [
        "Data platforms and analytics",
        "Human language technologies"
      ],
      "publication_date": "November 2024",
      "abstract": "Table reasoning tasks have shown remarkable progress with the development of large language models (LLMs), which involve interpreting and drawing conclusions from tabular data based on natural language (NL) questions. Existing solutions mainly tested on smaller tables face scalability issues and struggle with complex queries due to incomplete or dispersed data across different table sections. To alleviate these challenges, we propose TAP4LLM as a versatile pre-processor suite for leveraging LLMs in table-based tasks effectively. It covers several distinct components: (1) table sampling to decompose large tables into manageable sub-tables based on query semantics, (2) table augmentation to enhance tables with additional knowledge from external sources or models, and (3) table packing & serialization to convert tables into various formats suitable for LLMs’ understanding. In each module, we design and compare several common methods under various usage scenarios, aiming to shed light on the best practices for leveraging LLMs for table-reasoning tasks. Our experiments show that our method improves LLMs’ reasoning capabilities in various tabular tasks and enhances the interaction between LLMs and tabular data by employing effective pre-processing.",
      "url": "https://www.microsoft.com/en-us/research/publication/tap4llm-table-provider-on-sampling-augmenting-and-packing-semi-structured-data-for-large-language-model-reasoning/"
    },
    {
      "title": "Magentic-One: A Generalist Multi-Agent System for Solving Complex Tasks",
      "authors": [
        "Adam Fourney",
        "Ahmed Awadallah",
        "Cheng Tan",
        "Ece Kamar",
        "Eduardo Salinas",
        "Erkang (Eric) Zhu",
        "Friederike Niedtner",
        "Gagan Bansal",
        "Grace Proebsting",
        "Griffin Bassman",
        "Hussein Mozannar",
        "Jack Gerrits",
        "Jacob Alber",
        "Peter Chang",
        "Rafah Hosn",
        "Ricky Loynd",
        "Robert West",
        "Saleema Amershi",
        "Victor Dibia"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "November 2024",
      "abstract": "Modern AI agents, driven by advances in large foundation models, promise to enhance our productivity and transform our lives by augmenting our knowledge and capabilities. To achieve this vision, AI agents must effectively plan, perform multi-step reasoning and actions, respond to novel observations, and recover from errors, to successfully complete complex tasks across a wide range of scenarios. In this work, we introduce Magentic-One, a high-performing open-source agentic system for solving such tasks. Magentic-One uses a multi-agent architecture where a lead agent, the Orchestrator, plans, tracks progress, and re-plans to recover from errors. Throughout task execution, the Orchestrator also directs other specialized agents to perform tasks as needed, such as operating a web browser, navigating local files, or writing and executing Python code. Our experiments show that Magentic-One achieves statistically competitive performance to the state-of-the-art on three diverse and challenging agentic benchmarks: GAIA, AssistantBench, and WebArena. Notably, Magentic-One achieves these results without modification to core agent capabilities or to how they collaborate, demonstrating progress towards the vision of generalist agentic systems. Moreover, Magentic-One’s modular design allows agents to be added or removed from the team without additional prompt tuning or training, easing development and making it extensible to future scenarios. We provide an open-source implementation of Magentic-One and AutoGenBench, a standalone agentic evaluation tool. AutoGenBench provides built-in controls for repetition and isolation to run agentic benchmarks where actions may produce side-effects, in a rigorous and contained way. Magentic-One, AutoGenBench and detailed empirical performance evaluations of Magentic-One, including ablations and error analysis are available at https://aka.ms/magentic-one (opens in new tab).",
      "url": "https://www.microsoft.com/en-us/research/publication/magentic-one-a-generalist-multi-agent-system-for-solving-complex-tasks/"
    },
    {
      "title": "AutoRAG-HP: Automatic Online Hyper-Parameter Tuning for Retrieval-Augmented Generation",
      "authors": [
        "Dongmei Zhang",
        "Fangkai Yang",
        "Jia Fu",
        "Jue Zhang",
        "Lu Wang",
        "Qi Zhang",
        "Qingwei Lin 林庆维",
        "Saravan Rajmohan",
        "Xiaoting Qin",
        "Yubo Chen"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "November 2024",
      "abstract": "Recent advancements in Large Language Models have transformed ML/AI development, necessitating a reevaluation of AutoML principles for the Retrieval-Augmented Generation (RAG) systems. To address the challenges of hyper-parameter optimization and online adaptation in RAG, we propose the AutoRAG-HP framework, which formulates the hyper-parameter tuning as an online multi-armed bandit (MAB) problem and introduces a novel two-level Hierarchical MAB (Hier-MAB) method for efficient exploration of large search spaces. We conduct extensive experiments on tuning hyper-parameters, such as top-k retrieved documents, prompt compression ratio, and embedding methods, using the ALCE-ASQA and Natural Questions datasets. Our evaluation from jointly optimization all three hyper-parameters demonstrate that MAB-based online learning methods can achieve Recall@5 ≈ 0.8 for scenarios with prominent gradients in search space, using only ~ 20% of the LLM API calls required by the Grid Search approach. Additionally, the proposed Hier-MAB approach outperforms other baselines in more challenging optimization scenarios. The code will be made available at https://aka.ms/autorag.",
      "url": "https://www.microsoft.com/en-us/research/publication/autorag-hp-automatic-online-hyper-parameter-tuning-for-retrieval-augmented-generation/"
    },
    {
      "title": "Towards Safer Heuristics With XPlain",
      "authors": [
        "Behnaz Arzani",
        "Beibin Li",
        "Pantea Karimi",
        "Pooria Namyar",
        "Ryan Beckett",
        "Santiago Segarra",
        "Siva Kesava Reddy Kakarla",
        "Solal Pirelli"
      ],
      "research_areas": [
        "Systems and networking"
      ],
      "publication_date": "November 2024",
      "abstract": "Many problems that cloud operators solve are computationally expensive, and operators often use heuristic algorithms (that are faster and scale better than optimal) to solve them more efficiently. Heuristic analyzers enable operators to find when and by how much their heuristics underperform. However, these tools do not provide enough detail for operators to mitigate the heuristic’s impact in practice: they only discover a single input instance that causes the heuristic to underperform (and not the full set), and they do not explain why.\nWe propose XPlain, a tool that extends these analyzers and helps operators understand when and why their heuristics underperform. We present promising initial results that show such an extension is viable.",
      "url": "https://www.microsoft.com/en-us/research/publication/towards-safer-heuristics-with-xplain/"
    },
    {
      "title": "NeoMem: Hardware/Software Co-Design for CXL-Native Memory Tiering",
      "authors": [
        "Guangyu Sun",
        "Jie Zhang",
        "Lei Qu",
        "Peng Cheng",
        "Ran Shu",
        "Shuotao Xu",
        "Tao Zhang",
        "Yang Wang",
        "Yiqi Chen",
        "Yongqiang Xiong",
        "Zhe Zhou"
      ],
      "research_areas": [
        "Hardware and devices"
      ],
      "publication_date": "November 2024",
      "abstract": "The Compute Express Link (CXL) interconnect makes it feasible to integrate diverse types of memory into servers via its byte-addressable SerDes links. Considering the various access latency, harnessing the full potential of CXL-based heterogeneous memory systems requires efficient memory tiering. However, prior work can hardly make a fundamental progress owing to low-resolution and high-overhead memory access profiling techniques. To address this critical challenge, we propose a novel memory tiering solution called NeoMem, which features a hardware/software co-design. NeoMem offloads memory profiling functions to CXL device-side controllers, integrating a dedicated hardware unit called NeoProf. NeoProf readily monitors memory accesses and provides the OS with crucial page hotness statistics and other useful system state information. On the OS kernel side, we design a revamped memory-tiering strategy, enabling accurate and timely hot page promotion based on NeoProf statistics. We implement NeoMem on a real FPGA-based CXL memory platform and Linux kernel v6.3. Comprehensive evaluations demonstrate that NeoMem achieves 32% to 67% geomean speedup over several existing memory tiering solutions.",
      "url": "https://www.microsoft.com/en-us/research/publication/neomem-hardware-software-co-design-for-cxl-native-memory-tiering/"
    },
    {
      "title": "PromptIntern: Saving Inference Costs by Internalizing Recurrent Prompt during Large Language Model Fine-tuning",
      "authors": [
        "Dongmei Zhang",
        "Jiaru Zou",
        "Mengyu Zhou",
        "Shi Han",
        "Tao Li"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Human language technologies",
        "Programming languages and software engineering"
      ],
      "publication_date": "November 2024",
      "abstract": "Recent advances in fine-tuning large language models (LLMs) have greatly enhanced their usage in domain-specific tasks. Despite the success, fine-tuning continues to rely on repeated and lengthy prompts, which escalate computational expenses, require more resources, and lead to slower inference. In this paper, we present a novel approach, PromptIntern, which internalizes prompt knowledge during model fine-tuning to achieve efficient inference and save costs. Instead of compressing the prompts for a vanilla model, PromptIntern aims to embed the recurrent prompt directly into the model parameters. We design a fine-tuning pipeline that includes instruction template compression, few-shot example absorption, and a progressive internalization strategy, effectively diminishing the need for intricate prompts during inference. Comprehensive experiments on challenging NL2Code tasks demonstrate that our method reduces input tokens by more than 90%, accelerates inference by 4.2 times, and reduces monetary inference costs by 88.3%.",
      "url": "https://www.microsoft.com/en-us/research/publication/promptintern/"
    },
    {
      "title": "Current engagement with unreliable sites from web search driven by navigational search",
      "authors": [
        "Juan M. Lavista Ferres",
        "Kevin T. Greene",
        "Lucas A. Meyer",
        "Mayana Pereira",
        "Nilima Pisharody",
        "Rahul Dodhia"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Search and information retrieval"
      ],
      "publication_date": "November 2024",
      "abstract": "Do search engine algorithms systematically expose users to content from unreliable sites? There is widespread concern that they do, but little systematic evidence that search engine algorithms, rather than user-expressed preferences, are driving current exposure to and engagement with unreliable information sources. Using two datasets totaling roughly 14 billion search engine result pages (SERPs) from Bing, the second most popular search engine in the U.S., we show that search exposes users to few unreliable information sources. The vast majority of engagement with unreliable information sources from search occurs when users are explicitly searching for information from those sites, despite those searches being an extremely small share of the overall search volume. Our findings highlight the importance of accounting for user preference when examining engagement with unreliable sources from web search.",
      "url": "https://www.microsoft.com/en-us/research/publication/current-engagement-with-unreliable-sites-from-web-search-driven-by-navigational-search/"
    },
    {
      "title": "Self-maintaining [networked] systems: The rise of datacenter robotics!",
      "authors": [
        "Andromachi Chatzieleftheriou",
        "Ant Rowstron",
        "David Richardson",
        "David Sweeney",
        "Elliott Hogg",
        "Freddie Hong",
        "Hugh Williams",
        "Iason Sarantopoulos",
        "Yizhong Zhang"
      ],
      "research_areas": [
        "Systems and networking"
      ],
      "publication_date": "November 2024",
      "abstract": "The vision of self-maintaining systems is to make cloud hardware automatically servicing and repairing using robotics. We define a self-maintaining system as one where software can control robotics that can automatically perform hardware maintenance tasks and repair operations. This reduces failure service windows and lowers the risk of repairs causing further cascading failures and outages. Self-maintaining systems are not purely reactive to failures, but also do proactive maintenance before failures occur which reduces future hardware failures. Operating an entire datacenter as a self-maintaining system is many years away, and we present four stages of automation, analogous to levels used for autonomous vehicles, required to reach the full vision for datacenters.\n\n\n\nTo experiment with and learn about self-maintaining systems we have focused on datacenter networking. We have created basic robots that support common network maintenance tasks, such as reseating and cleaning optical transcei-vers and replacing optical fiber cables. The advantages of self-maintaining networks are lower costs and increased availability and reliability. Key is a cross-layering co-design approach; the core cloud services are co-designed with the robotic systems performing the repairs and maintenance. The services control the robots, and this is very analogous to how Software Defined Networking has evolved for broader network management.\n",
      "url": "https://www.microsoft.com/en-us/research/publication/self-maintaining-networked-systems-the-rise-of-datacenter-robotics/"
    },
    {
      "title": "Reviving Cloud Gaming Sessions",
      "authors": [
        "Lili Qiu",
        "Yifan Yang",
        "Yuqing Yang"
      ],
      "research_areas": [
        "Systems and networking"
      ],
      "publication_date": "November 2024",
      "abstract": "Cloud gaming is a multi-billion dollar industry. A client in cloud gaming sends its movement to the game server on the Internet, which renders and transmits the resulting video back. In order to provide a good gaming experience, a latency below 80 ms is required. This means that video rendering, encoding, transmission, decoding, and display have to finish within that time frame, which is especially challenging to achieve due to server overload, network congestion, and losses. In this paper, we propose a new method for recovering lost or corrupted video frames in cloud gaming. Unlike traditional video frame recovery, our approach uses game states to significantly enhance recovery accuracy and utilizes partially decoded frames to recover lost portions. We develop a holistic system that consists of (i) efficiently extracting game states, (ii) modifying H.264 video decoder to generate a mask to indicate which portions of video frames need recovery, and (iii) designing a novel neural network to recover either complete or partial video frames. Our approach is extensively evaluated using iPhone 12 and laptop implementations, and we demonstrate the utility of game states in the game video recovery and the effectiveness of our overall design.",
      "url": "https://www.microsoft.com/en-us/research/publication/enabling-real-time-neural-recovery-for-cloud-gaming-on-mobile-devices/"
    },
    {
      "title": "SciAgent: Tool-augmented Language Models for Scientific Reasoning",
      "authors": [
        "Aixin Sun",
        "Junheng Hao",
        "Liangming Pan",
        "Ruochen Xu",
        "Shuohang Wang",
        "Yixin Cao",
        "Yubo Ma",
        "Yujiu Yang",
        "Zhibin Gou"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Human language technologies"
      ],
      "publication_date": "November 2024",
      "abstract": "Scientific reasoning poses an excessive challenge for even the most advanced Large Language Models (LLMs). To make this task more practical and solvable for LLMs, we introduce a new task setting named tool-augmented scientific reasoning. This setting supplements LLMs with scalable toolsets, and shifts the focus from pursuing an omniscient problem solver to a proficient tool-user. To facilitate the research of such setting, we construct a tool-augmented training corpus named MathFunc which encompasses over 30,000 samples and roughly 6,000 tools. Building on MathFunc, we develop SciAgent to retrieve, understand and, if necessary, use tools for scientific problem solving. Additionally, we craft a benchmark, SciToolBench, spanning five scientific domains to evaluate LLMs’ abilities with tool assistance. Extensive experiments on SciToolBench confirm the effectiveness of SciAgent. Notably, SciAgent-Llama3-8B surpasses other LLMs with the comparable size by more than 8.0% in absolute accuracy. Furthermore, SciAgent-DeepMath-7B shows much superior performance than ChatGPT.",
      "url": "https://www.microsoft.com/en-us/research/publication/sciagent-tool-augmented-language-models-for-scientific-reasoning/"
    },
    {
      "title": "Uncovering Nested Data Parallelism and Data Reuse in DNN Computation with FractalTensor",
      "authors": [
        "Chao Yang",
        "Chengxiang Qi",
        "Fan Yang",
        "Mao Yang",
        "Siran Liu",
        "Weifang Hu",
        "Xuanhua Shi",
        "Ying Cao"
      ],
      "research_areas": [
        "Systems and networking"
      ],
      "publication_date": "November 2024",
      "abstract": "To speed up computation, deep neural networks (DNNs) usually rely on highly optimized tensor operators. Despite the effectiveness, tensor operators are often defined empirically with ad hoc semantics. This hinders the analysis and optimization across operator boundaries. FractalTensor is a programming framework that addresses this challenge. At the core, FractalTensor is a nested list-based abstract data type (ADT), where each element is a tensor with static shape or another FractalTensor (i.e., nested). DNNs are then defined by high-order compute operators like map/reduce/scan and data access operators like window/stride on FractalTensor. This new way of DNN definition explicitly exposes nested data parallelism and fine-grained data access patterns, opening new opportunities for whole program analysis and optimization. To exploit these opportunities, from the FractalTensor-based code the compiler extracts a nested multi-dimensional dataflow graph called Extended Task Dependence Graph (ETDG), which provides a holistic view of data dependency across different granularity. The ETDG is then transformed into an efficient implementation through graph coarsening, data reordering, and access materialization. Evaluation on six representative DNNs like RNN and FlashAttention on NVIDIA A100 shows that FractalTensor achieves speedup by up to 5.44x and 1.97x on average through a unified solution for diverse optimizations.",
      "url": "https://www.microsoft.com/en-us/research/publication/uncovering-nested-data-parallelism-and-data-reuse-in-dnn-computation-with-fractaltensor/"
    },
    {
      "title": "Cuttlefish: A Fair, Predictable Execution Environment for Cloud-hosted Financial Exchanges",
      "authors": [
        "Ilias Marinos",
        "Liangcheng Yu",
        "Prateesh Goyal",
        "Vincent Liu"
      ],
      "research_areas": [
        "Systems and networking"
      ],
      "publication_date": "November 2024",
      "abstract": "Recent years have seen rising interest in expanding and equalizing access to low-latency algorithmic trading for a wider group of participants, including retail traders. Cloud-hosted financial exchanges promise a cost-effective platform more accessible to traders.\nUnfortunately, achieving fairness in cloud environments is challenging due to unpredictable network latencies as well as non-deterministic computation times.\nThis work presents Cuttlefish, a fair-by-design algorithmic trading platform that can run in cloud environments.\nThe idea behind Cuttlefish, is the efficient and robust mapping of real operations to a novel formulation of `virtual time’.\nWith it, Cuttlefish pushes fairness to the extreme regardless of the underlying network communication and computation hardware.\nOur implementation and evaluation not only validates the practicality of Cuttlefish, but also shows its operational efficiency on public cloud platforms.",
      "url": "https://www.microsoft.com/en-us/research/publication/cuttlefish-a-fair-predictable-execution-environment-for-cloud-hosted-financial-exchanges/"
    },
    {
      "title": "Input-Dependent Power Usage in GPUs",
      "authors": [
        "Esha Choukse",
        "Pratyush Patel",
        "Theo Gregersen"
      ],
      "research_areas": [
        "Systems and networking"
      ],
      "publication_date": "November 2024",
      "abstract": "GPUs are known to be power-hungry, and given the boom in artificial intelligence, are currently the major contributors to high power demands of upcoming datacenters. Most GPU usage in these popular workloads consists of large matrix multiplications (GEMMs). The arithmetic intensity of various GEMM shapes and sizes has been deeply studied in the community. We show that modifying the input data, while maintaining data shapes and sizes, in GPU computation kernels can notably change their power consumption. We experiment with four kinds of input variations: value distribution, bit similarity, placement, and sparsity across different data types. Our findings indicate that these variations can change the GPU power usage during GEMM by almost 40%. We hypothesize that input-dependent power usage variations occur due to changes in the number of bit flips in the GPUs. We propose leveraging this property through compiler and scheduler\noptimizations to manage power and reduce energy consumption.",
      "url": "https://www.microsoft.com/en-us/research/publication/input-dependent-power-usage-in-gpus/"
    },
    {
      "title": "No Pose, No Problem: Surprisingly Simple 3D Gaussian Splats from Sparse Unposed Images",
      "authors": [
        "Botao Ye",
        "Haofei Xu",
        "Marc Pollefeys",
        "Ming-Hsuan Yang",
        "Sifei Liu",
        "Songyou Peng",
        "Xueting Li"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Computer vision"
      ],
      "publication_date": "October 2024",
      "abstract": "We introduce NoPoSplat, a feed-forward model capable of reconstructing 3D scenes parameterized by 3D Gaussians from \\textit{unposed} sparse multi-view images. Our model, trained exclusively with photometric loss, achieves real-time 3D Gaussian reconstruction during inference. To eliminate the need for accurate pose input during reconstruction, we anchor one input view’s local camera coordinates as the canonical space and train the network to predict Gaussian primitives for all views within this space. This approach obviates the need to transform Gaussian primitives from local coordinates into a global coordinate system, thus avoiding errors associated with per-frame Gaussians and pose estimation. To resolve scale ambiguity, we design and compare various intrinsic embedding methods, ultimately opting to convert camera intrinsics into a token embedding and concatenate it with image tokens as input to the model, enabling accurate scene scale prediction. We utilize the reconstructed 3D Gaussians for novel view synthesis and pose estimation tasks and propose a two-stage coarse-to-fine pipeline for accurate pose estimation. Experimental results demonstrate that our pose-free approach can achieve superior novel view synthesis quality compared to pose-required methods, particularly in scenarios with limited input image overlap. For pose estimation, our method, trained without ground truth depth or explicit matching loss, significantly outperforms the state-of-the-art methods with substantial improvements. This work makes significant advances in pose-free generalizable 3D reconstruction and demonstrates its applicability to real-world scenarios. Code and trained models are available at https://noposplat.github.io/.",
      "url": "https://www.microsoft.com/en-us/research/publication/no-pose-no-problem-surprisingly-simple-3d-gaussian-splats-from-sparse-unposed-images/"
    },
    {
      "title": "Bridging Geometric States via Generative Modeling",
      "authors": [
        "Di He",
        "Liwei Wang",
        "Shengjie Luo",
        "Shuxin Zheng",
        "Tie-Yan Liu",
        "Yixian Xu"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "October 2024",
      "abstract": "The accurate prediction of geometric state evolution in complex systems is critical for advancing scientific domains such as quantum chemistry and material modeling. Traditional experimental and computational methods face challenges in terms of environmental constraints and computational demands, while current deep learning approaches still fall short in terms of precision and generality. In this work, we introduce the Geometric Diffusion Bridge (GDB), a novel generative modeling framework that accurately bridges initial and target geometric states. GDB leverages a probabilistic approach to evolve geometric state distributions, employing an equivariant diffusion bridge derived by a modified version of Doob’s $h$-transform for connecting geometric states. This tailored diffusion process is anchored by initial and target geometric states as fixed endpoints and governed by equivariant transition kernels. Moreover, trajectory data can be seamlessly leveraged in our GDB framework by using a chain of equivariant diffusion bridges, providing a more detailed and accurate characterization of evolution dynamics. Theoretically, we conduct a thorough examination to confirm our framework’s ability to preserve joint distributions of geometric states and capability to completely model the underlying dynamics inducing trajectory distributions with negligible error. Experimental evaluations across various real-world scenarios show that GDB surpasses existing state-of-the-art approaches, opening up a new pathway for accurately bridging geometric states and tackling crucial scientific challenges with improved accuracy and applicability.",
      "url": "https://www.microsoft.com/en-us/research/publication/bridging-geometric-states-via-generative-modeling/"
    },
    {
      "title": "The Belief State Transformer",
      "authors": [
        "Ada Langford",
        "Alex Lamb",
        "Dinesh Jayaraman",
        "Edward S. Hu",
        "Haoran Xu",
        "John Langford",
        "Kwangjun Ahn",
        "Manan Tomar",
        "Qinghua Liu"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "October 2024",
      "abstract": "We introduce the “Belief State Transformer”, a next-token predictor that takes both a prefix and suffix as inputs, with a novel objective of predicting both the next token for the prefix and the previous token for the suffix. The Belief State Transformer effectively learns to solve challenging problems that conventional forward-only transformers struggle with, in a domain-independent fashion. Key to this success is learning a compact belief state that captures all relevant information necessary for accurate predictions. Empirical ablations show that each component of the model is essential in difficult scenarios where standard Transformers fall short. For the task of story writing with known prefixes and suffixes, our approach outperforms the Fill-in-the-Middle method for reaching known goals and demonstrates improved performance even when the goals are unknown. Altogether, the Belief State Transformer enables more efficient goal-conditioned decoding, better test-time inference, and high-quality text representations on small scale problems.\nWatch the Microsoft Research Forum session >",
      "url": "https://www.microsoft.com/en-us/research/publication/learning-to-achieve-goals-with-belief-state-transformers/"
    },
    {
      "title": "Bayesian Collaborative Bandits with Thompson Sampling for Improved Outreach in Maternal Health Program",
      "authors": [
        "A. Taneja",
        "Arpan Dasgupta",
        "Arun Suggala",
        "Gagan Jain",
        "Karthikeyan Shanmugam",
        "Milind Tambe"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "October 2024",
      "abstract": "Mobile health (mHealth) programs face a critical challenge in optimizing the timing of automated health information calls to beneficiaries. This challenge has been formulated as a collaborative multi-armed bandit problem, requiring online learning of a low-rank reward matrix. Existing solutions often rely on heuristic combinations of offline matrix completion and exploration strategies. In this work, we propose a principled Bayesian approach using Thompson Sampling for this collaborative bandit problem. Our method leverages prior information through efficient Gibbs sampling for posterior inference over the low-rank matrix factors, enabling faster convergence. We demonstrate significant improvements over state-of-the-art baselines on a real-world dataset from the world’s largest maternal mHealth program. Our approach achieves a $16\\%$ reduction in the number of calls compared to existing methods and a $47$\\% reduction compared to the deployed random policy. This efficiency gain translates to a potential increase in program capacity by $0.5-1.4$ million beneficiaries, granting them access to vital ante-natal and post-natal care information. Furthermore, we observe a $7\\%$ and $29\\%$ improvement in beneficiary retention (an extremely hard metric to impact) compared to state-of-the-art and deployed baselines, respectively. Synthetic simulations further demonstrate the superiority of our approach, particularly in low-data regimes and in effectively utilizing prior information. We also provide a theoretical analysis of our algorithm in a special setting using Eluder dimension.",
      "url": "https://www.microsoft.com/en-us/research/publication/bayesian-collaborative-bandits-with-thompson-sampling-for-improved-outreach-in-maternal-health-program/"
    },
    {
      "title": "Dittos: Personalized, Embodied Agents That Participate in Meetings When You Are Unavailable",
      "authors": [
        "Ed Cutrell",
        "Gregory Paul Baribault",
        "Joanne Leong",
        "John Tang",
        "Kori Inkpen",
        "Sasa Junuzovic"
      ],
      "research_areas": [
        "Human-computer interaction"
      ],
      "publication_date": "October 2024",
      "abstract": "Imagine being able to send a personalized embodied agent to meetings you are unable to attend. This paper explores the idea of a Ditto—an agent that visually resembles a person, sounds like them, possesses knowledge about them, and can represent them in meetings. This paper reports on results from two empirical investigations: 1) focus group sessions with six groups (n=24) and 2) a Wizard of Oz (WOz) study with 10 groups (n=39) recruited from within a large technology company. Results from the focus group sessions provide insights on what contexts are appropriate for Dittos, and issues around social acceptability and representation risk. The focus group results also provide feedback on visual design characteristics for Dittos. In the WOz study, teams participated in meetings with two different embodied agents: a Ditto and a Delegate (an agent which did not resemble the absent person). Insights from this research demonstrate the impact these embodied agents can have in meetings and highlight that Dittos in particular show promise in evoking feelings of presence and trust, as well as informing decision making. These results also highlight issues related to relationship dynamics such as maintaining social etiquette, managing one’s professional reputation, and upholding accountability. Overall, our investigation provides early evidence that Dittos could be beneficial to represent users when they are unable to be present but also outlines many factors that need to be carefully considered to successfully realize this vision.",
      "url": "https://www.microsoft.com/en-us/research/publication/dittos-personalized-embodied-agents-that-participate-in-meetings-when-you-are-unavailable/"
    },
    {
      "title": "IconDM: Text-Guided Icon Set Expansion Using Diffusion Models",
      "authors": [
        "Dongmei Zhang",
        "Jian-Guang Lou",
        "Jiaqi Guo",
        "Jiawei Lin",
        "Shizhao Sun",
        "Ting Liu",
        "Zhaoyun Jiang",
        "Zijiang Yang"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Computer vision"
      ],
      "publication_date": "October 2024",
      "abstract": "Icons are ubiquitous visual elements in graphic design, yet their creation is often complex and time-consuming. To resolve this problem, we draw inspiration from the booming text-to-image field and propose Text-Guided Icon Set Expansion, a novel task that helps users design high-quality icons using textual descriptions. Besides, users can control the style consistency of the created icons by inputting a few hand-crafted icons as style reference. Despite its practicality, the task poses two unique challenges. (i) Abstract Concept Visualization. Abstract concepts like technology and health are frequently encountered in icon creation, but their visualization is not straightforward and requires a grounding process that translates them into physical, easy-to-depict objects. (ii) Fine-grained Style Transfer. Unlike ordinary images, icons exhibit richer fine-grained stylistic elements, including tones, line widths, shapes, shadow effects, etc., which puts higher demands on capturing and preserving detailed styles during icon generation. To address the challenges, we propose IconDM, a method based on pre-trained text-to-image (T2I) diffusion models. Our approach incorporates a one-time domain adaptation process and an online style transfer process. In domain adaptation, we enhance the existing T2I model’s capability to understand abstract concepts by fine-tuning it on high-quality icon-text pairs. To achieve this, we construct a large-scale dataset IconBank containing 2.3 million icon samples, and leverage a state-of-the-art vision-language model to generate textual descriptions for each icon. In style transfer, we introduce a Style Enhancement Module into the T2I model. It explicitly extracts the fine-grained style features from the given reference icons and is jointly optimized with the T2I model during DreamBooth tuning. To assess IconDM, we present IconBench, a structured evaluation suite with 30 icon sets and 100 concepts (including 50 abstract concepts). Quantitative results, qualitative analysis, and extensive ablation studies demonstrate the effectiveness of IconDM.",
      "url": "https://www.microsoft.com/en-us/research/publication/icondm-text-guided-icon-set-expansion-using-diffusion-models/"
    },
    {
      "title": "PF3plat: Pose-Free Feed-Forward 3D Gaussian Splatting",
      "authors": [
        "Chong Luo",
        "Heeseong Shin",
        "Jaewoo Jung",
        "Jiaolong Yang",
        "Jisang Han",
        "Seungryong Kim",
        "Sung‐Jin Hong"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "October 2024",
      "abstract": "We consider the problem of novel view synthesis from unposed images in a single feed-forward. Our framework capitalizes on fast speed, scalability, and high-quality 3D reconstruction and view synthesis capabilities of 3DGS, where we further extend it to offer a practical solution that relaxes common assumptions such as dense image views, accurate camera poses, and substantial image overlaps. We achieve this through identifying and addressing unique challenges arising from the use of pixel-aligned 3DGS: misaligned 3D Gaussians across different views induce noisy or sparse gradients that destabilize training and hinder convergence, especially when above assumptions are not met. To mitigate this, we employ pre-trained monocular depth estimation and visual correspondence models to achieve coarse alignments of 3D Gaussians. We then introduce lightweight, learnable modules to refine depth and pose estimates from the coarse alignments, improving the quality of 3D reconstruction and novel view synthesis. Furthermore, the refined estimates are leveraged to estimate geometry confidence scores, which assess the reliability of 3D Gaussian centers and condition the prediction of Gaussian parameters accordingly. Extensive evaluations on large-scale real-world datasets demonstrate that PF3plat sets a new state-of-the-art across all benchmarks, supported by comprehensive ablation studies validating our design choices.",
      "url": "https://www.microsoft.com/en-us/research/publication/pf3plat-pose-free-feed-forward-3d-gaussian-splatting/"
    },
    {
      "title": "TamGen: drug design with target-aware molecule generation through a chemical language model",
      "authors": [
        "C. Chan",
        "Enhong Chen",
        "Haiguang Liu",
        "Han Guo",
        "Jinjiang Guo",
        "Jinzhi Wu",
        "Kehan Wu",
        "Liangliang Zhou",
        "Lijun Wu",
        "Nenghai Yu",
        "Pan Deng",
        "Qizhi Pei",
        "Renhe Liu",
        "Shawn Chen",
        "Shufang Xie",
        "Si Chen",
        "Song Hu",
        "Tao Qin",
        "Tie-Yan Liu",
        "Xi Lu",
        "Yingce Xia",
        "Yuan Zhang",
        "Yumeng Cui"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "October 2024",
      "abstract": "Generative drug design facilitates the creation of compounds effective against pathogenic target proteins. This opens up the potential to discover novel compounds within the vast chemical space and fosters the development of innovative therapeutic strategies. However, the practicality of generated molecules is often limited, as many designs focus on a narrow set of drug-related properties, failing to improve the success rate of subsequent drug discovery process. To overcome these challenges, we develop TamGen, a method that employs a GPT-like chemical language model and enables target-aware molecule generation and compound refinement. We demonstrate that the compounds generated by TamGen have improved molecular quality and viability. Additionally, we have integrated TamGen into a drug discovery pipeline and identified 14 compounds showing compelling inhibitory activity against the Tuberculosis ClpP protease, with the most effective compound exhibiting a half maximal inhibitory concentration (IC50) of 1.9 μM. Our findings underscore the practical potential and real-world applicability of generative drug design approaches, paving the way for future advancements in the field.",
      "url": "https://www.microsoft.com/en-us/research/publication/tamgen-drug-design-with-target-aware-molecule-generation-through-a-chemical-language-model/"
    },
    {
      "title": "SlowFast-VGen: Slow-Fast Learning for Action-Driven Long Video Generation",
      "authors": [
        "Beide Liu",
        "Chung-Ching Lin",
        "Jianfeng Wang",
        "K. Lin",
        "Kai-Wei Chang",
        "Lijuan Wang",
        "Linjie Li",
        "Maxine Wu",
        "Yingnian Wu",
        "Yining Hong",
        "Yuanhao Zhai",
        "Zhengyuan Yang"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Computer vision"
      ],
      "publication_date": "October 2024",
      "abstract": "Human beings are endowed with a complementary learning system, which bridges the slow learning of general world dynamics with fast storage of episodic memory from a new experience. Previous video generation models, however, primarily focus on slow learning by pre-training on vast amounts of data, overlooking the fast learning phase crucial for episodic memory storage. This oversight leads to inconsistencies across temporally distant frames when generating longer videos, as these frames fall beyond the model’s context window. To this end, we introduce SlowFast-VGen, a novel dual-speed learning system for action-driven long video generation. Our approach incorporates a masked conditional video diffusion model for the slow learning of world dynamics, alongside an inference-time fast learning strategy based on a temporal LoRA module. Specifically, the fast learning process updates its temporal LoRA parameters based on local inputs and outputs, thereby efficiently storing episodic memory in its parameters. We further propose a slow-fast learning loop algorithm that seamlessly integrates the inner fast learning loop into the outer slow learning loop, enabling the recall of prior multi-episode experiences for context-aware skill learning. To facilitate the slow learning of an approximate world model, we collect a large-scale dataset of 200k videos with language action annotations, covering a wide range of scenarios. Extensive experiments show that SlowFast-VGen outperforms baselines across various metrics for action-driven video generation, achieving an FVD score of 514 compared to 782, and maintaining consistency in longer videos, with an average of 0.37 scene cuts versus 0.89. The slow-fast learning loop algorithm significantly enhances performances on long-horizon planning tasks as well. Project Website: https://slowfast-vgen.github.io",
      "url": "https://www.microsoft.com/en-us/research/publication/slowfast-vgen-slow-fast-learning-for-action-driven-long-video-generation/"
    },
    {
      "title": "Automated Proof Generation for Rust Code via Self-Evolution",
      "authors": [
        "Chenyuan Yang",
        "Fan Yang",
        "Hao Yu",
        "Lidong Zhou",
        "Md Rakib Hossain Misu",
        "Nan Duan",
        "Peng Cheng",
        "Shan Lu",
        "Shuai Lu",
        "Shuvendu Lahiri",
        "Tao Xie",
        "Tianyu Chen",
        "Xuheng Li",
        "Yeyun Gong"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Programming languages and software engineering",
        "Systems and networking"
      ],
      "publication_date": "October 2024",
      "abstract": "Ensuring correctness is crucial for code generation. Formal verification offers a definitive assurance of correctness, but demands substantial human effort in proof construction and hence raises a pressing need for automation. The primary obstacle lies in the severe lack of data – there is much less proof than code for LLMs to train upon. In this paper, we introduce SAFE, a novel framework that overcomes the lack of human-written proof to enable automated proof generation of Rust code. SAFE establishes a self-evolving cycle where data synthesis and fine-tuning collaborate to enhance the model capability, leveraging the definitive power of a symbolic verifier in telling correct proof from incorrect ones. SAFE also re-purposes the large number of synthesized incorrect proofs to train the self-debugging capability of the fine-tuned models, empowering them to fix incorrect proofs based on the verifier’s feedback. SAFE demonstrates superior efficiency and precision compared to GPT-4o. Through tens of thousands of synthesized proofs and the self-debugging mechanism, we improve the capability of open-source models, initially unacquainted with formal verification, to automatically write proof for Rust code. This advancement leads to a significant improvement in performance, achieving a 70.50% accuracy rate in a benchmark crafted by human experts, a significant leap over GPT-4o’s performance of 24.46%.",
      "url": "https://www.microsoft.com/en-us/research/publication/automated-proof-generation-for-rust-code-via-self-evolution/"
    },
    {
      "title": "Choosing your Surgeon Wisely: Using Provider-Procedure-Volume-Specific Data for Informed Choice",
      "authors": [
        "Bill Weeks",
        "Bruno Demuro Segundo",
        "Dan M Lee",
        "Divya Michael",
        "James Weinstein",
        "Juan M. Lavista Ferres"
      ],
      "research_areas": [
        "Data platforms and analytics",
        "Medical, health and genomics"
      ],
      "publication_date": "October 2024",
      "abstract": "Background: There is a longstanding association between provider-specific procedure volumes and outcomes. “To inform patients and caregivers about clinicians’ experience,” the Centers for Medicare and Medicaid Services (CMS) recently released datasets on procedure- and provider-specific volumes for twelve procedures provided to Traditional Medicare and Medicare Advantage enrolees in 2021. We sought to determine the utility of these datasets for consumers’ informed decision making.\nMethods: Conducting a retrospective analysis of those datasets, we assigned proceduralists to procedure-specific volume quintiles, and, for each procedure and volume quintile, calculated the mean number of procedures performed, the number and specialties of providers performing them, and, at the hospital-referral region (HRR) level, the number of each procedure performed per 100,000 Medicare enrolees and the proportion of specified-procedure-providing proceduralists in the highest volume quintiles.\nResults: A substantial amount of data processing was necessary to merge relevant files and identify reasonable primary proceduralist specialties. Narrowing our analysis to that subset, we found considerable procedure specific variation in the mean number of procedures performed and the number of high-volume proceduralists per 100,000 Medicare enrolees. When a specialist performed multiple procedure types, excepting orthopaedic surgeons, performing a greater number of procedure types was associated with lower likelihood of being in the top volume quintile.\nConclusions: While CMS surfaced these datasets to help inform consumers, the data are complex and require a substantial amount of data manipulation to be informative. To improve consumer friendliness, CMS might identify high-volume proceduralists on its “Find & compare providers near you” tool.",
      "url": "https://www.microsoft.com/en-us/research/publication/choosing-your-surgeon-wisely-using-provider-procedure-volume-specific-data-for-informed-choice/"
    },
    {
      "title": "Multi-Field Adaptive Retrieval",
      "authors": [
        "Ben Van Durme",
        "Millicent Li",
        "Patrick Xia",
        "Tongfei Chen"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Search and information retrieval"
      ],
      "publication_date": "October 2024",
      "abstract": "Document retrieval for tasks such as search and retrieval-augmented generation typically involves datasets that are unstructured: free-form text without explicit internal structure in each document. However, documents can have a structured form, consisting of fields such as an article title, message body, or HTML header. To address this gap, we introduce Multi-Field Adaptive Retrieval (MFAR), a flexible framework that accommodates any number of and any type of document indices on structured data. Our framework consists of two main steps: (1) the decomposition of an existing document into fields, each indexed independently through dense and lexical methods, and (2) learning a model which adaptively predicts the importance of a field by conditioning on the document query, allowing on-the-fly weighting of the most likely field(s). We find that our approach allows for the optimized use of dense versus lexical representations across field types, significantly improves in document ranking over a number of existing retrievers, and achieves state-of-the-art performance for multi-field structured data.",
      "url": "https://www.microsoft.com/en-us/research/publication/multi-field-adaptive-retrieval/"
    },
    {
      "title": "The Potential and Value of AI Chatbot in Personalized Cognitive Training",
      "authors": [
        "Geli Guo",
        "Lili Qiu",
        "Ling Yue",
        "Luna K. Qiu",
        "Nan Chen",
        "Shiqi Jiang",
        "Yang Ou",
        "Yuqing Yang",
        "Zilong Wang"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "October 2024",
      "abstract": "In recent years, the rapid aging of the global population has led to an increase in cognitive disorders, such as Alzheimer’s disease, presenting significant public health challenges. Although no effective treatments currently exist to reverse Alzheimer’s, prevention and early intervention, including cognitive training, are critical. This report explores the potential of AI chatbots in enhancing personalized cognitive training. We introduce ReMe, a web-based framework designed to create AI chatbots that facilitate cognitive training research, specifically targeting episodic memory tasks derived from personal life logs. By leveraging large language models, ReMe provides enhanced user-friendly, interactive, and personalized training experiences. Case studies demonstrate ReMe’s effectiveness in engaging users through life recall and open-ended language puzzles, highlighting its potential to improve cognitive training design. Despite promising results, further research is needed to validate training effectiveness through large-scale studies that include cognitive ability evaluations. Overall, ReMe offers a promising approach to personalized cognitive training, utilizing AI capabilities to meet the growing demand for non-pharmacological interventions in cognitive health, with future research aiming to expand its applications and efficacy.",
      "url": "https://www.microsoft.com/en-us/research/publication/the-potential-and-value-of-ai-chatbot-in-personalized-cognitive-training/"
    },
    {
      "title": "Exploring Continual Fine-Tuning for Enhancing Language Ability in Large Language Model",
      "authors": [
        "Divyanshu Aggarwal",
        "Navin Goyal",
        "Sankarshan Damle",
        "Satya Lokam",
        "Sunayana Sitaram"
      ],
      "research_areas": [
        "Security, privacy, and cryptography"
      ],
      "publication_date": "October 2024",
      "abstract": "A common challenge towards the adaptability of Large Language Models (LLMs) is their ability to learn new languages over time without hampering the model’s performance on languages in which the model is already proficient (usually English). Continual fine-tuning (CFT) is the process of sequentially fine-tuning an LLM to enable the model to adapt to downstream tasks with varying data distributions and time shifts. This paper focuses on the language adaptability of LLMs through CFT. We study a two-phase CFT process in which an English-only end-to-end fine-tuned LLM from Phase 1 (predominantly Task Ability) is sequentially fine-tuned on a multilingual dataset — comprising task data in new languages — in Phase 2 (predominantly Language Ability). We observe that the “similarity” of Phase 2 tasks with Phase 1 determines the LLM’s adaptability. For similar phase-wise datasets, the LLM after Phase 2 does not show deterioration in task ability. In contrast, when the phase-wise datasets are not similar, the LLM’s task ability deteriorates. We test our hypothesis on the open-source \\mis\\ and \\llm\\ models with multiple phase-wise dataset pairs. To address the deterioration, we analyze tailored variants of two CFT methods: layer freezing and generative replay. Our findings demonstrate their effectiveness in enhancing the language ability of LLMs while preserving task performance, in comparison to relevant baselines.",
      "url": "https://www.microsoft.com/en-us/research/publication/exploring-continual-fine-tuning-for-enhancing-language-ability-in-large-language-model/"
    },
    {
      "title": "1-bit AI Infra: Part 1.1, Fast and Lossless BitNet b1.58 Inference on CPUs",
      "authors": [
        "Furu Wei",
        "Hansong Zhou",
        "Hongyu Wang",
        "Jinheng Wang",
        "Shaoguang Mao",
        "Shuming Ma",
        "Ting Song",
        "Yan Xia"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "October 2024",
      "abstract": "Recent advances in 1-bit Large Language Models (LLMs), such as BitNet and BitNet b1.58, present a promising approach to enhancing the efficiency of LLMs in terms of speed and energy consumption. These developments also enable local LLM deployment across a broad range of devices. In this work, we introduce bitnet.cpp, a tailored software stack designed to unlock the full potential of 1-bit LLMs. Specifically, we develop a set of kernels to support fast and lossless inference of ternary BitNet b1.58 LLMs on CPUs. Extensive experiments demonstrate that bitnet.cpp achieves significant speedups, ranging from 2.37x to 6.17x on x86 CPUs and from 1.37x to 5.07x on ARM CPUs, across various model sizes. The code is available at https://github.com/microsoft/BitNet (opens in new tab).",
      "url": "https://www.microsoft.com/en-us/research/publication/1-bit-ai-infra-part-1-1-fast-and-lossless-bitnet-b1-58-inference-on-cpus/"
    },
    {
      "title": "Little Giants: Synthesizing High-Quality Embedding Data at Scale",
      "authors": [
        "Furu Wei",
        "Haonan Chen",
        "Liang Wang",
        "Nan Yang",
        "Yutao Zhu",
        "Zhicheng Dou",
        "Ziliang Zhao"
      ],
      "research_areas": [
        "Human language technologies",
        "Search and information retrieval"
      ],
      "publication_date": "October 2024",
      "abstract": "Synthetic data generation has become an increasingly popular way of training models without the need for large, manually labeled datasets. For tasks like text embedding, synthetic data offers diverse and scalable training examples, significantly reducing the cost of human annotation. However, most current approaches rely heavily on proprietary models like GPT-4, which are expensive and inefficient for generating large-scale embedding data. In this paper, we introduce SPEED, a framework that aligns open-source small models (8B) to efficiently generate large-scale synthetic embedding data. Through supervised fine-tuning, preference optimization, and self-improvement, SPEED enables small open-source models to produce high-quality data. Remarkably, SPEED uses only less than 1/10 of the GPT API calls, outperforming the state-of-the-art embedding model E5_mistral when both are trained solely on their synthetic data. Using this efficient generator, we conduct a comprehensive study on how various factors within the alignment pipeline impact data quality and reveal the scaling law for synthetic embedding data.",
      "url": "https://www.microsoft.com/en-us/research/publication/little-giants-synthesizing-high-quality-embedding-data-at-scale/"
    },
    {
      "title": "VideoWebArena: Evaluating Long Context Multimodal Agents with Video Understanding Web Tasks",
      "authors": [
        "Charles Ding",
        "Dan Zhao",
        "Justin Lin",
        "Kazuhito Koishida",
        "Lawrence Jang",
        "Paul Pu Liang",
        "Rogerio Bonatti",
        "Yinheng Li"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Graphics and multimedia"
      ],
      "publication_date": "October 2024",
      "abstract": "Videos are often used to learn or extract the necessary information to complete tasks in ways different than what text and static imagery alone can provide. However, many existing agent benchmarks neglect long-context video understanding, instead focusing on text or static image inputs. To bridge this gap, we introduce VideoWebArena (VideoWA), a benchmark for evaluating the capabilities of long-context multimodal agents for video understanding. VideoWA consists of 2,021 web agent tasks based on manually crafted video tutorials, which total almost four hours of content. For our benchmark, we define a taxonomy of long-context video-based agent tasks with two main areas of focus: skill retention and factual retention. While skill retention tasks evaluate whether an agent can use a given human demonstration to complete a task efficiently, the factual retention task evaluates whether an agent can retrieve instruction-relevant information from a video to complete a task. We find that the best model achieves 13.3% success on factual retention tasks and 45.8% on factual retention QA pairs, far below human performance at 73.9% and 79.3%, respectively. On skill retention tasks, long-context models perform worse with tutorials than without, exhibiting a 5% performance decrease in WebArena tasks and a 10.3% decrease in VisualWebArena tasks. Our work highlights the need to improve the agentic abilities of long-context multimodal models and provides a testbed for future development with long-context video agents.",
      "url": "https://www.microsoft.com/en-us/research/publication/videowebarena-evaluating-long-context-multimodal-agents-with-video-understanding-web-tasks/"
    },
    {
      "title": "Not All Heads Matter: A Head-Level KV Cache Compression Method with Integrated Retrieval and Reasoning",
      "authors": [
        "Abedelkadir Asi",
        "Wayne Xiong",
        "Wen Xiao",
        "Yu Fu",
        "Yue Dong",
        "Zefan Cai"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "October 2024",
      "abstract": "Key-Value (KV) caching is a common technique to enhance the computational efficiency of Large Language Models (LLMs), but its memory overhead grows rapidly with input length. Prior work has shown that not all tokens are equally important for text generation, proposing layer-level KV cache compression to selectively retain key information. Recognizing the distinct roles of attention heads in generation, we propose HeadKV, a head-level KV cache compression method, and HeadKV-R2, which leverages a novel contextual reasoning ability estimation for compression. Our approach operates at the level of individual heads, estimating their importance for contextual QA tasks that require both retrieval and reasoning capabilities. Extensive experiments across diverse benchmarks (LongBench, LooGLE), model architectures (e.g., Llama-3-8B-Instruct, Mistral-7B-Instruct), and long-context abilities tests demonstrate that our head-level KV cache compression significantly outperforms strong baselines, particularly in low-resource settings (KV size = 64&128). Notably, our method retains just 1.5% of the KV cache while achieving 97% of the performance of the full KV cache on the contextual question answering benchmark.Codes are available at https://github.com/FYYFU/HeadKV",
      "url": "https://www.microsoft.com/en-us/research/publication/not-all-heads-matter-a-head-level-kv-cache-compression-method-with-integrated-retrieval-and-reasoning/"
    },
    {
      "title": "ProtNote: a multimodal method for protein-function annotation",
      "authors": [
        "Ava P. Amini",
        "Kevin Kaichuang Yang",
        "Nathaniel Corley",
        "Samir Char",
        "Sarah Alamdari"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Medical, health and genomics"
      ],
      "publication_date": "October 2024",
      "abstract": "Understanding the protein sequence-function relationship is essential for advancing protein biology and engineering. However, fewer than 1% of known protein sequences have human-verified functions. While deep learning methods have demonstrated promise for protein function prediction, current models are limited to predicting only those functions on which they were trained. Here, we introduce ProtNote, a multimodal deep learning model that leverages free-form text to enable both supervised and zero-shot protein function prediction. ProtNote not only maintains near state-of-the-art performance for annotations in its train set, but also generalizes to unseen and novel functions in zero-shot test settings. We envision that ProtNote will enhance protein function discovery by enabling scientists to use free text inputs, without restriction to predefined labels – a necessary capability for navigating the dynamic landscape of protein biology.",
      "url": "https://www.microsoft.com/en-us/research/publication/protnote-a-multimodal-method-for-protein-function-annotation-2/"
    },
    {
      "title": "SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs",
      "authors": [
        "Dayou Du",
        "Fan Yang",
        "Hayden Kwok-Hay So",
        "Mao Yang",
        "Shijie Cao",
        "Ting Cao",
        "Yizhao Gao",
        "Zhichen Zeng"
      ],
      "research_areas": [
        "Systems and networking"
      ],
      "publication_date": "October 2024",
      "abstract": "Attention is the cornerstone of modern Large Language Models (LLMs). Yet its quadratic complexity limits the efficiency and scalability of LLMs, especially for those with a long-context window. A promising approach addressing this limitation is to leverage the sparsity in attention. However, existing sparsity-based solutions predominantly rely on predefined patterns or heuristics to approximate sparsity. This practice falls short to fully capture the dynamic nature of attention sparsity in language-based tasks. This paper argues that attention sparsity should be learned rather than predefined. To this end, we design SeerAttention, a new Attention mechanism that augments the conventional attention with a learnable gate that adaptively selects significant blocks in an attention map and deems the rest blocks sparse. Such block-level sparsity effectively balances accuracy and speedup. To enable efficient learning of the gating network, we develop a customized FlashAttention implementation that extracts the block-level ground truth of attention map with minimum overhead. SeerAttention not only applies to post-training, but also excels in long-context fine-tuning. Our results show that at post-training stages, SeerAttention significantly outperforms state-of-the-art static or heuristic-based sparse attention methods, while also being more versatile and flexible to adapt to varying context lengths and sparsity ratios. When applied to long-context fine-tuning with YaRN, SeerAttention can achieve a remarkable 90% sparsity ratio at a 32k context length with minimal perplexity loss, offering a 5.67x speedup over FlashAttention-2.",
      "url": "https://www.microsoft.com/en-us/research/publication/seerattention-learning-intrinsic-sparse-attention-in-your-llms/"
    },
    {
      "title": "Making Every Frame Matter: Continuous Video Understanding for Large Models via Adaptive State Modeling",
      "authors": [
        "Donglin Bai",
        "Fengyuan Xu",
        "Hao Wu",
        "Qianxi Zhang",
        "Shiqi Jiang",
        "Ting Cao",
        "Yifan Yang"
      ],
      "research_areas": [
        "Computer vision"
      ],
      "publication_date": "October 2024",
      "abstract": "Video understanding has become increasingly important with the rise of multi-modality applications. Understanding continuous video poses considerable challenges due to the fast expansion of streaming video, which contains multi-scale and untrimmed events. We introduce a novel system, C-VUE, to overcome these issues through adaptive state modeling. C-VUE has three key designs. The first is a long-range history modeling technique that uses a video-aware approach to retain historical video information. The second is a spatial redundancy reduction technique, which enhances the efficiency of history modeling based on temporal relations. The third is a parallel training structure that incorporates the frame-weighted loss to understand multi-scale events in long videos. Our C-VUE offers high accuracy and efficiency. It runs at speeds>30 FPS on typical edge devices and outperforms all baselines in accuracy. Moreover, applying C-VUE to a video foundation model as a video encoder in our case study resulted in a 0.46-point enhancement (on a 5-point scale) on the in-distribution dataset, and an improvement ranging from 1.19\\% to 4\\% on zero-shot datasets.",
      "url": "https://www.microsoft.com/en-us/research/publication/making-every-frame-matter-continuous-video-understanding-for-large-models-via-adaptive-state-modeling/"
    },
    {
      "title": "CREAM: Consistency Regularized Self-Rewarding Language Models",
      "authors": [
        "Chetan Bansal",
        "Huaxiu Yao",
        "Weilei He",
        "Weitong Zhang",
        "Xuchao Zhang",
        "Ying Wei",
        "Zhaoyang Wang",
        "Zhiyuan Liang"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "October 2024",
      "abstract": "Recent self-rewarding large language models (LLM) have successfully applied LLM-as-a-Judge to iteratively improve the alignment performance without the need of human annotations for preference data. These methods commonly utilize the same LLM to act as both the policy model (which generates responses) and the reward model (which scores and ranks those responses). The ranked responses are then used as preference pairs to train the LLM via direct alignment technologies (e.g. DPO). However, it is noteworthy that throughout this process, there is no guarantee of accuracy in the rewarding and ranking, which is critical for ensuring accurate rewards and high-quality preference data. Empirical results from relatively small LLMs (e.g., 7B parameters) also indicate that improvements from self-rewarding may diminish after several iterations in certain situations, which we hypothesize is due to accumulated bias in the reward system. This bias can lead to unreliable preference data for training the LLM. To address this issue, we first formulate and analyze the generalized iterative preference fine-tuning framework for self-rewarding language model. We then introduce the regularization to this generalized framework to mitigate the overconfident preference labeling in the self-rewarding process. Based on this theoretical insight, we propose a Consistency Regularized sElf-rewarding lAnguage Model (CREAM) that leverages the rewarding consistency across different iterations to regularize the self-rewarding training, helping the model to learn from more reliable preference data. With this explicit regularization, our empirical results demonstrate the superiority of CREAM in improving both reward consistency and alignment performance. The code is publicly available at https://github.com/Raibows/CREAM.",
      "url": "https://www.microsoft.com/en-us/research/publication/cream-consistency-regularized-self-rewarding-language-models/"
    },
    {
      "title": "A theoretical and computational analysis of full strong-branching",
      "authors": [
        "Marco Molinaro",
        "Prachi Shah",
        "Santanu S. Dey",
        "Yatharth Dubey"
      ],
      "research_areas": [
        "Algorithms"
      ],
      "publication_date": "October 2024",
      "abstract": "Full strong-branching (henceforth referred to as strong-branching) is a well-known variable\nselection rule that is known experimentally to produce signi cantly smaller branch-and-bound\ntrees in comparison to all other known variable selection rules. In this paper, we attempt\nan analysis of the performance of the strong-branching rule both from a theoretical and a\ncomputational perspective. On the positive side for strong-branching we identify vertex cover as\na class of instances where this rule provably works well. In particular, for vertex cover we present\nan upper bound on the size of the branch-and-bound tree using strong-branching as a function\nof the additive integrality gap, show how the Nemhauser-Trotter property of persistency which\ncan be used as a pre-solve technique for vertex cover is being recursively and consistently used\nthrough-out the strong-branching based branch-and-bound tree, and finally provide an example\nof a vertex cover instance where not using strong-branching leads to a tree that has at least\nexponentially more nodes than the branch-and-bound tree based on strong-branching. On the\nnegative side for strong-branching, we identify another class of instances where strong-branching\nbased branch-and-bound tree has exponentially larger tree in comparison to another branch-and\nbound tree for solving these instances. On the computational side, we first present a dynamic\nprogramming algorithm to find an optimal branch-and-bound tree for any mixed integer linear\nprogram (MILP) with n binary variables whose running time is poly(data(I))*3^n. Then we\nconduct experiments on various types of instances like the lot-sizing problem and its variants,\npacking integer programs (IP), covering IPs, chance constrained IPs, vertex cover, etc., to\nunderstand how much larger is the size of the strong-branching based branch-and-bound tree in\ncomparison to the optimal branch-and-bound tree. The main take-away from these experiments\nis that for all these instances, the size of the strong-branching based branch-and-bound tree is\nwithin a factor of two of the size of the optimal branch-and-bound tree.",
      "url": "https://www.microsoft.com/en-us/research/publication/a-theoretical-and-computational-analysis-of-full-strong-branching/"
    },
    {
      "title": "SAFREE: Training-Free and Adaptive Guard for Safe Text-to-Image And Video Generation",
      "authors": [
        "Huaxiu Yao",
        "Jaehong Yoon",
        "Mohit Bansal",
        "Shoubin Yu",
        "Vaidehi Patil"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Computer vision"
      ],
      "publication_date": "October 2024",
      "abstract": "Recent advances in diffusion models have significantly enhanced their ability to generate high-quality images and videos, but they have also increased the risk of producing unsafe content. Existing unlearning/editing-based methods for safe generation remove harmful concepts from models but face several challenges: (1) They cannot instantly remove harmful concepts without training. (2) Their safe generation capabilities depend on collected training data. (3) They alter model weights, risking degradation in quality for content unrelated to toxic concepts. To address these, we propose SAFREE, a novel, training-free approach for safe T2I and T2V, that does not alter the model’s weights. Specifically, we detect a subspace corresponding to a set of toxic concepts in the text embedding space and steer prompt embeddings away from this subspace, thereby filtering out harmful content while preserving intended semantics. To balance the trade-off between filtering toxicity and preserving safe concepts, SAFREE incorporates a novel self-validating filtering mechanism that dynamically adjusts the denoising steps when applying the filtered embeddings. Additionally, we incorporate adaptive re-attention mechanisms within the diffusion latent space to selectively diminish the influence of features related to toxic concepts at the pixel level. In the end, SAFREE ensures coherent safety checking, preserving the fidelity, quality, and safety of the output. SAFREE achieves SOTA performance in suppressing unsafe content in T2I generation compared to training-free baselines and effectively filters targeted concepts while maintaining high-quality images. It also shows competitive results against training-based methods. We extend SAFREE to various T2I backbones and T2V tasks, showcasing its flexibility and generalization. SAFREE provides a robust and adaptable safeguard for ensuring safe visual generation.",
      "url": "https://www.microsoft.com/en-us/research/publication/safree-training-free-and-adaptive-guard-for-safe-text-to-image-and-video-generation/"
    },
    {
      "title": "Fine-Grained Verifiers: Preference Modeling as Next-token Prediction in Vision-Language Alignment",
      "authors": [
        "An Zhang",
        "Chenhang Cui",
        "Gelei Deng",
        "Huaxiu Yao",
        "Tat-Seng Chua",
        "Yiyang Zhou",
        "Zhaorun Chen"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Computer vision"
      ],
      "publication_date": "October 2024",
      "abstract": "The recent advancements in large language models (LLMs) and pre-trained vision models have accelerated the development of vision-language large models (VLLMs), enhancing the interaction between visual and linguistic modalities. Despite their notable success across various domains, VLLMs face challenges in modality alignment, which can lead to issues like hallucinations and unsafe content generation. Current alignment techniques often rely on coarse feedback and external datasets, limiting scalability and performance. In this paper, we propose FiSAO (Fine-Grained Self-Alignment Optimization), a novel self-alignment method that utilizes the model’s own visual encoder as a fine-grained verifier to improve vision-language alignment without the need for additional data. By leveraging token-level feedback from the vision encoder, FiSAO significantly improves vision-language alignment, even surpassing traditional preference tuning methods that require additional data. Through both theoretical analysis and experimental validation, we demonstrate that FiSAO effectively addresses the misalignment problem in VLLMs, marking the first instance of token-level rewards being applied to such models.",
      "url": "https://www.microsoft.com/en-us/research/publication/fine-grained-verifiers-preference-modeling-as-next-token-prediction-in-vision-language-alignment/"
    },
    {
      "title": "SpaceBlender: Creating Context-Rich Collaborative Spaces Through Generative 3D Scene Blending",
      "authors": [
        "Andrew D. Wilson",
        "Balasaravanan Thoravi Kumaravel",
        "Nels Numan",
        "Nicolai Marquardt",
        "Shwetha Rajaram"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Graphics and multimedia",
        "Human-computer interaction"
      ],
      "publication_date": "October 2024",
      "abstract": "There is increased interest in using generative AI to create 3D spaces for Virtual Reality (VR) applications. However, today’s models produce artificial environments, falling short of supporting collaborative tasks that benefit from incorporating the user’s physical context. To generate environments that support VR telepresence, we introduce SpaceBlender, a novel pipeline that utilizes generative AI techniques to blend users’ physical surroundings into unified virtual spaces. This pipeline transforms user-provided 2D images into context-rich 3D environments through an iterative process consisting of depth estimation, mesh alignment, and diffusion-based space completion guided by geometric priors and adaptive text prompts. In a preliminary within-subjects study, where 20 participants performed a collaborative VR affinity diagramming task in pairs, we compared SpaceBlender with a generic virtual environment and a state-of-the-art scene generation framework, evaluating its ability to create virtual spaces suitable for collaboration. Participants appreciated the enhanced familiarity and context provided by SpaceBlender but also noted complexities in the generative environments that could detract from task focus. Drawing on participant feedback, we propose directions for improving the pipeline and discuss the value and design of blended spaces for different scenarios.",
      "url": "https://www.microsoft.com/en-us/research/publication/spaceblender-creating-context-rich-collaborative-spaces-through-generative-3d-scene-blending/"
    },
    {
      "title": "OSAIRIS: Lessons Learned from the Hospital-Based Implementation and Evaluation of an Open-Source Deep-Learning Model for Radiotherapy Image Segmentation",
      "authors": [
        "Alexandra Constantinou",
        "Alexandros Mourounas",
        "Amy Edwards",
        "Andrew Hoole",
        "Andrew Robinson",
        "Anita Anthony",
        "Aviva Grisby",
        "Barry Evans",
        "C. Sanghera",
        "David C. Wong",
        "E. Gatfield",
        "G. S. Sagoo",
        "Gill Barnett",
        "Ian Gleeson",
        "Javier Alvarez-Valle",
        "K. T. Jayaprakash",
        "K. Wildschut",
        "Kenji Takeda",
        "Liam Stubbington",
        "Louisa Stockton",
        "Marian Toomey",
        "Niall Bolger",
        "Raj Jena",
        "Rebecca Sen",
        "Renteng Hou",
        "Richard Benson",
        "Rose McMullen",
        "Stephanie Brown",
        "Thiraviyam Elumalai",
        "Tian Wang",
        "Tom Griffiths",
        "Yvonne Rimmer"
      ],
      "research_areas": [
        "Medical, health and genomics"
      ],
      "publication_date": "October 2024",
      "abstract": "Several studies report the benefits and accuracy of using autosegmentation for organ at risk (OAR) outlining in radiotherapy treatment planning. Typically, evaluations focus on accuracy metrics, and other parameters such as perceived utility and safety are routinely ignored. Here we report our finding from the implementation and clinical evaluation of OSAIRIS, an open-source AI model for radiotherapy image segmentation, that was carried out as part of its development into a medical device. The device contours OARs in the head and neck and male pelvis (referred to as the prostate model), and is designed to be used as a time-saving workflow device, alongside a clinician. Unlike standard evaluation processes, which heavily rely on accuracy metrics alone, our evaluation sought to demonstrate the tangible benefits, quantify utility and assess risk within a specific clinical workflow. We evaluated the time-saving benefit this device affords to clinicians, and how this time-saving might be linked to accuracy metrics, as well as the clinicians’ assessment of the usability of the OSAIRIS contours in comparison to their colleagues’ contours and those from other commercial AI contouring devices. Our safety evaluation focused on whether clinicians can notice and correct any errors should they be included in the output of the device.\n\n\nWe found that OSAIRIS affords a significant time-saving of 36% (5.4 ± 2.1 minutes) when used for prostate contouring and 67% (30.3 ± 8.7 minutes) for head and neck contouring. Combining editing time data with accuracy metrics, we found the Hausdorff distance best correlated with editing-time, outperforming dice, the industry-standard, with a Spearman correlation coefficient of 0.70, and a Kendall coefficient of 0.52. Our safety and risk-mitigation exercise showed that anchoring bias is present when clinicians edit AI-generated contours, with the effect seemingly more pronounced for some structures over others. Most errors, however, were corrected by clinicians, with 72% of the head and neck errors 81% of the prostate errors removed in the editing step. Notably, our blinded clinician contour rating exercise showed that gold standard clinician contours are not rated more highly than the AI-generated contours.\n\n\nWe conclude that evaluations of AI in a clinical setting must consider the clinical workflow in which the device will be used, and not rely on accuracy metrics alone, in order to reliably assess the benefits, utility and safety of the device. The effects of human-AI inter-operation must be evaluated to accurately assess the practical usability and potential uptake of the technology, as demonstrated in our blinded clinical utility review. The clinical risks posed by the use of the device must be studied and mitigated as far as possible, and our ‘Mystery Shopping’ experiment provides a template for future such assessments.",
      "url": "https://www.microsoft.com/en-us/research/publication/osairis-lessons-learned-from-the-hospital-based-implementation-and-evaluation-of-an-open-source-deep-learning-model-for-radiotherapy-image-segmentation/"
    },
    {
      "title": "Unearthing Skill-Level Insights for Understanding Trade-Offs of Foundation Models",
      "authors": [
        "Besmira Nushi",
        "Mazda Moayeri",
        "Neel Joshi",
        "S. Feizi",
        "Safoora Yousefi",
        "Thomas Fel",
        "Varun Chandrasekaran",
        "Vibhav Vineet",
        "Vidhisha Balachandran"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "October 2024",
      "abstract": "With models getting stronger, evaluations have grown more complex, testing multiple skills in one benchmark and even in the same instance at once. However, skill-wise performance is obscured when inspecting aggregate accuracy, under-utilizing the rich signal modern benchmarks contain. We propose an automatic approach to recover the underlying skills relevant for any evaluation instance, by way of inspecting model-generated rationales. After validating the relevance of rationale-parsed skills and inferring skills for $46$k instances over $12$ benchmarks, we observe many skills to be common across benchmarks, resulting in the curation of hundreds of skill-slices (i.e. sets of instances testing a common skill). Inspecting accuracy over these slices yields novel insights on model trade-offs: e.g., compared to GPT-4o and Claude 3.5 Sonnet, on average, Gemini 1.5 Pro is $18\\%$ more accurate in”computing molar mass”, but $19\\%$ less accurate in”applying constitutional law”, despite the overall accuracies of the three models differing by a mere $0.4\\%$. Furthermore, we demonstrate the practical utility of our approach by showing that insights derived from skill slice analysis can generalize to held-out instances: when routing each instance to the model strongest on the relevant skills, we see a $3\\%$ accuracy improvement over our $12$ dataset corpus. Our skill-slices and framework open a new avenue in model evaluation, leveraging skill-specific analyses to unlock a more granular and actionable understanding of model capabilities.",
      "url": "https://www.microsoft.com/en-us/research/publication/unearthing-skill-level-insights-for-understanding-trade-offs-of-foundation-models/"
    },
    {
      "title": "MMed-RAG: Versatile Multimodal RAG System for Medical Vision Language Models",
      "authors": [
        "Haoran Li",
        "Huaxiu Yao",
        "James Zou",
        "Kangyu Zhu",
        "Linjun Zhang",
        "Peng Xia",
        "Sheng Wang",
        "Weijia Shi"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Medical, health and genomics"
      ],
      "publication_date": "October 2024",
      "abstract": "Artificial Intelligence (AI) has demonstrated significant potential in healthcare, particularly in disease diagnosis and treatment planning. Recent progress in Medical Large Vision-Language Models (Med-LVLMs) has opened up new possibilities for interactive diagnostic tools. However, these models often suffer from factual hallucination, which can lead to incorrect diagnoses. Fine-tuning and retrieval-augmented generation (RAG) have emerged as methods to address these issues. However, the amount of high-quality data and distribution shifts between training data and deployment data limit the application of fine-tuning methods. Although RAG is lightweight and effective, existing RAG-based approaches are not sufficiently general to different medical domains and can potentially cause misalignment issues, both between modalities and between the model and the ground truth. In this paper, we propose a versatile multimodal RAG system, MMed-RAG, designed to enhance the factuality of Med-LVLMs. Our approach introduces a domain-aware retrieval mechanism, an adaptive retrieved contexts selection method, and a provable RAG-based preference fine-tuning strategy. These innovations make the RAG process sufficiently general and reliable, significantly improving alignment when introducing retrieved contexts. Experimental results across five medical datasets (involving radiology, ophthalmology, pathology) on medical VQA and report generation demonstrate that MMed-RAG can achieve an average improvement of 43.8% in the factual accuracy of Med-LVLMs. Our data and code are available in https://github.com/richard-peng-xia/MMed-RAG.",
      "url": "https://www.microsoft.com/en-us/research/publication/mmed-rag-versatile-multimodal-rag-system-for-medical-vision-language-models/"
    },
    {
      "title": "Exploring Invariance in Images through One-way Wave Equations",
      "authors": [
        "Dongdong Chen",
        "Lu Yuan",
        "Mengchen Liu",
        "Xiyang Dai",
        "Yinan Feng",
        "Yinpeng Chen",
        "Youzuo Lin",
        "Zicheng Liu"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Graphics and multimedia"
      ],
      "publication_date": "October 2024",
      "abstract": "In this paper, we empirically reveal an invariance over images-images share a set of one-way wave equations with latent speeds. Each image is uniquely associated with a solution to these wave equations, allowing for its reconstruction with high fidelity from an initial condition. We demonstrate it using an intuitive encoder-decoder framework where each image is encoded into its corresponding initial condition (a single vector). Subsequently, the initial condition undergoes a specialized decoder, transforming the one-way wave equations into a first-order norm + linear autoregressive process. This process propagates the initial condition along the x and y directions, generating a high-resolution feature map (up to the image resolution), followed by a few convolutional layers to reconstruct image pixels. The revealed invariance, rooted in the shared wave equations, offers a fresh perspective for comprehending images, establishing a promising avenue for further exploration.",
      "url": "https://www.microsoft.com/en-us/research/publication/exploring-invariance-in-images-through-one-way-wave-equations/"
    },
    {
      "title": "Differential Transformer",
      "authors": [
        "Furu Wei",
        "Gao Huang",
        "Li Dong",
        "Tianzhu Ye",
        "Yi Zhu",
        "Yuqing Xia",
        "Yutao Sun"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "October 2024",
      "abstract": "Transformer tends to overallocate attention to irrelevant context. In this work, we introduce Diff Transformer, which amplifies attention to the relevant context while canceling noise. Specifically, the differential attention mechanism calculates attention scores as the difference between two separate softmax attention maps. The subtraction cancels noise, promoting the emergence of sparse attention patterns. Experimental results on language modeling show that Diff Transformer outperforms Transformer in various settings of scaling up model size and training tokens. More intriguingly, it offers notable advantages in practical applications, such as long-context modeling, key information retrieval, hallucination mitigation, in-context learning, and reduction of activation outliers. By being less distracted by irrelevant context, Diff Transformer can mitigate hallucination in question answering and text summarization. For in-context learning, Diff Transformer not only enhances accuracy but is also more robust to order permutation, which was considered as a chronic robustness issue. The results position Diff Transformer as a highly effective and promising architecture to advance large language models.",
      "url": "https://www.microsoft.com/en-us/research/publication/differential-transformer/"
    },
    {
      "title": "Improving Instruction-Following in Language Models through Activation Steering",
      "authors": [
        "Alessandro Stolfo",
        "Besmira Nushi",
        "Eric Horvitz",
        "Safoora Yousefi",
        "Vidhisha Balachandran"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "October 2024",
      "abstract": "The ability to follow instructions is crucial for numerous real-world applications of language models. In pursuit of deeper insights and more powerful capabilities, we derive instruction-specific vector representations from language models and use them to steer models accordingly. These vectors are computed as the difference in activations between inputs with and without instructions, enabling a modular approach to activation steering. We demonstrate how this method can enhance model adherence to constraints such as output format, length, and word inclusion, providing inference-time control over instruction following. Our experiments across four models demonstrate how we can use the activation vectors to guide models to follow constraints even without explicit instructions and to enhance performance when instructions are present. Additionally, we explore the compositionality of activation steering, successfully applying multiple instructions simultaneously. Finally, we demonstrate that steering vectors computed on instruction-tuned models can transfer to improve base models. Our findings demonstrate that activation steering offers a practical and scalable approach for fine-grained control in language generation. Our code and data are available at https://github.com/microsoft/llm-steer-instruct (opens in new tab).",
      "url": "https://www.microsoft.com/en-us/research/publication/improving-instruction-following-in-language-models-through-activation-steering/"
    },
    {
      "title": "MMIE: Massive Multimodal Interleaved Comprehension Benchmark for Large Vision-Language Models",
      "authors": [
        "Chenhang Cui",
        "Huaxiu Yao",
        "Lijuan Wang",
        "Linjie Li",
        "Mingyu Ding",
        "Peng Xia",
        "Shi Qiu",
        "Siwei Han",
        "Wenhao Zheng",
        "Yiyang Zhou",
        "Zhaorun Chen",
        "Zhaoyang Wang"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Computer vision"
      ],
      "publication_date": "October 2024",
      "abstract": "Interleaved multimodal comprehension and generation, enabling models to produce and interpret both images and text in arbitrary sequences, have become a pivotal area in multimodal learning. Despite significant advancements, the evaluation of this capability remains insufficient. Existing benchmarks suffer from limitations in data scale, scope, and evaluation depth, while current evaluation metrics are often costly or biased, lacking in reliability for practical applications. To address these challenges, we introduce MMIE, a large-scale knowledge-intensive benchmark for evaluating interleaved multimodal comprehension and generation in Large Vision-Language Models (LVLMs). MMIE comprises 20K meticulously curated multimodal queries, spanning 3 categories, 12 fields, and 102 subfields, including mathematics, coding, physics, literature, health, and arts. It supports both interleaved inputs and outputs, offering a mix of multiple-choice and open-ended question formats to evaluate diverse competencies. Moreover, we propose a reliable automated evaluation metric, leveraging a scoring model fine-tuned with human-annotated data and systematic evaluation criteria, aimed at reducing bias and improving evaluation accuracy. Extensive experiments demonstrate the effectiveness of our benchmark and metrics in providing a comprehensive evaluation of interleaved LVLMs. Specifically, we evaluate eight LVLMs, revealing that even the best models show significant room for improvement, with most achieving only moderate results. We believe MMIE will drive further advancements in the development of interleaved LVLMs. We publicly release our benchmark and code in https://mmie-bench.github.io/.",
      "url": "https://www.microsoft.com/en-us/research/publication/mmie-massive-multimodal-interleaved-comprehension-benchmark-for-large-vision-language-models/"
    },
    {
      "title": "Controllable Safety Alignment: Inference-Time Adaptation to Diverse Safety Requirements",
      "authors": [
        "Ahmed Elgohary",
        "Ahmed Magooda",
        "Ben Van Durme",
        "Daniel Khashabi",
        "Jingyu (Jack) Zhang"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "October 2024",
      "abstract": "The current paradigm for safety alignment of large language models (LLMs) follows a one-size-fits-all approach: the model refuses to interact with any content deemed unsafe by the model provider. This approach lacks flexibility in the face of varying social norms across cultures and regions. In addition, users may have diverse safety needs, making a model with static safety standards too restrictive to be useful, as well as too costly to be re-aligned. We propose Controllable Safety Alignment (CoSA), a framework designed to adapt models to diverse safety requirements without re-training. Instead of aligning a fixed model, we align models to follow safety configs — free-form natural language descriptions of the desired safety behaviors — that are provided as part of the system prompt. To adjust model safety behavior, authorized users only need to modify such safety configs at inference time. To enable that, we propose CoSAlign, a data-centric method for aligning LLMs to easily adapt to diverse safety configs. Furthermore, we devise a novel controllability evaluation protocol that considers both helpfulness and configured safety, summarizing them into CoSA-Score, and construct CoSApien, a human-authored benchmark that consists of real-world LLM use cases with diverse safety requirements and corresponding evaluation prompts. We show that CoSAlign leads to substantial gains of controllability over strong baselines including in-context alignment. Our framework encourages better representation and adaptation to pluralistic human values in LLMs, and thereby increasing their practicality.",
      "url": "https://www.microsoft.com/en-us/research/publication/controllable-safety-alignment-inference-time-adaptation-to-diverse-safety-requirements/"
    },
    {
      "title": "BlendScape: Enabling End-User Customization of Video-Conferencing Environments through Generative AI",
      "authors": [
        "Andrew D. Wilson",
        "Balasaravanan Thoravi Kumaravel",
        "Nels Numan",
        "Nicolai Marquardt",
        "Shwetha Rajaram"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Graphics and multimedia",
        "Human-computer interaction"
      ],
      "publication_date": "October 2024",
      "abstract": "Today’s video-conferencing tools support a rich range of professional and social activities, but their generic meeting environments cannot be dynamically adapted to align with distributed collaborators’ needs. To enable end-user customization, we developed BlendScape, a rendering and composition system for video-conferencing participants to tailor environments to their meeting context by leveraging AI image generation techniques. BlendScape supports flexible representations of task spaces by blending users’ physical or digital backgrounds into unified environments and implements multimodal interaction techniques to steer the generation. Through an exploratory study with 15 end-users, we investigated whether and how they would find value in using generative AI to customize video-conferencing environments. Participants envisioned using a system like BlendScape to facilitate collaborative activities in the future, but required further controls to mitigate distracting or unrealistic visual elements. We implemented scenarios to demonstrate BlendScape’s expressiveness for supporting environment design strategies from prior work and propose composition techniques to improve the quality of environments.",
      "url": "https://www.microsoft.com/en-us/research/publication/blendscape-enabling-end-user-customization-of-video-conferencing-environments-through-generative-ai/"
    },
    {
      "title": "TableBot: Getting a Handle on Hybrid Collaboration by Negotiating Control of a Tabletop Telepresence Robot",
      "authors": [
        "Alexander Langagergaard Vastrup",
        "Andriana Boudouraki",
        "Clemens Nylandsted Klokmose",
        "Jens Emil Sloth Grønbæk",
        "Johannes Ellemose",
        "Maja Dybboe",
        "Marianne Graves Petersen",
        "Sean Rintel"
      ],
      "research_areas": [
        "Human-computer interaction",
        "Social sciences"
      ],
      "publication_date": "October 2024",
      "abstract": "Mobile Remote telePresence robots (MRPs) have been explored as a promising approach to strengthen the agency and presence of remote participants in hybrid work settings. Despite the need and interest in how they might better support hybrid collaboration, substantial challenges remain in terms of their price, availability, and successful application in meeting room contexts. In response to these challenges, this paper explores the opportunities for designing lightweight telepresence robots supporting negotiation of control in hybrid meeting contexts. This paper describes a serial Research-through-Design process, exploring three iterations of design and evaluation of TableBot, a novel tabletop telepresence robot. Based on this work, we present the design of TableBot, and articulate the design space of telepresence robots for hybrid meetings in terms of three trade-offs and a framework for analysing telepresence systems regarding negotiation of control.",
      "url": "https://www.microsoft.com/en-us/research/publication/tablebot-getting-a-handle-on-hybrid-collaboration-by-negotiating-control-of-a-tabletop-telepresence-robot/"
    },
    {
      "title": "Farmer.Chat: Scaling AI-Powered Agricultural Services for Smallholder Farmers",
      "authors": [
        "Akshay Nambi",
        "Jacqueline Wang'ombe",
        "Jayasankar G K",
        "Jona Repishti",
        "Mohammed Irfan Rafiq",
        "Namita Singh",
        "Nereah Okanga",
        "Rajsekar Manokaran",
        "Rikin Gandhi",
        "Sanjeev Mishra",
        "Tetyana Zelenska",
        "Vineet Singh"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Human-computer interaction",
        "Technology for emerging markets"
      ],
      "publication_date": "October 2024",
      "abstract": "Small and medium-sized agricultural holders face challenges like limited access to localized, timely information, impacting productivity and sustainability. Traditional extension services, which rely on in-person agents, struggle with scalability and timely delivery, especially in remote areas. We introduce FarmerChat, a generative AI-powered chatbot designed to address these issues. Leveraging Generative AI, FarmerChat offers personalized, reliable, and contextually relevant advice, overcoming limitations of previous chatbots in deterministic dialogue flows, language support, and unstructured data processing. Deployed in four countries, FarmerChat has engaged over 15,000 farmers and answered over 300,000 queries. This paper highlights how FarmerChat’s innovative use of GenAI enhances agricultural service scalability and effectiveness. Our evaluation, combining quantitative analysis and qualitative insights, highlights FarmerChat’s effectiveness in improving farming practices, enhancing trust, response quality, and user engagement.",
      "url": "https://www.microsoft.com/en-us/research/publication/farmer-chat-scaling-ai-powered-agricultural-services-for-smallholder-farmers/"
    },
    {
      "title": "KBLaM: Knowledge Base augmented Language Model",
      "authors": [
        "James Hensman",
        "Liana Mikaelyan",
        "Taketomo Isazawa",
        "Xi Wang"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "October 2024",
      "abstract": "In this paper, we propose Knowledge Base augmented Language Model (KBLaM), a new method for augmenting Large Language Models (LLMs) with external knowledge. KBLaM works with a knowledge base (KB) constructed from a corpus of documents, transforming each piece of knowledge in the KB into continuous key-value vector pairs via pre-trained sentence encoders with linear adapters and integrating them into pre-trained LLMs via a specialized rectangular attention mechanism. Unlike Retrieval-Augmented Generation, KBLaM eliminates external retrieval modules, and unlike in-context learning, its computational overhead scales linearly with KB size rather than quadratically. Our approach enables integrating a large KB of more than 10K triples into an 8B pre-trained LLM of only 8K context window on one single A100 80GB GPU and allows for dynamic updates without model fine-tuning or retraining. Experiments demonstrate KBLaM’s effectiveness in various tasks, including question-answering and open-ended reasoning, while providing interpretable insights into its use of the augmented knowledge.",
      "url": "https://www.microsoft.com/en-us/research/publication/kblam-knowledge-base-augmented-language-model-2/"
    },
    {
      "title": "POROver: Improving Safety and Reducing Overrefusal in Large Language Models with Overgeneration and Preference Optimization",
      "authors": [
        "Alon Benhaim",
        "Batuhan K. Karaman",
        "I. Zabir",
        "M. Sabuncu",
        "Vishrav Chaudhary",
        "Xia Song"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "October 2024",
      "abstract": "Balancing safety and usefulness in large language models has become a critical challenge in recent years. Models often exhibit unsafe behavior or adopt an overly cautious approach, leading to frequent overrefusal of benign prompts, which reduces their usefulness. Addressing these issues requires methods that maintain safety while avoiding overrefusal. In this work, we examine how the overgeneration of training data using advanced teacher models (e.g., GPT-4o), including responses to both general-purpose and toxic prompts, influences the safety and overrefusal balance of instruction-following language models. Additionally, we present POROver, a strategy to use preference optimization methods in order to reduce overrefusal, via employing a superior teacher model’s completions. Our results show that overgenerating completions for general-purpose prompts significantly improves the balance between safety and usefulness. Specifically, the F1 score calculated between safety and usefulness increases from 70.8% to 88.3%. Moreover, overgeneration for toxic prompts substantially reduces overrefusal, decreasing it from 94.4% to 45.2%. Furthermore, preference optimization algorithms, when applied with carefully curated preference data, can effectively reduce a model’s overrefusal from 45.2% to 15.0% while maintaining comparable safety levels. Our code and data are available at https://github.com/batuhankmkaraman/POROver.",
      "url": "https://www.microsoft.com/en-us/research/publication/porover-improving-safety-and-reducing-overrefusal-in-large-language-models-with-overgeneration-and-preference-optimization/"
    },
    {
      "title": "EditRoom: LLM-parameterized Graph Diffusion for Composable 3D Room Layout Editing",
      "authors": [
        "Jianfeng Wang",
        "Jing Gu",
        "K. Lin",
        "Kaizhi Zheng",
        "Lijuan Wang",
        "Linjie Li",
        "Xiaotong Chen",
        "Xin Eric Wang",
        "Xuehai He",
        "Zhengyuan Yang"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Computer vision",
        "Graphics and multimedia",
        "Human-computer interaction"
      ],
      "publication_date": "October 2024",
      "abstract": "Given the steep learning curve of professional 3D software and the time-consuming process of managing large 3D assets, language-guided 3D scene editing has significant potential in fields such as virtual reality, augmented reality, and gaming. However, recent approaches to language-guided 3D scene editing either require manual interventions or focus only on appearance modifications without supporting comprehensive scene layout changes. In response, we propose Edit-Room, a unified framework capable of executing a variety of layout edits through natural language commands, without requiring manual intervention. Specifically, EditRoom leverages Large Language Models (LLMs) for command planning and generates target scenes using a diffusion-based method, enabling six types of edits: rotate, translate, scale, replace, add, and remove. To address the lack of data for language-guided 3D scene editing, we have developed an automatic pipeline to augment existing 3D scene synthesis datasets and introduced EditRoom-DB, a large-scale dataset with 83k editing pairs, for training and evaluation. Our experiments demonstrate that our approach consistently outperforms other baselines across all metrics, indicating higher accuracy and coherence in language-guided scene layout editing.",
      "url": "https://www.microsoft.com/en-us/research/publication/editroom-llm-parameterized-graph-diffusion-for-composable-3d-room-layout-editing/"
    },
    {
      "title": "DermaVQA: A Multilingual Visual Question Answering Dataset for Dermatology",
      "authors": [
        "Asma Ben Abacha",
        "Fei Xia",
        "Meliha Yetisgen",
        "Wen-wai Yim",
        "Yujuan Fu",
        "Zhaoyi Sun"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Medical, health and genomics"
      ],
      "publication_date": "October 2024",
      "abstract": "Remote medical care has become commonplace with the establishment of patient portals, the maturation of web technologies, and the proliferation of personal devices. However, though on-demand care provides convenience and expands patient access, this same phenomenon may lead to increased workload for healthcare providers. Drafting candidate responses may help speed up physician workflows answering electronic messages. One specialty that may benefit from the latest multi-modal vision-language foundational models is dermatology. However, there is no existing dataset that incorporate dermatological health queries along with user-generated images. In this work, we contribute a new dataset, DermaVQA (https://osf.io/72rp3/), for the task of dermatology question answering and we benchmark the performance of state-of-the-art multi-modal models on multilingual response generation using relevant multi-reference metrics. The dataset and corresponding code are available on our project’s GitHub repository (https://github.com/velvinnn/DermaVQA (opens in new tab)).",
      "url": "https://www.microsoft.com/en-us/research/publication/__trashed-7/"
    },
    {
      "title": "IRGen: Generative Modeling for Image Retrieval",
      "authors": [
        "Baining Guo",
        "Dong Chen",
        "Fan Yang",
        "Hao Sun",
        "Jingdong Wang",
        "Mao Yang",
        "Qi Chen",
        "Qi Zhang",
        "Qingmin Liao",
        "Ting Zhang",
        "Weiwei Deng",
        "Xing Xie",
        "Yidan Zhang",
        "Yujing Wang"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "October 2024",
      "abstract": "While generative modeling has become prevalent across numerous research fields, its integration into the realm of image retrieval remains largely unexplored and underjustified. In this paper, we present a novel methodology, reframing image retrieval as a variant of generative modeling and employing a sequence-to-sequence model. This approach is harmoniously aligned with the current trend towards unification in research, presenting a cohesive framework that allows for end-to-end differentiable searching. This, in turn, facilitates superior performance via direct optimization techniques. The development of our model, dubbed IRGen, addresses the critical technical challenge of converting an image into a concise sequence of semantic units, which is pivotal for enabling efficient and effective search. Extensive experiments demonstrate that our model achieves state-of-the-art performance on three widely-used image retrieval benchmarks as well as two million-scale datasets, yielding significant improvement compared to prior competitive retrieval methods. In addition, the notable surge in precision scores facilitated by generative modeling presents the potential to bypass the reranking phase, which is traditionally indispensable in practical retrieval workflows. The code is publicly available at https://github.com/yakt00/IRGen.",
      "url": "https://www.microsoft.com/en-us/research/publication/irgen-generative-modeling-for-image-retrieval/"
    },
    {
      "title": "MRTransformer: Transforming Avatar Non-verbal Behavior for Remote MR Collaboration in Incongruent Spaces",
      "authors": [
        "Andrea Stevenson Won",
        "Cheng Yao Wang",
        "Eyal Ofek",
        "Hyunju Kim",
        "Mar Gonzalez Franco",
        "Payod Panda"
      ],
      "research_areas": [
        "Human-computer interaction"
      ],
      "publication_date": "October 2024",
      "abstract": "Demonstrating MRTransformer’s dynamic management of collaboration spaces in real-time. (Left) Alice and Bob collaborate within a designated mapped area, (Middle) Bob independently adjusts the collaboration area during the activity, and (Right) The movements of both Alice’s and Bob’s avatars are seamlessly preserved as the collaboration area is adjusted.\n \nAvatar-mediated remote MR collaboration allows users in different spaces to interact as if they were together. However, directly applying a user’s motion to an avatar in incongruent spaces leads to ambiguous and error-prone communication. This paper introduces MRTransformer, a technique enabling dynamic MR collaboration across dissimilar spaces. By adapting transformations to user movements, MRTransformer preserves non-verbal cues and spatial context. It also allows flexible management of collaboration areas and remote object visualization, enhancing remote collaborations. A user study evaluated MRTransformer’s effectiveness in preserving non-verbal cues and spatial awareness, and examined social presence and privacy concerns. Findings offer implications for future remote MR collaboration research and design.\n ",
      "url": "https://www.microsoft.com/en-us/research/publication/mrtransformer-transforming-avatar-non-verbal-behavior-for-remote-mr-collaboration-in-incongruent-spaces/"
    },
    {
      "title": "AI-powered Microgrids Facilitate Energy Resilience and Equity in Regional Communities",
      "authors": [
        "Peeyush Kumar"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Ecology and environment",
        "Economics",
        "Social sciences",
        "Technology for emerging markets"
      ],
      "publication_date": "October 2024",
      "abstract": "This article explores the transformative potential of AI-powered microgrids in enhancing energy resilience and equity in regional communities. It addresses the critical attributes of AI systems necessary to support decentralized renewable energy infrastructures. The article highlights the significant energy burdens faced by low-income households and the environmental impact of the energy sector. It discusses the rise of community-centered renewable microgrids as a solution to these challenges, emphasizing their scalability, integration with renewable energy sources, and ability to provide equitable energy distribution. The article also delves into the role of AI in optimizing microgrid operations, improving grid design, and supporting regulatory frameworks. Through various collaborative projects, including partnerships with academic institutions and community organizations, the article showcases practical implementations of AI-enhanced microgrids and their societal benefits. The article concludes by emphasizing the importance of flexible AI architectures, interoperability, and multi-modal integration in advancing microgrid technology. This paper supports the transition to a more resilient and equitable energy future through a multi-dimensional design and participatory planning.",
      "url": "https://www.microsoft.com/en-us/research/publication/ai-powered-microgrids-facilitate-energy-resilience-and-equitability-in-regional-communities/"
    },
    {
      "title": "How do Active Dendrite Networks Mitigate Catastrophic Forgetting?",
      "authors": [
        "Navin Goyal",
        "Sankarshan Damle",
        "Satya Lokam"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "October 2024",
      "abstract": "We investigate the efficacy of Active Dendrite Networks (ADNs) in mitigating catastrophic forgetting in Continual Learning (CL). We consider Sparse Parity and Modular Addition to be our CL-task sequences. ADNs mitigate forgetting for Sparse Parity, but not for Modular Addition. For Sparse Parity, we perform an interpretability analysis to highlight the effectiveness of orthogonal (or uncorrelated) context vectors and task vectors in mitigation. We demonstrate that uncorrelated context vectors facilitate the creation of distinct subnetworks within ADNs, aiding in task separation. We also look at task uncorrelatedness to explain the difference in ADN’s performance for Sparse Parity and Modular Addition",
      "url": "https://www.microsoft.com/en-us/research/publication/how-do-active-dendrite-networks-mitigate-catastrophic-forgetting/"
    },
    {
      "title": "Scalable, compressed phenotypic screening using pooled perturbations.",
      "authors": [
        "Alex K. Shalek",
        "Anita Vrcic",
        "Ava P. Amini",
        "Benjamin E. Mead",
        "Brian Cleary",
        "Bryan D. Bryson",
        "Christian K Soule",
        "Conner Kummerlowe",
        "Jaime H Cheah",
        "Jane K. McIninch",
        "Joshua M. Peters",
        "Kristen E. Lowder",
        "Lorin Crawford",
        "Manuel Guzman",
        "Nuo Liu",
        "Paul C. Blainey",
        "Peter S. Winter",
        "Sarah Ingabire",
        "Sergio Triana",
        "Srivatsan Raghavan",
        "Thomas Cheng",
        "Tyler T. Dao",
        "Walaa E. Kattan",
        "William C. Hahn"
      ],
      "research_areas": [
        "Medical, health and genomics"
      ],
      "publication_date": "October 2024",
      "abstract": "High-throughput phenotypic screens using biochemical perturbations and high-content readouts are constrained by limitations of scale. To address this, we establish a method of pooling exogenous perturbations followed by computational deconvolution to reduce required sample size, labor and cost. We demonstrate the increased efficiency of compressed experimental designs compared to conventional approaches through benchmarking with a bioactive small-molecule library and a high-content imaging readout. We then apply compressed screening in two biological discovery campaigns. In the first, we use early-passage pancreatic cancer organoids to map transcriptional responses to a library of recombinant tumor microenvironment protein ligands, uncovering reproducible phenotypic shifts induced by specific ligands distinct from canonical reference signatures and correlated with clinical outcome. In the second, we identify the pleotropic modulatory effects of a chemical compound library with known mechanisms of action on primary human peripheral blood mononuclear cell immune responses. In sum, our approach empowers phenotypic screens with information-rich readouts to advance drug discovery efforts and basic biological inquiry.",
      "url": "https://www.microsoft.com/en-us/research/publication/scalable-compressed-phenotypic-screening-using-pooled-perturbations/"
    },
    {
      "title": "Studying and Mitigating Biases in Sign Language Understanding Models",
      "authors": [
        "Danielle Bragg",
        "Katherine Atwell",
        "Malihe Alikhani"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "October 2024",
      "abstract": "Ensuring that the benefits of sign language technologies are distributed equitably among all community members is crucial. Thus, it is important to address potential biases and inequities that may arise from the design or use of these resources. Crowd-sourced sign language datasets, such as the ASL Citizen dataset, are great resources for improving accessibility and preserving linguistic diversity, but they must be used thoughtfully to avoid reinforcing existing biases.In this work, we utilize the rich information about participant demographics and lexical features present in the ASL Citizen dataset to study and document the biases that may result from models trained on crowd-sourced sign datasets. Further, we apply several bias mitigation techniques during model training, and find that these techniques reduce performance disparities without decreasing accuracy. With the publication of this work, we release the demographic information about the participants in the ASL Citizen dataset to encourage future bias mitigation work in this space.",
      "url": "https://www.microsoft.com/en-us/research/publication/studying-and-mitigating-biases-in-sign-language-understanding-models/"
    },
    {
      "title": "ToolGen: Unified Tool Retrieval and Calling via Generation",
      "authors": [
        "Haonan Li",
        "Lei Ji",
        "Renxi Wang",
        "Shu Wang",
        "Timothy Baldwin",
        "Xudong Han"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "October 2024",
      "abstract": "As large language models (LLMs) advance, their inability to autonomously execute tasks by directly interacting with external tools remains a critical limitation. Traditional methods rely on inputting tool descriptions as context, which is constrained by context length and requires separate, often inefficient, retrieval mechanisms. We introduce ToolGen, a paradigm shift that integrates tool knowledge directly into the LLM’s parameters by representing each tool as a unique token. This enables the LLM to generate tool calls and arguments as part of its next token prediction capabilities, seamlessly blending tool invocation with language generation. Our framework allows the LLM to access and utilize a vast amount of tools with no additional retrieval step, significantly enhancing both performance and scalability. Experimental results with over 47,000 tools show that ToolGen not only achieves superior results in both tool retrieval and autonomous task completion but also sets the stage for a new era of AI agents that can adapt to tools across diverse domains. By fundamentally transforming tool retrieval into a generative process, ToolGen paves the way for more versatile, efficient, and autonomous AI systems. ToolGen enables end-to-end tool learning and opens opportunities for integration with other advanced techniques such as chain-of-thought and reinforcement learning, thereby expanding the practical capabilities of LLMs.",
      "url": "https://www.microsoft.com/en-us/research/publication/toolgen-unified-tool-retrieval-and-calling-via-generation/"
    },
    {
      "title": "i-Code Studio: A Configurable and Composable Framework for Integrative AI",
      "authors": [
        "Chenguang Zhu",
        "Lu Yuan",
        "Mahmoud Khademi",
        "Michael Zeng",
        "Reid Pryzant",
        "Takuya Yoshioka",
        "Xuedong Huang",
        "Yao Qian",
        "Yichong Xu",
        "Yuwei Fang",
        "Ziyi Yang"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Computer vision",
        "Human language technologies"
      ],
      "publication_date": "October 2024",
      "abstract": "Artificial General Intelligence (AGI) requires comprehensive understanding and generation capabilities for a variety of tasks spanning different modalities and functionalities. Integrative AI is one important direction to approach AGI, through combining multiple models to tackle complex multimodal tasks. However, there is a lack of a flexible and composable platform to facilitate efficient and effective model composition and coordination. In this paper, we propose the i-Code Studio, a configurable and composable framework for Integrative AI. The i-Code Studio orchestrates multiple pre-trained models in a finetuning-free fashion to conduct complex multimodal tasks. Instead of simple model composition, the i-Code Studio provides an integrative, flexible, and composable setting for developers to quickly and easily compose cutting-edge services and technologies tailored to their specific requirements. The i-Code Studio achieves impressive results on a variety of zero-shot multimodal tasks, such as video-to-text retrieval, speech-to-speech translation, and visual question answering. We also demonstrate how to quickly build a multimodal agent based on the i-Code Studio that can communicate and personalize for users. The project page with demonstrations and code is at https://i-code-studio.github.io/.\n",
      "url": "https://www.microsoft.com/en-us/research/publication/i-code-studio-a-configurable-and-composable-framework-for-integrative-ai/"
    },
    {
      "title": "ExACT: Teaching AI Agents to Explore with Reflective-MCTS and Exploratory Learning",
      "authors": [
        "Baolin Peng",
        "Hao Cheng",
        "Jianfeng Gao",
        "Michel Galley",
        "Vineeth Vajipey",
        "Xiao Yu",
        "Zhou Yu"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Human language technologies",
        "Human-computer interaction"
      ],
      "publication_date": "October 2024",
      "abstract": "Autonomous agents have demonstrated significant potential in automating complex multistep decision-making tasks. However, even state-of-the-art vision-language models (VLMs), such as GPT-4o, still fall short of human-level performance, particularly in intricate web environments and long-horizon tasks. To address these limitations, we present ExACT, an approach to combine test-time search and self-learning to build o1-like models for agentic applications. We first introduce Reflective Monte Carlo Tree Search (R-MCTS), a novel test time algorithm designed to enhance AI agents’ ability to explore decision space on the fly. R-MCTS extends traditional MCTS by 1) incorporating contrastive reflection, allowing agents to learn from past interactions and dynamically improve their search efficiency; and 2) using multi-agent debate for reliable state evaluation. Next, we introduce Exploratory Learning, a novel learning strategy to teach agents to search at inference time without relying on any external search algorithms. On the challenging VisualWebArena benchmark, our GPT-4o based R-MCTS agent achieves a 6% to 30% relative improvement across various tasks compared to the previous state-of-the-art. Additionally, we show that the knowledge and experience gained from test-time search can be effectively transferred back to GPT-4o via fine-tuning. After Exploratory Learning, GPT-4o 1) demonstrates the ability to explore the environment, evaluate a state, and backtrack to viable ones when it detects that the current state cannot lead to success, and 2) matches 87% of R-MCTS’s performance while using significantly less compute. Notably, our work demonstrates the compute scaling properties in both training – data collection with R-MCTS – and testing time. These results suggest a promising research direction to enhance VLMs’ capabilities for agentic applications via test-time search and self-learning.",
      "url": "https://www.microsoft.com/en-us/research/publication/exact-teaching-ai-agents-to-explore-with-reflective-mcts-and-exploratory-learning/"
    },
    {
      "title": "METAREFLECTION: Learning Instructions for Language Agents using Past Reflections",
      "authors": [
        "Ananya Singha",
        "Arjun Radhakrishna",
        "Gustavo Soares",
        "Priyanshu Gupta",
        "Shashank Kirtania",
        "Sherry Shi",
        "Sumit Gulwani"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "October 2024",
      "abstract": "The popularity of Large Language Models (LLMs) have unleashed a new age of Language Agents for solving a diverse range of tasks. While contemporary frontier LLMs are capable enough to power reasonably good Language agents, the closed-API model makes it hard to improve in cases they perform sub-optimally. To address this, recent works have explored ways to improve their performance using techniques like self-reflection and prompt optimization. Unfortunately, techniques like self-reflection can be used only in an online setup, while contemporary prompt optimization techniques are designed and tested to work on simple tasks. To this end, we introduce MetaReflection, a novel offline reinforcement learning technique that enhances the performance of Language Agents by augmenting a semantic memory based on experiential learnings from past trials. We demonstrate the efficacy of MetaReflection by evaluating across multiple domains, including complex logical reasoning, biomedical semantic similarity, open world question answering, and vulnerability threat detection, in Infrastructure-as-Code, spanning different agent designs. MetaReflection boosts Language agents’ performance by 4% to 16.82% over the raw GPT-4 baseline and performs on par with existing state-of-the-art prompt optimization techniques while requiring fewer LLM calls. We release our experimental code at: aka.ms/metareflection-code (opens in new tab)",
      "url": "https://www.microsoft.com/en-us/research/publication/metareflection-learning-instructions-for-language-agents-using-past-reflections/"
    },
    {
      "title": "Budget-aware Query Tuning: An AutoML Perspective",
      "authors": [
        "Chi Wang",
        "Wentao Wu"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Data platforms and analytics"
      ],
      "publication_date": "October 2024",
      "abstract": "Modern database systems rely on cost-based query optimizers to come up with good execution plans for input queries. Such query optimizers rely on cost models to estimate the costs of candidate query execution plans. A cost model represents a function from a set of cost units to query execution cost, where each cost unit specifies the unit cost of executing a certain type of query processing operation (such as table scan or join). These cost units are traditionally viewed as constants, whose values only depend on the platform configuration where the database system runs on top of but are invariant for queries processed by the database system. In this paper, we challenge this classic view by thinking of these cost units as variables instead. We show that, by varying the cost-unit values one can obtain query plans that significantly outperform the default query plans returned by the query optimizer when viewing the cost units as constants. We term this cost-unit tuning process “query tuning” (QT) and show that it is similar to the well-known hyper-parameter optimization (HPO) problem in AutoML. As a result, any state-of-the-art HPO technologies can be applied to QT. We study the QT problem in the context of anytime tuning, which is desirable in practice by constraining the total time spent on QT within a given budget — we call this problem budget-aware query tuning. We further extend our study from tuning a single query to tuning a workload with multiple queries, and we call this generalized problem budget-aware workload tuning (WT), which aims for minimizing the execution time of the entire workload. WT is more challenging as one needs to further prioritize individual query tuning within the given time budget. We propose solutions to both QT and WT and experimental evaluation using both benchmark and real workloads demonstrates the efficacy of our proposed solutions.",
      "url": "https://www.microsoft.com/en-us/research/publication/budget-aware-query-tuning-an-automl-perspective/"
    },
    {
      "title": "Reinforcement Learning Under Latent Dynamics: Toward Statistical and Algorithmic Modularity",
      "authors": [
        "Akshay Krishnamurthy",
        "Dylan Foster",
        "Nan Jiang",
        "Philip Amortila",
        "Zakaria Mhammedi"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "October 2024",
      "abstract": "Real-world applications of reinforcement learning often involve environments where agents operate on complex, high-dimensional observations, but the underlying (“latent”) dynamics are comparatively simple. However, beyond restrictive settings such as tabular latent dynamics, the fundamental statistical requirements and algorithmic principles for *reinforcement learning under latent dynamics* are poorly understood. This paper addresses the question of reinforcement learning under *general latent dynamics* from a statistical and algorithmic perspective. On the statistical side, our main negative result shows that *most* well-studied settings for reinforcement learning with function approximation become intractable when composed with rich observations; we complement this with a positive result, identifying *latent pushforward coverability* as a general condition that enables statistical tractability. Algorithmically, we develop provably efficient *observable-to-latent* reductions —that is, reductions that transform an arbitrary algorithm for the latent MDP into an algorithm that can operate on rich observations— in two settings: one where the agent has access to hindsight observations of the latent dynamics (Lee et al., 2023) and one where the agent can estimate *self-predictive* latent models (Schwarzer et al., 2020). Together, our results serve as a first step toward a unified statistical and algorithmic theory for reinforcement learning under latent dynamics.",
      "url": "https://www.microsoft.com/en-us/research/publication/reinforcement-learning-under-latent-dynamics-toward-statistical-and-algorithmic-modularity/"
    },
    {
      "title": "Gorilla: Teaching LLMs to Use Tools",
      "authors": [
        "Xin Wang"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "October 2024",
      "abstract": "Large Language Models (LLMs) have seen an im-pressive wave of advances recently, with models now excelling in a variety of tasks, such as mathematical reasoning and program synthesis. How-ever, their potential to effectively use tools via API calls remains unfulfilled. This is a challenging task even for today’s state-of- the-art LLMs such as GPT-4 largely due to their unawareness of what APIs are available and how to use them in a frequently updated toolset. We develop Gorilla, a finetuned LLaMA model that surpasses the performance of GPT-4 on writing API calls. When combined with a document retriever, Gorilla demonstrates a strong capability to adapt to test-time document changes, enabling flexible user updates or version changes. It also substantially mitigates the issue of hallucination, commonly encountered when prompting LLMs directly. To evaluate the model’s ability, we introduce APIBench, a comprehensive dataset consisting of HuggingFace, TorchHub, and TensorHubAPIs. The successful integration of the retrieval system with Gorilla demonstrates the potential for LLMs to use tools more accurately, keep up with frequently updated documentation, and consequently increase the reliability and applicability of their outputs. Gorilla’s code, model, and data will be open-sourced.",
      "url": "https://www.microsoft.com/en-us/research/publication/gorilla-teaching-llms-to-use-tools/"
    },
    {
      "title": "Navigating the Unknown: A Chat-Based Collaborative Interface for Personalized Exploratory Tasks",
      "authors": [
        "Dongmei Zhang",
        "Jue Zhang",
        "Qi Zhang",
        "Qingwei Lin 林庆维",
        "Saravan Rajmohan",
        "Xiaoting Qin",
        "Xu Yang",
        "Yingzhe Peng",
        "Zhiyang Zhang"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "October 2024",
      "abstract": "The rise of large language models (LLMs) has revolutionized user interactions with knowledge-based systems, enabling chatbots to synthesize vast amounts of information and assist with complex, exploratory tasks. However, LLM-based chatbots often struggle to provide personalized support, particularly when users start with vague queries or lack sufficient contextual information. This paper introduces the Collaborative Assistant for Personalized Exploration (CARE), a system designed to enhance personalization in exploratory tasks by combining a multi-agent LLM framework with a structured user interface. CARE’s interface consists of a Chat Panel, Solution Panel, and Needs Panel, enabling iterative query refinement and dynamic solution generation. The multi-agent framework collaborates to identify both explicit and implicit user needs, delivering tailored, actionable solutions. In a within-subject user study with 22 participants, CARE was consistently preferred over a baseline LLM chatbot, with users praising its ability to reduce cognitive load, inspire creativity, and provide more tailored solutions. Our findings highlight CARE’s potential to transform LLM-based systems from passive information retrievers to proactive partners in personalized problem-solving and exploration.",
      "url": "https://www.microsoft.com/en-us/research/publication/navigating-the-unknown-a-chat-based-collaborative-interface-for-personalized-exploratory-tasks/"
    },
    {
      "title": "Physical Consistency Bridges Heterogeneous Data in Molecular Multi-Task Learning",
      "authors": [
        "Chang Liu",
        "Dihan Zheng",
        "Jiyan He",
        "Lin Huang",
        "Peiran Jin",
        "Shengjie Luo",
        "Tao Qin",
        "Tie-Yan Liu",
        "Yu Shi",
        "Yuxuan Ren"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "October 2024",
      "abstract": "In recent years, machine learning has demonstrated impressive capability in handling molecular science tasks. To support various molecular properties at scale, machine learning models are trained in the multi-task learning paradigm. Nevertheless, data of different molecular properties are often not aligned: some quantities, e.g. equilibrium structure, demand more cost to compute than others, e.g. energy, so their data are often generated by cheaper computational methods at the cost of lower accuracy, which cannot be directly overcome through multi-task learning. Moreover, it is not straightforward to leverage abundant data of other tasks to benefit a particular task. To handle such data heterogeneity challenges, we exploit the specialty of molecular tasks that there are physical laws connecting them, and design consistency training approaches that allow different tasks to exchange information directly so as to improve one another. Particularly, we demonstrate that the more accurate energy data can improve the accuracy of structure prediction. We also find that consistency training can directly leverage force and off-equilibrium structure data to improve structure prediction, demonstrating a broad capability for integrating heterogeneous data.",
      "url": "https://www.microsoft.com/en-us/research/publication/physical-consistency-bridges-heterogeneous-data-in-molecular-multi-task-learning/"
    },
    {
      "title": "WaveCoder: Widespread And Versatile Enhanced Instruction Tuning with Refined Data Generation",
      "authors": [
        "Can Xu",
        "Ning Shang",
        "Qiufeng Yin",
        "Wenxiang Hu",
        "Xin Zhang",
        "Yangyu Huang",
        "Yishujie Zhao",
        "Zhaojian Yu"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "October 2024",
      "abstract": "Recent work demonstrates that, after instruction tuning, Code Large Language Models (Code LLMs) can obtain impressive capabilities to address a wide range of code-related tasks. However, current instruction tuning methods for Code LLMs mainly focus on the traditional code generation task, resulting in poor performance in complex multi-task scenarios. In this paper, we concentrate on multiple code-related tasks and present WaveCoder, a series of Code LLMs trained with Widespread And Versatile Enhanced instruction data. To enable the models to tackle complex coderelated tasks, we propose a method to stably generate diverse, high-quality instruction data from open source code dataset in multitask scenarios and obtain CodeSeaXDataset, a dataset comprising 19,915 instruction instances across 4 code-related tasks, which is aimed at improving the generalization ability of Code LLM. Our experiments demonstrate that WaveCoder models significantly outperform other open-source models in terms of the generalization ability across different code-related tasks.",
      "url": "https://www.microsoft.com/en-us/research/publication/wavecoder-widespread-and-versatile-enhanced-instruction-tuning-with-refined-data-generation/"
    },
    {
      "title": "Alchemy: Amplifying Theorem-Proving Capability through Symbolic Mutation",
      "authors": [
        "Nan Duan",
        "Ping Wei",
        "Shaonan Wu",
        "Shuai Lu",
        "Yeyun Gong"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "",
      "abstract": "Formal proofs are challenging to write even for experienced experts. Recent progress in Neural Theorem Proving (NTP) shows promise in expediting this process. However, the formal corpora available on the Internet are limited compared to the general text, posing a significant data scarcity challenge for NTP. To address this issue, this work proposes Alchemy, a general framework for data synthesis that constructs formal theorems through symbolic mutation. Specifically, for each candidate theorem in Mathlib, we identify all invocable theorems that can be used to rewrite or apply to it. Subsequently, we mutate the candidate theorem by replacing the corresponding term in the statement with its equivalent form or antecedent. As a result, our method increases the number of theorems in Mathlib by an order of magnitude, from 110k to 6M. Furthermore, we perform continual pretraining and supervised finetuning on this augmented corpus for large language models. Experimental results demonstrate the effectiveness of our approach, achieving a 4.70% absolute performance improvement on Leandojo benchmark. Additionally, our approach achieves a 2.47% absolute performance gain on the out-of-distribution miniF2F benchmark based on the synthetic this http URL provide further insights, we conduct a comprehensive analysis of synthetic data composition and the training paradigm, offering valuable guidance for developing a strong theorem prover.",
      "url": "https://www.microsoft.com/en-us/research/publication/alchemy-amplifying-theorem-proving-capability-through-symbolic-mutation/"
    },
    {
      "title": "Confidential Container Groups: Implementing Confidential Computing on Azure Container Instances",
      "authors": [
        "Christoph M. Wintersteiger",
        "John Starks",
        "Ken Gordon",
        "Manuel Costa",
        "Matthew Johnson",
        "Sean T. Allen",
        "Stavros Volos",
        "Sylvan Clebsch"
      ],
      "research_areas": [
        "Security, privacy, and cryptography",
        "Systems and networking"
      ],
      "publication_date": "October 2024",
      "abstract": "Container-based technologies empower cloud tenants to develop highly portable software and deploy services in the cloud at a rapid pace. Cloud privacy, meanwhile, is important as a large number of container deployments operate on privacy-sensitive data, but challenging due to the increasing frequency and sophistication of attacks. State-of-the-art confidential container-based designs leverage process-based trusted execution environments (TEEs), but face security and compatibility issues that limit their practical deployment. \nWe present the Parma architecture, which provides lift-and-shift deployment of unmodified containers while providing strong security protection against a powerful attacker who controls the untrusted host and hypervisor. Parma leverages VM-level isolation to execute a container group within a unique VM-based TEE. Besides container integrity and user data confidentiality and integrity, Parma also offers container attestation and execution integrity based on an attested execution policy. This policy, which is specified by the customer, delimits the actions that the cloud service provider is allowed to take on their behalf when managing the container group. \nOur evaluation of Parma across a diverse set of workloads shows that container groups can enjoy the security guarantees of TEEs with at most 1% additional overhead. Finally, we have deployed Parma as the underlying technology driving Confidential Containers on Azure Container Instances.",
      "url": "https://www.microsoft.com/en-us/research/publication/confidential-container-groups/"
    },
    {
      "title": "DeepStack: Deeply Stacking Visual Tokens is Surprisingly Simple and Effective for LMMs",
      "authors": [
        "Jianfeng Gao",
        "Jianwei Yang",
        "Lingchen Meng",
        "Rui Tian",
        "Xiyang Dai",
        "Yu-Gang Jiang",
        "Zuxuan Wu"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "October 2024",
      "abstract": "Large multimodal models (LMMs) have shown tremendous improvements over the past year for multimodal understanding and reasoning. Currently, most (if not all) of the works attempt to connect vision and LLMs by feeding into a large language model (LLM) a string of visual tokens extracted from pretrained vision encoders (e.g., CLIP). Nevertheless, such a strategy brings considerable compute and memory overhead to the original LLMs due to extra visual tokens, which is particularly significant for high-resolution images and videos. Despite some efforts to mitigate this with sophisticated token compressions, the methods usually struggle to reach a good trade-off between efficacy and efficiency. In this work, we propose a new strategy for connecting vision and language transformers in large multimodal models (LMMs). Instead of stringing visual tokens as a sequence, we stack the visual tokens into multiple layers and then feed the subset at each layer into a corresponding transformer layer in LLMs, shown in Fig. 1. In the end, we propose DeepStack, a new architecture to connect vision and language in the context of LMMs. This simple strategy significantly unleash the power of LLMs for modeling the dependencies across a large number of visual tokens while keeping the compute marginally changed. Concretely, using the same recipe as LLaVA-1.5, our DeepStack uses the same context length but achieves significant gain over a wide range of vision-language benchmarks. In particular, our model brings 4.2, 11.0, and 4.0 performance gains on TextVQA, DocVQA and InfoVQA compared to LLaVA-1.5-7B, respectively.",
      "url": "https://www.microsoft.com/en-us/research/publication/dont-just-string-tokens-stack-them-improving-multimodal-transformers-with-layer-stack/"
    },
    {
      "title": "Multimodal Large Language Models Make Text-to-Image Generative Models Align Better",
      "authors": [
        "Furu Wei",
        "Shaohan Huang",
        "Xun Wu"
      ],
      "research_areas": [
        "Computer vision"
      ],
      "publication_date": "October 2024",
      "abstract": "Recent studies have demonstrated the exceptional potentials of leveraging human preference datasets to refine text-to-image generative models, enhancing the alignment between generated images and textual prompts. Despite these advances, current human preference datasets are either prohibitively expensive to construct or suffer from a lack of diversity in preference dimensions, resulting in limited applicability for instruction tuning in open-source text-to-image generative models and hinder further exploration. To address these challenges and promote the alignment of generative models through instruction tuning, we leverage multimodal large language models to create VisionPrefer, a high-quality and fine-grained preference dataset that captures multiple preference aspects. We aggregate feedback from AI annotators across four aspects: prompt-following, aesthetic, fidelity, and harmlessness to construct VisionPrefer. To validate the effectiveness of VisionPrefer, we train a reward model VP-Score over VisionPrefer to guide the training of text-to-image generative models and the preference prediction accuracy of VP-Score is comparable to human annotators. Furthermore, we use two reinforcement learning methods to supervised fine-tune generative models to evaluate the performance of VisionPrefer, and extensive experimental results demonstrate that VisionPrefer significantly improves text-image alignment in compositional image generation across diverse aspects, e.g., aesthetic, and generalizes better than previous human-preference metrics across various image distributions. Moreover, VisionPrefer indicates that the integration of AI-generated synthetic data as a supervisory signal is a promising avenue for achieving improved alignment with human preferences in vision generative models.",
      "url": "https://www.microsoft.com/en-us/research/publication/multimodal-large-language-models-make-text-to-image-generative-models-align-better/"
    },
    {
      "title": "What can Foundation Models’ Embeddings do?",
      "authors": [
        "Jianwei Yang",
        "Lijuan Wang",
        "Linjie Li"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "October 2024",
      "abstract": "Foundation models possess strong capabilities in reasoning and memorizing across modalities. To further unleash the power of foundation models, we present FIND, a generalized interface for aligning foundation models’ embeddings with unified image and dataset-level understanding spanning modality and granularity. As shown in Fig.1, a lightweight transformer interface without tuning any foundation model weights is enough for segmentation, grounding, and retrieval in an interleaved manner. The proposed interface has the following favorable attributes: (1) Generalizable. It applies to various tasks spanning retrieval, segmentation, etc., under the same architecture and weights. (2) Interleavable. With the benefit of multi-task multi-modal training, the proposed interface creates an interleaved shared embedding space. (3) Extendable. The proposed interface is adaptive to new tasks, and new models. In light of the interleaved embedding space, we introduce FIND-Bench, which introduces new training and evaluation annotations to the COCO dataset for interleaved segmentation and retrieval. We are the first work aligning foundations models’ embeddings for interleave understanding. Meanwhile, our approach achieves state-of-the-art performance on FIND-Bench and competitive performance on standard retrieval and segmentation settings.",
      "url": "https://www.microsoft.com/en-us/research/publication/what-can-foundation-models-embeddings-do/"
    },
    {
      "title": "Topic-Conversation Relevance (TCR) Dataset and Benchmarks",
      "authors": [
        "Jamie Pool",
        "Ross Cutler",
        "Senja Filipi",
        "Yaran Fan"
      ],
      "research_areas": [
        "Human-computer interaction"
      ],
      "publication_date": "October 2024",
      "abstract": "Workplace meetings are vital to organizational collaboration, yet a large percentage of meetings are rated as ineffective. To help improve meeting effectiveness by understanding if the conversation is on topic, we create a comprehensive Topic-Conversation Relevance (TCR) Dataset that covers a variety of domains and meeting styles. The TCR dataset includes 1,500 unique meetings, 22,000 words in transcripts, and over 15,000 meeting topics, sourced from both newly collected Speech Interruption Meeting (SIM) data and existing public datasets. Along with the text data, we also open-source scripts to generate synthetic meetings or create augmented meetings from the TCR dataset to enhance the data diversity. For each data source, benchmarks are created using GPT-4 to evaluate the model accuracy in understanding transcription-topic relevance.",
      "url": "https://www.microsoft.com/en-us/research/publication/topic-conversation-relevance-tcr-dataset-and-benchmarks/"
    },
    {
      "title": "Ranking with Multiple Objectives",
      "authors": [
        "Nikhil Devanur",
        "Sivakanth Gopi"
      ],
      "research_areas": [
        "Algorithms"
      ],
      "publication_date": "October 2024",
      "abstract": "In search and advertisement ranking, it is often required to simultaneously maximize multiple objectives. For example, the objectives can correspond to multiple intents of a search query, or in the context of advertising, they can be relevance and revenue. It is important to efficiently find rankings which strike a good balance between such objectives.\nMotivated by such applications, we formulate a general class of problems where\n(1) each result gets a different score corresponding to each objective,\n(2) the results of a ranking are aggregated by taking, for each objective, a weighted sum of the scores in the order of the ranking, and\n(3) an arbitrary concave function of the aggregates is maximized.\nCombining the aggregates using a concave function will naturally lead to more balanced outcomes. We give an approximation algorithm in a bicriteria/resource augmentation setting: the algorithm with a slight advantage does as well as the optimum. In particular, if the aggregation step is just the sum of the top k results, then the algorithm outputs k+1 results which do as well the as the optimal top k results. Our proof relies on a topological argument to reduce a convex optimization problem to simple binary search, thus making our algorithms run in nearly linear time.\nWe show how this approach helps with balancing different objectives via simulations on synthetic data as well as on real data from LinkedIn.",
      "url": "https://www.microsoft.com/en-us/research/publication/ranking-with-multiple-objectives/"
    },
    {
      "title": "Implicit Curriculum in Procgen Made Explicit",
      "authors": [
        "Kaixin Wang",
        "Xinchao Wang"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "October 2024",
      "abstract": "Procedurally generated environments such as Procgen Benchmark provides a testbed for evaluating the agent’s ability to robustly learn a relevant skill, by situating the agent in ever-changing levels. The diverse levels associated with varying contexts are naturally connected to curriculum learning. Existing works mainly focus on arranging the levels to explicitly form a curriculum. In this work, we take a close look at the learning process itself under the multi-level training in Procgen. Interestingly, the learning process exhibits a gradual shift from easy contexts to hard contexts, suggesting an implicit curriculum in multi-level training. Our analysis is made possible through C-Procgen, a benchmark we build upon Procgen that enables explicit control of the contexts. We believe our findings will foster a deeper understanding of learning in diverse contexts, and our benchmark will benefit future research in curriculum reinforcement learning.",
      "url": "https://www.microsoft.com/en-us/research/publication/implicit-curriculum-in-procgen-made-explicit/"
    },
    {
      "title": "Improving Steering and Verification in AI-Assisted Data Analysis with Interactive Task Decomposition",
      "authors": [
        "Advait Sarkar",
        "Austin Henley",
        "Carina Negreanu",
        "Ian Drosos",
        "Jack Williams",
        "Majeed Kazemitabaar",
        "Tovi Grossman"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Data platforms and analytics",
        "Human-computer interaction"
      ],
      "publication_date": "October 2024",
      "abstract": "LLM-powered tools like ChatGPT Data Analysis, have the potential to help users tackle the challenging task of data analysis programming, which requires expertise in data processing, programming, and statistics. However, our formative study (n=15) uncovered serious challenges in verifying AI-generated results and steering the AI (i.e., guiding the AI system to produce the desired output). We developed two contrasting approaches to address these challenges. The first (Stepwise) decomposes the problem into step-by-step subgoals with pairs of editable assumptions and code until task completion, while the second (Phasewise) decomposes the entire problem into three editable, logical phases: structured input/output assumptions, execution plan, and code. A controlled, within-subjects experiment (n=18) compared these systems against a conversational baseline. Users reported significantly greater control with the Stepwise and Phasewise systems, and found intervention, correction, and verification easier, compared to the baseline. The results suggest design guidelines and trade-offs for AI-assisted data analysis tools",
      "url": "https://www.microsoft.com/en-us/research/publication/improving-steering-and-verification-in-ai-assisted-data-analysis-with-interactive-task-decomposition/"
    },
    {
      "title": "AvatarPilot: Decoupling One-to-One Motions from their Semantics with Weighted Interpolations",
      "authors": [
        "Andrea Stevenson Won",
        "Cheng Yao Wang",
        "Eyal Ofek",
        "Hyunju Kim",
        "Mar Gonzalez Franco",
        "Payod Panda"
      ],
      "research_areas": [
        "Human-computer interaction"
      ],
      "publication_date": "October 2024",
      "abstract": "Physical restrictions of the real spaces where users are situated present challenges to remote XR and spatial computing interactions using avatars. Users may not have available space in their physical environment to duplicate the physical set-up of their collaborators, but if avatars are relocated, one-to-one motions may no longer preserve meaning. We propose a solution: using weighted interpolations we can guarantee that everybody is looking or pointing at the same objects, both locally and remotely. At the same time, this preserves the meaning of gestures and postures that are not object-directed (i.e., that are close to the body). We extend this work to locomotion and direct interactions in near space such as grabbing of objects; exploring the limits of our social and scene understanding and finding a new use for Inverse Kinematics (IK). We discuss limitations and applications and open-source the AvatarPilot for general use.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
      "url": "https://www.microsoft.com/en-us/research/publication/avatarpilot-decoupling-one-to-one-motions-from-their-semantics-with-weighted-interpolations/"
    },
    {
      "title": "Predictability of identifier naming with Copilot: A case study for mixed-initiative programming tools",
      "authors": [
        "Advait Sarkar",
        "Alan F. Blackwell",
        "Michael Jing Long Lee"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Human-computer interaction",
        "Programming languages and software engineering"
      ],
      "publication_date": "October 2024",
      "abstract": "Studies show that predictive text entry systems make writing faster, but written content more predictable. We consider if these trade-offs extend to code synthesis tools such as GitHub Copilot. While Copilot can make developers produce code faster, it may also affect how they choose identifiers for methods and classes. This may have non-trivial effects on the activity of programming, because identifier names are a primary semantic signal in code, and play important roles in authoring, debugging, and developer communication. In a controlled, within-subjects experiment (n=12), we compared identifiers chosen in the presence and absence of Copilot suggestions. We find that identifiers chosen in the presence of Copilot suggestions were significantly more predictable (have lower mean entropy), even when suggestions were only visible and could not be automatically accepted. These results imply that mixed-initiative systems can take an active role in shaping programmer intentions and potentially impact their sense of agency. We consider whether an increased convergence towards predictable names is an asset or a liability for the practice of programming, and suggest design opportunities for surfacing surprising identifiers and conceptual refactoring tools.",
      "url": "https://www.microsoft.com/en-us/research/publication/predictability-of-identifier-naming-with-copilot-a-case-study-for-mixed-initiative-programming-tools/"
    },
    {
      "title": "IGOR: Image-GOal Representations are the Atomic Control Units for Foundation Models in Embodied AI",
      "authors": [
        "Chuheng Zhang",
        "Derek Yang",
        "Jiang Bian",
        "Junliang Guo",
        "Li Zhao",
        "Pushi Zhang",
        "Tianyu He",
        "Xiaoyu Chen"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "October 2024",
      "abstract": "We introduce Image-GOal Representations (IGOR), aiming to learn a unified, semantically consistent action space across human and various robots. Through this unified latent action space, IGOR enables knowledge transfer among large-scale robot and human activity data. We achieve this by compressing visual changes between an initial image and its goal state into latent actions. IGOR allows us to generate latent action labels for internet-scale video data. This unified latent action space enables the training of foundation policy and world models across a wide variety of tasks performed by both robots and humans. We demonstrate that: (1) IGOR learns a semantically consistent action space for both human and robots, characterizing various possible motions of objects representing the physical interaction knowledge; (2) IGOR can “migrate” the movements of the object in the one video to other videos, even across human and robots, by jointly using the latent action model and world model; (3) IGOR can learn to align latent actions with natural language through the foundation policy model, and integrate latent actions with a low-level policy model to achieve effective robot control. We believe IGOR opens new possibilities for human-to-robot knowledge transfer and control.",
      "url": "https://www.microsoft.com/en-us/research/publication/igor-image-goal-representations-are-the-atomic-control-units-for-foundation-models-in-embodied-ai/"
    },
    {
      "title": "Scaling the Codebook Size of VQ-GAN to 100,000 with a Utilization Rate of 99%",
      "authors": [
        "Dong Chen",
        "Fangyun Wei"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "October 2024",
      "abstract": "In the realm of image quantization exemplified by VQGAN, the process encodes images into discrete tokens drawn from a codebook with a predefined size. Recent advancements, particularly with LLAMA 3, reveal that enlarging the codebook significantly enhances model performance. However, VQGAN and its derivatives, such as VQGAN-FC (Factorized Codes) and VQGAN-EMA, continue to grapple with challenges related to expanding the codebook size and enhancing codebook utilization. For instance, VQGAN-FC is restricted to learning a codebook with a maximum size of 16,384, maintaining a typically low utilization rate of less than 12% on ImageNet. In this work, we propose a novel image quantization model named VQGAN-LC (Large Codebook), which extends the codebook size to 100,000, achieving an utilization rate exceeding 99%. Unlike previous methods that optimize each codebook entry, our approach begins with a codebook initialized with 100,000 features extracted by a pre-trained vision encoder. Optimization then focuses on training a projector that aligns the entire codebook with the feature distributions of the encoder in VQGAN-LC. We demonstrate the superior performance of our model over its counterparts across a variety of tasks, including image reconstruction, image classification, auto-regressive image generation using GPT, and image creation with diffusion- and flow-based generative models.",
      "url": "https://www.microsoft.com/en-us/research/publication/scaling-the-codebook-size-of-vq-gan-to-100000-with-a-utilization-rate-of-99/"
    },
    {
      "title": "Meta-Diffu$B$: A Contextualized Sequence-to-Sequence Text Diffusion Model with Meta-Exploration",
      "authors": [
        "Kevin Lin"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "October 2024",
      "abstract": "The diffusion model, a new generative modeling paradigm, has achieved significant success in generating images, audio, video, and text. It has been adapted for sequence-to-sequence text generation (Seq2Seq) through DiffuSeq, termed S2S Diffusion. Existing S2S-Diffusion models predominantly rely on fixed or hand-crafted rules to schedule noise during the diffusion and denoising processes. However, these models are limited by non-contextualized noise, which fails to fully consider the characteristics of Seq2Seq tasks. In this paper, we propose the Meta-Diffu$B$ framework—a novel scheduler-exploiter S2S-Diffusion paradigm designed to overcome the limitations of existing S2S-Diffusion models. We employ Meta-Exploration to train an additional scheduler model dedicated to scheduling contextualized noise for each sentence. Our exploiter model, an S2S-Diffusion model, leverages the noise scheduled by our scheduler model for updating and generation. Meta-Diffu$B$ achieves state-of-the-art performance compared to previous S2S-Diffusion models and fine-tuned pre-trained language models (PLMs) across four Seq2Seq benchmark datasets. We further investigate and visualize the impact of Meta-Diffu$B$’s noise scheduling on the generation of sentences with varying difficulties. Additionally, our scheduler model can function as a “plug-and-play” model to enhance DiffuSeq without the need for fine-tuning during the inference stage.",
      "url": "https://www.microsoft.com/en-us/research/publication/meta-diffub-a-contextualized-sequence-to-sequence-text-diffusion-model-with-meta-exploration/"
    },
    {
      "title": "Semi-Random Matrix Completion via Flow-Based Adaptive Reweighting",
      "authors": [
        "Jerry Li"
      ],
      "research_areas": [
        "Algorithms"
      ],
      "publication_date": "October 2024",
      "abstract": "We consider the well-studied problem of completing a rank-$r$, $\\mu$-incoherent matrix $\\mathbf{M} \\in \\mathbb{R}^{d \\times d}$ from incomplete observations. We focus on this problem in the semi-random setting where each entry is independently revealed with probability at least $p = \\frac{\\textup{poly}(r, \\mu, \\log d)}{d}$. Whereas multiple nearly-linear time algorithms have been established in the more specialized fully-random setting where each entry is revealed with probability exactly $p$, the only known nearly-linear time algorithm in the semi-random setting is due to [CG18], whose sample complexity has a polynomial dependence on the inverse accuracy and condition number and thus cannot achieve high-accuracy recovery. Our main result is the first high-accuracy nearly-linear time algorithm for solving semi-random matrix completion, and an extension to the noisy observation setting. Our result builds upon the recent short-flat decomposition framework of [KLLST23a, KLLST23b] and leverages fast algorithms for flow problems on graphs to solve adaptive reweighting subproblems efficiently.",
      "url": "https://www.microsoft.com/en-us/research/publication/semi-random-matrix-completion-via-flow-based-adaptive-reweighting/"
    },
    {
      "title": "COIN: Chance-Constrained Imitation Learning for Safe and Adaptive Resource Oversubscription under Uncertainty",
      "authors": [
        "Bo Qiao",
        "Chao Du",
        "Chetan Bansal",
        "Dongmei Zhang",
        "Fangkai Yang",
        "Hang Dong",
        "Lu Wang",
        "Mayukh Das",
        "Qi Zhang",
        "Qingwei Lin 林庆维",
        "Saravan Rajmohan",
        "Si Qin"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Systems and networking"
      ],
      "publication_date": "October 2024",
      "abstract": "We address the real problem of safe, robust, adaptive resource oversubscription in uncertain environments with our proposed novel technique of chance-constrained imitation learning. Our objective is to enhance resource efficiency while ensuring safety against congestion risk. Traditional supervised or forecasting models are ineffective in learning adaptive oversubscription policies, and conventional online optimization or reinforcement learning is difficult to deploy on real systems. Offline policy learning methods, such as Imitation Learning (IL) can leverage historical resource utilization telemetry data to learn effective policies if we can ensure robustness and safety from the underlying uncertainty in the domain, and thus the data. Our work investigates the nature of this uncertainty, how it can be quantified and proposes a novel chance-constrained IL that implicitly models such uncertainty in a principled manner via additional knowledge in the form of stochastic constraints on the associated risk, to learn provably safe and robust policies. We show empirically a substantial improvement (~ 3-4x) in capacity efficiency and congestion safety in test as well as real deployments.",
      "url": "https://www.microsoft.com/en-us/research/publication/coin-chance-constrained-imitation-learning-for-safe-and-adaptive-resource-oversubscription-under-uncertainty/"
    },
    {
      "title": "Predictor-Corrector Enhanced Transformers with Exponential Moving Average Coefficient Learning",
      "authors": [
        "Junliang Guo",
        "Rui Wang",
        "Xu Tan"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "October 2024",
      "abstract": "Residual networks, as discrete approximations of Ordinary Differential Equations (ODEs), have inspired significant advancements in neural network design, including multistep methods, high-order methods, and multi-particle dynamical systems. The precision of the solution to ODEs significantly affects parameter optimization, thereby impacting model performance. In this work, we present a series of advanced explorations of Transformer architecture design to minimize the error compared to the true “solution.” First, we introduce a predictor-corrector learning framework to minimize truncation errors, which consist of a high-order predictor and a multistep corrector. Second, we propose an exponential moving average-based coefficient learning method to further strengthen our higher-order predictor. Extensive experiments on large-scale machine translation, abstractive summarization, language modeling, and natural language understanding benchmarks demonstrate the superiority of our approach. On the WMT’14 English-German and English-French tasks, our model achieved BLEU scores of 30.95 and 44.27, respectively. Additionally, on the OPUS multilingual machine translation task, our model surpasses a robust 3.8B DeepNet by an average of 2.9 SacreBLEU, using only one-third of the parameters.",
      "url": "https://www.microsoft.com/en-us/research/publication/predictor-corrector-enhanced-transformers-with-exponential-moving-average-coefficient-learning/"
    },
    {
      "title": "Mesa-Extrapolation: A Weave Position Encoding Method for Enhanced Extrapolation in LLMs",
      "authors": [
        "Jingjing Liu"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "October 2024",
      "abstract": "Large language models (LLMs), although having revolutionized many fields, still suffer from the challenging extrapolation problem, where the inference ability of LLMs sharply declines beyond their max training lengths. In this work, we conduct a theoretical analysis to better understand why No Position Encoding (NoPE) fails outside its effective range, as well as examining the power of Position Encoding (PE) in this context. Our findings reveal that with meticulous weave position, PE can indeed be extended beyond effective range. Our theorems establish that LLMs equipped with weave PE can achieve improved extrapolation performance without additional cost. Furthermore, we introduce a novel weave PE method, Mesa-Extrapolation, which utilizes a chunk-based triangular attention matrix and applies Stair PE to manage the final chunk. This method not only retains competitive performance but also offers substantial benefits such as significantly reduced memory demand and faster inference speed. Extensive experiments validate the effectiveness of Mesa-Extrapolation, demonstrating its potential as a scalable solution to enhancing LLMs’ applicative reach.",
      "url": "https://www.microsoft.com/en-us/research/publication/mesa-extrapolation-a-weave-position-encoding-method-for-enhanced-extrapolation-in-llms/"
    },
    {
      "title": "AdaWiFi, Collaborative WiFi Sensing for Cross-Environment Adaptation",
      "authors": [
        "Chuchu Dong",
        "Naiyu Zheng",
        "Rongchun Yao",
        "Shiqi Jiang",
        "Yuanchun Li",
        "Yuanzhe Li"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Systems and networking"
      ],
      "publication_date": "October 2024",
      "abstract": "Deep learning (DL) based Wi-Fi sensing has witnessed great development in recent years. Although decent results have been achieved in certain scenarios, Wi-Fi based activity recognition is still difficult to deploy in real smart homes due to the limited cross-environment adaptability, i.e. a well-trained Wi-Fi sensing neural network in one environment is hard to adapt to other environments. To address this challenge, we propose AdaWiFi , a DL-based Wi-Fi sensing framework that allows multiple Internet-of-Things (IoT) devices to collaborate and adapt to various environments effectively. The key innovation of AdaWiFi includes a collective sensing model architecture that utilizes complementary information between distinct devices and avoids the biased perception of individual sensors and an accompanying model adaptation technique that can transfer the sensing model to new environments with limited data. We evaluate our system on a public dataset and a custom dataset collected from three complex sensing environments. The results demonstrate that AdaWiFi is able to achieve significantly better sensing adaptation effectiveness (e.g. 30% higher accuracy with one-shot adaptation) as compared with state-of-the-art baselines.",
      "url": "https://www.microsoft.com/en-us/research/publication/adawifi-collaborative-wifi-sensing-for-cross-environment-adaptation/"
    },
    {
      "title": "Early Bird: Ensuring Reliability of Cloud Systems Through Early Failure Prediction",
      "authors": [
        "Chetan Bansal",
        "Dongmei Zhang",
        "Minghua Ma",
        "Pu Zhao",
        "Qingwei Lin 林庆维",
        "Saravan Rajmohan",
        "Yudong Liu"
      ],
      "research_areas": [
        "Algorithms"
      ],
      "publication_date": "October 2024",
      "abstract": "As cloud service continues to dominate various sectors, the reliability of cloud infrastructures becomes crucial. Traditional methods of failure prediction often fall short in providing sufficient time for preventative measures. This paper presents a failure prediction framework, Early Bird, designed to address these challenges by integrating novel data handling and prediction strategies. Our approach utilizes enhanced sample generation techniques and a unique adaptive loss function within a unified prediction model, aiming for early and precise failure detection. We present a comprehensive analysis conducted at Microsoft, demonstrating the ability to predict potential failures up to 20 minutes earlier than conventional methods while maintaining accuracy across various prediction models, including LSTM and Transformer.",
      "url": "https://www.microsoft.com/en-us/research/publication/early-bird-ensuring-reliability-of-cloud-systems-through-early-failure-prediction/"
    },
    {
      "title": "The New Digital Divide",
      "authors": [
        "Allen Kim",
        "Juan M. Lavista Ferres",
        "Lucia Ronchi Darre",
        "Mayana Pereira",
        "Prasanna Tambe",
        "Raffaella Sadun",
        "Rahul Dodhia",
        "Shane Greenstein",
        "Tammy Glazer"
      ],
      "research_areas": [
        "Data platforms and analytics"
      ],
      "publication_date": "October 2024",
      "abstract": "We build and analyze new metrics of digital usage that leverage telemetry data collected by Microsoft during operating system updates across forty million Windows devices in US households. These measures of US household digital usage are much more comprehensive than those made available through any existing commercial or government survey. We construct representations of devices in ZIP codes and find evidence of significant variation in usage reflecting an urban-rural divide. We also show the existence of substantial disparities in usage even within narrowly defined Metropolitan Statistical Areas. Income and education correlate with these observed differences. These effects are large and suggest digital literacy gaps that extend beyond the availability of essential IT infrastructure at the local level. These findings call for interventions beyond the traditional focus on infrastructure access and address usage and skills development. The indices are made publicly available to support future research.",
      "url": "https://www.microsoft.com/en-us/research/publication/the-new-digital-divide/"
    },
    {
      "title": "A Large-Scale Human-Centric Benchmark for Referring Expression Comprehension in the LMM Era",
      "authors": [
        "Fangyun Wei"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "October 2024",
      "abstract": "Prior research in human-centric AI has primarily addressed single-modality tasks like pedestrian detection, action recognition, and pose estimation. However, the emergence of large multimodal models (LMMs) such as GPT-4V has redirected attention towards integrating language with visual content. Referring expression comprehension (REC) represents a prime example of this multimodal approach. Current human-centric REC benchmarks, typically sourced from general datasets, fall short in the LMM era due to their limitations, such as insufficient testing samples, overly concise referring expressions, and limited vocabulary, making them inadequate for evaluating the full capabilities of modern REC models. In response, we present HC-RefLoCo (Human-Centric Referring Expression Comprehension with Long Context), a benchmark that includes 13,452 images, 24,129 instances, and 44,738 detailed annotations, encompassing a vocabulary of 18,681 words. Each annotation, meticulously reviewed for accuracy, averages 93.2 words and includes topics such as appearance, human-object interaction, location, action, celebrity, and OCR. HC-RefLoCo provides a wider range of instance scales and diverse evaluation protocols, encompassing accuracy with various IoU criteria, scale-aware evaluation, and subject-specific assessments. Our experiments, which assess 24 models, highlight HC-RefLoCo’s potential to advance human-centric AI by challenging contemporary REC models with comprehensive and varied data. Our benchmark, along with the evaluation code, are available at https://github.com/ZhaoJingjing713/HC-RefLoCo (opens in new tab).",
      "url": "https://www.microsoft.com/en-us/research/publication/a-large-scale-human-centric-benchmark-for-referring-expression-comprehension-in-the-lmm-era/"
    },
    {
      "title": "ARLON: Boosting Diffusion Transformers with Autoregressive Models for Long Video Generation",
      "authors": [
        "Furu Wei",
        "Hefei Ling",
        "Jeongsoo Choi",
        "Jinyu Li",
        "Lingwei Meng",
        "Long Zhou",
        "Shujie Hu",
        "Shujie Liu",
        "Xun Guo",
        "Zongyi Li"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "",
      "abstract": "Text-to-video models have recently undergone rapid and substantial advancements. Nevertheless, due to limitations in data and computational resources, achieving efficient generation of long videos with rich motion dynamics remains a significant challenge. To generate high-quality, dynamic, and temporally consistent long videos, this paper presents ARLON, a novel framework that boosts diffusion Transformers with autoregressive models for long video generation, by integrating the coarse spatial and long-range temporal information provided by the AR model to guide the DiT model. Specifically, ARLON incorporates several key innovations: 1) A latent Vector Quantized Variational Autoencoder (VQ-VAE) compresses the input latent space of the DiT model into compact visual tokens, bridging the AR and DiT models and balancing the learning complexity and information density; 2) An adaptive norm-based semantic injection module integrates the coarse discrete visual units from the AR model into the DiT model, ensuring effective guidance during video generation; 3) To enhance the tolerance capability of noise introduced from the AR inference, the DiT model is trained with coarser visual latent tokens incorporated with an uncertainty sampling module. Experimental results demonstrate that ARLON significantly outperforms the baseline OpenSora-V1.2 on eight out of eleven metrics selected from VBench, with notable improvements in dynamic degree and aesthetic quality, while delivering competitive results on the remaining three and simultaneously accelerating the generation process. In addition, ARLON achieves state-of-the-art performance in long video generation. Detailed analyses of the improvements in inference efficiency are presented, alongside a practical application that demonstrates the generation of long videos using progressive text prompts.",
      "url": "https://www.microsoft.com/en-us/research/publication/arlon-boosting-diffusion-transformers-with-autoregressive-models-for-long-video-generation/"
    },
    {
      "title": "“For Us By Us”: Intentionally Designing Technology for Lived Black Experiences",
      "authors": [
        "Brittany Johnson",
        "Christina N. Harrington",
        "Denae Ford",
        "Leslie Coney",
        "Lisa Egede"
      ],
      "research_areas": [
        "Human-computer interaction"
      ],
      "publication_date": "October 2024",
      "abstract": "HCI research to date has only scratched the surface of the unique approaches racially minoritized communities take to building, designing, and using technology systems. While there has been an increase in understanding how people across racial groups create community across different platforms, there is still a lack of studies that explicitly center on how Black technologists design with and for their own communities. In this paper, we present findings from a series of semi-structured interviews with Black technologists who have used, created, or curated resources to support lived Black experiences. From their experiences, we find a multifaceted approach to design as a means of survival, to stay connected, for cultural significance, and to bask in celebratory joy. Further, we provide considerations that emphasize the need for centering lived Black experiences in design and share approaches that can empower the broader research community to conduct further inquiries into design focused on those in the margins.",
      "url": "https://www.microsoft.com/en-us/research/publication/for-us-by-us-intentionally-designing-technology-for-lived-black-experiences/"
    },
    {
      "title": "Context Pruning for More Robust SMT-based Program Verification",
      "authors": [
        "Bryan Parno",
        "Jay Bosamiya",
        "Jessica Li",
        "Marijn Heule",
        "Yi Zhou"
      ],
      "research_areas": [
        "Programming languages and software engineering"
      ],
      "publication_date": "October 2024",
      "abstract": "SMT solvers provide powerful proof automation for program verification. However, relying on SMT solvers also leads to proof instability, where a previously successful proof may fail after the developer makes trivial modifications to the source program. Such instability is a major headache for developers, but the causes and potential mitigations for it have received limited attention. In this study, we find that irrelevant query context accounts for 78% of the instability in existing program-verification query sets. As a result, we design SHAKE, a novel technique that leverages the structure in program-verification SMT queries in order to filter out irrelevant context from such queries. SHAKE is the first SMT-level technique that targets instability, and we implement it as a pre-processing step for SMT solvers. We evaluate SHAKE on real-world, large-scale query sets, and we find that it leads to large reduction in context and a 29% and 41% improvement in query stability on Z3 and cvc5, with minor performance overhead.",
      "url": "https://www.microsoft.com/en-us/research/publication/context-pruning-for-more-robust-smt-based-program-verification/"
    },
    {
      "title": "FXAM: A unified and fast interpretable model for predictive analytics",
      "authors": [
        "Dongmei Zhang",
        "Rui Ding",
        "Shi Han",
        "Tianchi Qiao",
        "Yuanyuan Jiang",
        "Yunan Zhu"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Data platforms and analytics"
      ],
      "publication_date": "October 2024",
      "abstract": "Generalized Additive Model (GAM) is a standard for interpretability. However, due to the one-to-many and many-to-one phenomena which appear commonly in real-world scenarios, existing GAMs have limitations to serve predictive analytics in terms of both accuracy and training efficiency. In this paper, we propose FXAM (Fast and eXplainable Additive Model), a unified and fast interpretable model for predictive analytics. FXAM extends GAM’s modeling capability with a unified additive model for numerical, categorical, and temporal features. FXAM conducts a novel training procedure called Three-Stage Iteration (TSI). TSI corresponds to learning over numerical, categorical, and temporal features respectively. Each stage learns a local optimum by fixing the parameters of other stages. We design joint learning over categorical features and partial learning over temporal features to achieve high accuracy and training efficiency. We prove that TSI is guaranteed to converge to the global optimum. We further propose a set of optimization techniques to speed up FXAM’s training algorithm to meet the needs of interactive analysis.",
      "url": "https://www.microsoft.com/en-us/research/publication/fxam-a-unified-and-fast-interpretable-model-for-predictive-analytics/"
    },
    {
      "title": "CaesarNeRF: Calibrated Semantic Representation for Few-shot Generalizable Neural Rendering",
      "authors": [
        "Haidong Zhu",
        "Ilya Zharkov",
        "Luming Liang",
        "Ram Nevatia",
        "Tianyi Chen",
        "Tianyu Ding"
      ],
      "research_areas": [
        "Computer vision"
      ],
      "publication_date": "October 2024",
      "abstract": "Generalizability and few-shot learning are key challenges in Neural Radiance Fields (NeRF), often due to the lack of a holistic understanding in pixel-level rendering. We introduce CaesarNeRF, an end-to-end approach that leverages scene-level CAlibratEd SemAntic Representation along with pixel-level representations to advance few-shot, generalizable neural rendering, facilitating a holistic understanding without compromising high-quality details. CaesarNeRF explicitly models pose differences of reference views to combine scene-level semantic representations, providing a calibrated holistic understanding. This calibration process aligns various viewpoints with precise location and is further enhanced by sequential refinement to capture varying details. Extensive experiments on public datasets, including LLFF, Shiny, mip-NeRF 360, and MVImgNet, show that CaesarNeRF delivers state-of-the-art performance across varying numbers of reference views, proving effective even with a single reference image.",
      "url": "https://www.microsoft.com/en-us/research/publication/caesarnerf-calibrated-semantic-representation-for-few-shot-generalizable-neural-rendering/"
    },
    {
      "title": "Boosting Text-to-Video Generative Model with MLLMs Feedback",
      "authors": [
        "Furu Wei",
        "Shaohan Huang",
        "Xun Wu"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "October 2024",
      "abstract": "Recent advancements in text-to-video generative models, such as Sora, have showcased impressive capabilities. These models have attracted significant interest for their potential applications. However, they often rely on extensive datasets of variable quality, which can result in generated videos that lack aesthetic appeal and do not accurately reflect the input text prompts. A promising approach to mitigate these issues is to leverage Reinforcement Learning from Human Feedback (RLHF), which aims to align the outputs of text-to-video generative with human preferences. However, the considerable costs associated with manual annotation have led to a scarcity of comprehensive preference datasets. In response to this challenge, our study begins by investigating the efficacy of Multimodal Large Language Models (MLLMs) generated annotations in capturing video preferences, discovering a high degree of concordance with human judgments. Building upon this finding, we utilize MLLMs to perform fine-grained video preference annotations across two dimensions, resulting in the creation of VideoPrefer, which includes 135,000 preference annotations. Utilizing this dataset, we introduce VideoRM, the first general-purpose reward model tailored for video preference in the text-to-video domain. Our comprehensive experiments confirm the effectiveness of both VideoPrefer and VideoRM, representing a significant step forward in the field.",
      "url": "https://www.microsoft.com/en-us/research/publication/boosting-text-to-video-generative-model-with-mllms-feedback/"
    },
    {
      "title": "Active, anytime-valid risk controlling prediction sets",
      "authors": [
        "Nikos Karampatziakis",
        "Paul Mineiro"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "October 2024",
      "abstract": "Rigorously establishing the safety of black-box machine learning models with respect to critical risk measures is important for providing guarantees about the behavior of the model. Recently, a notion of a risk controlling prediction set (RCPS) has been introduced by Bates et. al. (JACM ’24) for producing prediction sets that are statistically guaranteed to have low risk from machine learning models. Our method extends this notion to the sequential setting, where we provide guarantees even when the data is collected adaptively, and ensures the risk guarantee is anytime-valid, i.e., simultaneously holds at all time steps. Further, we propose a framework for constructing RCPSes for active labeling, i.e., allowing one to use a labeling policy that chooses whether to query the true label for each received data point, and ensures the expected proportion data points whose labels are queried are below a predetermined label budget. We also describe how to use predictors (e.g., the machine learning model we are providing risk control guarantees for) to further improve the utility of our RCPSes by estimating the expected risk conditioned on the covariates. We characterize the optimal choices of label policy under a fixed label budget, and predictor, and show a regret result that relates the estimation error of the optimal labeling policy and predictor to the wealth process that underlies our RCPSes. Lastly, we present practical ways of formulating label policies and we empirically show that our label policies use fewer labels to reach higher utility than naive baseline labeling strategies (e.g., labeling all points, randomly labeling points) on both simulations and real data.",
      "url": "https://www.microsoft.com/en-us/research/publication/active-anytime-valid-risk-controlling-prediction-sets/"
    },
    {
      "title": "MedImageInsight: An Open-Source Embedding Model for General Domain Medical Imaging",
      "authors": [
        "Alan McMillan",
        "Alberto Santamaria-Pang",
        "Asma Ben Abacha",
        "Daniel Holstein",
        "Gaurav Rajguru",
        "Ho Hin Lee",
        "Hoifung Poon",
        "Ivan Tarapov",
        "Javier Alvarez-Valle",
        "Jenq-Neng Hwang",
        "John Garett",
        "Madhu Maddi",
        "Matthew P Lungren",
        "Mu Wei",
        "Natieek Sangani",
        "Naveen Gaur",
        "Nick Mecklenburg",
        "Nilesh Vijayrania",
        "Noel Codella",
        "Reehan Bhimai",
        "Rupal Jain",
        "Sheng Zhang",
        "Shrey Jain",
        "Shruthi Bannur",
        "Stephanie Hyland",
        "Thomas Lin",
        "Vijay Aski",
        "Will Guyman",
        "Xue Li",
        "Yu Gu"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Computer vision",
        "Medical, health and genomics"
      ],
      "publication_date": "October 2024",
      "abstract": "In this work, we present MedImageInsight, an open-source medical imaging embedding model. MedImageInsight is trained on medical images with associated text and labels across a diverse collection of domains, including X-Ray, CT, MRI, dermoscopy, OCT, fundus photography, ultrasound, histopathology, and mammography. Rigorous evaluations demonstrate MedImageInsight’s ability to achieve state-of-the-art (SOTA) or human expert level performance across classification, image-image search, and fine-tuning tasks. Specifically, on public datasets, MedImageInsight achieves SOTA in CT 3D medical image retrieval, as well as SOTA in disease classification and search for chest X-ray, dermatology, and OCT imaging. Furthermore, MedImageInsight achieves human expert performance in bone age estimation (on both public and partner data), as well as AUC above 0.9 in most other domains. When paired with a text decoder, MedImageInsight achieves near SOTA level single image report findings generation with less than 10% the parameters of other models. Compared to fine-tuning GPT-4o with only MIMIC-CXR data for the same task, MedImageInsight outperforms in clinical metrics, but underperforms on lexical metrics where GPT-4o sets a new SOTA. Importantly for regulatory purposes, MedImageInsight can generate ROC curves, adjust sensitivity and specificity based on clinical need, and provide evidence-based decision support through image-image search (which can also enable retrieval augmented generation). In an independent clinical evaluation of image-image search in chest X-ray, MedImageInsight outperformed every other publicly available foundation model evaluated by large margins (over 6 points AUC), and significantly outperformed other models in terms of AI fairness (across age and gender). We hope releasing MedImageInsight will help enhance collective progress in medical imaging AI research and development.",
      "url": "https://www.microsoft.com/en-us/research/publication/medimageinsight-an-open-source-embedding-model-for-general-domain-medical-imaging/"
    },
    {
      "title": "Ironies of Generative AI: Understanding and Mitigating Productivity Loss in Human-AI Interaction",
      "authors": [
        "Abigail Sellen",
        "Auste Simkute",
        "Ava Elizabeth Scott",
        "Lev Tankelevitch",
        "Sean Rintel",
        "Victor Kewenig"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Human-computer interaction",
        "Social sciences"
      ],
      "publication_date": "October 2024",
      "abstract": "Generative AI (GenAI) systems offer opportunities to increase user productivity in many tasks, such as programming and writing. However, while they boost productivity in some studies, many others show that users are working ineffectively with GenAI systems and losing productivity. Despite the apparent novelty of these usability challenges, these ‘ironies of automation’ have been observed for over three decades in Human Factors research on the introduction of automation in domains such as aviation, automated driving, and intelligence. We draw on this extensive research alongside recent GenAI user studies to outline four key reasons for productivity loss with GenAI systems: a shift in users’ roles from production to evaluation, unhelpful restructuring of workflows, interruptions, and a tendency for automation to make easy tasks easier and hard tasks harder. We then suggest how Human Factors research can also inform GenAI system design to mitigate productivity loss by using approaches such as continuous feedback, system personalization, ecological interface design, task stabilization, and clear task allocation. Thus, we ground developments in GenAI system usability in decades of Human Factors research, ensuring that the design of human-AI interactions in this rapidly moving field learns from history instead of repeating it.\nKEYWORDS: Generative AI, automation, productivity, efficiency, ironies, human factors, workplace",
      "url": "https://www.microsoft.com/en-us/research/publication/ironies-of-generative-ai-understanding-and-mitigating-productivity-loss-in-human-ai-interaction/"
    },
    {
      "title": "The Power of Resets in Online Reinforcement Learning",
      "authors": [
        "Alexander Rakhlin",
        "Dylan Foster",
        "Zakaria Mhammedi"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "October 2024",
      "abstract": "Simulators are a pervasive tool in reinforcement learning, but most existing algorithms cannot efficiently exploit simulator access — particularly in high-dimensional domains that require general function approximation. We explore the power of simulators through online reinforcement learning with local simulator access (or, local planning), an RL protocol where the agent is allowed to reset to previously observed states and follow their dynamics during training. We use local simulator access to unlock new statistical guarantees that were previously out of reach:- We show that MDPs with low coverability (Xie et al. 2023) — a general structural condition that subsumes Block MDPs and Low-Rank MDPs — can be learned in a sample-efficient fashion with only Q⋆-realizability (realizability of the optimal state-value function); existing online RL algorithms require significantly stronger representation conditions.- As a consequence, we show that the notorious Exogenous Block MDP problem (Efroni et al. 2022) is tractable under local simulator access.The results above are achieved through a computationally inefficient algorithm. We complement them with a more computationally efficient algorithm, RVFS (Recursive Value Function Search), which achieves provable sample complexity guarantees under a strengthened statistical assumption known as pushforward coverability. RVFS can be viewed as a principled, provable counterpart to a successful empirical paradigm that combines recursive search (e.g., MCTS) with value function approximation.",
      "url": "https://www.microsoft.com/en-us/research/publication/the-power-of-resets-in-online-reinforcement-learning/"
    },
    {
      "title": "Gaussian Elimination of Side-channels: Linear Algebra for Memory Coloring",
      "authors": [
        "Boris Köpf",
        "Cédric Fournet",
        "Jana Hofmann",
        "Stavros Volos"
      ],
      "research_areas": [
        "Security, privacy, and cryptography"
      ],
      "publication_date": "October 2024",
      "abstract": "Memory coloring is a software-based technique to ensure microarchitectural isolation between trust domains sharing a CPU. Prior coloring schemes target individual microarchitectural components and thus provide only partial solutions. In this paper, we provide theoretical foundations and practical algorithms to infer comprehensive coloring schemes for modern cloud CPUs.\nTo this end, we first formulate the requirements for effective memory coloring schemes in a set-theoretic model, including definitions for simultaneous isolation of shared components and uniform utilization of private components. We then algebraically characterize these requirements for microarchitectural components that are indexed by linear functions, which is the prevalent case in today’s CPUs. Based on this, we develop efficient algorithms for computing multi-resource coloring schemes from linear indexing functions, and for reverse-engineering unknown linear indexing functions under minimal assumptions.\nIn a case study, we use our algorithms to compute coloring schemes for recent Intel CPUs, and we show how to design indexing functions that maximize the number of supported trust domains.",
      "url": "https://www.microsoft.com/en-us/research/publication/gaussian-elimination-of-side-channels-linear-algebra-for-memory-coloring/"
    },
    {
      "title": "Using Noise to Infer Aspects of Simplicity Without Learning",
      "authors": [
        "Lesia Semenova"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "October 2024",
      "abstract": "Noise in data significantly influences decision-making in the data science process. In fact, it has been shown that noise in data generation processes motivates practitioners to choose simpler hypothesis spaces. However, an open question still remains: what is the degree of model simplification we can expect under different noise levels? In this work, we address this question by investigating the relationship between the amount of noise and model simplicity across various hypothesis spaces, focusing on decision trees and linear models. We formally show that noise acts as an implicit regularizer for different noise models. Furthermore, we prove that Rashomon sets (sets of near-optimal models) constructed with noisy data tend to contain simpler models than corresponding Rashomon sets with non-noisy data. Additionally, we show that noise expands the set of “good” features, and increases the set of models that use at least one good feature. Our work offers theoretical guarantees and practical insights for both practitioners and policymakers on whether simple-yet-accurate machine learning models are likely to exist, based on knowledge of noise levels in the data generation process.",
      "url": "https://www.microsoft.com/en-us/research/publication/using-noise-to-infer-aspects-of-simplicity-without-learning/"
    },
    {
      "title": "Distributed AI Platform for the 6G RAN",
      "authors": [
        "Bozidar Radunovic",
        "Ganesh Ananthanarayanan",
        "Xenofon Foukas",
        "Yongguang Zhang"
      ],
      "research_areas": [
        "Systems and networking"
      ],
      "publication_date": "October 2024",
      "abstract": "Cellular Radio Access Networks (RANs) are rapidly evolving towards 6G, driven by the need to reduce costs and introduce new revenue streams for operators and enterprises. In this context, AI emerges as a key enabler in solving complex RAN problems spanning both the management and application domains. Unfortunately, and despite the undeniable promise of AI, several practical challenges still remain, hindering the widespread adoption of AI applications in the RAN space. This article attempts to shed light to these challenges and argues that existing approaches in addressing them are inadequate for realizing the vision of a truly AI-native 6G network. Motivated by this lack of solutions, it proposes a generic distributed AI platform architecture, tailored to the needs of an AI-native RAN and discusses its alignment with ongoing standardization efforts.",
      "url": "https://www.microsoft.com/en-us/research/publication/distributed-ai-platform-for-the-6g-ran/"
    },
    {
      "title": "Can We Trust Auto-Mitigation? Improving Cloud Failure Prediction with Uncertain Positive Learning",
      "authors": [
        "Chetan Bansal",
        "Dongmei Zhang",
        "Haozhe Li",
        "Minghua Ma",
        "Qingwei Lin 林庆维",
        "Saravan Rajmohan"
      ],
      "research_areas": [
        "Algorithms"
      ],
      "publication_date": "October 2024",
      "abstract": "In the rapidly expanding domain of cloud computing, a variety of software services have been deployed in the cloud. To ensure the reliability of cloud services, prior studies focus on the prediction of failure instances, such as disks, nodes, switches, etc. The mitigation actions are initiated to resolve the underlying issue once the prediction output is positive. However, our real-world practice in Microsoft Azure revealed a decline in prediction accuracy, approximate 9\\%, after model retraining. The decrease is attributed to the mitigation actions, which can result in uncertain positive instances. Since these instances cannot be verified after mitigation, they may introduce additional noise into the model updating process. To the best of our knowledge, we are the first to identify this Uncertain Positive Learning (UPLearning) issue in the real-world cloud failure prediction scenario, and we design an Uncertain Positive Learning Risk Estimator (Uptake) approach to address this problem. By utilizing two real-world datasets for disk failure prediction and conducting node prediction experiments in Azure, which is a top-tier cloud provider serving millions of users. We demonstrate that our Uptake method can significantly enhance failure prediction accuracy by an average of 5\\%.",
      "url": "https://www.microsoft.com/en-us/research/publication/can-we-trust-auto-mitigation-improving-cloud-failure-prediction-with-uncertain-positive-learning/"
    },
    {
      "title": "SureMap: Simultaneous mean estimation for single-task and multi-task disaggregated evaluation",
      "authors": [
        "Alex Chouldechova",
        "Lester Mackey",
        "Miro Dudík"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "October 2024",
      "abstract": "Disaggregated evaluation—estimating the performance of a machine learning model on different subpopulations—is an important task in the performance and group-fairness assessment of AI systems. However, evaluation data is scarce, and subpopulations arising from intersections of attributes (e.g., race, sex, gender) are often tiny. Today, it is common for multiple clients to procure the same AI model from a model developer, and the challenge of disaggregated evaluation is faced by each customer individually.  This gives rise to what we call the \\textit{multi-task disaggregated evaluation problem}, wherein multiple clients seek to conduct a disaggregated evaluation of a given model in their own data setting (task).  In this work we develop a disaggregated evaluation method called **SureMap** that has high estimation accuracy for both multi-task *and* single-task disaggregated evaluations. SureMap’s efficiency gains come from (1) transforming the problem into structured simultaneous Gaussian mean estimation and (2) incorporating external data, e.g. from the AI system creator or from their other clients.  The method combines *maximum a posteriori* (MAP) estimation using a well-chosen prior together with cross-validation-free tuning via Stein’s unbiased risk estimate (SURE). We evaluate SureMap on disaggregated evaluation tasks in multiple domains, observing significant accuracy improvements over several strong baselines.",
      "url": "https://www.microsoft.com/en-us/research/publication/suremap-simultaneous-mean-estimation-for-single-task-and-multi-task-disaggregated-evaluation/"
    },
    {
      "title": "RedCode: Risky Code Execution and Generation Benchmark for Code Agents",
      "authors": [
        "Andy Zhou",
        "Bo Li",
        "Chengquan Guo",
        "Chulin Xie",
        "Dawn Song",
        "Xun Liu",
        "Yi Zeng",
        "Zinan Lin"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Security, privacy, and cryptography"
      ],
      "publication_date": "October 2024",
      "abstract": "With the rapidly increasing capabilities and adoption of code agents for AI-assisted coding and software development, safety and security concerns, such as generating or executing malicious code, have become significant barriers to the real-world deployment of these agents. To provide comprehensive and practical evaluations on the safety of code agents, we propose RedCode, an evaluation platform with benchmarks grounded in four key principles: real interaction with systems, holistic evaluation of unsafe code generation and execution, diverse input formats, and high-quality safety scenarios and tests. RedCode consists of two parts to evaluate agents’ safety in unsafe code execution and generation: (1) RedCode-Exec provides challenging code prompts in Python as inputs, aiming to evaluate code agents’ ability to recognize and handle unsafe code. We then map the Python code to other programming languages (e.g., Bash) and natural text summaries or descriptions for evaluation, leading to a total of over 4,000 testing instances. We provide 25 types of critical vulnerabilities spanning various domains, such as websites, file systems, and operating systems. We provide a Docker sandbox environment to evaluate the execution capabilities of code agents and design corresponding evaluation metrics to assess their execution results. (2) RedCode-Gen provides 160 prompts with function signatures and docstrings as input to assess whether code agents will follow instructions to generate harmful code or software. Our empirical findings, derived from evaluating three agents based on various LLMs, provide insights into code agents’ vulnerabilities. For instance, evaluations on RedCode-Exec show that agents are more likely to reject executing unsafe operations on the operating system. Unsafe operations described in natural text lead to a lower rejection rate than those in code format. Additionally, evaluations on RedCode-Gen reveal that more capable base models and agents with stronger overall coding abilities, such as GPT-4, tend to produce more sophisticated and effective harmful software. Our findings highlight the need for stringent safety evaluations for diverse code agents. Our dataset and code are publicly available at https://github.com/AI-secure/RedCode.",
      "url": "https://www.microsoft.com/en-us/research/publication/redcode-multi-dimensional-safety-benchmark-for-code-agents/"
    },
    {
      "title": "ReXTime: A Benchmark Suite for Reasoning-Across-Time in Videos",
      "authors": [
        "Hsi-Che Lin",
        "Jr-Jen Chen",
        "Yen-Chun Chen",
        "Yu-Chiang Frank Wang",
        "Yu-Chien Liao",
        "Yu-Chu Yu"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "October 2024",
      "abstract": "We introduce ReXTime, a benchmark designed to rigorously test AI models’ ability to perform temporal reasoning within video events. Specifically, ReXTime focuses on reasoning across time, i.e. human-like understanding when the question and its corresponding answer occur in different video segments. This form of reasoning, requiring advanced understanding of cause-and-effect relationships across video segments, poses significant challenges to even the frontier multimodal large language models. To facilitate this evaluation, we develop an automated pipeline for generating temporal reasoning question-answer pairs, significantly reducing the need for labor-intensive manual annotations. Our benchmark includes 921 carefully vetted validation samples and 2,143 test samples, each manually curated for accuracy and relevance. Evaluation results show that while frontier large language models outperform academic models, they still lag behind human performance by a significant 14.3\\% accuracy gap. Additionally, our pipeline creates a training dataset of 9,695 machine generated samples without manual effort, which empirical studies suggest can enhance the across-time reasoning via fine-tuning.",
      "url": "https://www.microsoft.com/en-us/research/publication/rextime-a-benchmark-suite-for-reasoning-across-time-in-videos/"
    },
    {
      "title": "BPQP: A Differentiable Convex Optimization Framework for Efficient End-to-End Learning",
      "authors": [
        "Jiang Bian",
        "Lewen Wang",
        "Weiqing Liu",
        "Xiao Yang",
        "Xu Yang"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "October 2024",
      "abstract": "Data-driven decision-making processes increasingly utilize end-to-end learnable deep neural networks to render final decisions. Sometimes, the output of the forward functions in certain layers is determined by the solutions to mathematical optimization problems, leading to the emergence of differentiable optimization layers that permit gradient back-propagation. However, real-world scenarios often involve large-scale datasets and numerous constraints, presenting significant challenges. Current methods for differentiating optimization problems typically rely on implicit differentiation, which necessitates costly computations on the Jacobian matrices, resulting in low efficiency. In this paper, we introduce BPQP, a differentiable convex optimization framework designed for efficient end-to-end learning. To enhance efficiency, we reformulate the backward pass as a simplified and decoupled quadratic programming problem by leveraging the structural properties of the Karush–Kuhn–Tucker (KKT) matrix. This reformulation enables the use of first-order optimization algorithms in calculating the backward pass gradients, allowing our framework to potentially utilize any state-of-the-art solver. As solver technologies evolve, BPQP can continuously adapt and improve its efficiency. Extensive experiments on both simulated and real-world datasets demonstrate that BPQP achieves a significant improvement in efficiency—typically an order of magnitude faster in overall execution time compared to other differentiable optimization layers. Our results not only highlight the efficiency gains of BPQP but also underscore its superiority over differential optimization layer baselines.",
      "url": "https://www.microsoft.com/en-us/research/publication/bpqp-a-differentiable-convex-optimization-framework-for-efficient-end-to-end-learning/"
    },
    {
      "title": "FLASH: A Workflow Automation Agent for Diagnosing Recurring Incidents",
      "authors": [
        "Chetan Bansal",
        "Hao Huang",
        "Minghua Ma",
        "Rujia Wang",
        "Saravan Rajmohan",
        "Tanish Mittal",
        "Xuchao Zhang",
        "Zhixin Ren"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Programming languages and software engineering"
      ],
      "publication_date": "October 2024",
      "abstract": "Recurring incidents, typically raised by system monitors, often occur repeatedly, demanding significant human effort for troubleshooting. Automating the diagnosis process for these recurring incidents is crucial for minimizing service downtime, reducing customer impact, and decreasing manual labor. While recent agent approaches based on Large Language Models (LLMs) have demonstrated effectiveness in handling complex tasks requiring multiple logical steps, they still suffer from the reliability issue due to a lack of specific diagnostic knowledge. To enhance diagnostic reliability, we propose a workFLow Automation agent with Status supervision and Hindsight integration (FLASH), which significantly improves diagnostic accuracy by incorporating status supervision to break down the complex instructions into manageable pieces aligned with identified status. Moreover, we generate hindsight using LLMs from past failure experiences, progressively enhancing diagnostic reliability for subsequent incidents. We conduct extensive study over 250 production incidents from Microsoft in five different workflow automation scenarios. The results reveal that our FLASH agent approach outperforms state-of-the-art agent models by an average of 13.2% in terms of accuracy. These compelling results underscore the viability of automating the diagnostic process for recurring incidents.",
      "url": "https://www.microsoft.com/en-us/research/publication/flash-a-workflow-automation-agent-for-diagnosing-recurring-incidents/"
    },
    {
      "title": "Not All Tokens Are What You Need for Pretraining",
      "authors": [
        "Jian Jiao",
        "Nan Duan",
        "Ruochen Xu",
        "Weizhu Chen",
        "Xiao Liu",
        "Yelong Shen",
        "Yeyun Gong"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "October 2024",
      "abstract": "Previous language model pre-training methods have uniformly applied a next-token prediction loss to all training tokens. Challenging this norm, we posit that “Not all tokens in a corpus are equally important for language model training”. Our initial analysis examines token-level training dynamics of language model, revealing distinct loss patterns for different tokens. Leveraging these insights, we introduce a new language model called Rho-1. Unlike traditional LMs that learn to predict every next token in a corpus, Rho-1 employs Selective Language Modeling (SLM), which selectively trains on useful tokens that aligned with the desired distribution. This approach involves scoring pretraining tokens using a reference model, and then training the language model with a focused loss on tokens with higher scores. When continual pretraining on 15B OpenWebMath corpus, Rho-1 yields an absolute improvement in few-shot accuracy of up to 30% in 9 math tasks. After fine-tuning, Rho-1-1B and 7B achieved state-of-the-art results of 40.6% and 51.8% on MATH dataset, respectively – matching DeepSeekMath with only 3% of the pretraining tokens. Furthermore, when pretraining on 80B general tokens, Rho-1 achieves 6.8% average enhancement across 15 diverse tasks, increasing both efficiency and performance of the language model pre-training.",
      "url": "https://www.microsoft.com/en-us/research/publication/not-all-tokens-are-what-you-need-for-pretraining/"
    },
    {
      "title": "EEG2Video: Towards Decoding Dynamic Visual Perception from EEG Signals",
      "authors": [
        "Yansen Wang",
        "Zilong Wang"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "October 2024",
      "abstract": "Our visual experience in daily life are dominated by dynamic change. Decoding such dynamic information from brain activity can enhance the understanding of the brain’s visual processing system. However, previous studies predominately focus on reconstructing static visual stimuli. In this paper, we explore to decode dynamic visual perception from electroencephalography (EEG), a neuroimaging technique able to record brain activity with high temporal resolution (1000 Hz) for capturing rapid changes in brains. Our contributions are threefold: Firstly, we develop a large dataset recording signals from 20 subjects while they were watching 1400 dynamic video clips of 40 concepts. This dataset fills the gap in the lack of EEG-video pairs. Secondly, we annotate each video clips to investigate the potential for decoding some specific meta information (e.g., color, dynamic, human or not) from EEG. Thirdly, we propose a novel baseline EEG2Video for video reconstruction from EEG signals that better aligns dynamic movements with high temporal resolution brain signals by Seq2Seq architecture. EEG2Video achieves a 2-way accuracy of 79.8% in semantic classification tasks and 0.256 in structural similarity index (SSIM). Overall, our works takes an important step towards decoding dynamic visual perception from EEG signals. Our dataset and code will be released soon.",
      "url": "https://www.microsoft.com/en-us/research/publication/eeg2video-towards-decoding-dynamic-visual-perception-from-eeg-signals/"
    },
    {
      "title": "Energy-efficient Reinforcement Learning by Discovering Neural Pathways",
      "authors": [
        "Alessandro Sordoni"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "October 2024",
      "abstract": "Reinforcement learning (RL) algorithms have been very successful at tackling complex control problems, such as AlphaGo or fusion control. However, current research mainly emphasizes solution quality, often achieved by using large models trained on large amounts of data, and does not account for the financial, environmental, and societal costs associated with developing and deploying such models. Modern neural networks are often overparameterized and a significant number of parameters can be pruned without meaningful loss in performance, resulting in more efficient use of the model’s capacity lottery ticket. We present a methodology for identifying sub-networks within a larger network in reinforcement learning (RL). We call such sub-networks, neural pathways. We show empirically that even very small learned sub-networks, using less than 5% of the large network’s parameters, can provide very good quality solutions. We also demonstrate the training of multiple pathways within the same networks in a multitask setup, where each pathway is encouraged to tackle a separate task. We evaluate empirically our approach on several continuous control tasks, in both online and offline training",
      "url": "https://www.microsoft.com/en-us/research/publication/energy-efficient-reinforcement-learning-by-discovering-neural-pathways/"
    },
    {
      "title": "Motion Graph Unleashed: A Novel Approach to Video Prediction",
      "authors": [
        "Ilya Zharkov",
        "Luming Liang"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Computer vision"
      ],
      "publication_date": "October 2024",
      "abstract": "We introduce motion graph, a novel approach to address the video prediction problem, i.e., predicting future video frames from limited past data. The motion graph transforms patches of video frames into interconnected graph nodes, to comprehensively describe the spatial-temporal relationships among them. This representation overcomes the limitations of existing motion representations such as image differences, optical flow, and motion matrix that either fall short in capturing complex motion patterns or suffer from excessive memory consumption. We further present a video prediction pipeline empowered by motion graph, exhibiting substantial performance improvements and cost reductions. Extensive experiments on various datasets, including UCF Sports, KITTI and Cityscapes, highlight the strong representative ability of motion graph. Especially on UCF Sports, our method matches and outperforms the SOTA methods with a significant reduction in model size by 78% and a substantial decrease in GPU memory utilization by 47%.",
      "url": "https://www.microsoft.com/en-us/research/publication/motion-graph-unleashed-a-novel-approach-to-video-prediction/"
    },
    {
      "title": "Leveraging LLMs for Program Verification",
      "authors": [
        "Adharsh Kamath",
        "Aditya Senthilnathan",
        "Akash Lal",
        "Aseem Rastogi",
        "Pantazis Deligiannis",
        "Rahul Sharma",
        "Saikat Chakraborty",
        "Shuvendu Lahiri",
        "Subhajit Roy"
      ],
      "research_areas": [
        "Programming languages and software engineering"
      ],
      "publication_date": "October 2024",
      "abstract": "Loop invariants are fundamental to reasoning about programs with loops. They establish properties about a given loop’s behavior. When they additionally are inductive, they become useful for the task of formal verification that seeks to establish strong mathematical guarantees about program’s runtime behavior. The inductiveness ensures that the invariants can be checked locally without consulting the entire program, thus are indispensable artifacts in a formal proof of correctness. Finding inductive loop invariants is an undecidable problem, and despite a long history of research towards practical solutions, it remains far from a solved problem. This paper investigates the capabilities of the Large Language Models (LLMs) in offering a new solution towards this old, yet important problem. To that end, we first curate a dataset of verification problems on programs with loops. Next, we design a prompt for exploiting LLMs, obtaining inductive loop invariants, that are checked for correctness using sound symbolic tools. Finally, we explore the effectiveness of using an efficient combination of a symbolic tool and an LLM on our dataset and compare it against a purely symbolic baseline. Our results demonstrate that LLMs can help improve the state-of-the-art in automated program verification.",
      "url": "https://www.microsoft.com/en-us/research/publication/finding-inductive-loop-invariants-using-large-language-models/"
    },
    {
      "title": "Modeling health risks using neural network ensembles",
      "authors": [
        "Antonio Criminisi",
        "Brandon M. Smith",
        "Neeraj Sood",
        "Noam Sorek",
        "Steven B. heymsfield",
        "Yaar Harari"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "October 2024",
      "abstract": "This study aims to demonstrate that demographics combined with biometrics can be used to predict obesity related chronic disease risk and produce a health risk score that outperforms body mass index (BMI)—the most commonly used biomarker for obesity. We propose training an ensemble of small neural networks to fuse demographics and biometrics inputs. The categorical outputs of the networks are then turned into a multi-dimensional risk map, which associates diverse inputs with stratified, output health risk. Our ensemble model is optimized and validated on disjoint subsets of nationally representative data (N~100,000) from the National Health and Nutrition Examination Survey (NHANES). To broaden applicability of the proposed method, we consider only non-invasive inputs that can be easily measured through modern devices. Our results show that: (a) neural networks can predict individual conditions (e.g., diabetes, hypertension) or the union of multiple (e.g., nine) health conditions; (b) Softmax model outputs can be used to stratify individual- or any-condition risk; (c) ensembles of neural networks improve generalizability; (d) multiple-input models outperform BMI (e.g., 75.1% area under the receiver operator curve for eight-input, any-condition models compared to 64.2% for BMI); (e) small neural networks are as effective as larger ones for the inference tasks considered; the proposed models are small enough that they can be expressed as human-readable equations, and they can be adapted to clinical settings to identify high-risk, undiagnosed populations.",
      "url": "https://www.microsoft.com/en-us/research/publication/modeling-health-risks-using-neural-network-ensembles/"
    },
    {
      "title": "A Survey on Deep Learning for Theorem Proving",
      "authors": [
        "Jialiang Sun",
        "Kaiyu Yang",
        "Logan Murphy",
        "Qidong Su",
        "Xian Zhang",
        "Xujie Si",
        "Zenan Li",
        "Zhaoyu Li"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Mathematics"
      ],
      "publication_date": "October 2024",
      "abstract": "Theorem proving is a fundamental aspect of mathematics, spanning from informal reasoning in natural language to rigorous derivations in formal systems. In recent years, the advancement of deep learning, especially the emergence of large language models, has sparked a notable surge of research exploring these techniques to enhance the process of theorem proving. This paper presents a comprehensive survey of deep learning for theorem proving by offering (i) a thorough review of existing approaches across various tasks such as autoformalization, premise selection, proofstep generation, and proof search; (ii) an extensive summary of curated datasets and strategies for synthetic data generation; (iii) a detailed analysis of evaluation metrics and the performance of state-of-the-art methods; and (iv) a critical discussion on the persistent challenges and the promising avenues for future exploration. Our survey aims to serve as a foundational reference for deep learning approaches in theorem proving, inspiring and catalyzing further research endeavors in this rapidly growing field. A curated list of papers is available at https://github.com/zhaoyu-li/DL4TP.",
      "url": "https://www.microsoft.com/en-us/research/publication/a-survey-on-deep-learning-for-theorem-proving/"
    },
    {
      "title": "AI Should Challenge, Not Obey",
      "authors": [
        "Advait Sarkar"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Human language technologies",
        "Human-computer interaction",
        "Social sciences"
      ],
      "publication_date": "October 2024",
      "abstract": "Let’s transform our robot secretaries into Socratic gadflies.",
      "url": "https://www.microsoft.com/en-us/research/publication/ai-should-challenge-not-obey/"
    },
    {
      "title": "Social Conjuring: Multi-User Runtime Collaboration with AI in Building Virtual 3D Worlds",
      "authors": [
        "Amina Kobenova",
        "Andrzej Banburski-Fahey",
        "C. DeVeaux",
        "Jaron Lanier",
        "Judith Amores",
        "Samyak Parajuli"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Human-computer interaction"
      ],
      "publication_date": "September 2024",
      "abstract": "Generative artificial intelligence has shown promise in prompting virtual worlds into existence, yet little attention has been given to understanding how this process unfolds as social interaction. We present Social Conjurer, a framework for AI-augmented dynamic 3D scene co-creation, where multiple users collaboratively build and modify virtual worlds in real-time. Through an expanded set of interactions, including social and tool-based engagements as well as spatial reasoning, our framework facilitates the creation of rich, diverse virtual environments. Findings from a preliminary user study (N=12) provide insight into the user experience of this approach, how social contexts shape the prompting of spatial environments, and perspective on social applications of prompt-based 3D co-creation. In addition to highlighting the potential of AI-supported multi-user world creation and offering new pathways for AI-augmented creative processes in VR, this article presents a set of implications for designing human-centered interfaces that incorporate AI models into 3D content generation.",
      "url": "https://www.microsoft.com/en-us/research/publication/social-conjuring-multi-user-runtime-collaboration-with-ai-in-building-virtual-3d-worlds/"
    },
    {
      "title": "Robust Incremental Structure-from-Motion with Hybrid Features",
      "authors": [
        "Johannes L. Schönberger",
        "Marc Pollefeys",
        "Remi Pautrat",
        "Shaohui Liu",
        "Tianyi Zhang",
        "Viktor Larsson",
        "Yidan Gao"
      ],
      "research_areas": [
        "Computer vision"
      ],
      "publication_date": "September 2024",
      "abstract": "Structure-from-Motion (SfM) has become a ubiquitous tool for camera calibration and scene reconstruction with many downstream applications in computer vision and beyond. While the state-of-the-art SfM pipelines have reached a high level of maturity in well-textured and well-configured scenes over the last decades, they still fall short of robustly solving the SfM problem in challenging scenarios. In particular, weakly textured scenes and poorly constrained configurations oftentimes cause catastrophic failures or large errors for the primarily keypoint-based pipelines. In these scenarios, line segments are often abundant and can offer complementary geometric constraints. Their large spatial extent and typically structured configurations lead to stronger geometric constraints as compared to traditional keypoint-based methods. In this work, we introduce an incremental SfM system that, in addition to points, leverages lines and their structured geometric relations. Our technical contributions span the entire pipeline (mapping, triangulation, registration) and we integrate these into a comprehensive end-to-end SfM system that we share as an open-source software with the community. We also present the first analytical method to propagate uncertainties for 3D optimized lines via sensitivity analysis. Experiments show that our system is consistently more robust and accurate compared to the widely used point-based state of the art in SfM — achieving richer maps and more precise camera registrations, especially under challenging conditions. In addition, our uncertainty-aware localization module alone is able to consistently improve over the state of the art under both point-alone and hybrid setups.",
      "url": "https://www.microsoft.com/en-us/research/publication/robust-incremental-structure-from-motion-with-hybrid-features/"
    },
    {
      "title": "Maia-2: A Unified Model for Human-AI Alignment in Chess",
      "authors": [
        "Ashton Anderson",
        "Difan Jiao",
        "Jon Kleinberg",
        "Reid McIlroy-Young",
        "Siddhartha Sen",
        "Zhenwei Tang"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "September 2024",
      "abstract": "There are an increasing number of domains in which artificial intelligence (AI) systems both surpass human ability and accurately model human behavior. This introduces the possibility of algorithmically-informed teaching in these domains through more relatable AI partners and deeper insights into human decision-making. Critical to achieving this goal, however, is coherently modeling human behavior at various skill levels. Chess is an ideal model system for conducting research into this kind of human-AI alignment, with its rich history as a pivotal testbed for AI research, mature superhuman AI systems like AlphaZero, and precise measurements of skill via chess rating systems. Previous work in modeling human decision-making in chess uses completely independent models to capture human style at different skill levels, meaning they lack coherence in their ability to adapt to the full spectrum of human improvement and are ultimately limited in their effectiveness as AI partners and teaching tools. In this work, we propose a unified modeling approach for human-AI alignment in chess that coherently captures human style across different skill levels and directly captures how people improve. Recognizing the complex, non-linear nature of human learning, we introduce a skill-aware attention mechanism to dynamically integrate players’ strengths with encoded chess positions, enabling our model to be sensitive to evolving player skill. Our experimental results demonstrate that this unified framework significantly enhances the alignment between AI and human players across a diverse range of expertise levels, paving the way for deeper insights into human decision-making and AI-guided teaching tools.",
      "url": "https://www.microsoft.com/en-us/research/publication/maia-2-a-unified-model-for-human-ai-alignment-in-chess/"
    },
    {
      "title": "Mnemosyne: Parallelization Strategies for Efficiently Serving Multi-Million Context Length LLM Inference Requests Without Approximations",
      "authors": [
        "Alexey Tumanov",
        "Amey Agrawal",
        "Chaojie Zhang",
        "Esha Choukse",
        "Junda Chen",
        "Ramachandran Ramjee",
        "Íñigo Goiri"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "September 2024",
      "abstract": "As large language models (LLMs) evolve to handle increasingly longer contexts, serving inference requests for context lengths in the range of millions of tokens presents unique challenges. While existing techniques are effective for training, they fail to address the unique challenges of inference, such as varying prefill and decode phases and their associated latency constraints – like Time to First Token (TTFT) and Time Between Tokens (TBT). Furthermore, there are no long context inference solutions that allow batching requests to increase the hardware utilization today.\nIn this paper, we propose three key innovations for efficient interactive long context LLM inference, without resorting to any approximation: adaptive chunking to reduce prefill overheads in mixed batching, Sequence Pipeline Parallelism (SPP) to lower TTFT, and KV Cache Parallelism (KVP) to minimize TBT. These contributions are combined into a 3D parallelism strategy, enabling Mnemosyne to scale interactive inference to context lengths at least up to 10 million tokens with high throughput enabled with batching. To our knowledge, Mnemosyne is the first to be able to achieve support for 10 million long context inference efficiently, while satisfying production-grade SLOs on TBT (30ms) on contexts up to and including 10 million.\n",
      "url": "https://www.microsoft.com/en-us/research/publication/mnemosyne-parallelization-strategies-for-efficiently-serving-multi-million-context-length-llm-inference-requests-without-approximations/"
    },
    {
      "title": "Machine learning to evaluate the relationship between social determinants and diabetes prevalence in New York City",
      "authors": [
        "Ann Aerts",
        "Bill Weeks",
        "Darren Tanner",
        "Elizabeth Adamson",
        "Ji Eun Chang",
        "Juan M. Lavista Ferres",
        "Peter Speyer",
        "Yongkang Zhang"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Medical, health and genomics"
      ],
      "publication_date": "September 2024",
      "abstract": "Introduction Diabetes is a leading contributor to cardiovascular disease and mortality; social determinants of health (SDOH) are associated with disparities in diabetes risk. Quantifying the cumulative impact of SDOH and identifying the SDOH most associated with diabetes prevalence at the neighbourhood level can help policy-makers design and target local interventions to mitigate these disparities. Machine learning (ML) methods can provide novel insights and help inform public health intervention strategies in a place-based manner.\n\n\nMethods In a cross-sectional study, we used gradient boosting ML models to estimate the cumulative contribution of a set of SDOH variables to diabetes prevalence (%) at the census tract level within New York City (NYC); Shapley Additive Explanations were used to assess the magnitude and shape of relationships between our SDOH variables and model-predicted NYC diabetes prevalence. SDOH measures included socioeconomic position, educational attainment, food access, air quality, neighbourhood environment, housing conditions and insurance coverage.\n\n\nResults Across 2096 NYC census tracts (population 8 170 505), mean diabetes prevalence was 11.5% (SD 3.7%; range 1.9%–42.8%). A set of 16 SDOH variables representing a framework of 16 distinct SDOH concepts accounted for 67% of the between-tract variance in model-derived NYC diabetes prevalence estimates (95% CI 66% to 68%); a set of 81 variables representing these 16 concepts accounted for 80% of variance (95% CI 78% to 81%). Models showed excellent across-location generalisation. The most important variables driving model predictions within NYC were measures of low educational attainment and poverty.\n\n\nConclusions SDOH accounted for a substantial proportion of neighbourhood-level variation in diabetes prevalence within NYC, independent of the demographics and health behaviours associated with those SDOH. Our place-based findings suggest that, within NYC, where approximately one million residents have diabetes and there are legislative requirements to reduce the impacts from diabetes, policies reducing socioeconomic and educational inequality could have the greatest potential to equitably achieve this.\n",
      "url": "https://www.microsoft.com/en-us/research/publication/machine-learning-to-evaluate-the-relationship-between-social-determinants-and-diabetes-prevalence-in-new-york-city/"
    },
    {
      "title": "RAR: Retrieval Augmented Retrieval for Code Generation in Low Resource Languages",
      "authors": [
        "Avik Dutta",
        "Gust Verbruggen",
        "Mukul Singh",
        "Sumit Gulwani",
        "Vu Le"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Data platforms and analytics",
        "Programming languages and software engineering"
      ],
      "publication_date": "September 2024",
      "abstract": "Language models struggle in generating correct code for low resource programming languages, since these are underrepresented in training data. Popular approaches use either examples or documentation to improve the performance of these models. Instead of considering the independent retrieval of this information, we introduce retrieval augmented retrieval (RAR) as a two-step retrieval method for selecting relevant examples and documentation. Extensive experiments on two low resource languages (Power Query M and OfficeScript) show that RAR outperforms example or grammar retrieval techniques (2.81–26.14%).",
      "url": "https://www.microsoft.com/en-us/research/publication/rar-retrieval-augmented-retrieval-for-code-generation-in-low-resource-languages/"
    },
    {
      "title": "Easy2Hard-Bench: Standardized Difficulty Labels for Profiling LLM Performance and Generalization",
      "authors": [
        "A. Anandkumar",
        "Aakriti Agrawal",
        "Avi Schwarzschild",
        "Chenghao Deng",
        "Furong Huang",
        "Jocelyn Choo",
        "John Langford",
        "Mucong Ding",
        "Tianyi Zhou",
        "Tom Goldstein",
        "Zichu Wu"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "September 2024",
      "abstract": "While generalization over tasks from easy to hard is crucial to profile language models (LLMs), the datasets with fine-grained difficulty annotations for each problem across a broad range of complexity are still blank. Aiming to address this limitation, we present Easy2Hard-Bench, a consistently formatted collection of 6 benchmark datasets spanning various domains, such as mathematics and programming problems, chess puzzles, and reasoning questions. Each problem within these datasets is annotated with numerical difficulty scores. To systematically estimate problem difficulties, we collect abundant performance data on attempts to each problem by humans in the real world or LLMs on the prominent leaderboard. Leveraging the rich performance data, we apply well-established difficulty ranking systems, such as Item Response Theory (IRT) and Glicko-2 models, to uniformly assign numerical difficulty scores to problems. Moreover, datasets in Easy2Hard-Bench distinguish themselves from previous collections by a higher proportion of challenging problems. Through extensive experiments with six state-of-the-art LLMs, we provide a comprehensive analysis of their performance and generalization capabilities across varying levels of difficulty, with the aim of inspiring future research in LLM generalization. The datasets are available at https://huggingface.co/datasets/furonghuang-lab/Easy2Hard-Bench.",
      "url": "https://www.microsoft.com/en-us/research/publication/easy2hard-bench-standardized-difficulty-labels-for-profiling-llm-performance-and-generalization/"
    },
    {
      "title": "Neural P$^3$M: A Long-Range Interaction Modeling Enhancer for Geometric GNNs",
      "authors": [
        "Bin Shao",
        "Chaoran Cheng",
        "Ge Liu",
        "Nanning Zheng",
        "Pheng-Ann Heng",
        "Shaoning Li",
        "Yusong Wang",
        "Yuxuan Ren"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "September 2024",
      "abstract": "Geometric graph neural networks (GNNs) have emerged as powerful tools for modeling molecular geometry. However, they encounter limitations in effectively capturing long-range interactions in large molecular systems. To address this challenge, we introduce Neural P$^3$M, a versatile enhancer of geometric GNNs to expand the scope of their capabilities by incorporating mesh points alongside atoms and reimaging traditional mathematical operations in a trainable manner. Neural P$^3$M exhibits flexibility across a wide range of molecular systems and demonstrates remarkable accuracy in predicting energies and forces, outperforming on benchmarks such as the MD22 dataset. It also achieves an average improvement of 22% on the OE62 dataset while integrating with various architectures.",
      "url": "https://www.microsoft.com/en-us/research/publication/neural-p3m-a-long-range-interaction-modeling-enhancer-for-geometric-gnns/"
    },
    {
      "title": "Measuring User Experience Inclusivity in Human-AI Interaction via Five User Problem-Solving Styles",
      "authors": [
        "Andrew Anderson",
        "Fatima Moussaoui",
        "Jimena Noa Guevara",
        "Margaret Burnett",
        "Mihaela Vorvoreanu",
        "Tianyi Li"
      ],
      "research_areas": [
        "Human-computer interaction"
      ],
      "publication_date": "September 2024",
      "abstract": "Motivations: Recent research has emerged on generally how to improve AI products’ human-AI interaction (HAI) user experience (UX), but relatively little is known about HAI-UX inclusivity. For example, what kinds of users are supported, and who are left out? What product changes would make it more inclusive?\nObjectives: To help fill this gap, we present an approach to measuring what kinds of diverse users an AI product leaves out and how to act upon that knowledge. To bring actionability to the results, the approach focuses on users’ problem-solving diversity. Thus, our specific objectives were (1) to show how the measure can reveal which participants with diverse problem-solving styles were left behind in a set of AI products and (2) to relate participants’ problem-solving diversity to their demographic diversity, specifically gender and age.\nMethods: We performed 18 experiments, discarding two that failed manipulation checks. Each experiment was a 2\\(×\\)2 factorial experiment with online participants, comparing two AI products: one deliberately violating 1 of 18 HAI guidelines and the other applying the same guideline. For our first objective, we used our measure to analyze how much each AI product gained/lost HAI-UX inclusivity compared to its counterpart, where inclusivity meant supportiveness to participants with particular problem-solving styles. For our second objective, we analyzed how participants’ problem-solving styles aligned with their gender identities and ages.\nResults and Implications: Participants’ diverse problem-solving styles revealed six types of inclusivity results: (1) the AI products that followed an HAI guideline were almost always more inclusive across diversity of problem-solving styles than the products that did not follow that guideline—but “who” got most of the inclusivity varied widely by guideline and by problem-solving style; (2) when an AI product had risk implications, four variables’ values varied in tandem: participants’ feelings of control, their (lack of) suspicion, their trust in the product, and their certainty while using the product; (3) the more control an AI product offered users, the more inclusive it was; (4) whether an AI product was learning from “my” data or other people’s affected how inclusive that product was; (5) participants’ problem-solving styles skewed differently by gender and age group; and (6) almost all of the results suggested actions that HAI practitioners could take to improve their products’ inclusivity further. Together, these results suggest that a key to improving the demographic inclusivity of an AI product (e.g., across a wide range of genders, ages) can often be obtained by improving the product’s support of diverse problem-solving styles.",
      "url": "https://www.microsoft.com/en-us/research/publication/measuring-user-experience-inclusivity-in-human-ai-interaction-via-five-user-problem-solving-styles/"
    },
    {
      "title": "Interactive Speculative Planning: Enhance Agent Efficiency through Co-design of System and User Interface",
      "authors": [
        "Chi Wang",
        "Mengting Wan",
        "Ryan Nadel",
        "Shashank Vadrevu",
        "Wenyue Hua",
        "Yongfeng Zhang"
      ],
      "research_areas": [
        "Algorithms",
        "Artificial intelligence"
      ],
      "publication_date": "September 2024",
      "abstract": "Agents, as user-centric tools, are increasingly deployed for human task delegation, assisting with a broad spectrum of requests by generating thoughts, engaging with user proxies, and producing action plans. However, agents based on large language models (LLMs) often face substantial planning latency due to two primary factors: the efficiency limitations of the underlying LLMs due to their large size and high demand, and the structural complexity of the agents due to the extensive generation of intermediate thoughts to produce the final output. Given that inefficiency in service provision can undermine the value of automation for users, this paper presents a human-centered efficient agent planning method — Interactive Speculative Planning — aiming at enhancing the efficiency of agent planning through both system design and human-AI interaction. Our approach advocates for the co-design of the agent system and user interface, underscoring the importance of an agent system that can fluidly manage user interactions and interruptions. By integrating human interruptions as a fundamental component of the system, we not only make it more user-centric but also expedite the entire process by leveraging human-in-the-loop interactions to provide accurate intermediate steps. Code and data will be released.",
      "url": "https://www.microsoft.com/en-us/research/publication/interactive-speculative-planning-enhance-agent-efficiency-through-co-design-of-system-and-user-interface/"
    },
    {
      "title": "Toward Tailoring Just-in-Time Adaptive Intervention Systems for Workplace Stress Reduction: Exploratory Analysis of Intervention Implementation.",
      "authors": [
        "Esther Howe",
        "Javier Hernandez",
        "Jina Suh",
        "Koustuv Saha",
        "Mary Czerwinski",
        "Robert Lewis",
        "Tim Althoff"
      ],
      "research_areas": [
        "Human-computer interaction"
      ],
      "publication_date": "September 2024",
      "abstract": "BACKGROUND Integrating stress-reduction interventions into the workplace may improve the health and well-being of employees, and there is an opportunity to leverage ubiquitous everyday work technologies to understand dynamic work contexts and facilitate stress reduction wherever work happens. Sensing-powered just-in-time adaptive intervention (JITAI) systems have the potential to adapt and deliver tailored interventions, but such adaptation requires a comprehensive analysis of contextual and individual-level variables that may influence intervention outcomes and be leveraged to drive the system’s decision-making.\nOBJECTIVE This study aims to identify key tailoring variables that influence momentary engagement in digital stress reduction microinterventions to inform the design of similar JITAI systems.\nMETHODS To inform the design of such dynamic adaptation, we analyzed data from the implementation and deployment of a system that incorporates passively sensed data across everyday work devices to send just-in-time stress reduction microinterventions in the workplace to 43 participants during a 4-week deployment. We evaluated 27 trait-based factors (ie, individual characteristics), state-based factors (ie, workplace contextual and behavioral signals and momentary stress), and intervention-related factors (ie, location and function) across 1585 system-initiated interventions. We built logistical regression models to identify the factors contributing to momentary engagement, the choice of interventions, the engagement given an intervention choice, the user rating of interventions engaged, and the stress reduction from the engagement.\nRESULTS We found that women (odds ratio [OR] 0.41, 95% CI 0.21-0.77; P=.03), those with higher neuroticism (OR 0.57, 95% CI 0.39-0.81; P=.01), those with higher cognitive reappraisal skills (OR 0.69, 95% CI 0.52-0.91; P=.04), and those that chose calm interventions (OR 0.43, 95% CI 0.23-0.78; P=.03) were significantly less likely to experience stress reduction, while those with higher agreeableness (OR 1.73, 95% CI 1.10-2.76; P=.06) and those that chose prompt-based (OR 6.65, 95% CI 1.53-36.45; P=.06) or video-based (OR 5.62, 95% CI 1.12-34.10; P=.12) interventions were substantially more likely to experience stress reduction. We also found that work-related contextual signals such as higher meeting counts (OR 0.62, 95% CI 0.49-0.78; P<.001) and higher engagement skewness (OR 0.64, 95% CI 0.51-0.79; P<.001) were associated with a lower likelihood of engagement, indicating that state-based contextual factors such as being in a meeting or the time of the day may matter more for engagement than efficacy. In addition, a just-in-time intervention that was explicitly rescheduled to a later time was more likely to be engaged with (OR 1.77, 95% CI 1.32-2.38; P<.001).\nCONCLUSIONS JITAI systems have the potential to integrate timely support into the workplace. On the basis of our findings, we recommend that individual, contextual, and content-based factors be incorporated into the system for tailoring as well as for monitoring ineffective engagements across subgroups and contexts.",
      "url": "https://www.microsoft.com/en-us/research/publication/toward-tailoring-just-in-time-adaptive-intervention-systems-for-workplace-stress-reduction-exploratory-analysis-of-intervention-implementation/"
    },
    {
      "title": "AutoVerus: Automated Proof Generation for Rust Code",
      "authors": [
        "Chenyuan Yang",
        "Chris Hawblitzel",
        "Fan Yang",
        "Jay Lorch",
        "Jianan Yao",
        "Md Rakib Hossain Misu",
        "Shan Lu",
        "Shuai Lu",
        "Shuvendu Lahiri",
        "Weidong Cui",
        "Xuheng Li",
        "Yeyun Gong",
        "Ziqiao Zhou"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Programming languages and software engineering",
        "Systems and networking"
      ],
      "publication_date": "September 2024",
      "abstract": "AutoVerus as a VSCode Plugin\nGenerative AI has shown its values for many software engineering tasks. Still in its infancy, large language model (LLM)-based proof generation lags behind LLM-based code generation. In this paper, we present AutoVerus. AutoVerus uses LLM to automatically generate correctness proof for Rust code. AutoVerus is designed to match the unique features of Verus, a verification tool that can prove the correctness of Rust code using proofs and specifications also written in Rust. AutoVerus consists of a network of LLM agents that are crafted and orchestrated to mimic human experts’ three phases of proof construction: preliminary proof generation, proof refinement guided by generic tips, and proof debugging guided by verification errors. To thoroughly evaluate AutoVerus and help foster future research in this direction, we have built a benchmark suite of 150 non-trivial proof tasks, based on existing code-generation benchmarks and verification benchmarks. Our evaluation shows that AutoVerus can automatically generate correct proof for more than 90% of them, with more than half of them tackled in less than 30 seconds or 3 LLM calls.",
      "url": "https://www.microsoft.com/en-us/research/publication/autoverus-automated-proof-generation-for-rust-code/"
    },
    {
      "title": "RetrievalAttention: Accelerating Long-Context LLM Inference via Vector Retrieval",
      "authors": [
        "Bailu Ding",
        "Baotong Lu",
        "Chen Chen",
        "Chengruidong Zhang",
        "Di Liu",
        "Fan Yang",
        "Huiqiang Jiang",
        "Kai Zhang",
        "Lili Qiu",
        "Meng Chen",
        "Qi Chen",
        "Qianxi Zhang",
        "Yuqing Yang"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Human language technologies",
        "Systems and networking"
      ],
      "publication_date": "September 2024",
      "abstract": "Transformer-based Large Language Models (LLMs) have become increasingly important. However, due to the quadratic time complexity of attention computation, scaling LLMs to longer contexts incurs extremely slow inference speed and high GPU memory consumption for caching key-value (KV) vectors. This paper proposes RetrievalAttention, a training-free approach to both accelerate attention computation and reduce GPU memory consumption. By leveraging the dynamic sparsity of attention mechanism, RetrievalAttention proposes to build approximate nearest neighbour search (ANNS) indexes for KV vectors in CPU memory and retrieve the most relevant ones through vector search during generation. Unfortunately, we observe that the off-the-shelf ANNS indexes are often ineffective for such retrieval tasks due to the out-of-distribution (OOD) between query vectors and key vectors in the attention mechanism. RetrievalAttention addresses the OOD challenge by designing an attention-aware vector search algorithm that can adapt to the distribution of query vectors. Our evaluation demonstrates that RetrievalAttention achieves near full attention accuracy while only requiring access to 1–3% of the data. This leads to a significant reduction in the inference cost of long-context LLMs, with a much lower GPU memory footprint. In particular, RetrievalAttention only needs a single NVIDIA RTX4090 (24GB) to serve 128K tokens for LLMs with 8B parameters, which is capable of generating one token in 0.188 seconds.",
      "url": "https://www.microsoft.com/en-us/research/publication/retrievalattention-accelerating-long-context-llm-inference-via-vector-retrieval/"
    },
    {
      "title": "One-to-many testing for code generation from (just) natural language",
      "authors": [
        "Gust Verbruggen",
        "Mansi Uniyal",
        "Mukul Singh",
        "Sumit Gulwani",
        "Vu Le"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Data platforms and analytics",
        "Programming languages and software engineering"
      ],
      "publication_date": "September 2024",
      "abstract": "MBPP is a popular dataset for evaluating models on the task of code generation. Despite its popularity there are three problems with the original MBPP: (1) reliance on providing test cases to generate the right signature, (2) contamination of the exact phrasing being present in training datasets, and (3) poor alignment between instruction and evaluation testcases. To overcome this, we create MBUPP, by adapting the popular MBPP dataset for code generation from natural language to emphasize on the natural language aspect by evaluating generated code on multiple sets of assertions. Additionally, we update the text descriptions to remove ambiguity and instructions that are not evaluated by the assertions, like specific algorithms to use. This adapted dataset resolves the challenges around contamination, ambiguity and testcase alignment. Further, we compare popular open and closed weight models on the original (MBPP) and adapted (MBUPP) datasets.",
      "url": "https://www.microsoft.com/en-us/research/publication/one-to-many-testing-for-code-generation-from-just-natural-language/"
    },
    {
      "title": "Learnings from a Large-Scale Deployment of an LLM-Powered Expert-in-the-Loop Healthcare Chatbot",
      "authors": [
        "Bhuvan Sachdeva",
        "Dr. Kaushik Murali",
        "Geeta Fulari",
        "Mohit Jain",
        "Pragnya Ramjee"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Human-computer interaction",
        "Medical, health and genomics"
      ],
      "publication_date": "September 2024",
      "abstract": "Large Language Models (LLMs) are widely used in healthcare, but limitations like hallucinations, incomplete information, and bias hinder their reliability. To address these, researchers released the Build Your Own expert Bot (BYOeB) platform, enabling developers to create LLM-powered chatbots with integrated expert verification. CataractBot, its first implementation, provides expert-verified responses to cataract surgery questions. A pilot evaluation showed its potential; however the study had a small sample size and was primarily qualitative. In this work, we conducted a large-scale 24-week deployment of CataractBot involving 318 patients and attendants who sent 1,992 messages, with 91.71% of responses verified by seven experts. Analysis of interaction logs revealed that medical questions significantly outnumbered logistical ones, hallucinations were negligible, and experts rated 84.52% of medical answers as accurate. As the knowledge base expanded with expert corrections, system performance improved by 19.02%, reducing expert workload. These insights guide the design of future LLM-powered chatbots.",
      "url": "https://www.microsoft.com/en-us/research/publication/learnings-from-a-large-scale-deployment-of-an-llm-powered-expert-in-the-loop-healthcare-chatbot/"
    },
    {
      "title": "Promptriever: Instruction-Trained Retrievers Can Be Prompted Like Language Models",
      "authors": [
        "Ashwin Paranjape",
        "Ben Van Durme",
        "Dawn Lawrie",
        "Jack Hessel",
        "Orion Weller",
        "Yuhao Zhang"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Search and information retrieval"
      ],
      "publication_date": "September 2024",
      "abstract": "Instruction-tuned language models (LM) are able to respond to imperative commands, providing a more natural user interface compared to their base counterparts. In this work, we present Promptriever, the first retrieval model able to be prompted like an LM. To train Promptriever, we curate and release a new instance-level instruction training set from MS MARCO, spanning nearly 500k instances. Promptriever not only achieves strong performance on standard retrieval tasks, but also follows instructions. We observe: (1) large gains (reaching SoTA) on following detailed relevance instructions (+14.3 p-MRR / +3.1 nDCG on FollowIR), (2) significantly increased robustness to lexical choices/phrasing in the query+instruction (+12.9 Robustness@10 on InstructIR), and (3) the ability to perform hyperparameter search via prompting to reliably improve retrieval performance (+1.4 average increase on BEIR). Promptriever demonstrates that retrieval models can be controlled with prompts on a per-query basis, setting the stage for future work aligning LM prompting techniques with information retrieval.",
      "url": "https://www.microsoft.com/en-us/research/publication/promptriever-instruction-trained-retrievers-can-be-prompted-like-language-models/"
    },
    {
      "title": "Principal Type Inference under a Prefix (TR)",
      "authors": [
        "Daan Leijen",
        "Wenjia Ye"
      ],
      "research_areas": [
        "Programming languages and software engineering"
      ],
      "publication_date": "September 2024",
      "abstract": "At the heart of the Damas-Hindley-Milner (HM) type system lies the abstraction rule which derives a function type for a lambda expression. This rule allows the type of the parameter to be “guessed”, which allows for multiple possible types for functions like the identity function. The beauty of the HM system is that there always exists a most general type that encompasses all possible derivations. Algorithm W is used to infer these most general types in practice.\nUnfortunately, this property is also the weakness of the HM type rules. Many languages extend HM typing with additional features which often require complex side conditions to the type rules to maintain principal types. For example, various type systems for impredicative type inference, like HMF, FreezeML, or Boxy types, require let-bindings to always assign most general types. Such restriction is difficult to specify as a logical deduction rule though, as it ranges over all possible derivations. Despite these complications, the actual implementations of various type inference algorithms are usually straightforward extensions of algorithm\\ W, and from an implementation perspective, much of the complexity of various type system extensions, like boxes or polymorphic weights, is in some sense artificial.\nIn this article we rephrase the HM type rules as _type inference under a prefix_, called HMQ. HMQ is sound and complete with respect to the HM type rules, but always derives principal types that correspond to the types inferred by algorithm W. The HMQ type rules are close to the clarity of the declarative HM type rules, but also specific enough to “read off” an inference algorithm, and can form an excellent basis to describe type system extensions in practice. We show in particular how to describe the FreezeML and HMF systems in terms of inference under a prefix, and how we no longer require complex side conditions. We also show a novel formalization of static overloading in HMQ as implemented in Koka language.",
      "url": "https://www.microsoft.com/en-us/research/publication/principal-type-inference-under-a-prefix-tr/"
    },
    {
      "title": "Evaluating the risk of data loss due to particle radiation damage in a DNA data storage system",
      "authors": [
        "Bichlien Nguyen",
        "Carlo Cazzaniga",
        "Christopher D. Frost",
        "Christopher N. Takahashi",
        "David P. Ward",
        "Jake Smith",
        "Kumkum Ganguly",
        "Paolo Rech",
        "Sean Blanchard",
        "Steve Wender"
      ],
      "research_areas": [
        "Hardware and devices",
        "Search and information retrieval",
        "Technology for emerging markets"
      ],
      "publication_date": "September 2024",
      "abstract": "DNA data storage is a potential alternative to magnetic tape for archival storage purposes, promising substantial gains in information density. Critical to the success of DNA as a storage media is an understanding of the role of environmental factors on the longevity of the stored information. In this paper, we evaluate the effect of exposure to ionizing particle radiation, a cause of data loss in traditional magnetic media, on the longevity of data in DNA data storage pools. We develop a mass action kinetics model to estimate the rate of damage accumulation in DNA strands due to neutron interactions with both nucleotides and residual water molecules, then utilize the model to evaluate the effect several design parameters of a typical DNA data storage scheme have on expected data longevity. Finally, we experimentally validate our model by exposing dried DNA samples to different levels of neutron irradiation and analyzing the resulting error profile. Our results show that particle radiation is not a significant contributor to data loss in DNA data storage pools under typical storage conditions.\n\n\n",
      "url": "https://www.microsoft.com/en-us/research/publication/evaluating-the-risk-of-data-loss-due-to-particle-radiation-damage-in-a-dna-data-storage-system/"
    },
    {
      "title": "AMEGO: Active Memory from long EGOcentric videos",
      "authors": [
        "D. Damen",
        "Gabriele Goletto",
        "Giuseppe Averta",
        "Tushar Nagarajan"
      ],
      "research_areas": [
        "Computer vision"
      ],
      "publication_date": "September 2024",
      "abstract": "Egocentric videos provide a unique perspective into individuals’ daily experiences, yet their unstructured nature presents challenges for perception. In this paper, we introduce AMEGO, a novel approach aimed at enhancing the comprehension of very-long egocentric videos. Inspired by the human’s ability to maintain information from a single watching, AMEGO focuses on constructing a self-contained representations from one egocentric video, capturing key locations and object interactions. This representation is semantic-free and facilitates multiple queries without the need to reprocess the entire visual content. Additionally, to evaluate our understanding of very-long egocentric videos, we introduce the new Active Memories Benchmark (AMB), composed of more than 20K of highly challenging visual queries from EPIC-KITCHENS. These queries cover different levels of video reasoning (sequencing, concurrency and temporal grounding) to assess detailed video understanding capabilities. We showcase improved performance of AMEGO on AMB, surpassing other video QA baselines by a substantial margin.",
      "url": "https://www.microsoft.com/en-us/research/publication/amego-active-memory-from-long-egocentric-videos/"
    },
    {
      "title": "EUREKA: Evaluating and Understanding Large Foundation Models",
      "authors": [
        "Besmira Nushi",
        "Eduardo Salinas",
        "Hamid Palangi",
        "James Woffinden-Luey",
        "Jingya Chen",
        "Neel Joshi",
        "Safoora Yousefi",
        "Vibhav Vineet",
        "Vidhisha Balachandran"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Computer vision",
        "Human language technologies"
      ],
      "publication_date": "September 2024",
      "abstract": "Rigorous and reproducible evaluation of large foundation models is critical for assessing the state of the art, informing next steps in model improvement, and for guiding scientific advances in Artificial Intelligence (AI). Evaluation is also important for informing the increasing number of application developers that build services on foundation models. The evaluation process has however become challenging in practice due to several reasons that require immediate attention from the community, including benchmark saturation, lack of transparency in the methods being deployed for measurement, development challenges in extracting the right measurements for generative tasks, and, more generally, the extensive number of capabilities that need to be considered for showing a well-rounded comparison across models. In addition, despite the overwhelming numbers of side-by-side capability evaluations available, we still lack a deeper understanding about when and how different models fail for a given capability and whether the nature of failures is similar across different models being released over time.\nWe make three contributions to alleviate the above challenges. First, we present Eureka, a reusable and open evaluation framework for standardizing evaluations of large foundation models beyond single-score reporting and rankings. Second, we introduce Eureka-Bench as an extensible collection of benchmarks testing capabilities that (i) are still challenging for state-of-the-art foundation models and (ii) represent fundamental but overlooked capabilities for completing tasks in both language and vision modalities. The available space for improvement that comes inherently from non-saturated benchmarks, enables us to discover meaningful differences between models at a capability level. Third, using the framework and Eureka-Bench, we conduct an analysis of 12 state-of-the-art models, providing in-depth insights for failure understanding and model comparison by disaggregating the measurements across important subcategories of data. Such insights uncover granular weaknesses of models for a given capability and can then be further leveraged to plan more precisely on what areas are most promising for improvement. Eureka is available as open-source to foster transparent and reproducible evaluation practices.\nIn contrast to recent trends in evaluation reports and leaderboards showing absolute rankings and claims for one model or another to be the best, our analysis shows that there is no such best model. Different models have different strengths, but there are models that appear more often than others as best performers for several capabilities. Despite the many observed improvements, it also becomes obvious that current models still struggle with a number of fundamental capabilities including detailed image understanding, benefiting from multimodal input when available rather than fully relying on language, factuality and grounding for information retrieval, and over refusals.",
      "url": "https://www.microsoft.com/en-us/research/publication/eureka-evaluating-and-understanding-large-foundation-models/"
    },
    {
      "title": "Highly Accurate Real-space Electron Densities with Neural Networks",
      "authors": [
        "Adam Foster",
        "Derk Kooi",
        "Frank Noé",
        "Jan Hermann",
        "Jonas Köhler",
        "K.J.H. Giesbertz",
        "Lixue Cheng",
        "P. Bernát Szabó",
        "Paola Gori-Giorgi",
        "Zeno Schätzle"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "September 2024",
      "abstract": "Variational ab initio methods in quantum chemistry stand out among other methods in providing direct access to the wave function. This allows, in principle, straightforward extraction of any other observable of interest, besides the energy, but, in practice, this extraction is often technically difficult and computationally impractical. Here, we consider the electron density as a central observable in quantum chemistry and introduce a novel method to obtain accurate densities from real-space many-electron wave functions by representing the density with a neural network that captures known asymptotic properties and is trained from the wave function by score matching and noise-contrastive estimation. We use variational quantum Monte Carlo with deep-learning Ansätze to obtain highly accurate wave functions free of basis set errors and from them, using our novel method, correspondingly accurate electron densities, which we demonstrate by calculating dipole moments, nuclear forces, contact densities, and other density-based properties.",
      "url": "https://www.microsoft.com/en-us/research/publication/highly-accurate-real-space-electron-densities-with-neural-networks/"
    },
    {
      "title": "Overview of the MEDIQA-MAGIC Task at ImageCLEF 2024: Multimodal And Generative TelemedICine in Dermatology",
      "authors": [
        "Asma Ben Abacha",
        "Fei Xia",
        "Meliha Yetisgen",
        "Wen-wai Yim",
        "Yujuan Fu",
        "Zhaoyi Sun"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Computer vision",
        "Medical, health and genomics"
      ],
      "publication_date": "September 2024",
      "abstract": "Multimodal processing and language generation require models to internally represent both language and vision, and then generate contextually appropriate responses. To do so with arbitrary images and textual inputs in the medical field, requires additional high performance and fidelity. This paper presents the overview of the MEDIQA-MAGIC shared task at ImageCLEF 2024. In this dermatological visual question-answering (VQA) task, participants receive the input of an image and a textual consumer health query, and are expected to output a textual medical answer. A total of twenty two runs were submitted with a variety of general language-vision models and fine-tuned models, with the best team achieving 8.969 BLEU points. We hope that the findings and insights explored here will inspire future research directions to support improved patient care.",
      "url": "https://www.microsoft.com/en-us/research/publication/overview-of-the-mediqa-magic-task-at-imageclef-2024-multimodal-and-generative-telemedicine-in-dermatology/"
    },
    {
      "title": "TopoMap++: A faster and more space efficient technique to compute projections with topological guarantees",
      "authors": [
        "Claudio Silva",
        "Felipe Inagaki de Oliveira",
        "Harish Doraiswamy",
        "L. G. Nonato",
        "Vitória Guardieiro"
      ],
      "research_areas": [
        "Data platforms and analytics",
        "Human-computer interaction"
      ],
      "publication_date": "September 2024",
      "abstract": "High-dimensional data, characterized by many features, can be difficult to visualize effectively. Dimensionality reduction techniques, such as PCA, UMAP, and t-SNE, address this challenge by projecting the data into a lower-dimensional space while preserving important relationships. TopoMap is another technique that excels at preserving the underlying structure of the data, leading to interpretable visualizations. In particular, TopoMap maps the high-dimensional data into a visual space, guaranteeing that the 0-dimensional persistence diagram of the Rips filtration of the visual space matches the one from the high-dimensional data. However, the original TopoMap algorithm can be slow and its layout can be too sparse for large and complex datasets. In this paper, we propose three improvements to TopoMap: 1) a more space-efficient layout, 2) a significantly faster implementation, and 3) a novel TreeMap-based representation that makes use of the topological hierarchy to aid the exploration of the projections. These advancements make TopoMap, now referred to as TopoMap++, a more powerful tool for visualizing high-dimensional data which we demonstrate through different use case scenarios.",
      "url": "https://www.microsoft.com/en-us/research/publication/topomap-a-faster-and-more-space-efficient-technique-to-compute-projections-with-topological-guarantees/"
    },
    {
      "title": "Windows Agent Arena: Evaluating Multi-Modal OS Agents at Scale",
      "authors": [
        "A. Bucker",
        "Dan Zhao",
        "Dillon Dupont",
        "Francesco Bonacci",
        "Justin Wagle",
        "Kazuhito Koishida",
        "Lawrence Jang",
        "Rogerio Bonatti",
        "Sara Abdali",
        "Yadong Lu",
        "Yinheng Li",
        "Zack Hui"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "September 2024",
      "abstract": "Large language models (LLMs) show remarkable potential to act as computer agents, enhancing human productivity and software accessibility in multi-modal tasks that require planning and reasoning. However, measuring agent performance in realistic environments remains a challenge since: (i) most benchmarks are limited to specific modalities or domains (e.g. text-only, web navigation, Q&A, coding) and (ii) full benchmark evaluations are slow (on order of magnitude of days) given the multi-step sequential nature of tasks. To address these challenges, we introduce the Windows Agent Arena: a reproducible, general environment focusing exclusively on the Windows operating system (OS) where agents can operate freely within a real Windows OS and use the same wide range of applications, tools, and web browsers available to human users when solving tasks. We adapt the OSWorld framework (Xie et al., 2024) to create 150+ diverse Windows tasks across representative domains that require agent abilities in planning, screen understanding, and tool usage. Our benchmark is scalable and can be seamlessly parallelized in Azure for a full benchmark evaluation in as little as 20 minutes. To demonstrate Windows Agent Arena’s capabilities, we also introduce a new multi-modal agent, Navi. Our agent achieves a success rate of 19.5% in the Windows domain, compared to 74.5% performance of an unassisted human. Navi also demonstrates strong performance on another popular web-based benchmark, Mind2Web. We offer extensive quantitative and qualitative analysis of Navi’s performance, and provide insights into the opportunities for future research in agent development and data generation using Windows Agent Arena. Webpage: https://microsoft.github.io/WindowsAgentArena Code: https://github.com/microsoft/WindowsAgentArena",
      "url": "https://www.microsoft.com/en-us/research/publication/windows-agent-arena-evaluating-multi-modal-os-agents-at-scale/"
    },
    {
      "title": "Can We Count on LLMs? The Fixed-Effect Fallacy and Claims of GPT-4 Capabilities",
      "authors": [
        "Cormac Herley",
        "Shuo Chen",
        "Thomas Ball"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "September 2024",
      "abstract": "In this paper we explore evaluation of LLM capabilities. We present measurements of GPT-4 performance on several deterministic tasks; each task involves a basic calculation and takes as input parameter some element drawn from a large well-defined population (e.g., count elements in a list, multiply two k-digit numbers, etc). We examine several conditions per-task and perform enough trials so that statistically significant differences can be detected. This allows us to investigate the sensitivity of task-accuracy both to query phrasing and input parameter population. We find that seemingly trivial modifications in the task-prompt or input population can yield differences far larger than can be explained by sampling effects. For example, performance on a simple list-counting task varies with query-phrasing and list-length, but also with list composition (i.e., the thing-to-be-counted) and object frequency (e.g., success when an element accounts for \\(\\approx\\) 50\\% of a list is different from when it accounts for \\(\\approx\\) 70\\% etc). We conclude that efforts to quantify LLM capabilities easily succumb to the language-as-fixed-effect fallacy, where experimental observations are improperly generalized beyond what the data supports. A consequence appears to be that intuitions that have been formed based on interactions with humans form a very unreliable guide as to which input modifications should “make no difference” to LLM performance.",
      "url": "https://www.microsoft.com/en-us/research/publication/can-we-count-on-llms-the-fixed-effect-fallacy-and-claims-of-gpt-4-capabilities/"
    },
    {
      "title": "Policy Filtration for RLHF to Mitigate Noise in Reward Models",
      "authors": [
        "Chuheng Zhang",
        "Jiang Bian",
        "Li Zhao",
        "Wanchun Dou",
        "Wei Shen",
        "Xiaolong Xu",
        "Xuyun Zhang"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "September 2024",
      "abstract": "While direct policy optimization methods exist, pioneering LLMs are fine-tuned with reinforcement learning from human feedback (RLHF) to generate better responses under the supervision of a reward model learned from preference data. One major challenge of RLHF is the inaccuracy of the intermediate reward model, especially in the tasks that requires complex reasoning for the reward model to score a response. We find that the reliability of the reward model varies across responses assigned with different rewards. This motivates us to filter the samples whose rewards may be unreliable to improve the signal-to-noise ratio during policy learning, resulting in Policy Filtration for Proximal Policy Optimization (PF-PPO). To choose a proper policy filtering strategy, we use the coefficient of determination (R2) between the rewards and actual scores on filtered samples as the metrics to help us find promising strategies since it measures how well the rewards filtered by PF-PPO indicate real performance. We provide extensive experiments to validate the effectiveness of PF-PPO in code generation and math reasoning tasks. In code generation, PF-PPO achieves the state-of-the-art performance of 7-billion-parameter models on HumanEval (+7.9%), MBPP (+0.7%), and LeetCode Contest (+10.0%) which is a more challenging benchmark created by us. In math reasoning, PF-PPO yields performance increase using different reward models and benchmarks (Ape210K and CMATH). Code is available on https://github.com/swtheing/PF-PPO-RLHF.",
      "url": "https://www.microsoft.com/en-us/research/publication/policy-filtration-for-rlhf-to-mitigate-noise-in-reward-models/"
    },
    {
      "title": "MarS: a Financial Market Simulation Engine Powered by Generative Foundation Model",
      "authors": [
        "Chang Xu",
        "Jiang Bian",
        "Junjie Li",
        "Lewen Wang",
        "Shikai Fang",
        "Weiqing Liu",
        "Yang Liu"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Economics"
      ],
      "publication_date": "September 2024",
      "abstract": "Generative models aim to simulate realistic effects of various actions across different contexts, from text generation to visual effects. Despite efforts to build real-world simulators, leveraging generative models for virtual worlds, like financial markets, remains underexplored. In financial markets, generative models can simulate market effects of various behaviors, enabling interaction with market scenes and players, and training strategies without financial risk. This simulation relies on the finest structured data in financial market like orders thus building the finest realistic simulation. We propose Large Market Model (LMM), an order-level generative foundation model, for financial market simulation, akin to language modeling in the digital world. Our financial Market Simulation engine (MarS), powered by LMM, addresses the need for realistic, interactive and controllable order generation. Key objectives of this paper include evaluating LMM’s scaling law in financial markets, assessing MarS’s realism, balancing controlled generation with market impact, and demonstrating MarS’s potential applications. We showcase MarS as a forecast tool, detection system, analysis platform, and agent training environment. Our contributions include pioneering a generative model for financial markets, designing MarS to meet domain-specific needs, and demonstrating MarS-based applications’ industry potential.",
      "url": "https://www.microsoft.com/en-us/research/publication/mars-a-financial-market-simulation-engine-powered-by-generative-foundation-model/"
    },
    {
      "title": "AImagery: A Multisensory Approach to Anxiety Reduction with AI, Olfactory Stimuli, and Biofeedback-Enhanced Guided Imagery",
      "authors": [
        "Javier Hernandez",
        "Judith Amores"
      ],
      "research_areas": [
        "Human-computer interaction"
      ],
      "publication_date": "September 2024",
      "abstract": "We present AImagery, an AI-powered immersive relaxation experience tailored to user preferences and physiological feedback. In a study with 32 participants, half of them experienced a multisensory experience with scent, biofeedback, and a personalized AI audio story based on their heart rate, self-reported mood and custom scenery. The experimental group showed a significant anxiety reduction for those with moderate to high anxiety, as opposed to the control group (non-guided meditation/baseline resting control condition). User feedback was positive and received high ratings for enjoyment, immersion and, to a lesser extend, sleepiness. Our results highlight the potential of multisensory AI-driven relaxation tools for those with elevated anxiety.",
      "url": "https://www.microsoft.com/en-us/research/publication/aimagery/"
    },
    {
      "title": "Proceedings of The First Workshop on Large Language Models for Evaluation in Information Retrieval (LLM4Eval 2024)",
      "authors": [
        "Bhaskar Mitra",
        "Charles L. A. Clarke",
        "Clemencia Siro",
        "Emine Yilmaz",
        "Guglielmo Faggioli",
        "Hossein A. Rahmani",
        "Mohammad Aliannejadi",
        "Nick Craswell",
        "Paul Thomas"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Search and information retrieval"
      ],
      "publication_date": "September 2024",
      "abstract": "About The Workshop: Large language models (LLMs) have demonstrated increasing task-solving abilities not present in smaller models. Utilizing the capabilities and responsibilities of LLMs for automated evaluation (LLM4Eval) has recently attracted considerable attention in multiple research communities. For instance, LLM4Eval models have been studied in the context of automated judgments, natural language generation, and retrieval augmented generation systems. We believe that the information retrieval community can significantly contribute to this growing research area by designing, implementing, analyzing, and evaluating various aspects of LLMs with applications to LLM4Eval tasks. The main goal of LLM4Eval workshop is to bring together researchers from industry and academia to discuss various aspects of LLMs for evaluation in information retrieval, including automated judgments, retrieval-augmented generation pipeline evaluation, altering human evaluation, robustness, and trustworthiness of LLMs for evaluation in addition to their impact on real-world applications. We also plan to run an automated judgment challenge prior to the workshop, where participants will be asked to generate labels for a given dataset while maximising correlation with human judgments. The format of the workshop is interactive, including roundtable and keynote sessions and tends to avoid the one-sided dialogue of a mini-conference.\nThere were 26 papers submitted for peer-review to this workshop. Out of these, 7 papers were accepted for this volume as original research papers. Additionally, other 16 papers, 11 original contributions and 5 already published works, were accepted and presented during the workshop.",
      "url": "https://www.microsoft.com/en-us/research/publication/proceedings-of-the-first-workshop-on-large-language-models-for-evaluation-in-information-retrieval-llm4eval-2024/"
    },
    {
      "title": "Overview of ImageCLEFmedical 2024 – Caption Prediction and Concept Detection",
      "authors": [
        "Ahmad Idrissi-Yaghir",
        "Alba G. Seco de Herrera",
        "Asma Ben Abacha",
        "Benjamin Bracke",
        "Christoph M. Friedrich",
        "Cynthia Sabrina Schmidt",
        "Hendrik Damm",
        "Henning Müller",
        "Henning Schäfer",
        "Johannes Rückert",
        "Louise Bloch",
        "Raphael Brüngel",
        "Tabea M. G. Pakull"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Computer vision",
        "Medical, health and genomics"
      ],
      "publication_date": "September 2024",
      "abstract": "The ImageCLEFmedical 2024 Caption task on caption prediction and concept detection follows similar challenges held from 2017–2023. The goal is to extract Unified Medical Language System (UMLS) concept annotations and/or define captions from image data. Predictions are compared to original image captions. Images for both tasks are part of the Radiology Objects in COntext version 2 (ROCOv2) dataset. For concept detection, multi-label predictions are compared against UMLS terms extracted from the original captions with additional manually curated concepts via the F1-score. For caption prediction, the semantic similarity of the predictions to the original captions is evaluated using the BERTScore. The task attracted strong participation with 50 registered teams, 14 teams submitted 82 graded runs for the two subtasks. Participants mainly used multi-label classification systems for the concept detection subtask, the winning team DBS-HHU utilized an ensemble of four different Convolutional Neural Networks (CNNs). For the caption prediction subtask, most teams used encoder-decoder frameworks with various backbones, including transformer-based decoders and Long Short-Term Memories (LSTMs), with the winning team PCLmed using medical vision-language foundation models (Med-VLFMs) by combining general and specialist vision models.",
      "url": "https://www.microsoft.com/en-us/research/publication/overview-of-imageclefmedical-2024-caption-prediction-and-concept-detection/"
    },
    {
      "title": "Domain mismatch and data augmentation in speech emotion recognition",
      "authors": [
        "Dimitra Emmanouilidou",
        "Hannes Gamper",
        "Midia Yousefi"
      ],
      "research_areas": [
        "Audio and Acoustics"
      ],
      "publication_date": "September 2024",
      "abstract": "Large, pretrained model architectures have demonstrated potential in a wide range of audio recognition and classification tasks. These architectures are increasingly being used in Speech Emotion Recognition (SER) as well, an area that continues to grapple with the scarcity of data, and especially of labeled data for training. This study is motivated by the limited research available on the robustness and generalization capabilities of these models for SER and considers applicability beyond a restricted dataset. We invoke the widely adopted net- work architecture CNN14 and explore its ability to generalize across different datasets. Our analysis demonstrates a potential domain gap between datasets after analyzing the acoustic properties of each one. We bridge this gap with the introduction of acoustic and data variability, by invoking seven suitable augmentation methods. Our approach leads to up to 8% improvement for unseen datasets. However, bridging the acoustic mismatch seems to play a minor role only: an infelicitous finding involving partially scrambled (swapped) annotation labels hints to deeper domain mismatches during multi-dataset learning scenarios. Findings in this work are applicable to any large or pretrained network and contribute to the ongoing research on the robustness and generalization of SER models.\n \n \nFigure: Distribution of estimated mean opinion score (MOS), reverberation time (T60), and clarity (C50) for IEMOCAP and MSP-Podcast, with and without data augmentation. Each statistical distribution shown is averaged over 10,000 random samples. Notice how the application of the selected augmentations (row augm.) bridges the distribution gap across datasets.\n \nFigure: Effect of augmentations. Overall, the addition of augmentation seems to allow for performance improvement within a dataset and on unseen dataset scenarios, but it doesn’t seem to drastically improve cross-dataset generalization. The domain mismatches seem more intricate, and may go beyond acoustic conditions. Effect on baseline scenarios, Table 1: increasing amounts of augmentation improve wF1 by up to 5% for the large dataset MSP-Podcast, but the effect is not apparent for the smaller or less varied sets of IEMOCAP and CREMA-D. Effect on unseen data, Figure 2: the figure summarizes relative wF1 change at various rates of data augmentation during training (rows), and across datasets (columns). See Section 5 on interpreting relative wF1. Testing on unseen CREMA-D benefits from augmentation by as much as 3% column A, and 8% column B: from -0.36 wF1 drop to -0.28 drop. This is also evident in joint training column C. We further notice deterioration on unseen IEMOCAP column A, despite having bridged acoustic differences with augmentation.\n \n \n \nFigure: We introduce label scrambling only on IEMOCAP. We swap class labels Angry <-> Happy, and Sad<->Neutral. We then create a joined training scenario and add MSP-Podcast in training, together with the SCRAMBLED IEMOCAP. The assumption is, now that the acoustic conditions are matched between datasets, if the CNN14 model captures good/generalizable emotional features, then label scrambling should significantly confuse the model. The tables show before (left) and after (scrambling), where we see no particular effect in performance. The depicted table values correspond to relative change in wF1, relative compared to the baseline scenario when we train on the same dataset as the one we test on.\n ",
      "url": "https://www.microsoft.com/en-us/research/publication/domain-mismatch-and-data-augmentation-in-speech-emotion-recognition/"
    },
    {
      "title": "Domino: Eliminating Communication in LLM Training via Generic Tensor Slicing and Overlapping",
      "authors": [
        "Ang Li",
        "Chengming Zhang",
        "Guanhua Wang",
        "Olatunji Ruwase",
        "Zheyu Shen"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Systems and networking"
      ],
      "publication_date": "September 2024",
      "abstract": "Given the popularity of generative AI, Large Language Models (LLMs) often consume hundreds or thousands of GPUs for parallelizing and accelerating the training process. Communication overhead becomes more pronounced when training LLMs at scale. To eliminate communication overhead in distributed LLM training, we propose Domino, which provides a generic scheme to hide communication behind computation. By breaking data dependency of a single batch training into smaller independent pieces, Domino pipelines these independent pieces training and provides generic strategy of fine-grained communication and computation overlapping. Extensive results show that, comparing with Megatron-LM, Domino achieves up to 1.3x speedup for LLM training on Nvidia DGX-H100 GPUs.",
      "url": "https://www.microsoft.com/en-us/research/publication/domino-eliminating-communication-in-llm-training-via-generic-tensor-slicing-and-overlapping/"
    },
    {
      "title": "Uncover Nested Data Parallelism and Data Reuse in DNN Computation with FractalTensor",
      "authors": [
        "Fan Yang",
        "Mao Yang",
        "Ying Cao"
      ],
      "research_areas": [
        "Programming languages and software engineering",
        "Systems and networking"
      ],
      "publication_date": "September 2024",
      "abstract": "Abstract to come…",
      "url": "https://www.microsoft.com/en-us/research/publication/uncover-nested-data-parallelism-and-data-reuse-in-dnn-computation-with-fractaltensor/"
    },
    {
      "title": "AI detection of malicious push notifications in augmented reality in the workplace",
      "authors": [
        "Sarah Katz"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Security, privacy, and cryptography"
      ],
      "publication_date": "September 2024",
      "abstract": "Distraction caused by the visual processing of multiple objects during augmented reality (AR) immersion could make users more susceptible to malicious push notifications, thus potentially exposing organisations to unwitting insider threats. This case study consulted four experts in the field of AR application development to design a proposed artificial intelligence (AI) equipped feature that could detect possibly malicious artefacts entering the user’s line of sight during partial immersion in an augmented reality application at the workplace. Participants included a business partner at an AR company, a security engineering manager, an AI engineer focused on machine learning (ML) and a data analytics specialist. The case study determined that a security application natively implemented into the device could use heuristic analysis of user screen captured activity to assess potentially malicious push notifications in real time.",
      "url": "https://www.microsoft.com/en-us/research/publication/ai-detection-of-malicious-push-notifications-in-augmented-reality-in-the-workplace/"
    },
    {
      "title": "Retrieval Augmented Generation (RAG) and Beyond: A Comprehensive Survey on How to Make your LLMs use External Data More Wisely",
      "authors": [
        "Lili Qiu",
        "Luna K. Qiu",
        "Siyun Zhao",
        "Yuqing Yang",
        "Zhiyuan He",
        "Zilong Wang"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "September 2024",
      "abstract": "Large language models (LLMs) augmented with external data have demonstrated remarkable capabilities in completing real-world tasks. Techniques for integrating external data into LLMs, such as Retrieval-Augmented Generation (RAG) and fine-tuning, are gaining increasing attention and widespread application. Nonetheless, the effective deployment of data-augmented LLMs across various specialized fields presents substantial challenges. These challenges encompass a wide range of issues, from retrieving relevant data and accurately interpreting user intent to fully harnessing the reasoning capabilities of LLMs for complex tasks. We believe that there is no one-size-fits-all solution for data-augmented LLM applications. In practice, underperformance often arises from a failure to correctly identify the core focus of a task or because the task inherently requires a blend of multiple capabilities that must be disentangled for better resolution. In this survey, we propose a RAG task categorization method, classifying user queries into four levels based on the type of external data required and primary focus of the task: explicit fact queries, implicit fact queries, interpretable rationale queries, and hidden rationale queries. We define these levels of queries, provide relevant datasets, and summarize the key challenges and most effective techniques for addressing these challenges. Finally, we discuss three main forms of integrating external data into LLMs: context, small model, and fine-tuning, highlighting their respective strengths, limitations, and the types of problems they are suited to solve. This work aims to help readers thoroughly understand and decompose the data requirements and key bottlenecks in building LLM applications, offering solutions to the different challenges and serving as a guide to systematically developing such applications.",
      "url": "https://www.microsoft.com/en-us/research/publication/retrieval-augmented-generation-rag-and-beyond-a-comprehensive-survey-on-how-to-make-your-llms-use-external-data-more-wisely/"
    },
    {
      "title": "COSMIC: Data Efficient Instruction-tuning For Speech In-Context Learning",
      "authors": [
        "Jian Wu",
        "Jing Pan",
        "Jinyu Li",
        "Shujie Liu",
        "Sunit Sivasankaran",
        "Yashesh Gaur",
        "Zhuo Chen"
      ],
      "research_areas": [
        "Human language technologies"
      ],
      "publication_date": "September 2024",
      "abstract": "We present a cost-effective method to integrate speech into a large language model (LLM), resulting in a Contextual Speech Model with Instruction-following/in-context-learning Capabilities (COSMIC) multi-modal LLM. Using GPT-3.5, we generate Speech Comprehension Test Question-Answer (SQA) pairs from speech transcriptions for supervised instruction tuning. With under 30 million trainable parameters and only 450 hours of English speech data, COSMIC demonstrates emerging capabilities in instruction-following and in-context learning. Equipped with such capabilities, COSMIC achieves a maximum 33.18 BLEU score in 0-shot EN-to-X speech to text translation (S2TT) and a significant boost in the 1-shot setting. Additionally, there is an average 25.8% relative Word Error Rate (WER) reduction for 1-shot cross-domain adaptation. COSMIC exhibits a significant automatic speech recognition (ASR) accuracy gain in contextual biasing tasks due to its instruction-following capability.",
      "url": "https://www.microsoft.com/en-us/research/publication/cosmic-data-efficient-instruction-tuning-for-speech-in-context-learning/"
    },
    {
      "title": "Multi-label audio classification with a noisy zero-shot teacher",
      "authors": [
        "Hannes Gamper",
        "Sebastian Braun"
      ],
      "research_areas": [
        "Audio and Acoustics"
      ],
      "publication_date": "September 2024",
      "abstract": "We propose a novel training scheme using self-label correction and data augmentation methods designed to deal with noisy labels and improve real-world accuracy on a polyphonic audio content detection task. The augmentation method reduces label noise by mixing multiple audio clips and joining their labels, while being compatible with multiple active labels. We additionally show that performance can be improved by a self-label correction method using the same pretrained model. Finally, we show that it is feasible to use a strong zero-shot model such as CLAP to generate labels for unlabeled data and improve the results using the proposed training and label enhancement methods. The resulting model performs similar to CLAP while being an efficient mobile device friendly architecture and can be quickly adapted to unlabeled sound classes.",
      "url": "https://www.microsoft.com/en-us/research/publication/multi-label-audio-classification-with-a-noisy-zero-shot-teacher/"
    },
    {
      "title": "CMMD: Contrastive Multi-Modal Diffusion for Video-Audio Conditional Modeling",
      "authors": [
        "Hannes Gamper",
        "Ruihan Yang",
        "Sebastian Braun"
      ],
      "research_areas": [
        "Audio and Acoustics",
        "Graphics and multimedia"
      ],
      "publication_date": "September 2024",
      "abstract": "We introduce a multi-modal diffusion model tailored for the bi-directional conditional generation of video and audio. We propose a joint contrastive training loss to improve the synchronization between visual and auditory occurrences. We present experiments on two datasets to evaluate the efficacy of our proposed model. The assessment of generation quality and alignment performance is carried out from various angles, encompassing both objective and subjective metrics. Our findings demonstrate that the proposed model outperforms the baseline in terms of quality and generation speed through introduction of our novel cross-modal easy fusion architectural block. Furthermore, the incorporation of the contrastive loss results in improvements in audio-visual alignment, particularly in the high-correlation video-to-audio generation task.",
      "url": "https://www.microsoft.com/en-us/research/publication/cmmd-contrastive-multi-modal-diffusion-for-video-audio-conditional-modeling/"
    },
    {
      "title": "Gaussian Flow Bridges for Audio Domain Transfer with Unpaired Data",
      "authors": [
        "Eloi Moliner",
        "Hannes Gamper",
        "Sebastian Braun"
      ],
      "research_areas": [
        "Audio and Acoustics"
      ],
      "publication_date": "September 2024",
      "abstract": "Audio domain transfer is the process of modifying audio signals to match characteristics of a different domain, while retaining the original content. Examples include transferring room acoustics or altering audio effects such as distortion. This paper investigates the potential of Gaussian Flow Bridges, an emerging approach in generative modeling, for these problems. The presented framework addresses the transport problem across different distributions of audio signals through the implementation of a series of two deterministic probability flows. The proposed framework facilitates manipulation of the target distribution properties through a continuous control variable, which defines a certain aspect of the target domain. Notably, this approach does not rely on paired examples for training. To address identified challenges on maintaining the speech content consistent, we recommend a training strategy that incorporates chunk-based minibatch Optimal Transport couplings of data samples and noise. Comparing our unsupervised method with established baselines, we find competitive performance in tasks of reverberation and distortion manipulation. Despite encoutering limitations, the intriguing results obtained in this study underscore potential for further exploration.",
      "url": "https://www.microsoft.com/en-us/research/publication/gaussian-flow-bridges-for-audio-domain-transfer-with-unpaired-data/"
    },
    {
      "title": "Datacenter power and energy management: past, present, and future",
      "authors": [
        "Anand Sivasubramaniam",
        "Christian Belady",
        "Ricardo Bianchini"
      ],
      "research_areas": [
        "Systems and networking"
      ],
      "publication_date": "September 2024",
      "abstract": "This article overviews some of the key past developments in cloud datacenter power and energy management, where we are today, and what the future could be. This topic is gaining enormous, renewed interest in the context of the conflicting needs of the AI revolution and the climate crisis.",
      "url": "https://www.microsoft.com/en-us/research/publication/datacenter-power-and-energy-management-past-present-and-future/"
    },
    {
      "title": "Total-Duration-Aware Duration Modeling for Text-to-Speech Systems",
      "authors": [
        "Canrun Li",
        "Chung-Hsien Tsai",
        "Hemin Yang",
        "Jinyu Li",
        "Manthan Thakker",
        "Min Tang",
        "Naoyuki Kanda",
        "Sefik Emre Eskimez",
        "Sheng Zhao",
        "Xiaofei Wang",
        "Zhen Xiao",
        "Zirun Zhu"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Audio and Acoustics"
      ],
      "publication_date": "September 2024",
      "abstract": "Accurate control of the total duration of generated speech by adjusting the speech rate is crucial for various text-to-speech (TTS) applications. However, the impact of adjusting the speech rate on speech quality, such as intelligibility and speaker characteristics, has been underexplored. In this work, we propose a novel total-duration-aware (TDA) duration model for TTS, where phoneme durations are predicted not only from the text input but also from an additional input of the total target duration. We also propose a MaskGIT-based duration model that enhances the diversity and quality of the predicted phoneme durations. Our results demonstrate that the proposed TDA duration models achieve better intelligibility and speaker similarity for various speech rate configurations compared to the baseline models. We also show that the proposed MaskGIT-based model can generate phoneme durations with higher quality and diversity compared to its regression or flow-matching counterparts.",
      "url": "https://www.microsoft.com/en-us/research/publication/total-duration-aware-duration-modeling-for-text-to-speech-systems/"
    },
    {
      "title": "The Market Effects of Algorithms",
      "authors": [
        "Lindsey Raymond"
      ],
      "research_areas": [
        "Economics"
      ],
      "publication_date": "September 2024",
      "abstract": "While there is excitement about the potential of algorithms to optimize individual decision-making, changing individual behavior will, almost inevitably, impact markets. Yet little is known about these effects. In this paper, I study how the availability of algorithmic prediction changes entry, allocation, and prices in the U.S. residential real estate market, a key driver of household wealth. I identify a market-level natural experiment that generates variation in the cost of using algorithms to value houses: digitization, the transition from physical to digital housing records. I show that digitization leads to entry by investors using algorithms, but does not displace investors using human judgment. Instead, human investors shift towards houses that are difficult to predict algorithmically. Algorithm-using investors predominantly purchase minority-owned homes, an area where humans may be biased. Digitization increases the average sale price of minority-owned homes by 5% or $5,000 and nearly eliminates racial disparities in home prices. Algorithmic investors, via competition, affect the prices paid by humans, which drives most of the reduction in racial disparities.  This decrease in racial inequality underscores the potential of algorithms to mitigate human biases at the market level.",
      "url": "https://www.microsoft.com/en-us/research/publication/the-market-effects-of-algorithms/"
    },
    {
      "title": "Knowledge boosting during low-latency inference",
      "authors": [
        "Malek Itani",
        "Sefik Emre Eskimez",
        "Shyamnath Gollakota",
        "Takuya Yoshioka",
        "Tuochao Chen",
        "Vidya Srinivas"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Audio and Acoustics"
      ],
      "publication_date": "September 2024",
      "abstract": "Models for low-latency, streaming applications could benefit from the knowledge capacity of larger models, but edge devices cannot run these models due to resource constraints. A possible solution is to transfer hints during inference from a large model running remotely to a small model running on-device. However, this incurs a communication delay that breaks real-time requirements and does not guarantee that both models will operate on the same data at the same time. We propose knowledge boosting, a novel technique that allows a large model to operate on time-delayed input during inference, while still boosting small model performance. Using a streaming neural network that processes 8~ms chunks, we evaluate different speech separation and enhancement tasks with communication delays of up to six chunks or 48~ms. Our results show larger gains where the performance gap between the small and large models is wide, demonstrating a promising method for large-small model collaboration for low-latency applications. Code, dataset, and audio samples available at https://knowledgeboosting.cs.washington.edu/ (opens in new tab)",
      "url": "https://www.microsoft.com/en-us/research/publication/knowledge-boosting-during-low-latency-inference/"
    },
    {
      "title": "PAM: Prompting Audio-Language Models for Audio Quality Assessment",
      "authors": [
        "Benjamin Elizalde",
        "Bhiksha Raj",
        "Dareen Alharthi",
        "Hannes Gamper",
        "Huaming Wang",
        "Mahmoud Al Ismail",
        "Rita Singh",
        "Soham Deshmukh"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Audio and Acoustics"
      ],
      "publication_date": "September 2024",
      "abstract": "While audio quality is a key performance metric for various audio processing tasks, including generative modeling, its objective measurement remains a challenge. Audio-Language Models (ALMs) are pre-trained on audio-text pairs that may contain information about audio quality, the presence of artifacts, or noise. Given an audio input and a text prompt related to quality, an ALM can be used to calculate a similarity score between the two. Here, we exploit this capability and introduce PAM, a no-reference metric for assessing audio quality for different audio processing tasks. Contrary to other”reference-free”metrics, PAM does not require computing embeddings on a reference dataset nor training a task-specific model on a costly set of human listening scores. We extensively evaluate the reliability of PAM against established metrics and human listening scores on four tasks: text-to-audio (TTA), text-to-music generation (TTM), text-to-speech (TTS), and deep noise suppression (DNS). We perform multiple ablation studies with controlled distortions, in-the-wild setups, and prompt choices. Our evaluation shows that PAM correlates well with existing metrics and human listening scores. These results demonstrate the potential of ALMs for computing a general-purpose audio quality metric.",
      "url": "https://www.microsoft.com/en-us/research/publication/pam-prompting-audio-language-models-for-audio-quality-assessment/"
    },
    {
      "title": "Performance of explainable artificial intelligence in guiding the management of patients with a pancreatic cyst",
      "authors": [
        "Alison P Klein",
        "Ammar A Javed",
        "Anne Marie Lennon",
        "Bill Weeks",
        "Caleb Robinson",
        "Christopher L Wolfgang",
        "Elham Afghani",
        "Elliot K Fishman",
        "Felipe Oviedo",
        "Jin He",
        "Juan M. Lavista Ferres",
        "Ken Kinzler",
        "Linda Chu",
        "Mike Goggins",
        "Nick Papadopolous",
        "Rahul Dodhia",
        "Ralph H Hruban",
        "Satomi Kawamoto"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Medical, health and genomics"
      ],
      "publication_date": "September 2024",
      "abstract": "Background/objectives\nPancreatic cyst management can be distilled into three separate pathways – discharge, monitoring or surgery– based on the risk of malignant transformation. This study compares the performance of artificial intelligence (AI) models to clinical care for this task.\nMethods\nTwo explainable boosting machine (EBM) models were developed and evaluated using clinical features only, or clinical features and cyst fluid molecular markers (CFMM) using a publicly available dataset, consisting of 850 cases (median age 64; 65 % female) with independent training (429 cases) and holdout test cohorts (421 cases). There were 137 cysts with no malignant potential, 114 malignant cysts, and 599 IPMNs and MCNs.\nResults\nThe EBM and EBM with CFMM models had higher accuracy for identifying patients requiring monitoring (0.88 and 0.82) and surgery (0.66 and 0.82) respectively compared with current clinical care (0.62 and 0.58). For discharge, the EBM with CFMM model had a higher accuracy (0.91) than either the EBM model (0.84) or current clinical care (0.86). In the cohort of patients who underwent surgical resection, use of the EBM-CFMM model would have decreased the number of unnecessary surgeries by 59 % (n = 92), increased correct surgeries by 7.5 % (n = 11), identified patients who require monitoring by 122 % (n = 76), and increased the number of patients correctly classified for discharge by 138 % (n = 18) compared to clinical care.\nConclusions\nEBM models had greater sensitivity and specificity for identifying the correct management compared with either clinical management or previous AI models. The model predictions are demonstrated to be interpretable by clinicians.\n ",
      "url": "https://www.microsoft.com/en-us/research/publication/performance-of-explainable-artificial-intelligence-in-guiding-the-management-of-patients-with-a-pancreatic-cyst/"
    },
    {
      "title": "Target conversation extraction: Source separation using turn-taking dynamics",
      "authors": [
        "Bohan Wu",
        "Malek Itani",
        "Qirui Wang",
        "Sefik Emre Eskimez",
        "Shyamnath Gollakota",
        "Takuya Yoshioka",
        "Tuochao Chen"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Audio and Acoustics"
      ],
      "publication_date": "September 2024",
      "abstract": "Extracting the speech of participants in a conversation amidst interfering speakers and noise presents a challenging problem. In this paper, we introduce the novel task of target conversation extraction, where the goal is to extract the audio of a target conversation based on the speaker embedding of one of its participants. To accomplish this, we propose leveraging temporal patterns inherent in human conversations, particularly turn-taking dynamics, which uniquely characterize speakers engaged in conversation and distinguish them from interfering speakers and noise. Using neural networks, we show the feasibility of our approach on English and Mandarin conversation datasets. In the presence of interfering speakers, our results show an 8.19 dB improvement in signal-to-noise ratio for 2-speaker conversations and a 7.92 dB improvement for 2-4-speaker conversations. Code, dataset available at https://github.com/chentuochao/Target-Conversation-Extraction (opens in new tab)",
      "url": "https://www.microsoft.com/en-us/research/publication/target-conversation-extraction-source-separation-using-turn-taking-dynamics/"
    },
    {
      "title": "AI Delegates with a Dual Focus: Ensuring Privacy and Strategic Self-Disclosure",
      "authors": [
        "Chao Du",
        "Dongmei Zhang",
        "Fangkai Yang",
        "Hangxin Liu",
        "Qi Zhang",
        "Qingwei Lin 林庆维",
        "Saravan Rajmohan",
        "Xi Chen",
        "Xi Cheng",
        "Xiaoting Qin",
        "Zhiyang Zhang"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Human-computer interaction"
      ],
      "publication_date": "September 2024",
      "abstract": "Large language model (LLM)-based AI delegates are increasingly utilized to act on behalf of users, assisting them with a wide range of tasks through conversational interfaces. Despite their advantages, concerns arise regarding the potential risk of privacy leaks, particularly in scenarios involving social interactions. While existing research has focused on protecting privacy by limiting the access of AI delegates to sensitive user information, many social scenarios require disclosing private details to achieve desired outcomes, necessitating a balance between privacy protection and disclosure. To address this challenge, we conduct a pilot study to investigate user preferences for AI delegates across various social relations and task scenarios, and then propose a novel AI delegate system that enables privacy-conscious self-disclosure. Our user study demonstrates that the proposed AI delegate strategically protects privacy, pioneering its use in diverse and dynamic social interactions.",
      "url": "https://www.microsoft.com/en-us/research/publication/ai-delegates-with-a-dual-focus-ensuring-privacy-and-strategic-self-disclosure/"
    },
    {
      "title": "Adaptive Security, Erasures, and Network Assumptions in Communication-Local MPC",
      "authors": [
        "Ankit Kumar Misra",
        "Juan Garay",
        "Nishanth Chandran",
        "Rafail Ostrovsky",
        "Vassilis Zikas"
      ],
      "research_areas": [
        "Security, privacy, and cryptography"
      ],
      "publication_date": "September 2024",
      "abstract": "The problem of reliable/secure all-to-all communication over low-degree networks has been essential for communication-local (CL) n-party MPC (i.e., MPC protocols where every party directly communicates only with a few, typically polylogarithmic in n, parties) and more recently for communication over ad hoc networks, which are used in blockchain protocols. However, a limited number of adaptively secure solutions exist, and they all make relatively strong assumptions on the ability of parties to act in some specific manner before the adversary can corrupt them.\nTwo such assumptions were made in the work of Chandran et al. [ITCS ’15]—parties can (a) multisend messages to several receivers simultaneously; and (b) securely erase the message and the identities of the receivers, before the adversary gets a chance to corrupt the sender (even if a receiver is corrupted). A natural question to ask is: Are these assumptions necessary for adaptively secure CL MPC? In this paper, we characterize the feasibility landscape for all-to-all reliable message transmission (RMT) under these two assumptions, and use this characterization to obtain (asymptotically) tight feasibility results for CL MPC.\nFirst, we prove a strong impossibility result for a broad class of RMT protocols, termed here store-and-forward protocols, which includes all known communication protocols for CL MPC from standard cryptographic assumptions. Concretely, we show that no such protocol with a certain expansion rate can tolerate a constant fraction of parties being corrupted. – Next, under the assumption of only a PKI, we show that assuming secure erasures, we can obtain an RMT protocol between all pairs of parties with polylogarithmic locality (even without assuming multisend) for the honest majority setting. We complement this result by showing a negative result for the setting of dishonest majority. – Finally, and somewhat surprisingly, under stronger assumptions (i.e., trapdoor permutations with a reverse domain sampler, and compact and malicious circuit-private FHE), we construct a polylogarithmic-locality all-to-one RMT protocol, which is adaptively secure and tolerates any constant fraction of corruptions, without assuming either secure erasures or multisend. This last result uses a novel combination of adaptively secure (e.g., non-committing) encryption and (static) FHE to bypass the impossibility of compact adaptively secure FHE by Katz et al. [PKC’13], which we believe may be of independent interest. Intriguingly, even such assumptions do not allow reducing all-to-all RMT to all-to-one RMT (a reduction which is trivial in the non-CL setting). Still, we can implement what we call sublinear output-set RMT (SOS-RMT for short). We show how SOS-RMT can be used for SOS-MPC under the known bounds for feasibility of MPC in the standard (i.e., non-CL) setting assuming, in addition to SOS-RMT, an anonymous PKI.",
      "url": "https://www.microsoft.com/en-us/research/publication/adaptive-security-erasures-and-network-assumptions-in-communication-local-mpc/"
    },
    {
      "title": "Soft Language Identification for Language-Agnostic Many-to-One End-to-End Speech Translation",
      "authors": [
        "Aswin Shanmugam Subramanian",
        "Jian Xue",
        "Jinyu Li",
        "Junkun Chen",
        "Peidong Wang"
      ],
      "research_areas": [
        "Human language technologies"
      ],
      "publication_date": "September 2024",
      "abstract": "Language-agnostic many-to-one end-to-end speech translation models can convert audio signals from different source languages into text in a target language. These models do not need source language identification, which improves user experience. In some cases, the input language can be given or estimated. Our goal is to use this additional language information while preserving the quality of the other languages. We accomplish this by introducing a simple and effective linear input network. The linear input network is initialized as an identity matrix, which ensures that the model can perform as well as, or better than, the original model. Experimental results show that the proposed method can successfully enhance the specified language, while keeping the language-agnostic ability of the many-to-one ST models.",
      "url": "https://www.microsoft.com/en-us/research/publication/soft-language-identification-for-language-agnostic-many-to-one-end-to-end-speech-translation/"
    },
    {
      "title": "WildFeedback: Aligning LLMs With In-situ User Interactions And Feedback",
      "authors": [
        "Jennifer Neville",
        "Longqi Yang",
        "Mengting Wan",
        "Pei Zhou",
        "Sujay Kumar Jauhar",
        "Taiwei Shi",
        "Xia Song",
        "Xiaofeng Xu",
        "Ying-Chun Lin",
        "Zexue He",
        "Zhuoer Wang"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "August 2024",
      "abstract": "As large language models (LLMs) continue to advance, aligning these models with human preferences has emerged as a critical challenge. Traditional alignment methods, relying on human or LLM annotated datasets, are limited by their resource-intensive nature, inherent subjectivity, and the risk of feedback loops that amplify model biases. To overcome these limitations, we introduce WildFeedback, a novel framework that leverages real-time, in-situ user interactions to create preference datasets that more accurately reflect authentic human values. WildFeedback operates through a three-step process: feedback signal identification, preference data construction, and user-guided evaluation. We applied this framework to a large corpus of user-LLM conversations, resulting in a rich preference dataset that reflects genuine user preferences. This dataset captures the nuances of user preferences by identifying and classifying feedback signals within natural conversations, thereby enabling the construction of more representative and context-sensitive alignment data. Our extensive experiments demonstrate that LLMs fine-tuned on WildFeedback exhibit significantly improved alignment with user preferences, as evidenced by both traditional benchmarks and our proposed user-guided evaluation. By incorporating real-time feedback from actual users, WildFeedback addresses the scalability, subjectivity, and bias challenges that plague existing approaches, marking a significant step toward developing LLMs that are more responsive to the diverse and evolving needs of their users. In summary, WildFeedback offers a robust, scalable solution for aligning LLMs with true human values, setting a new standard for the development and evaluation of user-centric language models.",
      "url": "https://www.microsoft.com/en-us/research/publication/wildfeedback-aligning-llms-with-in-situ-user-interactions-and-feedback/"
    },
    {
      "title": "DDS: DPU-optimized Disaggregated Storage [Extended Report]",
      "authors": [
        "Badrish Chandramouli",
        "Jason Hu",
        "Phil Bernstein",
        "Qizhen Zhang",
        "Yiming Zheng"
      ],
      "research_areas": [
        "Data platforms and analytics"
      ],
      "publication_date": "August 2024",
      "abstract": "This extended report presents DDS, a novel disaggregated storage architecture enabled by emerging networking hardware, namely DPUs (Data Processing Units). DPUs can optimize the latency and CPU consumption of disaggregated storage servers. However, utilizing DPUs for DBMSs requires careful design of the network and storage paths and the interface exposed to the DBMS. To fully benefit from DPUs, DDS heavily uses DMA, zero-copy, and userspace I/O to minimize overhead when improving throughput. It also introduces an offload engine that eliminates host CPUs by executing client requests directly on the DPU. Adopting DDS’ API requires minimal DBMS modification. Our experimental study and production system integration show promising results — DDS achieves higher disaggregated storage throughput with an order of magnitude lower latency, and saves up to tens of CPU cores per storage server.",
      "url": "https://www.microsoft.com/en-us/research/publication/dds-dpu-optimized-disaggregated-storage-extended-report/"
    },
    {
      "title": "Rethinking Node Representation Interpretation through Relation Coherence",
      "authors": [
        "Cassiano Becker",
        "Jennifer Neville",
        "Nabiha Asghar",
        "Purvanshi Metha",
        "Vipul Agarwal",
        "Ying-Chun Lin"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Search and information retrieval"
      ],
      "publication_date": "September 2024",
      "abstract": "Understanding node representations in graph-based models is crucial for uncovering biases, diagnosing errors, and building trust in model decisions. However, previous work on explainable AI for node representations has primarily emphasized explanations (reasons for model predictions) rather than interpretations (mapping representations to understandable concepts). Furthermore, the limited research that focuses on interpretation lacks validation, and thus the reliability of such methods is unclear. We address this gap by proposing a novel interpretation method-Node Coherence Rate for Representation Interpretation (NCI)-which quantifies how well different node relations are captured in node representations. We also propose a novel method (IME) to evaluate the accuracy of different interpretation methods. Our experimental results demonstrate that NCI reduces the error of the previous best approach by an average of 39%. We then apply NCI to derive insights about the node representations produced by several graph-based methods and assess their quality in unsupervised settings.",
      "url": "https://www.microsoft.com/en-us/research/publication/rethinking-node-representation-interpretation-through-relation-coherence/"
    },
    {
      "title": "Understanding the Weakness of Large Language Model Agents within a Complex Android Environment",
      "authors": [
        "Fan Yang",
        "Hui Xue",
        "Mingzhe Xing",
        "Qi Chen",
        "Rongkai Zhang",
        "Zhen Xiao"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "August 2024",
      "abstract": "Large language models (LLMs) have empowered intelligent agents to execute intricate tasks within domain-specific software such as browsers and games. However, when applied to general-purpose software systems like operating systems, LLM agents face three primary challenges. Firstly, the action space is vast and dynamic, posing difficulties for LLM agents to maintain an up-to-date understanding and deliver accurate responses. Secondly, real-world tasks often require inter-application cooperation}, demanding farsighted planning from LLM agents. Thirdly, agents need to identify optimal solutions aligning with user constraints, such as security concerns and preferences. These challenges motivate AndroidArena, an environment and benchmark designed to evaluate LLM agents on a modern operating system. To address high-cost of manpower, we design a scalable and semi-automated method to construct the benchmark. In the task evaluation, AndroidArena incorporates accurate and adaptive metrics to address the issue of non-unique solutions. Our findings reveal that even state-of-the-art LLM agents struggle in cross-APP scenarios and adhering to specific constraints. Additionally, we identify a lack of four key capabilities, i.e., understanding, reasoning, exploration, and reflection, as primary reasons for the failure of LLM agents. Furthermore, we provide empirical analysis on the failure of reflection, and improve the success rate by 27% with our proposed exploration strategy. This work is the first to present valuable insights in understanding fine-grained weakness of LLM agents, and offers a path forward for future research in this area. Environment, benchmark, and evaluation code for AndroidArena are released at https://github.com/AndroidArenaAgent/AndroidArena.",
      "url": "https://www.microsoft.com/en-us/research/publication/understanding-the-weakness-of-large-language-model-agents-within-a-complex-android-environment/"
    },
    {
      "title": "Scalable Differentiable Causal Discovery in the Presence of Latent Confounders with Skeleton Posterior",
      "authors": [
        "Dongmei Zhang",
        "Jiaru Zhang",
        "Pingchuan Ma",
        "Qiang Fu",
        "Rui Ding",
        "Shi Han",
        "Shuai Wang"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "August 2024",
      "abstract": "Differentiable causal discovery has made significant advancements in the learning of directed acyclic graphs. However, its application to real-world datasets remains restricted due to the ubiquity of latent confounders and the requirement to learn maximal ancestral graphs (MAGs). To date, existing differentiable MAG learning algorithms have been limited to small datasets and failed to scale to larger ones (e.g., with more than 50 variables).\nThe key insight in this paper is that the causal skeleton, which is the undirected version of the causal graph, has potential for improving accuracy and reducing the search space of the optimization procedure, thereby enhancing the performance of differentiable causal discovery. Therefore, we seek to address a two-fold challenge to harness the potential of the causal skeleton for differentiable causal discovery in the presence of latent confounders: (1) scalable and accurate estimation of skeleton and (2) universal integration of skeleton estimation with differentiable causal discovery.\nTo this end, we propose SPOT (Skeleton Posterior-guided OpTimization), a two-phase framework that harnesses skeleton posterior for differentiable causal discovery in the presence of latent confounders. On the contrary to a “point-estimation”, SPOT seeks to estimate the posterior distribution of skeletons given the dataset. It first formulates the posterior inference as an instance of amortized inference problem and concretizes it with a supervised causal learning (SCL)-enabled solution to estimate the skeleton posterior. To incorporate the skeleton posterior with differentiable causal discovery, SPOT then features a skeleton posterior-guided stochastic optimization procedure to guide the optimization of MAGs.\nExtensive experiments on various datasets show that SPOT substantially outperforms SOTA methods for MAG learning. SPOT also demonstrates its effectiveness in the accuracy of skeleton posterior estimation in comparison with non-parametric bootstrap-based, or more recently, variational inference-based methods. Finally, we observe that the adoption of skeleton posterior exhibits strong promise in various causal discovery tasks.",
      "url": "https://www.microsoft.com/en-us/research/publication/scalable-differentiable-causal-discovery-in-the-presence-of-latent-confounders-with-skeleton-posterior/"
    },
    {
      "title": "HyperNova: Recursive arguments for customizable constraint systems",
      "authors": [
        "Abhiram Kothapalli",
        "Srinath Setty"
      ],
      "research_areas": [
        "Security, privacy, and cryptography"
      ],
      "publication_date": "August 2024",
      "abstract": "We introduce HyperNova, a new recursive argument for proving incremental computations whose steps are expressed with CCS (Setty et al. ePrint 2023/552), a customizable constraint system that simultaneously generalizes Plonkish, R1CS, and AIR without overheads. HyperNova makes four contributions, each resolving a major problem in the area of recursive arguments.\nFirst, it provides a folding scheme for CCS where the prover’s cryptographic cost is a single multi-scalar multiplication (MSM) of size equal to the number of variables in the constraint system, which is optimal when using an MSM-based commitment scheme. The folding scheme can fold multiple instances at once, making it easier to build generalizations of IVC such as PCD. Second, when proving program executions on stateful machines (e.g., EVM, RISC-V), the cost of proving a step of a program is proportional only to the size of the circuit representing the instruction invoked by the program step (“a la carte” cost profile). Third, we show how to achieve zero-knowledge for “free” and without the need to employ zero-knowledge SNARKs: we use a folding scheme to “randomize” IVC proofs. This highlights a new application of folding schemes. Fourth, we show how to efficiently instantiate HyperNova over a cycle of elliptic curves. For this, we provide a general technique, which we refer to as CycleFold, that applies to all modern folding-scheme-based recursive arguments.",
      "url": "https://www.microsoft.com/en-us/research/publication/hypernova-recursive-arguments-for-customizable-constraint-systems/"
    },
    {
      "title": "Bring Metric Functions into Diffusion Models",
      "authors": [
        "Jianfeng Wang",
        "Jie An",
        "Jiebo Luo",
        "Lijuan Wang",
        "Linjie Li",
        "Zhengyuan Yang",
        "Zicheng Liu"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "August 2024",
      "abstract": "We introduce a Cascaded Diffusion Model (Cas-DM) that improves a Denoising Diffusion Probabilistic Model (DDPM) by effectively incorporating additional metric functions in training. Metric functions such as the LPIPS loss have been proven highly effective in consistency models derived from the score matching. However, for the diffusion counterparts, the methodology and efficacy of adding extra metric functions remain unclear. One major challenge is the mismatch between the noise predicted by a DDPM at each step and the desired clean image that the metric function works well on. To address this problem, we propose Cas-DM, a network architecture that cascades two network modules to effectively apply metric functions to the diffusion model training. The first module, similar to a standard DDPM, learns to predict the added noise and is unaffected by the metric function. The second cascaded module learns to predict the clean image, thereby facilitating the metric function computation. Experiment results show that the proposed diffusion model backbone enables the effective use of the LPIPS loss, improving the image quality (FID, sFID) of diffusion models on various established benchmarks.",
      "url": "https://www.microsoft.com/en-us/research/publication/bring-metric-functions-into-diffusion-models/"
    },
    {
      "title": "Compositional 3D-aware Video Generation with LLM Director",
      "authors": [
        "Anni Tang",
        "Hanxin Zhu",
        "Jiang Bian",
        "Junliang Guo",
        "Tianyu He",
        "Zhibo Chen"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "August 2024",
      "abstract": "Significant progress has been made in text-to-video generation through the use of powerful generative models and large-scale internet data. However, substantial challenges remain in precisely controlling individual concepts within the generated video, such as the motion and appearance of specific characters and the movement of viewpoints. In this work, we propose a novel paradigm that generates each concept in 3D representation separately and then composes them with priors from Large Language Models (LLM) and 2D diffusion models. Specifically, given an input textual prompt, our scheme consists of three stages: 1) We leverage LLM as the director to first decompose the complex query into several sub-prompts that indicate individual concepts within the video~(\\textit{e.g.}, scene, objects, motions), then we let LLM to invoke pre-trained expert models to obtain corresponding 3D representations of concepts. 2) To compose these representations, we prompt multi-modal LLM to produce coarse guidance on the scales and coordinates of trajectories for the objects. 3) To make the generated frames adhere to natural image distribution, we further leverage 2D diffusion priors and use Score Distillation Sampling to refine the composition. Extensive experiments demonstrate that our method can generate high-fidelity videos from text with diverse motion and flexible control over each concept. Project page: https://aka.ms/c3v (opens in new tab).",
      "url": "https://www.microsoft.com/en-us/research/publication/compositional-3d-aware-video-generation-with-llm-director/"
    },
    {
      "title": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone",
      "authors": [
        "Abhishek Goswami",
        "Adil Salim",
        "Ahmed Awadallah",
        "Allie Del Giorno",
        "Alon Benhaim",
        "Amin Saied",
        "Amit Bahree",
        "Ammar Ahmad Awan",
        "Anh Nguyen",
        "Arash Bakhtiari",
        "Arindam Mitra",
        "Barun Patra",
        "Brandon Norick",
        "Caio César Teodoro Mendes",
        "Can Xu",
        "Chen Liang",
        "Chengruidong Zhang",
        "Corby Rosset",
        "Cyril Zhang",
        "Dan Iter",
        "Daniel Perez-Becker",
        "Donghan Yu",
        "Dongwoo Kim",
        "Emman Haider",
        "Fan Yang",
        "Guanhua Wang",
        "Gustavo de Rosa",
        "Hany Hassan Awadalla",
        "Hardik Modi",
        "Harkirat Behl",
        "Heyang Qin",
        "Hiteshi Sharma",
        "James R. Lee",
        "Jamie Huynh",
        "Jiahang Xu",
        "Jianwen Zhang",
        "Johan Bjorck",
        "Junheng Hao",
        "Jyoti Aneja",
        "Lev Kurilenko",
        "Li Lyna Zhang",
        "Mahmoud Khademi",
        "Marah I Abdin",
        "Marko Radmilac",
        "Martin Cai",
        "Matthew Dixon",
        "Michael Santacroce",
        "Michael Wyatt",
        "Misha Bilenko",
        "Mojan Javaheripi",
        "Nguyen Bach",
        "Nikos Karampatziakis",
        "Ning Shang",
        "Olatunji Ruwase",
        "Olli Saarikivi",
        "Parul Chopra",
        "Philipp Witte",
        "Piero Kauffmann",
        "Piyush Madan",
        "Rachel Ward",
        "Reid Pryzant",
        "Ronen Eldan",
        "Russell J. Hewett",
        "Sam Ade Jacobs",
        "Sambudha Roy",
        "Shital Shah",
        "Sonali Yadav",
        "Suriya Gunasekar",
        "Sébastien Bubeck",
        "Thomas Portet",
        "Vishrav Chaudhary",
        "Weijian Xu",
        "Weishung Liu",
        "Weizhu Chen",
        "Xia Song",
        "Xihui (Eric) Lin",
        "Xin Jin",
        "Xin Wang",
        "Xiren Zhou",
        "Yi Zhang",
        "Yin Tat Lee",
        "Yuanzhi Li",
        "Yunan Zhang",
        "Zeqi Lin",
        "Ziyi Yang"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "August 2024",
      "abstract": "We introduce phi-3-mini, a 3.8 billion parameter language model trained on 3.3 trillion tokens, whose overall performance, as measured by both academic benchmarks and internal testing, rivals that of models such as Mixtral 8x7B and GPT-3.5 (e.g., phi-3-mini achieves 69% on MMLU and 8.38 on MT-bench), despite being small enough to be deployed on a phone. The innovation lies entirely in our dataset for training, a scaled-up version of the one used for phi-2, composed of heavily filtered web data and synthetic data. The model is also further aligned for robustness, safety, and chat format. We also provide some initial parameter-scaling results with a 7B and 14B models trained for 4.8T tokens, called phi-3-small and phi-3-medium, both significantly more capable than phi-3-mini (e.g., respectively 75% and 78% on MMLU, and 8.7 and 8.9 on MT-bench).",
      "url": "https://www.microsoft.com/en-us/research/publication/phi-3-technical-report-a-highly-capable-language-model-locally-on-your-phone/"
    },
    {
      "title": "Training Ultra Long Context Language Model with Fully Pipelined Distributed Transformer",
      "authors": [
        "A. Shafi",
        "Dhabaleswar K. Panda",
        "H. Subramoni",
        "Jinghan Yao",
        "Masahiro Tanaka",
        "Olatunji Ruwase",
        "Sam Ade Jacobs"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "August 2024",
      "abstract": "Large Language Models (LLMs) with long context capabilities are integral to complex tasks in natural language processing and computational biology, such as text generation and protein sequence analysis. However, training LLMs directly on extremely long contexts demands considerable GPU resources and increased memory, leading to higher costs and greater complexity. Alternative approaches that introduce long context capabilities via downstream finetuning or adaptations impose significant design limitations. In this paper, we propose Fully Pipelined Distributed Transformer (FPDT) for efficiently training long-context LLMs with extreme hardware efficiency. For GPT and Llama models, we achieve a 16x increase in sequence length that can be trained on the same hardware compared to current state-of-the-art solutions. With our dedicated sequence chunk pipeline design, we can now train 8B LLM with 2 million sequence length on only 4 GPUs, while also maintaining over 55% of MFU. Our proposed FPDT is agnostic to existing training techniques and is proven to work efficiently across different LLM models.",
      "url": "https://www.microsoft.com/en-us/research/publication/training-ultra-long-context-language-model-with-fully-pipelined-distributed-transformer/"
    },
    {
      "title": "Decoding the AI Pen: Techniques and Challenges in Detecting  AI-Generated Text",
      "authors": [
        "CJ Barberan",
        "Jia He",
        "Richard Anarfi",
        "Sara Abdali"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Human language technologies",
        "Social sciences"
      ],
      "publication_date": "August 2024",
      "abstract": "Large Language Models (LLMs) have revolutionized the field of Natural Language Generation (NLG) by demonstrating an impressive ability to generate human-like text. However, their widespread us\nage introduces challenges that necessitate thoughtful examination, ethical scrutiny, and responsible practices. In this study, we delve into these challenges, explore existing strategies for mitigating them, with a particular emphasis on identifying AI-generated text as the ultimate solution. Additionally, we assess the feasibility of detection from a theoretical perspective and propose novel research directions to address the current limitations in this domain.",
      "url": "https://www.microsoft.com/en-us/research/publication/decoding-the-ai-pen-techniques-and-challenges-in-detecting-ai-generated-text/"
    },
    {
      "title": "Extreme Meta-Classification for Large-Scale Zero-Shot Retrieval",
      "authors": [
        "Anirudh Buvanesh",
        "Bhawna Paliwal",
        "Deepak Saini",
        "Jian Jiao",
        "Kunal Dahiya",
        "Manik Varma",
        "Sachin Yadav",
        "Siddarth Asokan",
        "Yashoteja Prabhu"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Data platforms and analytics"
      ],
      "publication_date": "August 2024",
      "abstract": "We develop accurate and efficient solutions for large-scale retrieval tasks where novel (zero-shot) items can arrive continuously at a rapid pace. Conventional Siamese-style approaches embed both queries and items through a small encoder and retrieve the items lying closest to the query. While this approach allows efficient addition and retrieval of novel items, the small encoder lacks sufficient capacity for the necessary world knowledge in complex retrieval tasks. The extreme classification approaches have addressed this by learning a separate classifier for each item observed in the training set which significantly increases the representation capacity of the model. Such classifiers outperform Siamese approaches on observed items, but cannot be trained for novel items due to data and latency constraints. To bridge these gaps, this paper develops: (1) A new algorithmic framework, EMMETT, which efficiently synthesizes classifiers on-the-fly for novel items, by relying on the readily available classifiers for observed items; (2) A new algorithm, IRENE, which is a simple and effective instance of EMMETT that is specifically suited for large-scale deployments, and (3) A new theoretical framework for analyzing the generalization performance in large-scale zero-shot retrieval which guides our algorithm and training related design decisions. Comprehensive experiments are conducted on a wide range of retrieval tasks which demonstrate that IRENE improves the zero-shot retrieval accuracy by up to 15% points in Recall@10 when added on top of leading encoders. Additionally, on an online A/B test in a large-scale ad retrieval task in a major search engine, IRENE improved the ad click-through rate by 4.2%. Lastly, we validate our design choices through extensive ablative experiments. The source code for IRENE is available at https://aka.ms/irene (opens in new tab).",
      "url": "https://www.microsoft.com/en-us/research/publication/extreme-meta-classification-for-large-scale-zero-shot-retrieval/"
    },
    {
      "title": "On the Criticality of Integrity Protection in 5G Fronthaul Networks",
      "authors": [
        "Daehyeok Kim",
        "Jiarong Xing",
        "Michael K. Reiter",
        "Sophia Yoo",
        "Xenofon Foukas"
      ],
      "research_areas": [
        "Security, privacy, and cryptography",
        "Systems and networking"
      ],
      "publication_date": "August 2024",
      "abstract": "The modern 5G fronthaul, which connects the base stations to radio units in cellular networks, is designed to deliver microsecond-level performance guarantees using Ethernet-based protocols. Unfortunately, due to potential performance overheads, as well as misconceptions about the low risk and impact of possible attacks, integrity protection is not considered a mandatory feature in the 5G fronthaul standards. In this work, we show how vulnerabilities from the lack of protection can be exploited, making attacks easier and more powerful than ever. We present a novel class of powerful attacks and a set of traditional attacks, which can both be fully launched from software over open packet-based interfaces, to cause performance degradation or denial of service to users over large geographical regions. Our attacks do not require a physical radio presence or signal-based attack mechanisms, do not affect the network’s operation (e.g., not crashing the radios), and are highly severe (e.g., impacting multiple cells). We demonstrate the impact of our attacks in an end-to-end manner on a commercial-grade, multi-cell 5G testbed, showing that adversaries can degrade performance of connected users by more than 80%, completely block a selected subset of users from ever attaching to the cell, or even generate signaling storm attacks of more than 2500 signaling messages per minute, with just two compromised cells and four mobile users. We also present an analysis of countermeasures that meet the strict performance requirements of the fronthaul.",
      "url": "https://www.microsoft.com/en-us/research/publication/on-the-criticality-of-integrity-protection-in-5g-fronthaul-networks/"
    },
    {
      "title": "Personhood credentials: Artificial intelligence and the value of privacy-preserving tools to distinguish who is real online",
      "authors": [
        "Alan Z. Rozenshtein",
        "Andrew Critch",
        "Andrew Trask",
        "Brian Christian",
        "Catherine Brewer",
        "Cedric Whitney",
        "Claire R. Leibowicz",
        "Connor Spelliscy",
        "David Schnurr",
        "Divya Siddarth",
        "Eddy Lazzarin",
        "Eric Ho",
        "Evan Shapiro",
        "Heather Flanagan",
        "John Bailey",
        "Kim Hamilton Duffy",
        "Lacey Strahm",
        "Manu Sporny",
        "Nouran Soliman",
        "Renée DiResta",
        "Ronnie Falcon",
        "Sean McGregor",
        "Shrey Jain",
        "Srikanth Nadhamuni",
        "Steven Adler",
        "Tobin South",
        "Tom Zick",
        "Varya Srivastava",
        "Wayne Chang",
        "Wendy Seltzer",
        "Zoe Weinberg",
        "Zoë Hitzig"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Security, privacy, and cryptography"
      ],
      "publication_date": "August 2024",
      "abstract": "Anonymity is an important principle online. However, malicious actors have long used misleading identities to conduct fraud, spread disinformation, and carry out other deceptive schemes. With the advent of increasingly capable AI, bad actors can amplify the potential scale and effectiveness of their operations, intensifying the challenge of balancing anonymity and trustworthiness online. In this paper, we analyze the value of a new tool to address this challenge: “personhood credentials” (PHCs), digital credentials that empower users to demonstrate that they are real people — not AIs — to online services, without disclosing any personal information. Such credentials can be issued by a range of trusted institutions — governments or otherwise. A PHC system, according to our definition, could be local or global, and does not need to be biometrics-based. Two trends in AI contribute to the urgency of the challenge: AI’s increasing indistinguishability (i.e., lifelike content and avatars, agentic activity) from people online, and AI’s increasing scalability (i.e., cost-effectiveness, accessibility). Drawing on a long history of research into anonymous credentials and “proof-of-personhood” systems, personhood credentials give people a way to signal their trustworthiness on online platforms, and offer service providers new tools for reducing misuse by bad actors. In contrast, existing countermeasures to automated deception — such as CAPTCHAs — are inadequate against sophisticated AI, while stringent identity verification solutions are insufficiently private for many use-cases. After surveying the benefits of personhood credentials, we also examine deployment risks and design challenges. We conclude with actionable next steps for policymakers, technologists, and standards bodies to consider in consultation with the public.",
      "url": "https://www.microsoft.com/en-us/research/publication/personhood-credentials-artificial-intelligence-and-the-value-of-privacy-preserving-tools-to-distinguish-who-is-real-online/"
    },
    {
      "title": "Controllable Financial Market Generation with Diffusion Guided Meta Agent",
      "authors": [
        "Chang Xu",
        "Jiang Bian",
        "Weiqing Liu",
        "Wu-Jun Li",
        "Yang Liu",
        "Yu-Hao Huang"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Economics"
      ],
      "publication_date": "August 2024",
      "abstract": "Order flow modeling stands as the most fundamental and essential financial task, as orders embody the minimal unit within a financial market. However, current approaches often result in unsatisfactory fidelity in generating order flow, and their generation lacks controllability, thereby limiting their application scenario. In this paper, we advocate incorporating controllability into the market generation process, and propose a Diffusion Guided meta Agent(DiGA) model to address the problem. Specifically, we utilize a diffusion model to capture dynamics of market state represented by time-evolving distribution parameters about mid-price return rate and order arrival rate, and define a meta agent with financial economic priors to generate orders from the corresponding distributions. Extensive experimental results demonstrate that our method exhibits outstanding controllability and fidelity in generation. Furthermore, we validate DiGA’s effectiveness as generative environment for downstream financial applications.",
      "url": "https://www.microsoft.com/en-us/research/publication/controllable-financial-market-generation-with-diffusion-guided-meta-agent/"
    },
    {
      "title": "ElectionGuard: a Cryptographic Toolkit to Enable Verifiable Elections",
      "authors": [
        "D. Wallach",
        "Josh Benaloh",
        "Michael Naehrig",
        "Olivier Pereira"
      ],
      "research_areas": [
        "Security, privacy, and cryptography"
      ],
      "publication_date": "August 2024",
      "abstract": ",ElectionGuard is a flexible set of open-source tools that—when used with traditional election systems—can produce end-to-end verifiable elections whose integrity can be verified by observers, candidates, media, and even voters themselves. ElectionGuard has been integrated into a variety of systems and used in actual public U.S. elections in Wisconsin, California, Idaho, Utah, and Maryland as well as in caucus elections in the U.S. Congress. It has also been used for civic voting in the Paris suburb of Neuilly-sur-Seine and for an online election by a Switzerland/Denmark-based organization.\nThe principal innovation of ElectionGuard is the separation of the cryptographic tools from the core mechanics and user interfaces of voting systems. This separation allows the cryptography to be designed and built by security experts without having to re-invent and replace the existing infrastructure. Indeed, in its preferred deployment, ElectionGuard does not replace the existing vote counting infrastructure but instead runs alongside and produces its own independently-verifiable tallies. Although much of the cryptography in ElectionGuard is, by design, not novel, some significant innovations are introduced which greatly simplify the process of verification.\nThis paper describes the design of ElectionGuard, its innovations, and many of the learnings from its implementation and growing number of real-world deployments.",
      "url": "https://www.microsoft.com/en-us/research/publication/electionguard-a-cryptographic-toolkit-to-enable-verifiable-elections/"
    },
    {
      "title": "LLexus: an AI agent system for incident management",
      "authors": [
        "Alok Kumbhare",
        "Pedro Las-Casas",
        "Rodrigo Fonseca",
        "Sharad Agarwal"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Systems and networking"
      ],
      "publication_date": "August 2024",
      "abstract": "When operating a software service on a cloud, the complexity of keeping multiple distributed components responsive is a significant challenge for engineering teams. Engineers frequently rely on Troubleshooting Guides (TSGs) to navigate how to mitigate performance or outage incidents. However, the effectiveness of TSGs is often hindered by their length, implicit reliance on tribal knowledge, and the variable quality of their content. This paper introduces LLexus, an agent-based AI system to automate the execution of TSGs.\n\n\n",
      "url": "https://www.microsoft.com/en-us/research/publication/llexus-an-ai-agent-system-for-incident-management/"
    },
    {
      "title": "On cycles of pairing-friendly abelian varieties",
      "authors": [
        "Craig Costello",
        "Maria Corte-Real Santos",
        "Michael Naehrig"
      ],
      "research_areas": [
        "Security, privacy, and cryptography"
      ],
      "publication_date": "August 2024",
      "abstract": ". One of the most promising avenues for realising scalable proof systems relies on the existence of 2-cycles of pairing-friendly elliptic curves. Such a cycle consists of two elliptic curves E / F p and E ′ / F q that both have a low embedding degree and also satisfy q = # E ( F p ) and p = # E ′ ( F q ). These constraints turn out to be rather restrictive; in the decade that has passed since 2-cycles were first proposed for use in proof systems, no new constructions of 2-cycles have been found.\nIn this paper, we generalise the notion of cycles of pairing-friendly elliptic curves to study cycles of pairing-friendly abelian varieties , with a view towards realising more efficient pairing-based SNARKs. We show that considering abelian varieties of dimension larger than 1 un-locks a number of interesting possibilities for finding pairing-friendly cycles, and we give several new constructions that can be instantiated at any security level.",
      "url": "https://www.microsoft.com/en-us/research/publication/on-cycles-of-pairing-friendly-abelian-varieties/"
    },
    {
      "title": "Intelligent Router for LLM Workloads: Improving Performance Through Workload-Aware Scheduling",
      "authors": [
        "A. Parayil",
        "Ankur Mallick",
        "Anoop Kulkarni",
        "Chetan Bansal",
        "Esha Choukse",
        "Jue Zhang",
        "Kunal Jain",
        "Rujia Wang",
        "Saravan Rajmohan",
        "Steve Kofsky",
        "Victor Ruehle",
        "Xiaoting Qin",
        "Íñigo Goiri"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Systems and networking"
      ],
      "publication_date": "August 2024",
      "abstract": "Large Language Model (LLM) workloads have distinct prefill and decode phases with different compute and memory requirements which should ideally be accounted for when scheduling input queries across different LLM instances in a cluster. However existing scheduling algorithms treat LLM workloads as monolithic jobs without considering the distinct characteristics of the two phases in each workload. This leads to sub-optimal scheduling and increased response latency. In this work, we propose a heuristic-guided reinforcement learning-based intelligent router for data-driven and workload-aware scheduling. Our router leverages a trainable response-length predictor, and a novel formulation for estimating the impact of mixing different workloads to schedule queries across LLM instances and achieve over 11% lower end-to-end latency than existing approaches.",
      "url": "https://www.microsoft.com/en-us/research/publication/intelligent-router-for-llm-workloads-improving-performance-through-workload-aware-scheduling/"
    },
    {
      "title": "A Methodology for Using Large Language Models to Create User-Friendly Applications for Medicaid Redetermination and Other Social Services",
      "authors": [
        "Aneesh Chopra",
        "Bill Weeks",
        "Juan M. Lavista Ferres",
        "Mayana Pereira",
        "Sumanth Ratna"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Medical, health and genomics"
      ],
      "publication_date": "August 2024",
      "abstract": "Background\nFollowing the unwinding of Medicaid’s continuous enrollment provision, states must redetermine Medicaid eligibility, creating uncertainty about coverage [1 (opens in new tab)] and the widespread administrative removal of beneficiaries from rolls [2 (opens in new tab)].\nExisting research demonstrates that Large Language Models (LLMs) can automate clinical trial eligibility query extraction [3 (opens in new tab)], generation [4 (opens in new tab)], and classification [5 (opens in new tab)]. Given that Medicaid redetermination follows eligibility rules similar to those in clinical trials, we thought LLMs might help with Medicaid redetermination, as well.\nTherefore, using the State of Washington, South Carolina, and North Dakota as examples, we applied LLMs to extract Medicaid rules from publicly available documents and transform those rules into a web application that could allow users to determine whether they are eligible for Medicaid. This paper describes the methodology we used.",
      "url": "https://www.microsoft.com/en-us/research/publication/a-methodology-for-using-large-language-models-to-create-user-friendly-applications-for-medicaid-redetermination-and-other-social-services/"
    },
    {
      "title": "Mutual Reasoning Makes Smaller LLMs Stronger Problem-Solvers",
      "authors": [
        "Fan Yang",
        "Jiahang Xu",
        "Li Lyna Zhang",
        "Mao Yang",
        "Mingyuan Ma",
        "Zhenting Qi"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "August 2024",
      "abstract": "This paper introduces rStar, a self-play mutual reasoning approach that significantly improves reasoning capabilities of small language models (SLMs) without fine-tuning or superior models. rStar decouples reasoning into a self-play mutual generation-discrimination process. First, a target SLM augments the Monte Carlo Tree Search (MCTS) with a rich set of human-like reasoning actions to construct higher quality reasoning trajectories. Next, another SLM, with capabilities similar to the target SLM, acts as a discriminator to verify each trajectory generated by the target SLM. The mutually agreed reasoning trajectories are considered mutual consistent, thus are more likely to be correct. Extensive experiments across five SLMs demonstrate rStar can effectively solve diverse reasoning problems, including GSM8K, GSM-Hard, MATH, SVAMP, and StrategyQA. Remarkably, rStar boosts GSM8K accuracy from 12.51% to 63.91% for LLaMA2-7B, from 36.46% to 81.88% for Mistral-7B, from 74.53% to 91.13% for LLaMA3-8B-Instruct.",
      "url": "https://www.microsoft.com/en-us/research/publication/mutual-reasoning-makes-smaller-llms-stronger-problem-solvers/"
    },
    {
      "title": "Interpretable User Satisfaction Estimation for Conversational Systems with Large Language Models",
      "authors": [
        "Brent Hecht",
        "Deepak Gupta",
        "Georg Buscher",
        "Jack W. Stokes",
        "Jaime Teevan",
        "Jennifer Neville",
        "Longqi Yang",
        "Mengting Wan",
        "Reid Andersen",
        "Saurabh Tiwary",
        "Scott Counts",
        "Siddharth Suri",
        "Sujay Kumar Jauhar",
        "Tara Safavi",
        "Xia Song",
        "Xiaofeng Xu",
        "Ying-Chun Lin"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Search and information retrieval"
      ],
      "publication_date": "August 2024",
      "abstract": "Accurate and interpretable user satisfaction estimation (USE) is critical for understanding, evaluating, and continuously improving conversational systems. Users express their satisfaction or dissatisfaction with diverse conversational patterns in both general-purpose (ChatGPT and Bing Copilot) and task-oriented (customer service chatbot) conversational systems. Existing approaches based on featurized ML models or text embeddings fall short in extracting generalizable patterns and are hard to interpret. In this work, we show that LLMs can extract interpretable signals of user satisfaction from their natural language utterances more effectively than embedding-based approaches. Moreover, an LLM can be tailored for USE via an iterative prompting framework using supervision from labeled examples. The resulting method, Supervised Prompting for User satisfaction Rubrics (SPUR), not only has higher accuracy but is more interpretable as it scores user satisfaction via learned rubrics with a detailed breakdown.",
      "url": "https://www.microsoft.com/en-us/research/publication/interpretable-user-satisfaction-estimation-for-conversational-systems-with-large-language-models/"
    },
    {
      "title": "Does Reasoning Emerge? Examining the Probabilities of Causation in Large Language Models",
      "authors": [
        "Aditya Nori",
        "Javier González"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "August 2024",
      "abstract": "Recent advances in AI have been significantly driven by the capabilities of large language models (LLMs) to solve complex problems in ways that resemble human thinking. However, there is an ongoing debate about the extent to which LLMs are capable of actual reasoning. Central to this debate are two key probabilistic concepts that are essential for connecting causes to their effects: the probability of necessity (PN) and the probability of sufficiency (PS). This paper introduces a framework that is both theoretical and practical, aimed at assessing how effectively LLMs are able to replicate real-world reasoning mechanisms using these probabilistic measures. By viewing LLMs as abstract machines that process information through a natural language interface, we examine the conditions under which it is possible to compute suitable approximations of PN and PS. Our research marks an important step towards gaining a deeper understanding of when LLMs are capable of reasoning, as illustrated by a series of math examples.",
      "url": "https://www.microsoft.com/en-us/research/publication/does-reasoning-emerge-examining-the-probabilities-of-causation-in-large-language-models/"
    },
    {
      "title": "A Survey on Model MoErging: Recycling and Routing Among Specialized Experts for Collaborative Learning",
      "authors": [
        "Alessandro Sordoni",
        "Colin Raffel",
        "Haokun Liu",
        "Leshem Choshen",
        "Lucas Caccia",
        "Mohammed Muqeeth",
        "Mohit Bansal",
        "Prateek Yadav",
        "Tian-Xiang Chen"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "August 2024",
      "abstract": "The availability of performant pre-trained models has led to a proliferation of fine-tuned expert models that are specialized to a particular domain or task. Model MoErging methods aim to recycle expert models to create an aggregate system with improved performance or generalization. A key component of MoErging methods is the creation of a router that decides which expert model(s) to use for a particular input or application. The promise, effectiveness, and large design space of MoErging has spurred the development of many new methods over the past few years. This rapid pace of development has made it challenging to compare different MoErging methods, which are rarely compared to one another and are often validated in different experimental setups. To remedy such gaps, we present a comprehensive survey of MoErging methods that includes a novel taxonomy for cataloging key design choices and clarifying suitable applications for each method. Apart from surveying MoErging research, we inventory software tools and applications that make use of MoErging. We additionally discuss related fields of study such as model merging, multitask learning, and mixture-of-experts models. Taken as a whole, our survey provides a unified overview of existing MoErging methods and creates a solid foundation for future work in this burgeoning field.",
      "url": "https://www.microsoft.com/en-us/research/publication/a-survey-on-model-moerging-recycling-and-routing-among-specialized-experts-for-collaborative-learning/"
    },
    {
      "title": "Anatomizing Deep Learning Inference in Web Browsers",
      "authors": [
        "Aoyu Li",
        "Qipeng Wang",
        "Shiqi Jiang",
        "Ting Cao",
        "Xu Cao",
        "Xuanzhe Liu",
        "Yuanchun Li",
        "Yun Ma",
        "Zhenpeng Chen"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Systems and networking"
      ],
      "publication_date": "August 2024",
      "abstract": "Web applications have increasingly adopted Deep Learning (DL) through in-browser inference, wherein DL inference performs directly within Web browsers. The actual performance of in-browser inference and its impacts on the quality of experience (QoE) remain unexplored, and urgently require new QoE measurements beyond traditional ones, e.g., mainly focusing on page load time. To bridge this gap, we make the first comprehensive performance measurement of in-browser inference to date. Our approach proposes new metrics to measure in-browser inference: responsiveness, smoothness, and inference accuracy. Our extensive analysis involves 9 representative DL models across Web browsers of 50 popular PC devices and 20 mobile devices. The results reveal that in-browser inference exhibits a substantial latency gap, averaging 16.9 times slower on CPU and 4.9 times slower on GPU compared to native inference on PC devices. The gap on mobile CPU and mobile GPU is 15.8 times and 7.8 times, respectively. Furthermore, we identify contributing factors to such latency gap, including underutilized hardware instruction sets, inherent overhead in the runtime environment, resource contention within the browser, and inefficiencies in software libraries and GPU abstractions. Additionally, in-browser inference imposes significant memory demands, at times exceeding 334.6 times the size of the DL models themselves, partly attributable to suboptimal memory management. We also observe that in-browser inference leads to a significant 67.2% increase in the time it takes for GUI components to render within Web browsers, significantly affecting the overall user QoE of Web applications reliant on this technology.",
      "url": "https://www.microsoft.com/en-us/research/publication/anatomizing-deep-learning-inference-in-web-browsers/"
    },
    {
      "title": "DeepSeek-Prover-V1.5: Harnessing Proof Assistant Feedback for Reinforcement Learning and Monte-Carlo Tree Search",
      "authors": [
        "Bo Liu (Benjamin Liu)",
        "C. Ruan",
        "Dejian Yang",
        "Fuli Luo",
        "Haocheng Wang",
        "Huajian Xin",
        "Jun-Mei Song",
        "Liyue Zhang",
        "Qihao Zhu",
        "Qiushi Du",
        "W. Gao",
        "Wanjia Zhao",
        "Xuan Lu",
        "Z. F. Wu",
        "Z. Ren",
        "Zhibin Gou",
        "Zhihong Shao"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "August 2024",
      "abstract": "We introduce DeepSeek-Prover-V1.5, an open-source language model designed for theorem proving in Lean 4, which enhances DeepSeek-Prover-V1 by optimizing both training and inference processes. Pre-trained on DeepSeekMath-Base with specialization in formal mathematical languages, the model undergoes supervised fine-tuning using an enhanced formal theorem proving dataset derived from DeepSeek-Prover-V1. Further refinement is achieved through reinforcement learning from proof assistant feedback (RLPAF). Beyond the single-pass whole-proof generation approach of DeepSeek-Prover-V1, we propose RMaxTS, a variant of Monte-Carlo tree search that employs an intrinsic-reward-driven exploration strategy to generate diverse proof paths. DeepSeek-Prover-V1.5 demonstrates significant improvements over DeepSeek-Prover-V1, achieving new state-of-the-art results on the test set of the high school level miniF2F benchmark ($63.5\\%$) and the undergraduate level ProofNet benchmark ($25.3\\%$).",
      "url": "https://www.microsoft.com/en-us/research/publication/deepseek-prover-v1-5-harnessing-proof-assistant-feedback-for-reinforcement-learning-and-monte-carlo-tree-search/"
    },
    {
      "title": "Reef: Fast Succinct Non-Interactive Zero-Knowledge Regex Proofs",
      "authors": [
        "Eleftherios Ioannidis",
        "Elizabeth Margolin",
        "Jess Woods",
        "Sebastian Angel",
        "Srinath Setty"
      ],
      "research_areas": [
        "Security, privacy, and cryptography"
      ],
      "publication_date": "August 2024",
      "abstract": "This paper presents Reef, a system for generating publicly verifiable succinct non-interactive zero-knowledge proofs that a committed document matches or does not match a regular expression. We describe applications such as proving the strength of passwords, the provenance of email despite redactions, the validity of oblivious DNS queries, and the existence of mutations in DNA. Reef supports the Perl Compatible Regular Expression syntax, including wildcards, alternation, ranges, capture groups, Kleene star, negations, and lookarounds. Reef introduces a new type of automata, Skipping Alternating Finite Automata (SAFA), that skips irrelevant parts of a document when producing proofs without undermining soundness, and instantiates SAFA with a lookup argument. Our experimental evaluation confirms that Reef can generate proofs for documents with 32M characters; the proofs are small and cheap to verify (under a second).",
      "url": "https://www.microsoft.com/en-us/research/publication/reef-fast-succinct-non-interactive-zero-knowledge-regex-proofs/"
    },
    {
      "title": "How to Solve Contextual Goal-Oriented Problems with Offline Datasets?",
      "authors": [
        "Adith Swaminathan",
        "Aditya Modi",
        "Ching-An Cheng",
        "Jingling Li",
        "Ying Fan"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "August 2024",
      "abstract": "We present a novel method, Contextual goal-Oriented Data Augmentation (CODA), which uses commonly available unlabeled trajectories and context-goal pairs to solve Contextual Goal-Oriented (CGO) problems. By carefully constructing an action-augmented MDP that is equivalent to the original MDP, CODA creates a fully labeled transition dataset under training contexts without additional approximation error. We conduct a novel theoretical analysis to demonstrate CODA’s capability to solve CGO problems in the offline data setup. Empirical results also showcase the effectiveness of CODA, which outperforms other baseline methods across various context-goal relationships of CGO problem. This approach offers a promising direction to solving CGO problems using offline datasets.",
      "url": "https://www.microsoft.com/en-us/research/publication/how-to-solve-contextual-goal-oriented-problems-with-offline-datasets/"
    },
    {
      "title": "LUT Tensor Core: Lookup Table Enables Efficient Low-Bit LLM Inference Acceleration",
      "authors": [
        "Fan Yang",
        "Jianyu Wei",
        "Jilong Xue",
        "Lei Wang",
        "Lingxiao Ma",
        "Mao Yang",
        "Naifeng Jing",
        "Shijie Cao",
        "Ting Cao",
        "Zhichen Zeng",
        "Zhiwen Mo"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "August 2024",
      "abstract": "As large language model (LLM) inference demands ever-greater resources, there is a rapid growing trend of using low-bit weights to shrink memory usage and boost inference efficiency. However, these low-bit LLMs introduce the need for mixed-precision matrix multiplication (mpGEMM), which is a crucial yet under-explored operation that involves multiplying lower-precision weights with higher-precision activations. Unfortunately, current hardware does not natively support mpGEMM, resulting in indirect and inefficient dequantization-based implementations. To address the mpGEMM requirements in low-bit LLMs, we explored the lookup table (LUT)-based approach for mpGEMM. However, a conventional LUT implementation falls short of its potential. To fully harness the power of LUT-based mpGEMM, we introduce LUT Tensor Core, a software-hardware co-design optimized for low-bit LLM inference. Specifically, we introduce software-based operator fusion and table symmetrization techniques to optimize table precompute and table storage, respectively. Then, LUT Tensor Core proposes the hardware design featuring an elongated tiling shape design to enhance table reuse and a bit-serial design to support various precision combinations in mpGEMM. Moreover, we design an end-to-end compilation stack with new instructions for LUT-based mpGEMM, enabling efficient LLM compilation and optimizations. The evaluation on low-bit LLMs (e.g., BitNet, LLAMA) shows that LUT Tensor Core achieves more than a magnitude of improvements on both compute density and energy efficiency. The system is open-sourced at https://github.com/microsoft/T-MAC.",
      "url": "https://www.microsoft.com/en-us/research/publication/lut-tensor-core-lookup-table-enables-efficient-low-bit-llm-inference-acceleration/"
    },
    {
      "title": "Player-Driven Emergence in LLM-Driven Game Narrative",
      "authors": [
        "Bill Dolan",
        "Chris Brockett",
        "Claire Jin",
        "Gabriel DesGarennes",
        "Jessica Quaye",
        "Jorge J. G. Leandro",
        "Ken Lobb",
        "Michael Xu",
        "Nebojsa Jojic",
        "Portia Botchway",
        "Sudha Rao",
        "Weijia Xu",
        "Xiangyu Peng"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Human language technologies"
      ],
      "publication_date": "August 2024",
      "abstract": "We explore how interaction with large language models (LLMs) can give rise to emergent behaviors, empowering players to participate in the evolution of game narratives. Our testbed is a text-adventure game in which players attempt to solve a mystery under a fixed narrative premise, but can freely interact with non-player characters generated by GPT-4, a large language model. We recruit 28 gamers to play the game and use GPT-4 to automatically convert the game logs into a node-graph representing the narrative in the player’s gameplay. We find that through their interactions with the non-deterministic behavior of the LLM, players are able to discover interesting new emergent nodes that were not a part of the original narrative but have potential for being fun and engaging. Players that created the most emergent nodes tended to be those that often enjoy games that facilitate discovery, exploration and experimentation.",
      "url": "https://www.microsoft.com/en-us/research/publication/player-driven-emergence-in-llm-driven-game-narrative/"
    },
    {
      "title": "AutoGen Studio: A No-Code Developer Tool for Building and Debugging Multi-Agent Systems",
      "authors": [
        "Adam Fourney",
        "Chi Wang",
        "Erkang (Eric) Zhu",
        "Gagan Bansal",
        "Jingya Chen",
        "Saleema Amershi",
        "Suff Syed",
        "Victor Dibia"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Human-computer interaction"
      ],
      "publication_date": "August 2024",
      "abstract": "Multi-agent systems, where multiple agents (generative AI models + tools) collaborate, are emerging as an effective pattern for solving long-running, complex tasks in numerous domains. However, specifying their parameters (such as models, tools, and orchestration mechanisms etc,.) and debugging them remains challenging for most developers. To address this challenge, we present AUTOGEN STUDIO, a no-code developer tool for rapidly prototyping, debugging, and evaluating multi-agent workflows built upon the AUTOGEN framework. AUTOGEN STUDIO offers a web interface and a Python API for representing LLM-enabled agents using a declarative (JSON-based) specification. It provides an intuitive drag-and-drop UI for agent workflow specification, interactive evaluation and debugging of workflows, and a gallery of reusable agent components. We highlight four design principles for no-code multi-agent developer tools and contribute an open-source implementation on GitHub (opens in new tab).\n \nAUTOGEN STUDIO provides a backend api (web, python, cli) and a UI which implements a playground (shown), build and gallery view. In the playground view, users can run tasks in a session based on a workflow. Users can also observe actions taken by agents, reviewing agent messages and metrics based on a profiler module.\n ",
      "url": "https://www.microsoft.com/en-us/research/publication/autogen-studio-a-no-code-developer-tool-for-building-and-debugging-multi-agent-systems/"
    },
    {
      "title": "Experiences from Running a Voice-Based Education Platform for Children and Teachers with Visual Impairments",
      "authors": [
        "Amit Prakash",
        "Bhagyashree Biradar",
        "Devidatta Ghosh",
        "Dipanjan Chakraborty",
        "Kavyansh Chourasia",
        "Manikanteswar Punnam",
        "Nagarathna R Bhat",
        "Rajesh S Paali",
        "Rajeswari Pandurangan",
        "Roshni Poddar",
        "Sudipta Ray Chaudhuri",
        "Swami Manohar",
        "Tarini Naik",
        "Venkatesh Deshpande"
      ],
      "research_areas": [
        "Human-computer interaction",
        "Medical, health and genomics",
        "Technology for emerging markets"
      ],
      "publication_date": "August 2024",
      "abstract": "India has the world’s largest population of children with vision impairments (CVIs), a group facing significant educational barriers including inadequate accessible resources, limited school access, and a shortage of trained teachers. The authors from four different organizations including a non-profit dedicated to enhancing STEM education for CVIs through interactive, play-based learning and teacher training have collaborated to create SEEDS, Scalable Educational Experiences with Digital Scaffolding. This is a voice-based platform for educational experiences delivery, featuring an Android Teacher App, an Interactive Voice Response (IVR) system, and a comprehensive website for content management. The Teacher app facilitates educational experiences via group calls with students, while the IVR system offers students independent access to educational content using feature phones. To assess effectiveness and usability of SEEDS among students and teachers, we conducted a mixed-methods study involving 29 visually impaired students across five schools and five facilitators (3 sighted and 2 visually impaired) from Vision Empower Trust . Despite challenges like internet connectivity issues causing call drops, the study highlighted strong student engagement and a collaborative environment among facilitators. By open-sourcing the SEEDS codebase, we aim to improve the platform and increase its adoption among teachers in schools for CVIs across India and beyond.",
      "url": "https://www.microsoft.com/en-us/research/publication/experiences-from-running-a-voice-based-education-platform-for-children-and-teachers-with-visual-impairments/"
    },
    {
      "title": "Call Me When Necessary: LLMs can Efficiently and Faithfully Reason over Structured Environments",
      "authors": [
        "Chaoyun Zhang",
        "Dongmei Zhang",
        "Fangkai Yang",
        "Ling Chen",
        "Qi Zhang",
        "Qingwei Lin 林庆维",
        "Saravan Rajmohan",
        "Sitao Cheng",
        "Xiang Huang",
        "Xiaoting Qin",
        "Yong Xu",
        "Ziyuan Zhuang"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "August 2024",
      "abstract": "Large Language Models (LLMs) have shown potential in reasoning over structured environments, e.g., knowledge graphs and tables. Such tasks typically require multi-hop reasoning, i.e., match natural language utterance with instances in the environment. Previous works adopt LLMs to incrementally build a reasoning path, where LLMs either invoke tools or pick up items by step-by-step interacting with the environment. We propose Reasoning-Path-Editing (Readi), a novel framework where LLMs can efficiently and faithfully reason over structured environments. In Readi, LLMs initially generate a reasoning path given a query, and edit the path only when necessary. We instantiate the path on structured environments and provide feedback to edit the path if anything goes wrong. Experimental results on three KGQA and two TableQA datasets show the effectiveness of Readi, significantly surpassing previous LLM-based methods (by 9.1% Hit@1 on WebQSP, 12.4% on MQA-3H and 9.5% on WTQ), comparable with state-of-the-art fine-tuned methods (67% on CWQ and 74.7% on WebQSP) and substantially boosting the vanilla LLMs (by 14.9% on CWQ). Our code will be available on https://aka.ms/readi.",
      "url": "https://www.microsoft.com/en-us/research/publication/call-me-when-necessary-llms-can-efficiently-and-faithfully-reason-over-structured-environments/"
    },
    {
      "title": "LLMJudge: LLMs for Relevance Judgments",
      "authors": [
        "Bhaskar Mitra",
        "Charles L A Clarke",
        "Clemencia Siro",
        "Emine Yilmaz",
        "Guglielmo Faggioli",
        "Hossein A Rahmani",
        "Mohammad Aliannejadi",
        "Nick Craswell",
        "Paul Thomas"
      ],
      "research_areas": [
        "Search and information retrieval"
      ],
      "publication_date": "August 2024",
      "abstract": "The LLMJudge challenge is organized as part of the LLM4Eval workshop at SIGIR 2024. Test collections are essential for evaluating information retrieval (IR) systems. The evaluation and tuning of a search system is largely based on relevance labels, which indicate whether a document is useful for a specific search and user. However, collecting relevance judgments on a large scale is costly and resource-intensive. Consequently, typical experiments rely on third-party labelers who may not always produce accurate annotations. The LLMJudge challenge aims to explore an alternative approach by using LLMs to generate relevance judgments. Recent studies have shown that LLMs can generate reliable relevance judgments for search systems. However, it remains unclear which LLMs can match the accuracy of human labelers, which prompts are most effective, how fine-tuned open-source LLMs compare to closed-source LLMs like GPT-4, whether there are biases in synthetically generated data, and if data leakage affects the quality of generated labels. This challenge will investigate these questions, and the collected data will be released as a package to support automatic relevance judgment research in information retrieval and search.",
      "url": "https://www.microsoft.com/en-us/research/publication/llmjudge-llms-for-relevance-judgments/"
    },
    {
      "title": "CacheGen: Fast Context Loading for Language Model Applications via KV Cache Streaming",
      "authors": [
        "Ari Holtzman",
        "Ganesh Ananthanarayanan",
        "Hanchen Li",
        "Henry Hoffmann",
        "Jiayi Yao",
        "Junchen Jiang",
        "Kuntai Du",
        "Michael Maire",
        "Qizheng Zhang",
        "Shan Lu",
        "Siddhant Ray",
        "Yihua Cheng",
        "Yuhan Liu",
        "Yuyang Huang"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Systems and networking"
      ],
      "publication_date": "August 2024",
      "abstract": "As large language models (LLMs) take on complex tasks, their inputs are supplemented with longer contexts that incorporate domain knowledge or user-specific information. Yet using long contexts poses a challenge for responsive LLM systems, as nothing can be generated until the whole context is processed by the LLM. While the context-processing delay can be reduced by reusing the KV cache of a context across different inputs, fetching the KV cache, which contains large tensors, over the network can cause extra network delays. \nCacheGen is a fast context-loading module for LLM systems. First, CacheGen uses a custom tensor encoder, which embraces KV cache’s distributional properties, to encode a KV cache into more compact bitstream representations with negligible encoding/decoding overhead. This reduces the bandwidth demand to fetch the KV cache. Second, to maintain low context-loading delay and high generation quality, CacheGen adapts the streaming strategies to cope with changes in available bandwidth. When available bandwidth drops, CacheGen may raise the compression level for a part of the context or choose to recompute its KV cache on the fly.\nWe test CacheGen on four popular LLMs of various sizes and four datasets (662 contexts in total). Compared to the recent systems that reuse the KV cache, CacheGen reduces the KV cache size by 3.5-4.3× and the total delay in fetching and processing contexts by 3.2-3.7× while having negligible impacton the LLM response quality in accuracy or perplexity.",
      "url": "https://www.microsoft.com/en-us/research/publication/cachegen-fast-context-loading-for-language-model-applications-via-kv-cache-streaming/"
    },
    {
      "title": "Large Language Models Can Provide Accurate and Interpretable Incident Triage",
      "authors": [
        "Chetan Bansal",
        "Dongmei Zhang",
        "Minghua Ma",
        "Qingwei Lin 林庆维",
        "Saravan Rajmohan",
        "Ze Li",
        "Zexin Wang"
      ],
      "research_areas": [
        "Systems and networking"
      ],
      "publication_date": "August 2024",
      "abstract": "Large-scale cloud services frequently experience incidents that can have a significant impact on their stability. Incident triage is a critical process that assigns incidents to dedicated teams for resolution. However, traditional rule-based methods, commonly employed in various systems, have limitations due to a finite set of rules that necessitate continuous updates, leading to suboptimal performance. Current state-of-the-art approaches primarily rely on textual information, utilizing classifiers or unsupervised clustering. Unfortunately, the abundance of textual information, combined with considerable noise, presents a significant challenge to the accuracy of these methods. To tackle these challenges, we introduce COMET, an innovative system that utilizes an AutoExtractor to filter out non-critical logs and employs a Large Language Model (LLM) for keyword extraction. This approach effectively mitigates the complexity arising from disordered textual information. Additionally, COMET incorporates significant domain knowledge during keyword extraction, enhancing the LLM’s comprehension of the text. We deployed COMET on multiple cloud services within Microsoft, where it has operated continuously for over six months. Offline and online evaluations have shown that COMET achieves enhanced accuracy and reduced Time to Mitigation (TTM).",
      "url": "https://www.microsoft.com/en-us/research/publication/large-language-models-can-provide-accurate-and-interpretable-incident-triage/"
    },
    {
      "title": "Scaling Deep Learning Computation over the Inter-Core Connected Intelligence Processor with T10",
      "authors": [
        "Jian Huang",
        "Jilong Xue",
        "Lingxiao Ma",
        "Yiqi Liu",
        "Yu Cheng",
        "Yuqi Xue",
        "Ziming Miao"
      ],
      "research_areas": [
        "Programming languages and software engineering",
        "Systems and networking"
      ],
      "publication_date": "August 2024",
      "abstract": "As AI chips incorporate numerous parallelized cores to scale deep learning (DL) computing, inter-core communication is enabled recently by employing high-bandwidth and low-latency interconnect links on the chip (e.g., Graphcore IPU). It allows each core to directly access the fast scratchpad memory in other cores, which enables new parallel computing paradigms. However, without proper support for the scalable inter-core connections in current DL compilers, it is hard for developers to exploit the benefits of this new architecture. We present T10, the first DL compiler to exploit the inter-core communication bandwidth and distributed on-chip memory on AI chips. To formulate the computation and communication patterns of tensor operators in this new architecture, T10 introduces a distributed tensor abstraction rTensor. T10 maps a DNN model to execution plans with a generalized compute-shift pattern, by partitioning DNN computation into sub-operators and mapping them to cores, so that the cores can exchange data following predictable patterns. T10 makes globally optimized trade-offs between on-chip memory consumption and inter-core communication overhead, selects the best execution plan from a vast optimization space, and alleviates unnecessary inter-core communications. Our evaluation with a real inter-core connected AI chip, the Graphcore IPU, shows up to 3.3$\\times$ performance improvement, and scalability support for larger models, compared to state-of-the-art DL compilers and vendor libraries.",
      "url": "https://www.microsoft.com/en-us/research/publication/scaling-deep-learning-computation-over-the-inter-core-connected-intelligence-processor-with-t10/"
    },
    {
      "title": "MMIU: Multimodal Multi-image Understanding for Evaluating Large Vision-Language Models",
      "authors": [
        "Chuanhao Li",
        "Fanqing Meng",
        "Hao Tian",
        "Jiaqi Liao",
        "Jifeng Dai",
        "Jin Wang",
        "Kaipeng Zhang",
        "Ping Luo",
        "Quanfeng Lu",
        "Wenqi Shao",
        "Xizhou Zhu",
        "Yu Qiao"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Computer vision"
      ],
      "publication_date": "August 2024",
      "abstract": "The capability to process multiple images is crucial for Large Vision-Language Models (LVLMs) to develop a more thorough and nuanced understanding of a scene. Recent multi-image LVLMs have begun to address this need. However, their evaluation has not kept pace with their development. To fill this gap, we introduce the Multimodal Multi-image Understanding (MMIU) benchmark, a comprehensive evaluation suite designed to assess LVLMs across a wide range of multi-image tasks. MMIU encompasses 7 types of multi-image relationships, 52 tasks, 77K images, and 11K meticulously curated multiple-choice questions, making it the most extensive benchmark of its kind. Our evaluation of 24 popular LVLMs, including both open-source and proprietary models, reveals significant challenges in multi-image comprehension, particularly in tasks involving spatial understanding. Even the most advanced models, such as GPT-4o, achieve only 55.7% accuracy on MMIU. Through multi-faceted analytical experiments, we identify key performance gaps and limitations, providing valuable insights for future model and data improvements. We aim for MMIU to advance the frontier of LVLM research and development, moving us toward achieving sophisticated multimodal multi-image user interactions.",
      "url": "https://www.microsoft.com/en-us/research/publication/mmiu-multimodal-multi-image-understanding-for-evaluating-large-vision-language-models/"
    },
    {
      "title": "GENEVA: GENErating and Visualizing branching narratives using LLMs",
      "authors": [
        "Bill Dolan",
        "Chris Brockett",
        "Jorge J. G. Leandro (jorgeleandro)",
        "Michael Xu",
        "Nebojsa Jojic",
        "Sudha Rao",
        "Weijia Xu"
      ],
      "research_areas": [
        "Human language technologies"
      ],
      "publication_date": "August 2024",
      "abstract": "Dialogue-based Role Playing Games (RPGs) require powerful storytelling. The narratives of these may take years to write and typically involve a large creative team. In this work, we demonstrate the potential of large generative text models to assist this process. \\textbf{GENEVA}, a prototype tool, generates a rich narrative graph with branching and reconverging storylines that match a high-level narrative description and constraints provided by the designer. A large language model (LLM), GPT-4, is used to generate the branching narrative and to render it in a graph format in a two-step process. We illustrate the use of GENEVA in generating new branching narratives for four well-known stories under different contextual constraints. This tool has the potential to assist in game development, simulations, and other applications with game-like properties. Link to the GENEVA tool: Visualizing Generated Narratives (msr-emergence.com) (opens in new tab)\n \n",
      "url": "https://www.microsoft.com/en-us/research/publication/geneva-generating-and-visualizing-branching-narratives-using-llms/"
    },
    {
      "title": "GEMS: Generative Expert Metric System through Iterative Prompt Priming",
      "authors": [
        "Carmen Badea",
        "Christian Bird",
        "Denae Ford",
        "Nicole Forsgren",
        "Robert DeLine",
        "Ti-Chung Cheng",
        "Tom Zimmermann"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Programming languages and software engineering"
      ],
      "publication_date": "August 2024",
      "abstract": "Across domains, metrics and measurements are fundamental to identifying challenges, informing decisions, and resolving conflicts. Despite the abundance of data available in this information age, not only can it be challenging for a single expert to work across multi-disciplinary data}, but non-experts can also find it unintuitive to create effective measures or transform theories into context-specific metrics that are chosen appropriately. This technical report addresses this challenge by examining software communities within large software corporations, where different measures are used as proxies to locate counterparts within the organization to transfer tacit knowledge. We propose a prompt-engineering framework inspired by neural activities, demonstrating that generative models can extract and summarize theories and perform basic reasoning, thereby transforming concepts into context-aware metrics to support software communities given software repository data. While this research zoomed in on software communities, we believe the framework’s applicability extends across various fields, showcasing expert-theory-inspired metrics that aid in triaging complex challenges.",
      "url": "https://www.microsoft.com/en-us/research/publication/gems-generative-expert-metric-system-through-iterative-prompt-priming/"
    },
    {
      "title": "Automatic Bug Detection in LLM-Powered Text-Based Games Using LLMs",
      "authors": [
        "Bill Dolan",
        "Chris Brockett",
        "Claire Jin",
        "Jessica Quaye",
        "Portia Botchway",
        "Sudha Rao",
        "Xiangyu Peng"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Human language technologies"
      ],
      "publication_date": "August 2024",
      "abstract": "Advancements in large language models (LLMs) are revolutionizing interactive game design, enabling dynamic plotlines and interactions between players and non-player characters (NPCs). However, LLMs may exhibit flaws such as hallucinations, forgetfulness, or misinterpretations of prompts, causing logical inconsistencies and unexpected deviations from intended designs. Automated techniques for detecting such game bugs are still lacking. To address this, we propose a systematic LLM-based method for automatically identifying such bugs from player game logs, eliminating the need for collecting additional data such as post-play surveys. Applied to a text-based game DejaBoom!, our approach effectively identifies bugs inherent in LLM-powered interactive games, surpassing unstructured LLM-powered bug-catching methods and filling the gap in automated detection of logical and design flaws.",
      "url": "https://www.microsoft.com/en-us/research/publication/automatic-bug-detection-in-llm-powered-text-based-games-using-llms/"
    },
    {
      "title": "Rethinking Machine Learning Collective Communication as a Multi Commodity Flow problem",
      "authors": [
        "Behnaz Arzani",
        "Liangyu Zhao",
        "Luke Marshall",
        "Miguel Castro",
        "Siva Kesava Reddy Kakarla",
        "Srikanth Kandula",
        "Vincent Liu",
        "Xuting Liu"
      ],
      "research_areas": [
        "Systems and networking"
      ],
      "publication_date": "August 2024",
      "abstract": "Cloud operators utilize collective communication optimizers to enhance the efficiency of the single-tenant, centrally managed training clusters they manage. However, current optimizers struggle to scale for such use cases and often compromise solution quality for scalability. Our solution, TE-CCL, adopts a traffic-engineering-based approach to collective communication. Compared to a state-of-the-art optimizer, TACCL, TE-CCL produced schedules with 2× better performance on topologies TACCL supports (and its solver took a similar amount of time as TACCL’s heuristic-based approach). TECCL additionally scales to larger topologies than TACCL. On our GPU testbed, TE-CCL outperformed TACCL by 2.14× and RCCL by 3.18× in terms of algorithm bandwidth.",
      "url": "https://www.microsoft.com/en-us/research/publication/rethinking-machine-learning-collective-communication-as-a-multi-commodity-flow-problem/"
    },
    {
      "title": "Securely Training Decision Trees Efficiently",
      "authors": [
        "Divya Gupta",
        "Divyanshu Bhardwaj",
        "Nishanth Chandran",
        "Sandhya Saravanan"
      ],
      "research_areas": [
        "Security, privacy, and cryptography"
      ],
      "publication_date": "August 2024",
      "abstract": "Decision trees are an important class of supervised learning algorithms. When multiple entities contribute data to train a decision tree (e.g. for fraud detection in the financial sector), data privacy concerns necessitate the use of a privacy-enhancing technology such as secure multi-party computation (MPC) in order to secure the underlying training data. Prior state-of-the-art (Hamada et al.) construct an MPC protocol for decision tree training with a communication of O (ℎ𝑚𝑁 log 𝑁 ), when building a decision tree of height ℎ for a training dataset of 𝑁 samples, each having 𝑚 attributes. \nIn this work, we significantly reduce the communication complexity of secure decision tree training. We construct a protocol with communication complexity O (𝑚𝑁 log 𝑁 + ℎ𝑚𝑁 + ℎ𝑁 log 𝑁 ), thereby achieving an improvement of ≈ min(ℎ, 𝑚, log 𝑁 ) over Hamada et al. At the core of our technique is an improved protocol to regroup sorted private elements further into additional groups (according to a flag vector) while maintaining their relative ordering. We implement our protocol in the MP-SPDZ framework and show that it requires 10× lesser communication and is 9× faster than Hamada et al.",
      "url": "https://www.microsoft.com/en-us/research/publication/securely-training-decision-trees-efficiently/"
    },
    {
      "title": "“I Want It That Way”: Enabling Interactive Decision Support Using Large Language Models and Constraint Programming",
      "authors": [
        "Bahar Sarrafzadeh",
        "Connor Lawless",
        "Cristina St. Hill",
        "Jakob Schoeffer",
        "Jina Suh",
        "Kael Rowan",
        "Lindy Le",
        "Shilad Sen"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Human-computer interaction"
      ],
      "publication_date": "August 2024",
      "abstract": "A critical factor in the success of many decision support systems is the accurate modeling of user preferences. Psychology research has demonstrated that users often develop their preferences during the elicitation process, highlighting the pivotal role of system-user interaction in developing personalized systems. This paper introduces a novel approach, combining Large Language Models (LLMs) with Constraint Programming to facilitate interactive decision support. We study this hybrid framework through the lens of meeting scheduling, a time-consuming daily activity faced by a multitude of information workers. We conduct three studies to evaluate the novel framework, including a diary study to characterize contextual scheduling preferences, a quantitative evaluation of the system’s performance, and a user study to elicit insights with a technology probe that encapsulates our framework. Our work highlights the potential for a hybrid LLM and optimization approach for iterative preference elicitation, and suggests design considerations for building systems that support humansystem collaborative decision-making processes.",
      "url": "https://www.microsoft.com/en-us/research/publication/i-want-it-that-way-enabling-interactive-decision-support-using-large-language-models-and-constraint-programming/"
    },
    {
      "title": "New frontiers in AI for biodiversity research and conservation with multimodal language models",
      "authors": [
        "Amrita Gupta",
        "Andres Hernandez Celis",
        "Chunyuan Li",
        "Jason Holmberg",
        "Juan M. Lavista Ferres",
        "Kaitlyn Gaynor",
        "Md Nasir",
        "Meredith Palmer",
        "Rahul Dodhia",
        "Sara Beery",
        "Wanhua Li",
        "Yuanhan Zhang",
        "Zalan Fabian",
        "Zhongqi Miao",
        "Ziwei Liu"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Ecology and environment"
      ],
      "publication_date": "August 2024",
      "abstract": "The integration of Artificial Intelligence (AI) into biodiversity research and conservation is growing rapidly, demonstrating great potential in reducing the intensive human labor required for data preprocessing, thereby, facilitating larger data collections that offer ecological insights at unprecedented scales. However, most of these AI applications for biodiversity are still in the early stages of development, hindered by challenges inherent in real-world datasets and the limited accessibility of these technologies to practitioners without extensive programming knowledge. The recent advent of multimodal language models, which can process and generate multiple data modalities, has significantly expanded the realm of possible AI applications in biodiversity research. These models have demonstrated the ability to classify species and recognize more complex concepts, such as animal postures and orientations, without prior exposure during training. Multimodal language models can also provide explanations for their predictions and interact with humans in natural language, thereby making them more transparent, intuitive, and accessible to non-specialists. Despite these advancements, the use of multimodal language models for biodiversity still needs to overcome unique barriers to application, including high computational and financial demands, reliance on prompt engineering for consistent model performance on large datasets, and insufficient open-source sharing of state-of-the-art methods. This paper explores the transformative potential of multimodal language models for biodiversity research, compared with traditional machine learning methods, and discusses several potential applications in biodiversity research. We also discuss challenges to implementing these models in real-world conservation scenarios and propose directions for future research to overcome these hurdles. Our goal is to encourage robust discussions and research into the integration of multimodal language models to advance AI for biodiversity research and conservation.",
      "url": "https://www.microsoft.com/en-us/research/publication/new-frontiers-in-ai-for-biodiversity-research-and-conservation-with-multimodal-language-models/"
    },
    {
      "title": "Natural Language Decomposition and Interpretation of Complex Utterances",
      "authors": [
        "Ben Van Durme",
        "Eran Levy",
        "Hao Fang",
        "Harsh Jhamtani",
        "Jacob Andreas",
        "Patrick Xia"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Human language technologies"
      ],
      "publication_date": "August 2024",
      "abstract": "Natural language interfaces often require supervised data to translate user requests into programs, database queries, or other structured intent representations. During data collection, it can be difficult to anticipate and formalize the full range of user needs — for example, in a system designed to handle simple requests (like find my meetings tomorrow or move my meeting with my manager to noon), users may also express more elaborate requests (like swap all my calls on Monday and Tuesday). We introduce an approach for equipping a simple language-to-code model to handle complex utterances via a process of hierarchical natural language decomposition. Our approach uses a pre-trained language model to decompose a complex utterance into a sequence of smaller natural language steps, then interprets each step using the language-to-code model. To test our approach, we collect and release DeCU — a new NL-to-program benchmark to evaluate Decomposition of Complex Utterances. Experiments show that the proposed approach enables the interpretation of complex utterances with almost no complex training data, while outperforming standard few-shot prompting approaches.",
      "url": "https://www.microsoft.com/en-us/research/publication/natural-language-decomposition-and-interpretation-of-complex-utterances/"
    },
    {
      "title": "Closed-Form Bounds for DP-SGD against Record-level Inference",
      "authors": [
        "Andrew Paverd",
        "Boris Köpf",
        "Giovanni Cherubin",
        "Lukas Wutschitz",
        "Santiago Zanella-Béguelin",
        "Shruti Tople"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Security, privacy, and cryptography"
      ],
      "publication_date": "August 2024",
      "abstract": "Machine learning models trained with differentially-private (DP) algorithms such as DP-SGD enjoy resilience against a wide range of privacy attacks. Although it is possible to derive bounds for some attacks based solely on an (ε,δ)-DP guarantee, meaningful bounds require a small enough privacy budget (i.e., injecting a large amount of noise), which results in a large loss in utility. This paper presents a new approach to evaluate the privacy of machine learning models against specific record-level threats, such as membership and attribute inference, without the indirection through DP. We focus on the popular DP-SGD algorithm, and derive simple closed-form bounds. Our proofs model DP-SGD as an information theoretic channel whose inputs are the secrets that an attacker wants to infer (e.g., membership of a data record) and whose outputs are the intermediate model parameters produced by iterative optimization. We obtain bounds for membership inference that match state-of-the-art techniques, whilst being orders of magnitude faster to compute. Additionally, we present a novel data-dependent bound against attribute inference. Our results provide a direct, interpretable, and practical way to evaluate the privacy of trained models against specific inference threats without sacrificing utility.",
      "url": "https://www.microsoft.com/en-us/research/publication/closed-form-bounds-for-dp-sgd-against-record-level-inference/"
    },
    {
      "title": "Uncovering Milestone Papers: A Network Diffusion and Game Theory Approach",
      "authors": [
        "Hao Liao",
        "Juyang Cao",
        "Manuel Sebastian Mariani",
        "Mingyang Zhou",
        "Wei Chen",
        "Wei Zhang",
        "Zhen-Zhen Wang"
      ],
      "research_areas": [
        "Algorithms",
        "Data platforms and analytics"
      ],
      "publication_date": "August 2024",
      "abstract": "Methods to rank documents in large-scale citation data are increasingly assessed in terms of their ability to identify small sets of expert-selected papers. Here, we propose an algorithm for the accurate identification of milestone papers from citation networks. The algorithm combines an influence propagation process with game theory concepts. It outperforms state-of-the-art metrics in the identification of milestone papers in aggregate citation network data, while potentially mitigating the ranking’s temporal bias compared with metrics that have similar milestone identification performance. The proposed method sheds light on the interplay between ranking accuracy and temporal bias.",
      "url": "https://www.microsoft.com/en-us/research/publication/uncovering-milestone-papers-a-network-diffusion-and-game-theory-approach/"
    },
    {
      "title": "Evaluating LLMs’ Mathematical Reasoning in Financial Document Question Answering",
      "authors": [
        "Dan Roth",
        "Manuj Malik",
        "Pragya Srivastava",
        "Tanuja Ganu",
        "Vivek Gupta"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "August 2024",
      "abstract": "Large Language Models (LLMs), excel in natural language understanding, but their capability for complex mathematical reasoning with an amalgamation of structured tables and unstructured text is uncertain. This study explores LLMs’ mathematical reasoning on four financial tabular question-answering datasets: TATQA, FinQA, ConvFinQA, and Multihiertt. Through extensive experiments with various models and prompting techniques, we assess how LLMs adapt to complex tables and mathematical tasks. We focus on sensitivity to table complexity and performance variations with an increasing number of arithmetic reasoning steps. The results provide insights into LLMs’ capabilities and limitations in handling complex mathematical scenarios for semi-structured tables. Ultimately, we introduce a novel prompting technique tailored to semi-structured documents, matching or outperforming other baselines in performance while providing a nuanced understanding of LLMs abilities for such a task.",
      "url": "https://www.microsoft.com/en-us/research/publication/evaluating-llms-mathematical-reasoning-in-financial-document-question-answering/"
    },
    {
      "title": "m3: Accurate Flow-Level Performance Estimation using Machine Learning",
      "authors": [
        "Arash Nasr-Esfahany",
        "Chenning Li",
        "Kevin Zhao",
        "Kimia Noorbakhsh",
        "Mohammad Alizadeh",
        "Prateesh Goyal",
        "Thomas Anderson"
      ],
      "research_areas": [
        "Systems and networking"
      ],
      "publication_date": "August 2024",
      "abstract": "Data center network operators often need accurate estimates of aggregate network performance, such as the frequency of poor tail latency events, to guide network configuration — when and where to add capacity as a function of increased load, which network congestion control algorithm to use and how best to tune its parameters, and so forth. Unfortunately, existing methods for estimating aggregate network statistics are either fast and systematically inaccurate, or are detailed but too slow to be practical at data center scale.\nIn this paper, we develop and evaluate a scale-free, fast, and accurate model for estimating data center network tail latency performance given a workload, topology, and network configuration. First, we show that path-level simulations — simulations of traffic that intersects a given path — produce almost the same aggregate statistics as full network-wide packet-level simulations. We use a simple and fast flow-level fluid simulation in a novel way to capture and summarize essential elements of the path workload, including the effect of cross-traffic on flows on that path. We use this inaccurate simulation as input to a simple machine-learning model to predict path-level behavior, and run it on a sample of paths to produce accurate network-wide estimates. Our model generalizes over the choice of congestion control (CC) protocol, CC protocol parameters, and routing. Relative to Parsimon, a state of the art system for rapidly estimating aggregate network tail latency, our approach is significantly faster, more accurate, and more robust.",
      "url": "https://www.microsoft.com/en-us/research/publication/m3-accurate-flow-level-performance-estimation-using-machine-learning/"
    },
    {
      "title": "DEX: Scalable Range Indexing on Disaggregated Memory",
      "authors": [
        "Baotong Lu",
        "Chieh-Jan Mike Liang",
        "Eric Lo",
        "Kaisong Huang",
        "Tianzheng Wang"
      ],
      "research_areas": [
        "Systems and networking"
      ],
      "publication_date": "August 2024",
      "abstract": "Memory disaggregation can potentially allow memory-optimized range indexes such as B+-trees to scale beyond one machine while attaining high hardware utilization and low cost. Designing scalable indexes on disaggregated memory, however, is challenging due to rudimentary caching, unprincipled offloading and excessive inconsistency among servers.\nThis paper proposes DEX, a new scalable B+-tree for memory disaggregation. DEX includes a set of techniques to reduce remote accesses, including logical partitioning, lightweight caching and cost-aware offloading. Our evaluation shows that DEX can outperform the state-of-the-art by 1.7–56.3X, and the advantage remains under various setups, such as cache size and skewness.",
      "url": "https://www.microsoft.com/en-us/research/publication/dex-scalable-range-indexing-on-disaggregated-memory/"
    },
    {
      "title": "Collaborative Quest Completion with LLM-driven Non-Player Characters in Minecraft",
      "authors": [
        "Bill Dolan",
        "Chris Brockett",
        "Gabriel DesGarennes",
        "Jorge J. G. Leandro",
        "Ken Lobb",
        "Michael Xu",
        "Sudha Rao",
        "Weijia Xu"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Human language technologies"
      ],
      "publication_date": "August 2024",
      "abstract": "The use of generative AI in video game development is on the rise, and as the conversational and other capabilities of large language models continue to improve, we expect LLM-driven non-player characters (NPCs) to become widely deployed. In this paper, we seek to understand how human players collaborate with LLM-driven NPCs to accomplish in-game goals. We design a minigame within Minecraft where a player works with two GPT4-driven NPCs to complete a quest. We perform a user study in which 28 Minecraft players play this minigame and share their feedback. On analyzing the game logs and recordings, we find that several patterns of collaborative behavior emerge from the NPCs and the human players. We also report on the current limitations of language-only models that do not have rich game-state or visual understanding. We believe that this preliminary study and analysis will inform future game developers on how to better exploit these rapidly improving generative AI models for collaborative roles in games.",
      "url": "https://www.microsoft.com/en-us/research/publication/collaborative-quest-completion-with-llm-driven-non-player-characters-in-minecraft/"
    },
    {
      "title": "Tabularis Revilio: Converting Text to Tables",
      "authors": [
        "Gust Verbruggen",
        "Mukul Singh",
        "Sumit Gulwani",
        "Vu Le"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Data platforms and analytics",
        "Programming languages and software engineering"
      ],
      "publication_date": "August 2024",
      "abstract": "Copying tables from documents and applications without proper tabular support, like PDF documents, web pages or images, surprisingly remains a challenge. In this paper, we present Revilio, a novel neurosymbolic system for reconstructing tables when their column boundaries have been lost. Revilio addresses this task by detecting headers, generating an initial table sketch using a large language model, and using that sketch as a guiding representation during an enumerate-and-test strategy that evaluates syntactic and semantic table structures. We evaluate Revilio on a diverse set of datasets, demonstrating significant improvements over existing table parsing methods. Revilio outperforms traditional techniques in both accuracy and scalability, handling large tables with over 100,000 rows. Our experiments find an increase in reconstruction accuracy by 5.8–11.3% over both neural and symbolic baseline systems.",
      "url": "https://www.microsoft.com/en-us/research/publication/tabularis-revilio-converting-text-to-tables/"
    },
    {
      "title": "Everything of Thoughts: Defying the Law of Penrose Triangle for Thought Generation",
      "authors": [
        "Chaoyun Zhang",
        "Dongmei Zhang",
        "Lu Wang",
        "Minghua Ma",
        "Qingwei Lin 林庆维",
        "Ruomeng Ding",
        "Saravan Rajmohan",
        "Si Qin",
        "Wei Zhang",
        "Yong Xu"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "August 2024",
      "abstract": "Recent advancements in Large Language Models (LLMs) have revolutionized decision-making by breaking down complex problems into more manageable language sequences referred to as ”thoughts”. An effective thought design should consider three key perspectives: performance, efficiency, and flexibility. However, existing thought can at most exhibit two of these attributes. To address these limitations, we introduce a novel thought prompting approach called ”Everything of Thoughts”(XoT) to defy the law of” Penrose triangle of existing thought paradigms. XoT leverages pretrained reinforcement learning and Monte Carlo Tree Search (MCTS) to incorporate external domain knowledge into thoughts, thereby enhancing LLMs’ capabilities and enabling them to generalize to unseen problems efficiently. Through the utilization of the MCTS-LLM collaborative thought revision framework, this approach autonomously produces high-quality comprehensive cognitive mappings with minimal LLM interactions. Additionally, XoT empowers LLMs to engage in unconstrained thinking, allowing for flexible cognitive mappings for problems with multiple solutions. We evaluate XoT on several challenging multi-solution problem-solving tasks, including Game of 24, 8-Puzzle, and Pocket Cube. Our results demonstrate that XoT significantly outperforms existing approaches. Notably, XoT can yield multiple solutions with just one LLM call, showcasing its remarkable proficiency in addressing complex problems across diverse domains.",
      "url": "https://www.microsoft.com/en-us/research/publication/everything-of-thoughts-defying-the-law-of-penrose-triangle-for-thought-generation-2/"
    },
    {
      "title": "LordNet: An efficient neural network for learning to solve parametric partial differential equations without simulated data",
      "authors": [
        "Jia Zhang",
        "Jiang Bian",
        "Mao Yang",
        "Tie-Yan Liu",
        "Wenlei Shi",
        "Xiaotian Gao",
        "Xinquan Huang",
        "Xinran wei"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "August 2024",
      "abstract": "Neural operators, as a powerful approximation to the non-linear operators between infinite-dimensional function spaces, have proved to be promising in accelerating the solution of partial differential equations (PDE). However, it requires a large amount of simulated data, which can be costly to collect. This can be avoided by learning physics from the physics-constrained loss, which we refer to it as mean squared residual (MSR) loss constructed by the discretized PDE. We investigate the physical information in the MSR loss, which we called long-range entanglements, and identify the challenge that the neural network requires the capacity to model the long-range entanglements in the spatial domain of the PDE, whose patterns vary in different PDEs. To tackle the challenge, we propose LordNet, a tunable and efficient neural network for modeling various entanglements. Inspired by the traditional solvers, LordNet models the long-range entanglements with a series of matrix multiplications, which can be seen as the low-rank approximation to the general fully-connected layers and extracts the dominant pattern with reduced computational cost. The experiments on solving Poisson’s equation and (2D and 3D) Navier-Stokes equation demonstrate that the long-range entanglements from the MSR loss can be well modeled by the LordNet, yielding better accuracy and generalization ability than other neural networks. The results show that the Lordnet can be 40× faster than traditional PDE solvers. In addition, LordNet outperforms other modern neural network architectures in accuracy and efficiency with the smallest parameter size.",
      "url": "https://www.microsoft.com/en-us/research/publication/lordnet-an-efficient-neural-network-for-learning-to-solve-parametric-partial-differential-equations-without-simulated-data/"
    },
    {
      "title": "Causal Reasoning and Large Language Models: Opening a New Frontier for Causality",
      "authors": [
        "Amit Sharma",
        "Chenhao Tan",
        "Emre Kiciman",
        "Robert Osazuwa Ness"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "August 2024",
      "abstract": "The causal capabilities of large language models (LLMs) is a matter of significant debate, with critical implications for the use of LLMs in societally impactful domains such as medicine, science, law, and policy. We further our understanding of LLMs and their causal implications, considering the distinctions between different types of causal reasoning tasks, as well as the entangled threats of construct and measurement validity. LLM-based methods establish new state-of-the-art accuracies on multiple causal benchmarks. Algorithms based on GPT-3.5 and 4 outperform existing algorithms on a pairwise causal discovery task (97%, 13 points gain), counterfactual reasoning task (92%, 20 points gain), and actual causality (86% accuracy in determining necessary and sufficient causes in vignettes). At the same time, LLMs exhibit unpredictable failure modes and we provide some techniques to interpret their robustness.\nCrucially, LLMs perform these causal tasks while relying on sources of knowledge and methods distinct from and complementary to non-LLM based approaches. Specifically, LLMs bring capabilities so far understood to be restricted to humans, such as using collected knowledge to generate causal graphs or identifying background causal context from natural language. We envision LLMs to be used alongside existing causal methods, as a proxy for human domain knowledge and to reduce human effort in setting up a causal analysis, one of the biggest impediments to the widespread adoption of causal methods. We also see existing causal methods as promising tools for LLMs to formalize, validate, and communicate their reasoning especially in high-stakes scenarios.\nIn capturing common sense and domain knowledge about causal mechanisms and supporting translation between natural language and formal methods, LLMs open new frontiers for advancing the research, practice, and adoption of causality.",
      "url": "https://www.microsoft.com/en-us/research/publication/causal-reasoning-and-large-language-models-opening-a-new-frontier-for-causality/"
    },
    {
      "title": "AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation",
      "authors": [
        "Ahmed Awadallah",
        "Beibin Li",
        "Chi Wang",
        "Doug Burger",
        "Erkang (Eric) Zhu",
        "Gagan Bansal",
        "Jieyu Zhang",
        "Li Jiang",
        "Qingyun Wu",
        "Ryen W. White",
        "Shaokun Zhang",
        "Xiaoyun Zhang",
        "Yiran Wu"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Human-computer interaction"
      ],
      "publication_date": "August 2024",
      "abstract": "We present AutoGen, an open-source framework that allows developers to build LLM applications by composing multiple agents to converse with each other to accomplish tasks. AutoGen agents are customizable, conversable, and can operate in various modes that employ combinations of LLMs, human inputs, and tools. It also enables developers to create flexible agent behaviors and conversation patterns for different applications using both natural language and code. AutoGen serves as a generic infrastructure and is widely used by AI practitioners and researchers to build diverse applications of various complexities and LLM capacities. We demonstrate the framework’s effectiveness with several pilot applications, on domains ranging from mathematics and coding to question-answering, supply-chain optimization, online decision-making, and entertainment.\n\n\n",
      "url": "https://www.microsoft.com/en-us/research/publication/autogen-enabling-next-gen-llm-applications-via-multi-agent-conversation-framework/"
    },
    {
      "title": "Let’s Fix this Together: Conversational Debugging with GitHub Copilot",
      "authors": [
        "Arjun Radhakrishna",
        "Bhavya Chopra",
        "Cagri Aslan",
        "Chris Parnin",
        "Dustin Coleman",
        "Gustavo Soares",
        "Param Biyani",
        "Sumit Gulwani",
        "Yasharth Bajpai"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Human-computer interaction",
        "Programming languages and software engineering"
      ],
      "publication_date": "August 2024",
      "abstract": "Despite advancements in IDE tooling, code understanding, generation, and automated repair, debugging continues to present significant challenges. Existing debugging strategies available to developers in literature are often too mechanical and rigid for day-to-day issues. Recent advances in Large Language Models (LLMs) promise practical solutions that allow for more free-form debugging strategies. While LLMs offer satisfactory assistance in some cases, they often leap to action without sufficient context, making implicit assumptions and providing inaccurate responses. Moreover, the dialogue between developers and LLMs predominantly takes the form of question-answer pairs, placing the burden of formulating the correct questions and sustaining multi-turn conversations on the developer.\nWe introduce ROBIN, a novel multi-agent conversational AI-assistant within GitHub Copilot Chat, specifically designed for debugging. ROBIN moves beyond the question-answer pairs by introducing the investigate & respond pattern, that focuses on using information gathered automatically from the IDE or gathered interactively from the developer before responding. ROBIN incorporates a general debugging strategy to systematically analyze bugs to sustain collaborative interactions while ensuring that the conversation does not deviate from the debugging task at hand. Through a within-subjects user study with 16 industry professionals, we find that equipping ROBIN to—(1) leverage the insert expansion interaction pattern, (2) facilitate turn-taking, and (3) utilize debugging strategies—leads to lowered conversation barriers, a 2.5x improvement in bug localization and a substantial 3.5x improvement in bug resolution compared to AI-assisted debugging in Visual Studio prior to ROBIN.",
      "url": "https://www.microsoft.com/en-us/research/publication/lets-fix-this-together-conversational-debugging-with-github-copilot/"
    },
    {
      "title": "A Glitch in the Matrix? Locating and Detecting Language Model Grounding with Fakepedia",
      "authors": [
        "Barun Patra",
        "Emre Kiciman",
        "Giovanni Monea",
        "Hamid Palangi",
        "Jason Eisner",
        "Martin Josifoski",
        "Maxime Peyrard",
        "Robert West",
        "Vishrav Chaudhary"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "August 2024",
      "abstract": "Large language models (LLMs) have an impressive ability to draw on novel information supplied in their context. Yet the mechanisms underlying this contextual grounding remain unknown, especially in situations where contextual information contradicts factual knowledge stored in the parameters, which LLMs also excel at recalling. Favoring the contextual information is critical for retrieval-augmented generation methods, which enrich the context with up-to-date information, hoping that grounding can rectify outdated or noisy stored knowledge. We present a novel method to study grounding abilities using Fakepedia, a novel dataset of counterfactual texts constructed to clash with a model’s internal parametric knowledge. In this study, we introduce Fakepedia, a counterfactual dataset designed to evaluate grounding abilities when the internal parametric knowledge clashes with the contextual information. We benchmark various LLMs with Fakepedia and conduct a causal mediation analysis of LLM components when answering Fakepedia queries, based on our Masked Grouped Causal Tracing (MGCT) method. Through this analysis, we identify distinct computational patterns between grounded and ungrounded responses. We finally demonstrate that distinguishing grounded from ungrounded responses is achievable through computational analysis alone. Our results, together with existing findings about factual recall mechanisms, provide a coherent narrative of how grounding and factual recall mechanisms interact within LLMs.",
      "url": "https://www.microsoft.com/en-us/research/publication/a-glitch-in-the-matrix-locating-and-detecting-language-model-grounding-with-fakepedia/"
    },
    {
      "title": "OmniParser for Pure Vision Based GUI Agent",
      "authors": [
        "Ahmed Awadallah",
        "Jianwei Yang",
        "Yadong Lu",
        "Yelong Shen"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "July 2024",
      "abstract": "The recent success of large vision language models shows great potential in driving the agent system operating on user interfaces. However, we argue that the power multimodal models like GPT-4V as a general agent on multiple operating systems across different applications is largely underestimated due to the lack of a robust screen parsing technique capable of: 1) reliably identifying interactable icons within the user interface, and 2) understanding the semantics of various elements in a screenshot and accurately associate the intended action with the corresponding region on the screen. To fill these gaps, we introduce \\textsc{OmniParser}, a comprehensive method for parsing user interface screenshots into structured elements, which significantly enhances the ability of GPT-4V to generate actions that can be accurately grounded in the corresponding regions of the interface. We first curated an interactable icon detection dataset using popular webpages and an icon description dataset. These datasets were utilized to fine-tune specialized models: a detection model to parse interactable regions on the screen and a caption model to extract the functional semantics of the detected elements. \\textsc{OmniParser} significantly improves GPT-4V’s performance on ScreenSpot benchmark. And on Mind2Web and AITW benchmark, \\textsc{OmniParser} with screenshot only input outperforms the GPT-4V baselines requiring additional information outside of screenshot.",
      "url": "https://www.microsoft.com/en-us/research/publication/omniparser-for-pure-vision-based-gui-agent/"
    },
    {
      "title": "Report on the 1st Workshop on Large Language Model for Evaluation in Information Retrieval (LLM4Eval 2024) at SIGIR 2024",
      "authors": [
        "Bhaskar Mitra",
        "Charles L. A. Clarke",
        "Clemencia Siro",
        "Emine Yilmaz",
        "Guglielmo Faggioli",
        "Hossein A. Rahmani",
        "Mohammad Aliannejadi",
        "Nick Craswell",
        "Paul Thomas"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Search and information retrieval"
      ],
      "publication_date": "August 2024",
      "abstract": "The first edition of the workshop on Large Language Model for Evaluation in Information Retrieval (LLM4Eval 2024) took place in July 2024, co-located with the ACM SIGIR Conference 2024 in the USA (SIGIR 2024). The aim was to bring information retrieval researchers together around the topic of LLMs for evaluation in information retrieval that gathered attention with the advancement of large language models and generative AI. Given the novelty of the topic, the workshop was focused around multi-sided discussions, namely panels and poster sessions of the accepted proceedings papers.",
      "url": "https://www.microsoft.com/en-us/research/publication/report-on-the-1st-workshop-on-large-language-model-for-evaluation-in-information-retrieval-llm4eval-2024-at-sigir-2024/"
    },
    {
      "title": "Efficient Policy-Rich Rate Enforcement with Phantom Queues",
      "authors": [
        "Ammar Tahir",
        "Ilias Marinos",
        "Mike Evans",
        "Prateesh Goyal",
        "Radhika Mittal"
      ],
      "research_areas": [
        "Systems and networking"
      ],
      "publication_date": "August 2024",
      "abstract": "Rate enforcement is routinely employed in modern networks (e.g. ISPs rate-limiting users traffic to the subscribed rates). In addition to correctly enforcing the desired rates, rate-limiting mechanisms must be able to support rich rate-sharing policies within each traffic aggregate (e.g. per-flow fairness, weighted fairness, and prioritization). And all of this must be done at scale to efficiently support the vast magnitude of users. There are two primary rate-limiting mechanisms – traffic shaping (that buffers packets in queues to enforce the desired rates and policies) and traffic policing (that filters packets as per the desired rates without buffering them). Policers are light-weight and scalable, but do not support rich policy enforcement and often provide poor rate enforcement (being notoriously hard to configure). Shapers, on the other hand, achieve desired rates and policies, but at the cost of high system resource (memory and CPU) utilization which impacts scalability. In this paper, we explore whether we can get the best of both worlds – the scalability of a policer with the rate and policy enforcement properties of a shaper. We answer this question in the affirmative with our system BC-PQP. BC-PQP augments a policer with (i) multiple phantom queues that simulate buffer occupancy using counters, and enable rich policy enforcement, and (ii) a novel burst control mechanism that enables auto-configuration of the queues for correct rate enforcement. We implement our rate-limiter as a middlebox over DPDK. Our evaluation shows how BC-PQP achieves the rate and policy enforcement properties close to that of a shaper while being up to 7 × more efficient.",
      "url": "https://www.microsoft.com/en-us/research/publication/efficient-policy-rich-rate-enforcement-with-phantom-queues/"
    },
    {
      "title": "Virchow2: Scaling Self-Supervised Mixed Magnification Models in Pathology",
      "authors": [
        "Adam Casson",
        "Eric Zimmermann",
        "Eugene Vorontsov",
        "George Shaikovski",
        "Jimmy Hall",
        "Julian Viret",
        "Kristen Severson",
        "Michal Zelechowski",
        "Neil Tenenholtz",
        "Nicolo Fusi",
        "Siqi Liu",
        "Thomas J. Fuchs"
      ],
      "research_areas": [
        "Computer vision",
        "Medical, health and genomics"
      ],
      "publication_date": "July 2024",
      "abstract": "Foundation models are rapidly being developed for computational pathology applications. However, it remains an open question which factors are most important for downstream performance with data scale and diversity, model size, and training algorithm all playing a role. In this work, we present the result of scaling both data and model size, surpassing previous studies in both dimensions, and introduce two new models: Virchow2, a 632M parameter vision transformer, and Virchow2G, a 1.85B parameter vision transformer, each trained with 3.1M histopathology whole slide images. To support this scale, we propose domain-inspired adaptations to the DINOv2 training algorithm, which is quickly becoming the default method in self-supervised learning for computational pathology. We achieve state of the art performance on twelve tile-level tasks, as compared to the top performing competing models. Our results suggest that data diversity and domain-specific training can outperform models that only scale in the number of parameters, but, on average, performance benefits from domain-tailoring, data scale, and model scale.",
      "url": "https://www.microsoft.com/en-us/research/publication/virchow-2-scaling-self-supervised-mixed-magnification-models-in-pathology/"
    },
    {
      "title": "Methods for recovering conditional independence graphs (Abstract Reprint)",
      "authors": [
        "Harsh Shrivastava",
        "Urszula Chajewska"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "July 2024",
      "abstract": "Conditional Independence (CI) graphs are a type of probabilistic graphical models that are primarily used to gain insights about feature relationships. Each edge represents the partial correlation between the connected features which gives information about their direct dependence. In this survey, we list out different methods and study the advances in techniques developed to recover CI graphs. We cover traditional optimization methods as well as recently developed deep learning architectures along with their recommended implementations. To facilitate wider adoption, we include preliminaries that consolidate associated operations, for example techniques to obtain covariance matrix for mixed datatypes.",
      "url": "https://www.microsoft.com/en-us/research/publication/methods-for-recovering-conditional-independence-graphs-abstract-reprint/"
    },
    {
      "title": "Mixture of Nested Experts: Adaptive Processing of Visual Tokens",
      "authors": [
        "Aditya Kusupati",
        "Anurag Arnab",
        "Arsha Nagrani",
        "Gagan Jain",
        "Nidhi Hegde",
        "Prateek Jain",
        "Shyamal Buch",
        "Sujoy Paul"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "July 2024",
      "abstract": "The visual medium (images and videos) naturally contains a large amount of information redundancy, thereby providing a great opportunity for leveraging efficiency in processing. While Vision Transformer (ViT) based models scale effectively to large data regimes, they fail to capitalize on this inherent redundancy, leading to higher computational costs. Mixture of Experts (MoE) networks demonstrate scalability while maintaining same inference-time costs, but they come with a larger parameter footprint. We present Mixture of Nested Experts (MoNE), which utilizes a nested structure for experts, wherein individual experts fall on an increasing compute-accuracy curve. Given a compute budget, MoNE learns to dynamically choose tokens in a priority order, and thus redundant tokens are processed through cheaper nested experts. Using this framework, we achieve equivalent performance as the baseline models, while reducing inference time compute by over two-fold. We validate our approach on standard image and video datasets – ImageNet-21K, Kinetics400, and Something-Something-v2. We further highlight MoNE$’$s adaptability by showcasing its ability to maintain strong performance across different inference-time compute budgets on videos, using only a single trained model.",
      "url": "https://www.microsoft.com/en-us/research/publication/mixture-of-nested-experts-adaptive-processing-of-visual-tokens/"
    },
    {
      "title": "Maternal Obesity and Risk of Sudden Unexpected Infant Death",
      "authors": [
        "Bill Weeks",
        "Darren Tanner",
        "Edwin A. Mitchell",
        "Jan-Marino Ramirez",
        "Juan M. Lavista Ferres"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Medical, health and genomics"
      ],
      "publication_date": "July 2024",
      "abstract": "Importance  Rates of maternal obesity are increasing in the US. Although obesity is a well-documented risk factor for numerous poor pregnancy outcomes, it is not currently a recognized risk factor for sudden unexpected infant death (SUID).\nObjective  To determine whether maternal obesity is a risk factor for SUID and the proportion of SUID cases attributable to maternal obesity.\nDesign, Setting, and Participants  This was a US nationwide cohort study using Centers for Disease Control and Prevention National Center for Health Statistics linked birth–infant death records for birth cohorts in 2015 through 2019. All US live births for the study years occurring at 28 weeks’ gestation or later from complete reporting areas were eligible; SUID cases were deaths occurring at 7 to 364 days after birth with International Statistical Classification of Diseases, Tenth Revision cause of death code R95 (sudden infant death syndrome), R99 (ill-defined and unknown causes), or W75 (accidental suffocation and strangulation in bed). Data were analyzed from October 1 through November 15, 2023.\nExposure  Maternal prepregnancy body mass index (BMI; calculated as weight in kilograms divided by height in meters squared).\nMain Outcome and Measure  SUID.\nResults  Of 18 857 694 live births eligible for analysis (median [IQR] age: maternal, 29 [9] years; paternal, 31 [9] years; gestational, 39 [2] weeks), 16 545 died of SUID (SUID rate, 0.88/1000 live births). After confounder adjustment, compared with mothers with normal BMI (BMI 18.5-24.9), infants born to mothers with obesity had a higher SUID risk that increased with increasing obesity severity. Infants of mothers with class I obesity (BMI 30.0-34.9) were at increased SUID risk (adjusted odds ratio [aOR], 1.10; 95% CI, 1.05-1.16); with class II obesity (BMI 35.0-39.9), a higher risk (aOR, 1.20; 95% CI, 1.13-1.27); and class III obesity (BMI ≥40.0), an even higher risk (aOR, 1.39; 95% CI, 1.31-1.47). A generalized additive model showed that increased BMI was monotonically associated with increased SUID risk, with an acceleration of risk for BMIs greater than approximately 25 to 30. Approximately 5.4% of SUID cases were attributable to maternal obesity.\nConclusions and Relevance  The findings suggest that infants born to mothers with obesity are at increased risk of SUID, with a dose-dependent association between increasing maternal BMI and SUID risk. Maternal obesity should be added to the list of known risk factors for SUID. With maternal obesity rates increasing, research should identify potential causal mechanisms for this association.",
      "url": "https://www.microsoft.com/en-us/research/publication/maternal-obesity-and-risk-of-sudden-unexpected-infant-death/"
    },
    {
      "title": "Diversification of Adaptive Policy for Effective Offline Reinforcement Learning",
      "authors": [
        "Chuheng Zhang",
        "Jiang Bian",
        "Kee-Eung Kim",
        "Lei Song",
        "Li Zhao",
        "Yunseon Choi"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "July 2024",
      "abstract": "Offline Reinforcement Learning (RL) aims to learn policies from pre-collected datasets that capture only a subset of the environment’s dynamics. The predominant approach has been to solve a constrained optimization formulation, which ensures that the policy visits state-action pairs within the support of the offline dataset. However, this approach has limited the ability to make decisions when the agent faces unknown parts of the environment at deployment time. To address the challenge of decision-making in out-of-support regions, model-based Bayes-adaptive approaches have been proposed by considering all dynamics models that could potentially be the true environment. Since it is generally infeasible to compute the posterior of all dynamics models based on the offline dataset, these approaches usually approximate the posterior by using a finite ensemble of highly probable dynamics models. Hence, the diversity of these models is the key to obtaining good policies. In this work, we propose MoDAP (Model-based Diverse Adaptive Policy Learning), an algorithm to enable the adaptive policy to make informed decisions in previously unexplored states. MoDAP adopts an iterative strategy that simultaneously training the policy and dynamics models. The policy optimization seeks to maximize expected returns across dynamics models, while the dynamics models are trained to promote policy diversification through the proposed information-theoretic objective. We evaluate MoDAP through experiments on the D4RL and NeoRL benchmarks, showcasing its performance superiority over state-of-the-art algorithms.",
      "url": "https://www.microsoft.com/en-us/research/publication/diversification-of-adaptive-policy-for-effective-offline-reinforcement-learning/"
    },
    {
      "title": "Cloud Actor-Oriented Database Transactions in Orleans",
      "authors": [
        "Asaf Cidon",
        "Junfeng Yang",
        "Phil Bernstein",
        "Reuben Bond",
        "Sebastian Burckhardt",
        "Tamer Eldeeb"
      ],
      "research_areas": [
        "Data platforms and analytics",
        "Systems and networking"
      ],
      "publication_date": "July 2024",
      "abstract": "Microsoft Orleans is a popular open source distributed programming framework and platform which invented the virtual actor model, and has since evolved into an actor-oriented database system with the addition of database abstractions such as ACID transactions. Properties of Orleans’ virtual actor model imply that any ACID transaction mechanism for operations spanning multiple actors must support distributed transactions on top of pluggable cloud storage drivers. Unfortunately, distributed transactions usually perform poorly in this environment, partly because of the high performance and contention overhead of performing two-phase commit (2PC) on slow cloud storage systems. In this paper we describe the design and implementation of ACID transactions in Orleans. The system uses two primary techniques to mask the high latency of cloud storage and enable high transaction throughput. First, Orleans pioneered the use of a distributed form of early lock release by releasing all of a transaction’s locks during phase one of 2PC, and by tracking commit dependencies to implement cascading abort. This avoids blocking transactions while running 2PC and enables a distributed form of group commit. Second, Orleans leverages reconnaissance queries to prefetch the state of all actors involved in a transaction from cloud storage prior to running the transaction and acquiring any locks, thus ensuring no locks are held while blocking on high latency cloud storage in most cases.",
      "url": "https://www.microsoft.com/en-us/research/publication/cloud-actor-oriented-database-transactions-in-orleans/"
    },
    {
      "title": "Commentary: Productivity implications for generative AI role-based prompts as a networked hermeneutic",
      "authors": [
        "Sean Rintel"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Human-computer interaction",
        "Social sciences"
      ],
      "publication_date": "August 2024",
      "abstract": "Commentary for Membership categorisation, sociological description and role prompt engineering with ChatGPT – William Housley, Patrik Dahl, 2024 (opens in new tab)\nAs Housley and Dahl (2024) (opens in new tab) demonstrate, role-based prompts for Generative AI (GenAI) systems are based on vernacular resources of membership categorization and action description, representing a networked hermeneutic of lay and professional sociology. As a Microsoft Human-Computer Interaction researcher, I see three implications for designing GenAI systems for productivity.",
      "url": "https://www.microsoft.com/en-us/research/publication/productivity-implications-for-generative-ai-role-based-prompts-as-a-networked-hermeneutic/"
    },
    {
      "title": "UX Matters: The Critical Role of UX in Responsible AI",
      "authors": [
        "Hariharan Subramonyam",
        "Lauren Wilcox",
        "Mihaela Vorvoreanu",
        "Q. Vera Liao"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "July 2024",
      "abstract": "Let’s imagine a scenario—inspired by true events—in which a company has deployed an AI-powered system in a hospital. The system provides recommendations for treatment plans. Some clinicians find that the new system requires them to change their routine and significantly adds to their workload, so they start resisting the use of the system. Other clinicians are amazed by this new, powerful technology and overly rely on the AI by accepting its recommendations even when they are incorrect, resulting in medical errors with negative effects on patients—especially women, for whom the AI system tends to underperform. Unfortunately, this is a scenario that happens too often as AI technologies are being deployed in various domains. A responsible approach to AI development and deployment should have aimed to prevent these issues. UX practitioners could have been involved to play an instrumental role. For example:\n\nUX researchers could have identified stakeholder needs, concerns, and values, including those of clinicians and patients, to inform better choices of AI use cases, datasets, model parameters, evaluation criteria, and so on.\nUX designers could have taken a leading role in designing the system interactions to be compatible with current work processes, and created interface features that help mitigate overreliance on AI\n",
      "url": "https://www.microsoft.com/en-us/research/publication/ux-matters-the-critical-role-of-ux-in-responsible-ai/"
    },
    {
      "title": "Provable Risk-Sensitive Distributional Reinforcement Learning with General Function Approximation",
      "authors": [
        "Longbo Huang",
        "Siwei Wang",
        "Xiangcheng Zhang",
        "Yu Chen"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "July 2024",
      "abstract": "In the realm of reinforcement learning (RL), accounting for risk is crucial for making decisions under uncertainty, particularly in applications where safety and reliability are paramount. In this paper, we introduce a general framework on Risk-Sensitive Distributional Reinforcement Learning (RS-DisRL), with static Lipschitz Risk Measures (LRM) and general function approximation. Our framework covers a broad class of risk-sensitive RL, and facilitates analysis of the impact of estimation functions on the effectiveness of RSRL strategies and evaluation of their sample complexity. We design two innovative meta-algorithms: \\texttt{RS-DisRL-M}, a model-based strategy for model-based function approximation, and \\texttt{RS-DisRL-V}, a model-free approach for general value function approximation. With our novel estimation techniques via Least Squares Regression (LSR) and Maximum Likelihood Estimation (MLE) in distributional RL with augmented Markov Decision Process (MDP), we derive the first $\\widetilde{\\mathcal{O}}(\\sqrt{K})$ dependency of the regret upper bound for RSRL with static LRM, marking a pioneering contribution towards statistically efficient algorithms in this domain.",
      "url": "https://www.microsoft.com/en-us/research/publication/provable-risk-sensitive-distributional-reinforcement-learning-with-general-function-approximation/"
    },
    {
      "title": "Large Language Models as Co-Pilots for Causal Inference in Medical Studies",
      "authors": [
        "Ahmed Alaa",
        "Emre Kiciman",
        "Laura B. Balzer",
        "M. V. D. Laan",
        "Maya Petersen",
        "Rachael V. Phillips"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Medical, health and genomics"
      ],
      "publication_date": "July 2024",
      "abstract": "The validity of medical studies based on real-world clinical data, such as observational studies, depends on critical assumptions necessary for drawing causal conclusions about medical interventions. Many published studies are flawed because they violate these assumptions and entail biases such as residual confounding, selection bias, and misalignment between treatment and measurement times. Although researchers are aware of these pitfalls, they continue to occur because anticipating and addressing them in the context of a specific study can be challenging without a large, often unwieldy, interdisciplinary team with extensive expertise. To address this expertise gap, we explore the use of large language models (LLMs) as co-pilot tools to assist researchers in identifying study design flaws that undermine the validity of causal inferences. We propose a conceptual framework for LLMs as causal co-pilots that encode domain knowledge across various fields, engaging with researchers in natural language interactions to provide contextualized assistance in study design. We provide illustrative examples of how LLMs can function as causal co-pilots, propose a structured framework for their grounding in existing causal inference frameworks, and highlight the unique challenges and opportunities in adapting LLMs for reliable use in epidemiological research.",
      "url": "https://www.microsoft.com/en-us/research/publication/large-language-models-as-co-pilots-for-causal-inference-in-medical-studies/"
    },
    {
      "title": "Q-Sparse: All Large Language Models can be Fully Sparsely-Activated",
      "authors": [
        "Furu Wei",
        "Hongyu Wang",
        "Ruiping Wang",
        "Shuming Ma"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "July 2024",
      "abstract": "We introduce, Q-Sparse, a simple yet effective approach to training sparsely-activated large language models (LLMs). Q-Sparse enables full sparsity of activations in LLMs which can bring significant efficiency gains in inference. This is achieved by applying top-K sparsification to the activations and the straight-through-estimator to the training. We also introduce Block Q-Sparse for batch training and inference. The key results from this work are, (1) Q-Sparse can achieve results comparable to those of baseline LLMs while being much more efficient at inference time; (2) We present an inference-optimal scaling law for sparsely-activated LLMs; (3) Q-Sparse is effective in different settings, including training-from-scratch, continue-training of off-the-shelf LLMs, and finetuning; (4) Q-Sparse works for both full-precision and 1-bit LLMs (e.g., BitNet b1.58). Particularly, the synergy of BitNet b1.58 and Q-Sparse (can be equipped with MoE) provides the cornerstone and a clear path to revolutionize the efficiency, including cost and energy consumption, of future LLMs.",
      "url": "https://www.microsoft.com/en-us/research/publication/q-sparse-all-large-language-models-can-be-fully-sparsely-activated/"
    },
    {
      "title": "DéjàVu: KV-cache Streaming for Fast, Fault-tolerant Generative LLM Serving",
      "authors": [
        "Amar Phanishayee",
        "Ana Klimovic",
        "Foteini Strati",
        "Jakub Tarnawski",
        "Sara Mcallister"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Systems and networking"
      ],
      "publication_date": "July 2024",
      "abstract": "Distributed LLM serving is costly and often underutilizes hardware accelerators due to three key challenges: bubbles in pipeline-parallel deployments caused by the bimodal latency of prompt and token processing, GPU memory overprovisioning, and long recovery times in case of failures. In this paper, we propose DéjàVu, a system to address all these challenges using a versatile and efficient KV cache streaming library (DéjàVuLib). Using DéjàVuLib, we propose and implement efficient prompt-token disaggregation to reduce pipeline bubbles, microbatch swapping for efficient GPU memory management, and state replication for fault-tolerance. We highlight the efficacy of these solutions on a range of large models across cloud deployments.",
      "url": "https://www.microsoft.com/en-us/research/publication/dejavu-kv-cache-streaming-for-fast-fault-tolerant-generative-llm-serving/"
    },
    {
      "title": "Arithmetic Solving in Z3",
      "authors": [
        "Lev Nachmanson",
        "Nikolaj Bjørner"
      ],
      "research_areas": [
        "Algorithms",
        "Mathematics",
        "Programming languages and software engineering"
      ],
      "publication_date": "July 2024",
      "abstract": "The theory of arithmetic is integral to many uses of SMT solvers. Z3 has implemented native solvers for arithmetic reasoning since its first release. We present a full re-implementation of Z3’s original arithmetic solver. It is based on substantial experiences from user feedback, engineering and experimentation. While providing a comprehensive overview of the main components we emphasize selected new insights we arrived at while developing and testing the solver.\n\n\n",
      "url": "https://www.microsoft.com/en-us/research/publication/arithmetic-solving-in-z3/"
    },
    {
      "title": "MGit: A Model Versioning and Management System",
      "authors": [
        "Amar Phanishayee",
        "Asaf Cidon",
        "Daniel Mendoza",
        "Deepak Narayanan",
        "Junfeng Yang",
        "Rafael da Silva",
        "Wei Hao"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Systems and networking"
      ],
      "publication_date": "July 2024",
      "abstract": "New ML models are often derived from existing ones (e.g., through fine-tuning, quantization or distillation), forming an ecosystem where models are related to each other and can share structure or even parameter values. Managing such a large and evolving ecosystem of model derivatives is challenging. For instance, the overhead of storing all such models is high, and models may inherit bugs from related models, complicating error attribution and debugging. In this paper, we propose a model versioning and management system called MGit that makes it easier to store, test, update, and collaborate on related models. MGit introduces a lineage graph that records the relationships between models, optimizations to efficiently store model parameters, and abstractions over this lineage graph that facilitate model testing, updating and collaboration. We find that MGit works well in practice: MGit is able to reduce model storage footprint by up to 7×. Additionally, in a user study with 20 ML practitioners, users complete a model updating task 3× faster on average with MGit.",
      "url": "https://www.microsoft.com/en-us/research/publication/mgit-a-model-versioning-and-management-system-3/"
    },
    {
      "title": "Integrated Hardware Architecture and Device Placement Search",
      "authors": [
        "Amar Phanishayee",
        "Divya Mahajan",
        "Irene Wang",
        "Jakub Tarnawski"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Systems and networking"
      ],
      "publication_date": "July 2024",
      "abstract": "Distributed execution of deep learning training involves a dynamic interplay between hardware accelerator architecture and device placement strategy. This is the first work to explore the co-optimization of determining the optimal architecture and device placement strategy through novel algorithms, improving the balance of computational resources, memory usage, and data distribution. Our architecture search leverages tensor and vector units, determining their quantity and dimensionality, and on-chip and off-chip memory configurations. It also determines the microbatch size and decides whether to recompute or stash activations, balancing the memory footprint of training and storage size. For each explored architecture configuration, we use an Integer Linear Program (ILP) to find the optimal schedule for executing operators on the accelerator. The ILP results then integrate with a dynamic programming solution to identify the most effective device placement strategy, combining data, pipeline, and tensor model parallelism across multiple accelerators. Our approach achieves higher throughput on large language models compared to the state-of-the-art TPUv4 and the Spotlight accelerator search framework. The entire source code of Phaze is available at https://github.com/msr-fiddle/phaze.",
      "url": "https://www.microsoft.com/en-us/research/publication/integrated-hardware-architecture-and-device-placement-search/"
    },
    {
      "title": "A Framework for Debugging Automated Program Verification Proofs via Proof Actions",
      "authors": [
        "Bryan Parno",
        "Chanhee Cho",
        "Jay Bosamiya",
        "Yi Zhou"
      ],
      "research_areas": [
        "Programming languages and software engineering"
      ],
      "publication_date": "July 2024",
      "abstract": "Many program verification tools provide automation via SMT solvers, allowing them to automatically discharge many proofs. However, when a proof fails, it can be hard to understand why it failed or how to fix it. The main feedback the developer receives is simply the verification result (i.e., success or failure), with no visibility into the solver’s internal state. To assist developers using such tools, we introduce ProofPlumber, a novel and extensible proof-action framework for understanding and debugging proof failures. Proof actions act on the developer’s source-level proofs (e.g., assertions and lemmas) to determine why they failed and potentially suggest remedies. We evaluate ProofPlumber by writing a collection of proof actions that capture common proof debugging practices. We produce 17 proof actions, each only 29–176 lines of code.",
      "url": "https://www.microsoft.com/en-us/research/publication/a-framework-for-debugging-automated-program-verification-proofs-via-proof-actions/"
    },
    {
      "title": "Generative AI in Real-World Workplaces",
      "authors": [
        "Alex Farach",
        "Alexia Cambon",
        "Brent Hecht",
        "Jaime Teevan",
        "Jenna Butler",
        "Michael Schwarz",
        "Neha Parikh Shah",
        "Sonia Jaffe"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Economics",
        "Human-computer interaction",
        "Search and information retrieval",
        "Social sciences"
      ],
      "publication_date": "July 2024",
      "abstract": "This report presents the most recent findings of Microsoft’s research initiative on AI and Productivity, which seeks to measure and understand the productivity gains associated with LLM-powered productivity tools like Microsoft Copilot. The report synthesizes research results from over a dozen recent studies conducted by researchers at Microsoft, with a focus on studies of generative AI in actual workplace environments. One of these is, to our knowledge, the largest, randomized controlled trial of the introduction of generative AI into organizations. Overall, the research suggests that generative AI is already aiding workers in becoming more productive in their day-to-day jobs in significant ways. However, the influence of generative AI is subject to variation by role, function, and organization and is contingent upon adoption and utilization. The report explores these variations and underscores the potential for AI to have even greater impact as individuals and organizations recalibrate their work practices to harness AI in the places where it provides the most value.\nSlides summarizing the main findings of the report are available Generative-AI-in-Real-World-Workplaces-Deck.pdf\nThis report is a follow up to the first productivity report released in Dec 2023: Early LLM-based Tools for Enterprise Information Workers Likely Provide Meaningful Boosts to Productivity",
      "url": "https://www.microsoft.com/en-us/research/publication/generative-ai-in-real-world-workplaces/"
    },
    {
      "title": "Combinatorial Multivariant Multi-Armed Bandits with Applications to Episodic Reinforcement Learning and Beyond",
      "authors": [
        "Han Zhong",
        "Jinhang Zuo",
        "John C.S. Lui",
        "Mohammad Hajiesmaili",
        "Shuai Li",
        "Siwei Wang",
        "Wei Chen",
        "Xuchuang Wang",
        "Xutong Liu",
        "Zhiyong Wang"
      ],
      "research_areas": [
        "Algorithms",
        "Artificial intelligence"
      ],
      "publication_date": "July 2024",
      "abstract": "We introduce a novel framework of combinatorial multi-armed bandits (CMAB) with multivariant and probabilistically triggering arms (CMABMT), where the outcome of each arm is a d-dimensional multivariant random variable and the feedback follows a general arm triggering process. Compared with existing CMAB works, CMABMT not only enhances the modeling power but also allows improved results by leveraging distinct statistical properties for multivariant random variables. For CMAB-MT, we propose a general 1-norm multivariant and triggering probability modulated smoothness condition, and an optimistic CUCB-MT algorithm built upon this condition. Our framework can include many important problems as applications, such as episodic reinforcement learning (RL) and probabilistic maximum coverage for goods distribution, all of which meet the above smoothness condition and achieve matching or improved regret bounds compared to existing works. Through our new framework, we build the first connection between the episodic RL and CMAB literature, by offering a new angle to solve the episodic RL through the lens of CMAB, which may encourage more interactions between these two important directions.",
      "url": "https://www.microsoft.com/en-us/research/publication/combinatorial-multivariant-multi-armed-bandits-with-applications-to-episodic-reinforcement-learning-and-beyond/"
    },
    {
      "title": "TrustRate: A Decentralized Platform for Hijack-Resistant Anonymous Reviews",
      "authors": [
        "Divya Gupta",
        "Muthian Sivathanu",
        "Nishanth Chandran",
        "Rohit Dwivedula",
        "Sambhav Satija",
        "Satya Lokam",
        "Sriram Sridhar"
      ],
      "research_areas": [
        "Security, privacy, and cryptography"
      ],
      "publication_date": "July 2024",
      "abstract": "Reviews and ratings by users form a central component in several widely used products today (e.g., product reviews, ratings of online content, etc.), but today’s platforms for managing such reviews are ad-hoc and vulnerable to various forms of tampering and hijack by fake reviews either by bots or motivated paid workers. We define a new metric called ‘hijack-resistance’ for such review platforms, and then present TrustRate, an end-to-end decentralized, hijack-resistant platform for authentic, anonymous, tamper-proof reviews. With a prototype implementation and evaluation at the scale of thousands of nodes, we demonstrate the efficacy and performance of our platform, towards a new paradigm for building products based on trusted reviews by end users without having to trust a single organization that manages the reviews.",
      "url": "https://www.microsoft.com/en-us/research/publication/trustrate-a-decentralized-platform-for-hijack-resistant-anonymous-reviews/"
    },
    {
      "title": "LookupViT: Compressing visual information to a limited number of tokens",
      "authors": [
        "Gagan Jain",
        "Prateek Jain",
        "Rajat Koner",
        "Sujoy Paul",
        "Volker Tresp"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "July 2024",
      "abstract": "Vision Transformers (ViT) have emerged as the de-facto choice for numerous industry grade vision solutions. But their inference cost can be prohibitive for many settings, as they compute self-attention in each layer which suffers from quadratic computational complexity in the number of tokens. On the other hand, spatial information in images and spatio-temporal information in videos is usually sparse and redundant. In this work, we introduce LookupViT, that aims to exploit this information sparsity to reduce ViT inference cost. LookupViT provides a novel general purpose vision transformer block that operates by compressing information from higher resolution tokens to a fixed number of tokens. These few compressed tokens undergo meticulous processing, while the higher-resolution tokens are passed through computationally cheaper layers. Information sharing between these two token sets is enabled through a bidirectional cross-attention mechanism. The approach offers multiple advantages – (a) easy to implement on standard ML accelerators (GPUs/TPUs) via standard high-level operators, (b) applicable to standard ViT and its variants, thus generalizes to various tasks, (c) can handle different tokenization and attention approaches. LookupViT also offers flexibility for the compressed tokens, enabling performance-computation trade-offs in a single trained model. We show LookupViT’s effectiveness on multiple domains – (a) for image-classification (ImageNet-1K and ImageNet-21K), (b) video classification (Kinetics400 and Something-Something V2), (c) image captioning (COCO-Captions) with a frozen encoder. LookupViT provides $2\\times$ reduction in FLOPs while upholding or improving accuracy across these domains. In addition, LookupViT also demonstrates out-of-the-box robustness and generalization on image classification (ImageNet-C,R,A,O), improving by up to $4\\%$ over ViT.",
      "url": "https://www.microsoft.com/en-us/research/publication/lookupvit-compressing-visual-information-to-a-limited-number-of-tokens/"
    },
    {
      "title": "Stealing Part of a Production Language Model",
      "authors": [
        "A. Feder Cooper",
        "Arthur Conmy",
        "Daniel Paleka",
        "David Rolnick",
        "Eric Wallace",
        "Florian Tramèr",
        "Itay Yona",
        "Jonathan Hayase",
        "Katherine Lee",
        "Krishnamurthy Dj Dvijotham",
        "Matthew Jagielski",
        "Milad Nasr",
        "Nicholas Carlini",
        "Thomas Steinke"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Security, privacy, and cryptography"
      ],
      "publication_date": "July 2024",
      "abstract": "We introduce the first model-stealing attack that extracts precise, nontrivial information from black-box production language models like OpenAI’s ChatGPT or Google’s PaLM-2. Specifically, our attack recovers the embedding projection layer (up to symmetries) of a transformer model, given typical API access. For under $20 USD, our attack extracts the entire projection matrix of OpenAI’s Ada and Babbage language models. We thereby confirm, for the first time, that these black-box models have a hidden dimension of 1024 and 2048, respectively. We also recover the exact hidden dimension size of the gpt-3.5-turbo model, and estimate it would cost under $2,000 in queries to recover the entire projection matrix. We conclude with potential defenses and mitigations, and discuss the implications of possible future work that could extend our attack.",
      "url": "https://www.microsoft.com/en-us/research/publication/stealing-part-of-a-production-language-model/"
    },
    {
      "title": "Intersecting-Boundary-Sensitive Fingerprinting for Tampering Detection of DNN Models",
      "authors": [
        "Bin Benjamin Zhu"
      ],
      "research_areas": [
        "Search and information retrieval",
        "Security, privacy, and cryptography"
      ],
      "publication_date": "July 2024",
      "abstract": "Cloud-based AI services offer numerous benefits but also introduce vulnerabilities, allowing for tampering with deployed DNN models, ranging from injecting malicious behaviors to reducing computing resources. Fingerprint samples are generated to query models to detect such tampering. In this paper, we present \\emph{Intersecting-Boundary-Sensitive Fingerprinting (IBSF)}, a novel method for black-box integrity verification of DNN models using only top-1 labels. Recognizing that tampering with a model alters its decision boundary, IBSF crafts fingerprint samples from normal samples by maximizing the partial Shannon entropy of a selected subset of categories to position the fingerprint samples near decision boundaries where the categories in the subset intersect. These fingerprint samples are almost indistinguishable from their source samples. We theoretically establish and confirm experimentally that these fingerprint samples’ expected sensitivity to tampering increases with the cardinality of the subset. Extensive evaluation demonstrates that IBSF surpasses existing state-of-the-art fingerprinting methods, particularly with larger subset cardinality, establishing its state-of-the-art performance in black-box tampering detection using only top-1 labels. The IBSF code is available at: https://github.com/CGCL-codes/IBSF.",
      "url": "https://www.microsoft.com/en-us/research/publication/intersecting-boundary-sensitive-fingerprinting-for-tampering-detection-of-dnn-models/"
    },
    {
      "title": "Convex Analysis at Infinity: An Introduction to Astral Space",
      "authors": [
        "Matus Telgarsky",
        "Miro Dudík",
        "Robert E. Schapire"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Mathematics"
      ],
      "publication_date": "July 2024",
      "abstract": "Not all convex functions on Rn have finite minimizers; some can only be minimized by a sequence as it heads to infinity. In this work, we aim to develop a theory for understanding such minimizers at infinity. We study astral space, a compact extension of Rn to which such points at infinity have been added. Astral space is constructed to be as small as possible while still ensuring that all linear functions can be continuously extended to the new space. Although astral space includes all of Rn, it is not a vector space, nor even a metric space. However, it is sufficiently well-structured to allow useful and meaningful extensions of such concepts as convexity, conjugacy, and subdifferentials. We develop these concepts and analyze various properties of convex functions on astral space, including the detailed structure of their minimizers, exact characterizations of continuity, and convergence of descent algorithms.",
      "url": "https://www.microsoft.com/en-us/research/publication/convex-analysis-at-infinity-an-introduction-to-astral-space/"
    },
    {
      "title": "As Generative Models Improve, People Adapt Their Prompts",
      "authors": [
        "Benjamin S. Manning",
        "Christos Nicolaides",
        "David Holtz",
        "E. Jahani",
        "Hong-Yi TuYe",
        "Joe Zhang",
        "Mohammed Alsobay",
        "Siddharth Suri"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "July 2024",
      "abstract": "In an online experiment with N = 1891 participants, we collected and analyzed over 18,000 prompts to explore how the importance of prompting will change as the capabilities of generative AI models continue to improve. Each participant in our experiment was randomly and blindly assigned to use one of three text-to-image diffusion models: DALL-E 2, its more advanced successor DALL-E 3, or a version of DALL-E 3 with automatic prompt revision. Participants were then asked to write prompts to reproduce a target image as closely as possible in 10 consecutive tries. We find that task performance was higher for participants using DALL-E 3 than for those using DALL-E 2. This performance gap corresponds to a noticeable difference in the similarity of participants’ images to their target images, and was caused in equal measure by: (1) the increased technical capabilities of DALL-E 3, and (2) endogenous changes in participants’ prompting in response to these increased capabilities. More specifically, despite being blind to the model they were assigned, participants assigned to DALL-E 3 wrote longer prompts that were more semantically similar to each other and contained a greater number of descriptive words. Furthermore, while participants assigned to DALL-E 3 with prompt revision still outperformed those assigned to DALL-E 2, automatic prompt revision reduced the benefits of using DALL-E 3 by 58%. Taken together, our results suggest that as models continue to progress, people will continue to adapt their prompts to take advantage of new models’ capabilities.",
      "url": "https://www.microsoft.com/en-us/research/publication/as-generative-models-improve-people-adapt-their-prompts/"
    },
    {
      "title": "Is Behavior Cloning All You Need? Understanding Horizon in Imitation Learning",
      "authors": [
        "Adam Block",
        "Dipendra Misra",
        "Dylan Foster"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "July 2024",
      "abstract": "Imitation learning (IL) aims to mimic the behavior of an expert in a sequential decision making task by learning from demonstrations, and has been widely applied to robotics, autonomous driving, and autoregressive text generation. The simplest approach to IL, behavior cloning (BC), is thought to incur sample complexity with unfavorable quadratic dependence on the problem horizon, motivating a variety of different online algorithms that attain improved linear horizon dependence under stronger assumptions on the data and the learner’s access to the expert. We revisit the apparent gap between offline and online IL from a learning-theoretic perspective, with a focus on general policy classes up to and including deep neural networks. Through a new analysis of behavior cloning with the logarithmic loss, we show that it is possible to achieve horizon-independent sample complexity in offline IL whenever (i) the range of the cumulative payoffs is controlled, and (ii) an appropriate notion of supervised learning complexity for the policy class is controlled. Specializing our results to deterministic, stationary policies, we show that the gap between offline and online IL is not fundamental: (i) it is possible to achieve linear dependence on horizon in offline IL under dense rewards (matching what was previously only known to be achievable in online IL); and (ii) without further assumptions on the policy class, online IL cannot improve over offline IL with the logarithmic loss, even in benign MDPs. We complement our theoretical results with experiments on standard RL tasks and autoregressive language generation to validate the practical relevance of our findings.",
      "url": "https://www.microsoft.com/en-us/research/publication/is-behavior-cloning-all-you-need-understanding-horizon-in-imitation-learning/"
    },
    {
      "title": "Low latency carbon budget analysis reveals a large decline of the land carbon sink in 2023",
      "authors": [
        "Ana Bastos",
        "D. Goll",
        "F. Chevallier",
        "Jeffeson Goncalves de Souza",
        "Jiang Bian",
        "Mike O’Sullivan",
        "P. Ciais",
        "P. Friedlingstein",
        "Piyu Ke",
        "S. Sitch",
        "Wanjing Li",
        "Wei Li",
        "Xiaofan Gui",
        "Yi Xi",
        "Yidi Xu",
        "Zhu Liu"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "July 2024",
      "abstract": "In 2023, the CO2 growth rate was 3.37 ± 0.11 ppm at Mauna Loa, 86% above the previous year, and hitting a record high since observations began in 1958, while global fossil fuel CO2 emissions only increased by 0.6 ± 0.5%. This implies an unprecedented weakening of land and ocean sinks, and raises the question of where and why this reduction happened. Here we show a global net land CO2 sink of 0.44 ± 0.21 GtC yr−1, the weakest since 2003. We used dynamic global vegetation models, satellites fire emissions, an atmospheric inversion based on OCO-2 measurements, and emulators of ocean biogeochemical and data driven models to deliver a fast-track carbon budget in 2023. Those models ensured consistency with previous carbon budgets. Regional flux anomalies from 2015–2022 are consistent between top-down and bottom-up approaches, with the largest abnormal carbon loss in the Amazon during the drought in the second half of 2023 (0.31 ± 0.19 GtC yr−1), extreme fire emissions of 0.58 ± 0.10 GtC yr−1 in Canada and a loss in South-East Asia (0.13 ± 0.12 GtC yr−1). Since 2015, land CO2 uptake north of 20°N declined by half to 1.13 ± 0.24 GtC yr−1 in 2023. Meanwhile, the tropics recovered from the 2015–16 El Niño carbon loss, gained carbon during the La Niña years (2020–2023), then switched to a carbon loss during the 2023 El Niño (0.56 ± 0.23 GtC yr−1). The ocean sink was stronger than normal in the equatorial eastern Pacific due to reduced upwelling from La Niña’s retreat in early 2023 and the development of El Niño later. Land regions exposed to extreme heat in 2023 contributed a gross carbon loss of 1.73 GtC yr−1, indicating that record warming in 2023 had a strong negative impact on the capacity of terrestrial ecosystems to mitigate climate change.",
      "url": "https://www.microsoft.com/en-us/research/publication/low-latency-carbon-budget-analysis-reveals-a-large-decline-of-the-land-carbon-sink-in-2023/"
    },
    {
      "title": "Improving Context-Aware Preference Modeling for Language Models",
      "authors": [
        "Alessandro Sordoni",
        "Nicolas Le Roux",
        "Silviu Pitis",
        "Ziang Xiao"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "July 2024",
      "abstract": "While finetuning language models from pairwise preferences has proven remarkably effective, the underspecified nature of natural language presents critical challenges. Direct preference feedback is uninterpretable, difficult to provide where multidimensional criteria may apply, and often inconsistent, either because it is based on incomplete instructions or provided by diverse principals. To address these challenges, we consider the two-step preference modeling procedure that first resolves the under-specification by selecting a context, and then evaluates preference with respect to the chosen context. We decompose reward modeling error according to these two steps, which suggests that supervising context in addition to context-specific preference may be a viable approach to aligning models with diverse human preferences. For this to work, the ability of models to evaluate context-specific preference is critical. To this end, we contribute context-conditioned preference datasets and accompanying experiments that investigate the ability of language models to evaluate context-specific preference. We use our datasets to (1) show that existing preference models benefit from, but fail to fully consider, added context, (2) finetune a context-aware reward model with context-specific performance exceeding that of GPT-4 and Llama 3 70B on tested datasets, and (3) investigate the value of context-aware preference modeling.",
      "url": "https://www.microsoft.com/en-us/research/publication/improving-context-aware-preference-modeling-for-language-models/"
    },
    {
      "title": "On Overcoming Miscalibrated Conversational Priors in LLM-based Chatbots",
      "authors": [
        "Adith Swaminathan",
        "Christine Herlihy",
        "Jennifer Neville",
        "Tobias Schnabel"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Search and information retrieval"
      ],
      "publication_date": "July 2024",
      "abstract": "We explore the use of Large Language Model (LLM-based) chatbots to power recommender systems. We observe that the chatbots respond poorly when they encounter under-specified requests (e.g., they make incorrect assumptions, hedge with a long response, or refuse to answer). We conjecture that such miscalibrated response tendencies (i.e., conversational priors) can be attributed to LLM fine-tuning using annotators — single-turn annotations may not capture multi-turn conversation utility, and the annotators’ preferences may not even be representative of users interacting with a recommender system.\nWe first analyze public LLM chat logs to conclude that query under-specification is common. Next, we study synthetic recommendation problems with configurable latent item utilities and frame them as Partially Observed Decision Processes (PODP). We find that pre-trained LLMs can be sub-optimal for PODPs and derive better policies that clarify under-specified queries when appropriate. Then, we re-calibrate LLMs by prompting them with learned control messages to approximate the improved policy. Finally, we show empirically that our lightweight learning approach effectively uses logged conversation data to re-calibrate the response strategies of LLM-based chatbots for recommendation tasks.",
      "url": "https://www.microsoft.com/en-us/research/publication/on-overcoming-miscalibrated-conversational-priors-in-llm-based-chatbots/"
    },
    {
      "title": "APriCoT: Action Primitives based on Contact-state Transition for In-Hand Tool Manipulation",
      "authors": [
        "Atsushi Kanehira",
        "Daichi Saito",
        "Hideki Koike",
        "Jun Takamatsu",
        "Katsushi Ikeuchi",
        "Kazuhiro Sasabuchi",
        "Naoki Wake"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "July 2024",
      "abstract": "In-hand tool manipulation is an operation that not only manipulates a tool within the hand (i.e., in-hand manipulation) but also achieves a grasp suitable for a task after the manipulation. This study aims to achieve an in-hand tool manipulation skill through deep reinforcement learning. The difficulty of learning the skill arises because this manipulation requires (A) exploring long-term contact-state changes to achieve the desired grasp and (B) highly-varied motions depending on the contact-state transition. (A) leads to a sparsity of a reward on a successful grasp, and (B) requires an RL agent to explore widely within the state-action space to learn highly-varied actions, leading to sample inefficiency. To address these issues, this study proposes Action Primitives based on Contact-state Transition (APriCoT). APriCoT decomposes the manipulation into short-term action primitives by describing the operation as a contact-state transition based on three action representations (detach, crossover, attach). In each action primitive, fingers are required to perform short-term and similar actions. By training a policy for each primitive, we can mitigate the issues from (A) and (B). This study focuses on a fundamental operation as an example of in-hand tool manipulation: rotating an elongated object grasped with a precision grasp by half a turn to achieve the initial grasp. Experimental results demonstrated that ours succeeded in both the rotation and the achievement of the desired grasp, unlike existing studies. Additionally, it was found that the policy was robust to changes in object shape.",
      "url": "https://www.microsoft.com/en-us/research/publication/apricot-action-primitives-based-on-contact-state-transition-for-in-hand-tool-manipulation/"
    },
    {
      "title": "Automated Root Causing of Cloud Incidents using In-Context Learning with GPT-4",
      "authors": [
        "Chetan Bansal",
        "Minghua Ma",
        "Rujia Wang",
        "Saravan Rajmohan",
        "Supriyo GHOSH",
        "Xuchao Zhang",
        "Yu Kang"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "July 2024",
      "abstract": "Root Cause Analysis (RCA) plays a pivotal role in the incident diagnosis process for cloud services, requiring on-call engineers to identify the primary issues and implement corrective actions to prevent future recurrences. Improving the incident RCA process is vital for minimizing service downtime, customer impact and manual toil. Recent advances in artificial intelligence have introduced state-of-the-art Large Language Models (LLMs) like GPT-4, which have proven effective in tackling various AIOps problems, ranging from code authoring to incident management. Nonetheless, the GPT-4 model’s immense size presents challenges when trying to fine-tune it on user data because of the significant GPU resource demand and the necessity for continuous model fine-tuning with the emergence of new data. To address the high cost of fine-tuning LLM, we propose an in-context learning approach for automated root causing, which eliminates the need for fine-tuning. We conduct extensive study over 100,000 production incidents, comparing several large language models using multiple metrics. The results reveal that our in-context learning approach outperforms the previous fine-tuned large language models such as GPT-3 by an average of 24.8\\% across all metrics, with an impressive 49.7\\% improvement over the zero-shot model. Moreover, human evaluation involving actual incident owners demonstrates its superiority over the fine-tuned model, achieving a 43.5\\% improvement in correctness and an 8.7\\% enhancement in readability. The impressive results demonstrate the viability of utilizing a vanilla GPT model for the RCA task, thereby avoiding the high computational and maintenance costs associated with a fine-tuned model.",
      "url": "https://www.microsoft.com/en-us/research/publication/automated-root-causing-of-cloud-incidents-using-in-context-learning-with-gpt-4/"
    },
    {
      "title": "Can Large Language Models Transform Natural Language Intent into Formal Method Postconditions?",
      "authors": [
        "Madeline Endres",
        "Saikat Chakraborty",
        "Sarah Fakhoury",
        "Shuvendu Lahiri"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Programming languages and software engineering"
      ],
      "publication_date": "July 2024",
      "abstract": "Informal natural language that describes code functionality, such as code comments or function documentation, may contain substantial information about a programs intent. However, there is typically no guarantee that a programs implementation and natural language documentation are aligned. In the case of a conflict, leveraging information in code-adjacent natural language has the potential to enhance fault localization, debugging, and code trustworthiness. In practice, however, this information is often underutilized due to the inherent ambiguity of natural language which makes natural language intent challenging to check programmatically. The emergent abilities of Large Language Models (LLMs) have the potential to facilitate the translation of natural language intent to programmatically checkable assertions. However, it is unclear if LLMs can correctly translate informal natural language specifications into formal specifications that match programmer intent. Additionally, it is unclear if such translation could be useful in practice. In this paper, we describe nl2postcond, the problem of leveraging LLMs for transforming informal natural language to formal method postconditions, expressed as program assertions. We introduce and validate metrics to measure and compare different nl2postcond approaches, using the correctness and discriminative power of generated postconditions. We then use qualitative and quantitative methods to assess the quality of nl2postcond postconditions, finding that they are generally correct and able to discriminate incorrect code. Finally, we find that nl2postcond via LLMs has the potential to be helpful in practice; nl2postcond generated postconditions were able to catch 64 real-world historical bugs from Defects4J.",
      "url": "https://www.microsoft.com/en-us/research/publication/formalizing-natural-language-intent-into-program-specifications-via-large-language-models/"
    },
    {
      "title": "What Matters in a Measure? A Perspective from Large-Scale Search Evaluation",
      "authors": [
        "Gabriella Kazai",
        "Nick Craswell",
        "Paul Thomas",
        "Seth Spielman"
      ],
      "research_areas": [
        "Search and information retrieval"
      ],
      "publication_date": "July 2024",
      "abstract": "Information retrieval (IR) has a large literature on evaluation, dating back decades and forming a central part of the research culture. The largest proportion of this literature discusses techniques to turn a sequence of relevance labels into a single number, reflecting the system’s performance: precision or cumulative gain, for example, or dozens of alternatives. Those techniques—metrics—are themselves evaluated, commonly by reference to sensitivity and validity.\nIn our experience measuring search in industrial settings, a measurement regime needs many other qualities to be practical. For example, we must also consider how much a metric costs; how robust it is to the happenstance of sampling; whether it is debuggable; and what activities are incentivised when a metric is taken as a goal.\nIn this perspective paper we discuss what makes a search metric successful in large-scale settings, including factors which are not often canvassed in IR research but which are important in “real-world” use. We illustrate this with examples, including from industrial settings, and offer suggestions for metrics as part of a working system.",
      "url": "https://www.microsoft.com/en-us/research/publication/what-matters-in-a-measure-a-perspective-from-large-scale-search-evaluation/"
    },
    {
      "title": "Large Language Models Can Accurately Predict Searcher Preferences",
      "authors": [
        "Bhaskar Mitra",
        "Nick Craswell",
        "Paul Thomas",
        "Seth Spielman"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Search and information retrieval"
      ],
      "publication_date": "July 2024",
      "abstract": "Much of the evaluation and tuning of a search system relies on relevance labels—annotations that say whether a document is useful for a given search and searcher. Ideally these come from real searchers, but it is hard to collect this data at scale, so typical experiments rely on third-party labellers who may or may not produce accurate annotations. Label quality is managed with ongoing auditing, training, and monitoring.\nWe discuss an alternative approach. We take careful feedback from real searchers and use this to select a large language model (LLM), and prompt, that agrees with this feedback; the LLM can then produce labels at scale. Our experiments show LLMs are as accurate as human  labellers and as useful for finding the best systems and hardest queries. LLM performance varies with prompt features, but also varies unpredictably with simple paraphrases. This unpredictability reinforces the need for high-quality “gold” labels.",
      "url": "https://www.microsoft.com/en-us/research/publication/large-language-models-can-accurately-predict-searcher-preferences/"
    },
    {
      "title": "SLIP: Securing LLMs IP Using Weights Decomposition",
      "authors": [
        "Adam Hakim",
        "Ben Fishman",
        "Lev Greenberg",
        "Satya Lokam",
        "Shachar Seidman",
        "Tal Aviv",
        "Yehonathan Refael"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Security, privacy, and cryptography"
      ],
      "publication_date": "July 2024",
      "abstract": "Large language models (LLMs) have recently seen widespread adoption, in both academia and industry. As these models grow, they become valuable intellectual property (IP), reflecting enormous investments by their owners. Moreover, the high cost of cloud-based deployment has driven interest towards deployment to edge devices, yet this risks exposing valuable parameters to theft and unauthorized use. Current methods to protect models’ IP on the edge have limitations in terms of practicality, loss in accuracy, or suitability to requirements. In this paper, we introduce a novel hybrid inference algorithm, named SLIP, designed to protect edge-deployed models from theft. SLIP is the first hybrid protocol that is both practical for real-world applications and provably secure, while having zero accuracy degradation and minimal impact on latency. It involves partitioning the model between two computing resources, one secure but expensive, and another cost-effective but vulnerable. This is achieved through matrix decomposition, ensuring that the secure resource retains a maximally sensitive portion of the model’s IP while performing a minimal amount of computations, and vice versa for the vulnerable resource. Importantly, the protocol includes security guarantees that prevent attackers from exploiting the partition to infer the secured information. Finally, we present experimental results that show the robustness and effectiveness of our method, positioning it as a compelling solution for protecting LLMs.",
      "url": "https://www.microsoft.com/en-us/research/publication/slip-securing-llms-ip-using-weights-decomposition/"
    },
    {
      "title": "LLM4Eval: Large Language Model for Evaluation in IR",
      "authors": [
        "Bhaskar Mitra",
        "Charles L. A. Clarke",
        "Clemencia Siro",
        "Emine Yilmaz",
        "Guglielmo Faggioli",
        "Hossein A. Rahmani",
        "Mohammad Aliannejadi",
        "Nick Craswell",
        "Paul Thomas"
      ],
      "research_areas": [
        "Search and information retrieval"
      ],
      "publication_date": "July 2024",
      "abstract": "Large language models (LLMs) have demonstrated increasing task-solving abilities not present in smaller models. Utilizing the capabilities and responsibilities of LLMs for automated evaluation (LLM4eval) has recently attracted considerable attention in multiple research communities. For instance, LLM4eval models have been studied in the context of automated judgments, natural language generation, and retrieval augmented generation systems. We believe that the information retrieval community can significantly contribute to this growing research area by designing, implementing, analyzing, and evaluating various aspects of LLMs with applications to LLM4eval tasks. The main goal of LLM4eval workshop is to bring together researchers from industry and academia to discuss various aspects of LLMs for evaluation in information retrieval, including automated judgments, retrieval-augmented generation pipeline evaluation, altering human evaluation, robustness, and trustworthiness of LLMs for evaluation in addition to their impact on real-world applications. We also plan to run an automated judgment challenge prior to the workshop, where participants will be asked to generate labels for a given dataset while maximising correlation with human judgments. The format of the workshop is interactive, including roundtable and keynote sessions and tends to avoid the one sided dialogue of a mini-conference.",
      "url": "https://www.microsoft.com/en-us/research/publication/llm4eval-large-language-model-for-evaluation-in-ir/"
    },
    {
      "title": "X-lifecycle Learning for Cloud Incident Management using LLMs",
      "authors": [
        "A. Parayil",
        "Aditya Singh",
        "Chetan Bansal",
        "Drishti Goel",
        "Fiza Husain",
        "Saravan Rajmohan",
        "Supriyo GHOSH",
        "Xuchao Zhang"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "July 2024",
      "abstract": "Incident management for large cloud services is a complex and tedious process and requires significant amount of manual efforts from on-call engineers (OCEs). OCEs typically leverage data from different stages of the software development lifecycle [SDLC] (e.g., codes, configuration, monitor data, service properties, service dependencies, trouble-shooting documents, etc.) to generate insights for detection, root causing and mitigating of incidents. Recent advancements in large language models [LLMs] (e.g., ChatGPT, GPT-4, Gemini) created opportunities to automatically generate contextual recommendations to the OCEs assisting them to quickly identify and mitigate critical issues. However, existing research typically takes a silo-ed view for solving a certain task in incident management by leveraging data from a single stage of SDLC. In this paper, we demonstrate that augmenting additional contextual data from different stages of SDLC improves the performance of two critically important and practically challenging tasks: (1) automatically generating root cause recommendations for dependency failure related incidents, and (2) identifying ontology of service monitors used for automatically detecting incidents. By leveraging 353 incident and 260 monitor dataset from Microsoft, we demonstrate that augmenting contextual information from different stages of the SDLC improves the performance over State-of-The-Art methods.",
      "url": "https://www.microsoft.com/en-us/research/publication/x-lifecycle-learning-for-cloud-incident-management-using-llms/"
    },
    {
      "title": "Workload estimator using EEG and eye-tracking",
      "authors": [
        "Christine Beauchene",
        "David Johnston",
        "Ivan Tashev",
        "Justin R. Estepp",
        "Nathaniel Bridges",
        "R. Michael Winters",
        "Yu Te Wang"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "July 2024",
      "abstract": "In this paper we present a workload estimator based on biological signals – electroencephalographic and eye-tracking. The workload estimator is person- and session- independent, designed to work in a virtual reality flight simulator environment and is a part of our adaptive training system. The novel component is using objective evaluation of the workload, based on the flight logs, as labels for training the regression neural network. As evaluation parameter is selected the correlation with the objective labels. The paper contains the results from using several feature sets and estimators, where the best estimator achieves correlation with objective labels of 0.84.\nIndex Terms—workload estimation, electroencephalography, eye-tracking, adaptive training system",
      "url": "https://www.microsoft.com/en-us/research/publication/workload-estimator-using-eeg-and-eye-tracking/"
    },
    {
      "title": "Autoregressive Speech Synthesis without Vector Quantization",
      "authors": [
        "Bing Han",
        "Furu Wei",
        "Helen Meng",
        "Jinyu Li",
        "Lingwei Meng",
        "Long Zhou",
        "Sanyuan Chen",
        "Sheng Zhao",
        "Shujie Hu",
        "Shujie Liu",
        "Xixin Wu",
        "Yanqing Liu"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Audio and Acoustics"
      ],
      "publication_date": "July 2024",
      "abstract": "We present MELLE, a novel continuous-valued tokens based language modeling approach for text to speech synthesis (TTS). MELLE autoregressively generates continuous mel-spectrogram frames directly from text condition, bypassing the need for vector quantization, which are originally designed for audio compression and sacrifice fidelity compared to mel-spectrograms. Specifically, (i) instead of cross-entropy loss, we apply regression loss with a proposed spectrogram flux loss function to model the probability distribution of the continuous-valued tokens. (ii) we have incorporated variational inference into MELLE to facilitate sampling mechanisms, thereby enhancing the output diversity and model robustness. Experiments demonstrate that, compared to the two-stage codec language models VALL-E and its variants, the single-stage MELLE mitigates robustness issues by avoiding the inherent flaws of sampling discrete codes, achieves superior performance across multiple metrics, and, most importantly, offers a more streamline paradigm.",
      "url": "https://www.microsoft.com/en-us/research/publication/autoregressive-speech-synthesis-without-vector-quantization/"
    },
    {
      "title": "ChameleonAPI: Automatic and Efficient Customization of Neural Networks for ML Applications",
      "authors": [
        "Chengcheng Wan",
        "Henry Hoffmann",
        "Junchen Jiang",
        "Kuntai Du",
        "Michael Maire",
        "Shan Lu",
        "Yuhan Liu"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Systems and networking"
      ],
      "publication_date": "July 2024",
      "abstract": "ML APIs have greatly relieved application developers of the burden to design and train their own neural network models—classifying objects in an image can now be as simple as one line of Python code to call an API. However, these APIs offer the same pre-trained models regardless of how their output is used by different applications. This can be suboptimal as not all ML inference errors can cause application failures, and the distinction between inference errors that can or cannot cause failures varies greatly across applications.\nTo tackle this problem, we first study 77 real-world applications, which collectively use six ML APIs from two providers, to reveal common patterns of how ML API output affects applications’ decision processes. Inspired by the findings, we propose ChameleonAPI, an optimization framework for ML APIs, which takes effect without changing the application source code. ChameleonAPI provides application developers with a parser that automatically analyzes the application to produce an abstract of its decision process, which is then used to devise an application-specific loss function that only penalizes API output errors critical to the application. ChameleonAPI uses the loss function to efficiently train a neural network model customized for each application and deploys it to serve API invocations from the respective application via existing interface. Compared to a baseline that selects the best-of-all commercial ML API, we show that ChameleonAPI reduces incorrect application decisions by 43%.",
      "url": "https://www.microsoft.com/en-us/research/publication/automatic-and-efficient-customization-of-neural-networks-for-ml-applications/"
    },
    {
      "title": "Optimizing Learning-to-Rank Models for Ex-Post Fair Relevance",
      "authors": [
        "Amit Deshpande",
        "Anand Louis",
        "Eshaan Bhansali",
        "Sruthi Gorantla"
      ],
      "research_areas": [
        "Algorithms",
        "Search and information retrieval"
      ],
      "publication_date": "July 2024",
      "abstract": "Learning-to-rank (LTR) models rank items based on specific features, aiming to maximize ranking utility by prioritizing highly relevant items. However, optimizing only for ranking utility can lead to representational harm and may fail to address implicit bias in relevance scores. Prior studies introduced algorithms to train stochastic ranking models, such as the Plackett-Luce ranking model, that maximize expected ranking utility while achieving fairness in expectation (ex-ante fairness). Still, every sampled ranking may not satisfy group fairness (ex-post fairness). Post-processing methods ensure ex-post fairness; however, the LTR model lacks awareness of this step, creating a mismatch between the objective function the LTR model optimizes and the one it is supposed to optimize. In this paper, we first propose a novel objective where the relevance (or the expected ranking utility) is computed over only those rankings that satisfy given representation constraints for groups of items. We call this the ex-post fair relevance. We then give a framework for training Group-Fair LTR models to maximize our proposed ranking objective.\nLeveraging an efficient sampler for ex-post group-fair rankings and efficient algorithms to train the Plackett-Luce LTR model, we demonstrate their use in training the Group-Fair Plackett-Luce model in our framework. Experiments on MovieLens and Kiva datasets reveal improved fairness and relevance with our group-fair Plackett-Luce model compared to post-processing. In scenarios with implicit bias, our algorithm generally outperforms existing LTR baselines in both fairness and relevance.",
      "url": "https://www.microsoft.com/en-us/research/publication/optimizing-learning-to-rank-models-for-ex-post-fair-relevance-3/"
    },
    {
      "title": "Optimizing Learning-to-Rank Models for Ex-Post Fair Relevance",
      "authors": [
        "Amit Deshpande",
        "Anand Louis",
        "Eshaan Bhansali",
        "Sruthi Gorantla"
      ],
      "research_areas": [
        "Algorithms",
        "Search and information retrieval"
      ],
      "publication_date": "July 2024",
      "abstract": "Learning-to-rank (LTR) models rank items based on specific features, aiming to maximize ranking utility by prioritizing highly relevant items. However, optimizing only for ranking utility can lead to representational harm and may fail to address implicit bias in relevance scores. Prior studies introduced algorithms to train stochastic ranking models, such as the Plackett-Luce ranking model, that maximize expected ranking utility while achieving fairness in expectation (ex-ante fairness). Still, every sampled ranking may not satisfy group fairness (ex-post fairness). Post-processing methods ensure ex-post fairness; however, the LTR model lacks awareness of this step, creating a mismatch between the objective function the LTR model optimizes and the one it is supposed to optimize. In this paper, we first propose a novel objective where the relevance (or the expected ranking utility) is computed over only those rankings that satisfy given representation constraints for groups of items. We call this the ex-post fair relevance. We then give a framework for training Group-Fair LTR models to maximize our proposed ranking objective.\nLeveraging an efficient sampler for ex-post group-fair rankings and efficient algorithms to train the Plackett-Luce LTR model, we demonstrate their use in training the Group-Fair Plackett-Luce model in our framework. Experiments on MovieLens and Kiva datasets reveal improved fairness and relevance with our group-fair Plackett-Luce model compared to post-processing. In scenarios with implicit bias, our algorithm generally outperforms existing LTR baselines in both fairness and relevance.",
      "url": "https://www.microsoft.com/en-us/research/publication/optimizing-learning-to-rank-models-for-ex-post-fair-relevance-2/"
    },
    {
      "title": "CLAVE: An Adaptive Framework for Evaluating Values of LLM Generated Responses",
      "authors": [
        "Jing Yao",
        "Xiaoyuan Yi",
        "Xing Xie"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "July 2024",
      "abstract": "The rapid progress in Large Language Models (LLMs) poses potential risks such as generating unethical content. Assessing LLMs’ values can help expose their misalignment, but relies on reference-free evaluators, e.g., fine-tuned LLMs or close-source ones like GPT-4, to identify values reflected in generated responses. Nevertheless, these evaluators face two challenges in open-ended value evaluation: they should align with changing human value definitions with minimal annotation, against their own bias (adaptability), and detect varying value expressions and scenarios robustly (generalizability). To handle these challenges, we introduce CLAVE, a novel framework which integrates two complementary LLMs, a large one to extract high-level value concepts from a few human labels, leveraging its extensive knowledge and generalizability, and a smaller one fine-tuned on such concepts to better align with human value understanding. This dual-model approach enables calibration with any value systems using<100 human-labeled samples per value type. Then we present ValEval, a comprehensive dataset comprising 13k+ (text,value,label) tuples across diverse domains, covering three major value systems. We benchmark the capabilities of 12+ popular LLM evaluators and analyze their strengths and weaknesses. Our findings reveal that combining fine-tuned small models and prompt-based large ones serves as a superior balance in value evaluation.",
      "url": "https://www.microsoft.com/en-us/research/publication/clave-an-adaptive-framework-for-evaluating-values-of-llm-generated-responses/"
    },
    {
      "title": "Accuracy is Not All You Need",
      "authors": [
        "Abhinav Dutta",
        "Nipun Kwatra",
        "Ramachandran Ramjee",
        "Sanjeev Krishnan"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "July 2024",
      "abstract": "When Large Language Models (LLMs) are compressed using techniques such as quantization, the predominant way to demonstrate the validity of such techniques is by measuring the model’s accuracy on various benchmarks.If the accuracies of the baseline model and the compressed model are close, it is assumed that there was negligible degradation in quality.However, even when the accuracy of baseline and compressed model are similar, we observe the phenomenon of flips, wherein answers change from correct to incorrect and vice versa in proportion.We conduct a detailed study of metrics across multiple compression techniques, models and datasets, demonstrating that the behavior of compressed models as visible to end-users is often significantly different from the baseline model, even when accuracy is similar.We further evaluate compressed models qualitatively and quantitatively using MT-Bench and show that compressed models are significantly worse than baseline models in this free-form generative task.Thus, we argue that compression techniques should also be evaluated using distance metrics.We propose two such metrics, KL-Divergence and flips, and show that they are well correlated.",
      "url": "https://www.microsoft.com/en-us/research/publication/accuracy-is-not-all-you-need/"
    },
    {
      "title": "CodePlan: Repository-level Coding using LLMs and Planning",
      "authors": [
        "Aditya Kanade",
        "Arun Shankar Iyer",
        "Atharv Sonwane",
        "B. Ashok",
        "Ramakrishna Bairi",
        "Shashank Shet",
        "Sriram Rajamani",
        "Suresh Parthasarathy",
        "Vageesh D C"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Programming languages and software engineering"
      ],
      "publication_date": "July 2024",
      "abstract": "Software engineering activities such as package migration, fixing error reports from static analysis or testing, and adding type annotations or other specifications to a codebase, involve pervasively editing the entire repository of code. We formulate these activities as repository-level coding tasks. Recent tools like GitHub Copilot, which are powered by Large Language Models (LLMs), have succeeded in offering high-quality solutions to localized coding problems. Repository-level coding tasks are more involved and cannot be solved directly using LLMs, since code within a repository is inter-dependent and the entire repository may be too large to fit into the prompt. We frame repository-level coding as a planning problem and present a task-agnostic, neuro-symbolic framework called CodePlan to solve it. CodePlan synthesizes a multi-step chain-of-edits (plan), where each step results in a call to an LLM on a code location with context derived from the entire repository, previous code changes and task-specific instructions. CodePlan is based on a novel combination of an incremental dependency analysis, a change may-impact analysis and an adaptive planning algorithm (symbolic components) with the neural LLMs. We evaluate the effectiveness of CodePlan on two repository-level tasks: package migration (C#) and temporal code edits (Python). Each task is evaluated on multiple code repositories, each of which requires inter-dependent changes to many files (between 2–97 files). Coding tasks of this level of complexity have not been automated using LLMs before. Our results show that CodePlan has better match with the ground truth compared to baselines. CodePlan is able to get 5/7 repositories to pass the validity checks (i.e., to build without errors and make correct code edits) whereas the baselines (without planning but with the same type of contextual information as CodePlan) cannot get any of the repositories to pass them. We provide our (non-proprietary) data, evaluation scripts and supplementary material at https://github.com/microsoft/codeplan (opens in new tab).",
      "url": "https://www.microsoft.com/en-us/research/publication/codeplan-repository-level-coding-using-llms-and-planning-2/"
    },
    {
      "title": "NuTime: Numerically Multi-Scaled Embedding for Large-Scale Time Series Pretraining",
      "authors": [
        "Chenguo Lin",
        "Congrui Huang",
        "Jiang Bian",
        "Stephen Lin",
        "Wei Cao",
        "Xumeng Wen",
        "Zhirong Wu"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "July 2024",
      "abstract": "Recent research on time-series self-supervised models shows great promise in learning semantic representations. However, it has been limited to small-scale datasets, e.g., thousands of temporal sequences. In this work, we make key technical contributions that are tailored to the numerical properties of time-series data and allow the model to scale to large datasets, e.g., millions of temporal sequences. We adopt the Transformer architecture by first partitioning the input into non-overlapping windows. Each window is then characterized by its normalized shape and two scalar values denoting the mean and standard deviation within each window. To embed scalar values that may possess arbitrary numerical amplitudes in a high-dimensional space, we propose a numerically multi-scaled embedding module enumerating all possible numerical scales for the scalars. The model undergoes pretraining with a simple contrastive objective on a large-scale dataset over a million sequences collected by merging existing public data. We study its transfer performance on a number of univariate and multivariate classification tasks, few shot learning, unsupervised clustering and anomaly detection benchmarks. Our method exhibits remarkable improvement against previous pretraining approaches and establishes the new state of the art, even compared with domain-specific non-learning-based methods.",
      "url": "https://www.microsoft.com/en-us/research/publication/nutime-numerically-multi-scaled-embedding-for-large-scale-time-series-pretraining/"
    },
    {
      "title": "Towards Effective AI Support for Developers: A Survey of Desires and Concerns",
      "authors": [
        "Brian Houck",
        "Mansi Khemka"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Human-computer interaction"
      ],
      "publication_date": "July 2024",
      "abstract": "“Towards Effective AI Support for Developers: A Survey of Desires and Concerns,” co-authored with Mansi Khemka, explores developers’ perspectives on AI integration in their workflows. This study prioritizes developers’ voices, revealing their top desires for AI assistance and major concerns. By conducting a comprehensive survey among 791 Microsoft developers, the research identifies key areas where AI can enhance productivity and highlights significant apprehensions about AI’s practicality and reliability. The findings provide actionable insights for product teams and leaders to create AI tools that truly support developers’ needs.",
      "url": "https://www.microsoft.com/en-us/research/publication/towards-effective-ai-support-for-developers-a-survey-of-desires-and-concerns/"
    },
    {
      "title": "VeriSMo: A Verified Security Module for Confidential VMs",
      "authors": [
        "Anjali",
        "Chris Hawblitzel",
        "Sishuai Gong",
        "Weidong Cui",
        "Weiteng Chen",
        "Ziqiao Zhou"
      ],
      "research_areas": [
        "Security, privacy, and cryptography"
      ],
      "publication_date": "July 2024",
      "abstract": "Hardware vendors have introduced confidential VM architectures (e.g., AMD SEV-SNP, Intel TDX and Arm CCA) in recent years. They eliminate the trust in the hypervisor and lead to the need for security modules such as AMD Secure VMService Module (SVSM). These security modules aim to provide a guest with security features that previously were offered by the hypervisor. Since the security of such modules is critical, Rust is used to implement them for its known memory safety features. However, using Rust for implementation does not guarantee correctness, and the use of unsafe Rust compromises the memory safety guarantee.\nIn this paper, we introduce VERISMO, the first verified security module for confidential VMs on AMD SEV-SNP. VERISMO is fully functional and provides security features such as code integrity, runtime measurement, and secret management. More importantly, as a Rust-based implementation, VERISMO is fully verified for functional correctness, secure information flow, and VM confidentiality and integrity. The key challenge in verifying VERISMO is that the untrusted hypervisor can interrupt VERISMO’s execution and modify the hardware state at any time. We address this challenge by dividing verification into two layers. The upper layer handles the concurrent hypervisor execution, while the lower layer handles VERISMO’s own concurrent execution. When compared with a C-based implementation, VERISMO achieves similar performance. When verifying VERISMO, we identified a subtle requirement for VM confidentiality and found that it was overlooked by AMD SVSM. This demonstrates the necessity for formal verification.",
      "url": "https://www.microsoft.com/en-us/research/publication/verismo-a-verified-security-module-for-confidential-vms/"
    },
    {
      "title": "MonitorAssistant: Simplifying Cloud Service Monitoring via Large Language Models",
      "authors": [
        "Changhua Pei",
        "Chaoyun Zhang",
        "Chetan Bansal",
        "Dan Pei",
        "Dongmei Zhang",
        "Minghua Ma",
        "Qingwei Lin 林庆维",
        "Saravan Rajmohan",
        "Si Qin",
        "Yingnong Dang",
        "Yu Kang",
        "Zhaoyang Yu"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "July 2024",
      "abstract": "In large-scale cloud service systems, monitoring metric data and conducting anomaly detection is an important way to maintain reliability and stability. However, great disparity exists between academic approaches and industrial practice to anomaly detection. Industry predominantly uses simple, efficient methods due to better interpretability and ease of implementation. In contrast, academically favor deep-learning methods, despite their advanced capabilities, face practical challenges in real-world applications. To address these challenges, this paper introduces MonitorAssistant, an end-to-end practical anomaly detection system via Large Language Models. MonitorAssistant automates model configuration recommendation achieving knowledge inheritance and alarm interpretation with guidance-oriented anomaly reports, facilitating a more intuitive engineer-system interaction through natural language. By deploying MonitorAssistant in Microsoft’s cloud service system, we validate its efficacy and practicality, marking a significant advancement in the field of practical anomaly detection for large-scale cloud services.",
      "url": "https://www.microsoft.com/en-us/research/publication/monitorassistant-simplifying-cloud-service-monitoring-via-large-language-models/"
    },
    {
      "title": "A Call for Research on Storage Emissions",
      "authors": [
        "Aaron Ogus",
        "Daniel S. Berger",
        "Fiodar Kazhamiaka",
        "G. R. Ganger",
        "George Amvrosiadis",
        "Kali Frost",
        "Maneesh Sah",
        "Nathan Beckmann",
        "Ricardo Bianchini",
        "Rodrigo Fonseca",
        "Sara McAllister"
      ],
      "research_areas": [
        "Systems and networking"
      ],
      "publication_date": "July 2024",
      "abstract": "Major cloud providers have committed to lowering carbon emissions by 2030 across their datacenters, and research has contributed many ideas on how this may be achieved. However, a major contributor to datacenter emissions has not received enough attention: storage. Storage – everything from file storage to inter-application messaging in datacenters – causes 33% of operational emissions and 61% of embodied emissions in Azure’s general-purpose cloud, based on a recent study. This paper identifies key sources of both operational and embodied emissions within distributed storage in datacenters. We also discuss strategies to reduce storage emissions and their challenges due to storage’s fundamentally stateful nature.",
      "url": "https://www.microsoft.com/en-us/research/publication/a-call-for-research-on-storage-emissions/"
    },
    {
      "title": "RUBICON: Rubric-based Evaluation of Domain Specific Human-AI Conversations",
      "authors": [
        "Arjun Radhakrishna",
        "Gustavo Soares",
        "Param Biyani",
        "Sumit Gulwani",
        "Yasharth Bajpai"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Human-computer interaction",
        "Programming languages and software engineering"
      ],
      "publication_date": "July 2024",
      "abstract": "The evaluation of conversational assistants, such as GitHub Copilot Chat, poses a significant challenge for tool builders in the domain of Software Engineering. These assistants rely on language models and chat-based user experiences, making evaluating them according to the quality of the Human-AI conversations complicated. Existing general-purpose conversational quality metrics from literature are inadequate for assessing domain-specific dialogues due to their lack of context sensitivity. In this paper, we present RUBICON, a technique for evaluating domain-specific Human-AI conversations. RUBICON leverages large language models to generate rubrics for assessing conversation quality. It employs a selection process to choose the subset of rubrics based on their performance in scoring conversations. In our experiments, RUBICON effectively learns to differentiate conversation quality, achieving higher accuracy and yield rates than existing baselines.",
      "url": "https://www.microsoft.com/en-us/research/publication/rubicon-rubric-based-evaluation-of-domain-specific-human-ai-conversations/"
    },
    {
      "title": "ImDiffusion: Imputed Diffusion Models for Multivariate Time Series Anomaly Detection",
      "authors": [
        "Bo Li",
        "C. Zhang",
        "Dongmei Zhang",
        "Minghua Ma",
        "Qingwei Lin 林庆维",
        "Ruomeng Ding",
        "S. Rajmohan",
        "Shilin He",
        "Yudong Liu",
        "Yuhang Chen"
      ],
      "research_areas": [
        "Algorithms",
        "Artificial intelligence",
        "Data platforms and analytics",
        "Programming languages and software engineering",
        "Systems and networking"
      ],
      "publication_date": "July 2024",
      "abstract": "Anomaly detection in multivariate time series data is of paramount importance for ensuring the efficient operation of large-scale systems across diverse domains. However, accurately detecting anomalies in such data poses significant challenges. Existing approaches, including forecasting and reconstruction-based methods, struggle to address these challenges effectively. To overcome these limitations, we propose a novel anomaly detection framework named ImDiffusion, which combines time series imputation and diffusion models to achieve accurate and robust anomaly detection. The imputation-based approach employed by ImDiffusion leverages the information from neighboring values in the time series, enabling precise modeling of temporal and inter-correlated dependencies, reducing uncertainty in the data, thereby enhancing the robustness of the anomaly detection process. ImDiffusion further leverages diffusion models as time series imputers to accurately capturing complex dependencies. We leverage the step-by-step denoised outputs generated during the inference process to serve as valuable signals for anomaly prediction, resulting in improved accuracy and robustness of the detection process. We evaluate the performance of ImDiffusion via extensive experiments on benchmark datasets. The results demonstrate that our proposed framework significantly outperforms state-of-the-art approaches in terms of detection accuracy and timeliness. ImDiffusion is further integrated into the real production system in Microsoft and observe a remarkable 11.4% increase in detection F1 score compared to the legacy approach. To the best of our knowledge, ImDiffusion represents a pioneering approach that combines imputation-based techniques with time series anomaly detection, while introducing the novel use of diffusion models to the field.",
      "url": "https://www.microsoft.com/en-us/research/publication/imdiffusion-imputed-diffusion-models-for-multivariate-time-series-anomaly-detection/"
    },
    {
      "title": "A Generative Approach to Control Complex Physical Systems",
      "authors": [
        "Haodong Feng",
        "Long Wei",
        "Peiyan Hu",
        "Rui Wang",
        "Ruiqi Feng",
        "Tailin Wu",
        "Tao Zhang",
        "Yixuan Du",
        "Yue Wang",
        "Zhi-Ming Ma"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "July 2024",
      "abstract": "Controlling the evolution of complex physical systems is a fundamental task across science and engineering. Classical techniques suffer from limited applicability or huge computational costs. On the other hand, recent deep learning and reinforcement learning-based approaches often struggle to optimize long-term control sequences under the constraints of system dynamics. In this work, we introduce Diffusion Physical systems Control (DiffPhyCon), a new class of method to address the physical systems control problem. DiffPhyCon excels by simultaneously minimizing both the learned generative energy function and the predefined control objectives across the entire trajectory and control sequence. Thus, it can explore globally and plan near-optimal control sequences. Moreover, we enhance DiffPhyCon with prior reweighting, enabling the discovery of control sequences that significantly deviate from the training distribution. We test our method on three tasks: 1D Burgers’ equation, 2D jellyfish movement control, and 2D high-dimensional smoke control, where our generated jellyfish dataset is released as a benchmark for complex physical system control research. Our method outperforms widely applied classical approaches and state-of-the-art deep learning and reinforcement learning methods. Notably, DiffPhyCon unveils an intriguing fast-close-slow-open pattern observed in the jellyfish, aligning with established findings in the field of fluid dynamics. The project website, jellyfish dataset, and code can be found at https://github.com/AI4Science-WestlakeU/diffphycon (opens in new tab).",
      "url": "https://www.microsoft.com/en-us/research/publication/a-generative-approach-to-control-complex-physical-systems/"
    },
    {
      "title": "Etalon: Holistic Performance Evaluation Framework for LLM Inference Systems",
      "authors": [
        "Alexey Tumanov",
        "Amey Agrawal",
        "Anmol Agarwal",
        "Jayashree Mohan",
        "Nipun Kwatra",
        "Nitin Kedia",
        "Ramachandran Ramjee",
        "Souvik Kundu"
      ],
      "research_areas": [
        "Systems and networking"
      ],
      "publication_date": "July 2024",
      "abstract": "Serving large language models (LLMs) in production can incur substantial costs, which has prompted recent advances in inference system optimizations. Today, these systems are evaluated against conventional latency and throughput metrics (eg. TTFT, TBT, Normalised Latency and TPOT). However, these metrics fail to fully capture the nuances of LLM inference, leading to an incomplete assessment of user-facing performance crucial for real-time applications such as chat and translation. In this paper, we first identify the pitfalls of current performance metrics in evaluating LLM inference systems. We then propose Etalon, a comprehensive performance evaluation framework that includes fluidity-index — a novel metric designed to reflect the intricacies of the LLM inference process and its impact on real-time user experience. Finally, we evaluate various existing open-source platforms and model-as-a-service offerings using Etalon, discussing their strengths and weaknesses. Etalon is available at this https URL.",
      "url": "https://www.microsoft.com/en-us/research/publication/etalon-holistic-performance-evaluation-framework-for-llm-inference-systems/"
    },
    {
      "title": "AgentInstruct: Toward Generative Teaching with Agentic Flows",
      "authors": [
        "Ahmed Awadallah",
        "Andres Codas",
        "Arindam Mitra",
        "Corby Rosset",
        "Dany Rouhana",
        "Fillipe Silva",
        "Guoqing Zheng",
        "Hamed Khanpour",
        "Luciano Del Corro",
        "Olga Vrousgou",
        "Shweti Mahajan",
        "Wei-ge Chen",
        "Yadong Lu",
        "Yash Lara"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "July 2024",
      "abstract": "Synthetic data is becoming increasingly important for accelerating the development of language models, both large and small. Despite several successful use cases, researchers also raised concerns around model collapse and drawbacks of imitating other models. This discrepancy can be attributed to the fact that synthetic data varies in quality and diversity. Effective use of synthetic data usually requires significant human effort in curating the data. We focus on using synthetic data for post-training, specifically creating data by powerful models to teach a new skill or behavior to another model, we refer to this setting as Generative Teaching. We introduce AgentInstruct, an extensible agentic framework for automatically creating large amounts of diverse and high-quality synthetic data. AgentInstruct can create both the prompts and responses, using only raw data sources like text documents and code files as seeds. We demonstrate the utility of AgentInstruct by creating a post training dataset of 25M pairs to teach language models different skills, such as text editing, creative writing, tool usage, coding, reading comprehension, etc. The dataset can be used for instruction tuning of any base model. We post-train Mistral-7b with the data. When comparing the resulting model Orca-3 to Mistral-7b-Instruct (which uses the same base model), we observe significant improvements across many benchmarks. For example, 40% improvement on AGIEval, 19% improvement on MMLU, 54% improvement on GSM8K, 38% improvement on BBH and 45% improvement on AlpacaEval. Additionally, it consistently outperforms other models such as LLAMA-8B-instruct and GPT-3.5-turbo.",
      "url": "https://www.microsoft.com/en-us/research/publication/agentinstruct-toward-generative-teaching-with-agentic-flows/"
    },
    {
      "title": "Video In-context Learning",
      "authors": [
        "Jiang Bian",
        "Junliang Guo",
        "Li Zhao",
        "Linli Xu",
        "Tianyu He",
        "Wentao Zhang"
      ],
      "research_areas": [
        "Computer vision"
      ],
      "publication_date": "July 2024",
      "abstract": "In-context learning for vision data has been underexplored compared with that in natural language. Previous works studied image in-context learning, urging models to generate a single image guided by demonstrations. In this paper, we propose and study video in-context learning, where the model starts from an existing video clip and generates diverse potential future sequences, each semantically guided by the prompted video demonstrations. To achieve this, we provide a clear definition of the task, and train an autoregressive Transformer on video datasets. We thoroughly analyze the effect of different datasets and represent frames as discrete tokens, and then model them by next token predictions. We design various evaluation metrics, including both objective and subjective measures, to demonstrate the visual quality and semantic accuracy of generation results. Our model follows the scaling law and generates high-quality video clips that accurately align with the semantic guidance provided by in-context examples.",
      "url": "https://www.microsoft.com/en-us/research/publication/video-in-context-learning/"
    },
    {
      "title": "Virtual Voices: Exploring Individual Differences in Written and Verbal Participation in Meetings",
      "authors": [
        "Lev Tankelevitch",
        "Liana Kreamer",
        "Sean Rintel",
        "Steven G. Rogelberg"
      ],
      "research_areas": [
        "Human-computer interaction",
        "Social sciences"
      ],
      "publication_date": "July 2024",
      "abstract": "A key component of team performance is participation among group members. One widespread organizational function that provides a stage for participation is the workplace meeting. With the shift to remote work, roughly half of all meetings are conducted virtually. One encouraging opportunity that can elevate meeting participation in this context is the use of written chat. Chat offers a second avenue of participation during a meeting, where attendees can synchronously contribute to the conversation through writing. This study explores factors influencing participation in virtual meetings, drawing on individual differences (status characteristics theory), psychological safety perceptions, and group communication. Results reveal gender and job level nuances: women engage more in chat, while men verbally participate more frequently. Further, we found men highest in job level verbally contribute the most in virtual meetings, whereas women highest in job level use the chat the most frequently. Regarding type of chats sent, women use emoji reactions more often than men, and men send more attachments than women. Additionally, results revealed psychological safety moderated the relationship between job level and overall chat participation, such that employees low in job level with high perceptions of psychological safety sent more chats than their counterparts. This study provides insights into communication patterns and the impact of psychological safety on participation in technology-mediated spaces.",
      "url": "https://www.microsoft.com/en-us/research/publication/virtual-voices-exploring-individual-differences-in-written-and-verbal-participation-in-meetings/"
    },
    {
      "title": "Optimizing Learning-to-Rank Models for Ex-Post Fair Relevance",
      "authors": [
        "Amit Deshpande",
        "Anand Louis",
        "Eshaan Bhansali",
        "Sruthi Gorantla"
      ],
      "research_areas": [
        "Algorithms",
        "Search and information retrieval"
      ],
      "publication_date": "July 2024",
      "abstract": "Learning-to-rank (LTR) models rank items based on specific features, aiming to maximize ranking utility by prioritizing highly relevant items. However, optimizing only for ranking utility can lead to representational harm and may fail to address implicit bias in relevance scores. Prior studies introduced algorithms to train stochastic ranking models, such as the Plackett-Luce ranking model, that maximize expected ranking utility while achieving fairness in expectation (ex-ante fairness). Still, every sampled ranking may not satisfy group fairness (ex-post fairness). Post-processing methods ensure ex-post fairness; however, the LTR model lacks awareness of this step, creating a mismatch between the objective function the LTR model optimizes and the one it is supposed to optimize. In this paper, we first propose a novel objective where the relevance (or the expected ranking utility) is computed over only those rankings that satisfy given representation constraints for groups of items. We call this the ex-post fair relevance. We then give a framework for training Group-Fair LTR models to maximize our proposed ranking objective.\nLeveraging an efficient sampler for ex-post group-fair rankings and efficient algorithms to train the Plackett-Luce LTR model, we demonstrate their use in training the Group-Fair Plackett-Luce model in our framework. Experiments on MovieLens and Kiva datasets reveal improved fairness and relevance with our group-fair Plackett-Luce model compared to post-processing. In scenarios with implicit bias, our algorithm generally outperforms existing LTR baselines in both fairness and relevance.",
      "url": "https://www.microsoft.com/en-us/research/publication/optimizing-learning-to-rank-models-for-ex-post-fair-relevance/"
    },
    {
      "title": "FairyWREN: A Sustainable Cache for Emerging Write-Read-Erase Flash Interfaces",
      "authors": [
        "Benjamin Berg",
        "Daniel S. Berger",
        "G. R. Ganger",
        "George Amvrosiadis",
        "Nathan Beckmann",
        "Sara McAllister",
        "Sherry Wang"
      ],
      "research_areas": [
        "Systems and networking"
      ],
      "publication_date": "July 2024",
      "abstract": "Datacenters need to reduce embodied carbon emissions, particularly for ﬂash, which accounts for 40% of embodied carbon in servers. However, decreasing ﬂash’s embodied emissions is challenging due to ﬂash’s limited write endurance, which more than halves with each generation of denser ﬂash. Reducing embodied emissions requires extending ﬂash life-time, stressing its limited write endurance even further. The legacy Logical Block-Addressable Device (LBAD) interface exacerbates the problem by forcing devices to perform garbage collection, leading to even more writes.\nFlash-based caches in particular write frequently, limiting the lifetimes and densities of the devices they use. These ﬂash caches illustrate the need to break away from LBAD and switch to the new Write-Read-Erase iNterfaces (WREN) now coming to market. WREN affords applications con-trol over data placement and garbage collection. We present FairyW REN 1 , a ﬂash cache designed for WREN. FairyW REN reduces writes by co-designing caching policies and ﬂash garbage collection. FairyW REN provides a 12.5 × write reduction over state-of-the-art LBAD caches. This decrease in writes allows ﬂash devices to last longer, decreasing ﬂash cost by 35% and ﬂash carbon emissions by 33%.",
      "url": "https://www.microsoft.com/en-us/research/publication/fairyw-ren-a-sustainable-cache-for-emerging-write-read-erase-flash-interfaces/"
    },
    {
      "title": "How Far Can Fairness Constraints Help Recover From Biased Data?",
      "authors": [
        "Amit Deshpande",
        "Mohit Sharma"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "July 2024",
      "abstract": "A general belief in fair classification is that fairness constraints incur a trade-off with accuracy, which biased data may worsen. Contrary to this belief, Blum&Stangl (2019) show that fair classification with equal opportunity constraints even on extremely biased data can recover optimally accurate and fair classifiers on the original data distribution. Their result is interesting because it demonstrates that fairness constraints can implicitly rectify data bias and simultaneously overcome a perceived fairness-accuracy trade-off. Their data bias model simulates under-representation and label bias in underprivileged population, and they show the above result on a stylized data distribution with i.i.d. label noise, under simple conditions on the data distribution and bias parameters. We propose a general approach to extend the result of Blum&Stangl (2019) to different fairness constraints, data bias models, data distributions, and hypothesis classes. We strengthen their result, and extend it to the case when their stylized distribution has labels with Massart noise instead of i.i.d. noise. We prove a similar recovery result for arbitrary data distributions using fair reject option classifiers. We further generalize it to arbitrary data distributions and arbitrary hypothesis classes, i.e., we prove that for any data distribution, if the optimally accurate classifier in a given hypothesis class is fair and robust, then it can be recovered through fair classification with equal opportunity constraints on the biased distribution whenever the bias parameters satisfy certain simple conditions. Finally, we show applications of our technique to time-varying data bias in classification and fair machine learning pipelines.",
      "url": "https://www.microsoft.com/en-us/research/publication/how-far-can-fairness-constraints-help-recover-from-biased-data/"
    },
    {
      "title": "Certainly Uncertain: A Benchmark and Metric for Multimodal Epistemic and Aleatoric Awareness",
      "authors": [
        "Anas Awadalla",
        "J. Park",
        "Jack Hessel",
        "Khyathi Raghavi Chandu",
        "Lijuan Wang",
        "Linjie Li",
        "Ximing Lu",
        "Yejin Choi"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Computer vision"
      ],
      "publication_date": "July 2024",
      "abstract": "The ability to acknowledge the inevitable uncertainty in their knowledge and reasoning is a prerequisite for AI systems to be truly truthful and reliable. In this paper, we present a taxonomy of uncertainty specific to vision-language AI systems, distinguishing between epistemic uncertainty (arising from a lack of information) and aleatoric uncertainty (due to inherent unpredictability), and further explore finer categories within. Based on this taxonomy, we synthesize a benchmark dataset, CertainlyUncertain, featuring 178K visual question answering (VQA) samples as contrastive pairs. This is achieved by 1) inpainting images to make previously answerable questions into unanswerable ones; and 2) using image captions to prompt large language models for both answerable and unanswerable questions. Additionally, we introduce a new metric confidence-weighted accuracy, that is well correlated with both accuracy and calibration error, to address the shortcomings of existing metrics.",
      "url": "https://www.microsoft.com/en-us/research/publication/certainly-uncertain-a-benchmark-and-metric-for-multimodal-epistemic-and-aleatoric-awareness/"
    },
    {
      "title": "Evaluating LLM-driven User-Intent Formalization for Verification-Aware Languages",
      "authors": [
        "Shuvendu Lahiri"
      ],
      "research_areas": [
        "Programming languages and software engineering"
      ],
      "publication_date": "July 2024",
      "abstract": "Verification-aware programming languages such as Dafny and F* provide means to formally specify and prove properties of programs. Although the problem of checking an implementation against a specification can be defined mechanically, there is no algorithmic way of ensuring the correctness of the user-intent formalization for programs — that a specification adheres to the user’s intent behind the program. The intent or requirement is expressed informally in natural language and the specification is a formal artefact. The advent of large language models (LLMs) has made strides bridging the gap between informal intent and formal program implementations recently, driven in large parts due to benchmarks and automated metrics for evaluation.\nRecent work has proposed evaluating {\\it user-intent formalization} problem for mainstream programming languages~\\cite{endres-fse24}. However, such an approach does not readily extend to verification-aware languages that support rich specifications (containing quantifiers and ghost variables) that cannot be evaluated through dynamic execution. Previous work also required generating program mutants using LLMs to create the benchmark. We advocate an alternate approach of {\\it symbolically testing specifications} to provide an intuitive metric for evaluating the quality of specifications for verification-aware languages. We demonstrate that our automated metric agrees closely with mostly GPT-4 generated and human-labeled dataset of roughly 150 Dafny specifications for the popular MBPP code-generation benchmark, yet demonstrates cases where the human labeling is not perfect. We believe our work provides a stepping stone to enable the establishment of a benchmark and research agenda for the problem of user-intent formalization for programs.",
      "url": "https://www.microsoft.com/en-us/research/publication/evaluating-llm-driven-user-intent-formalization-for-verification-aware-languages/"
    },
    {
      "title": "VisEval: A Benchmark for Data Visualization in the Era of Large Language Models",
      "authors": [
        "Jiahang Xu",
        "Kan Ren",
        "Nan Chen",
        "Yuge Zhang",
        "Yuqing Yang"
      ],
      "research_areas": [
        "Human language technologies",
        "Human-computer interaction"
      ],
      "publication_date": "July 2024",
      "abstract": "Translating natural language to visualization (NL2VIS) has shown great promise for visual data analysis, but it remains a challenging task that requires multiple low-level implementations, such as natural language processing and visualization design. Recent advancements in pre-trained large language models (LLMs) are opening new avenues for generating visualizations from natural language. However, the lack of a comprehensive and reliable benchmark hinders our understanding of LLMs’ capabilities in visualization generation. In this paper, we address this gap by proposing a new NL2VIS benchmark called VisEval. Firstly, we introduce a high-quality and large-scale dataset. This dataset includes 2,524 representative queries covering 146 databases, paired with accurately labeled ground truths. Secondly, we advocate for a comprehensive automated evaluation methodology covering multiple dimensions, including validity, legality, and readability. By systematically scanning for potential issues with a number of heterogeneous checkers, VisEval provides reliable and trustworthy evaluation outcomes. We run VisEval on a series of state-of-the-art LLMs. Our evaluation reveals prevalent challenges and delivers essential insights for future advancements.",
      "url": "https://www.microsoft.com/en-us/research/publication/viseval-a-benchmark-for-data-visualization-in-the-era-of-large-language-models/"
    },
    {
      "title": "Differentially Private Synthetic Data via Foundation Model APIs 2: Text",
      "authors": [
        "Arturs Backurs",
        "Bo Li",
        "Chulin Xie",
        "Da Yu",
        "Haotian Jiang",
        "Harsha Nori",
        "Huishuai Zhang",
        "Huseyin Inan",
        "Sergey Yekhanin",
        "Sivakanth Gopi",
        "Yin Tat Lee",
        "Zinan Lin"
      ],
      "research_areas": [
        "Algorithms",
        "Artificial intelligence",
        "Mathematics",
        "Security, privacy, and cryptography"
      ],
      "publication_date": "July 2024",
      "abstract": "Text data has become extremely valuable due to the emergence of machine learning algorithms that learn from it. A lot of high-quality text data generated in the real world is private and therefore cannot be shared or used freely due to privacy concerns. Generating synthetic replicas of private text data with a formal privacy guarantee, i.e., differential privacy (DP), offers a promising and scalable solution. However, existing methods necessitate DP finetuning of large language models (LLMs) on private data to generate DP synthetic data. This approach is not viable for proprietary LLMs (e.g., GPT-3.5) and also demands considerable computational resources for open-source LLMs. Lin et al. (2024) recently introduced the Private Evolution (PE) algorithm to generate DP synthetic images with only API access to diffusion models. In this work, we propose an augmented PE algorithm, named Aug-PE, that applies to the complex setting of text. We use API access to an LLM and generate DP synthetic text without any model training. We conduct comprehensive experiments on three benchmark datasets. Our results demonstrate that Aug-PE produces DP synthetic text that yields competitive utility with the SOTA DP finetuning baselines. This underscores the feasibility of relying solely on API access of LLMs to produce high-quality DP synthetic texts, thereby facilitating more accessible routes to privacy-preserving LLM applications.",
      "url": "https://www.microsoft.com/en-us/research/publication/differentially-private-synthetic-data-via-foundation-model-apis-2-text/"
    },
    {
      "title": "CORE: Resolving Code Quality Issues using LLMs",
      "authors": [
        "Aditya Kanade (kanadeaditya)",
        "Atharv Sonwane",
        "Jui Pradhan",
        "Nagarajan Natarajan",
        "Nalin Wadhwa",
        "Sriram Rajamani (sriram)",
        "Suresh Parthasarathy (supartha)",
        "Surya Prakash Sahu"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Programming languages and software engineering"
      ],
      "publication_date": "July 2024",
      "abstract": "As software projects progress, quality of code assumes paramount importance as it affects reliability, maintainability and security of software. For this reason, static analysis tools are used in developer workflows to flag code quality issues. However, developers need to spend extra efforts to revise their code to improve code quality based on the tool findings. In this work, we investigate the use of (instruction-following) large language models (LLMs) to assist developers in revising code to resolve code quality issues.\nWe present a tool, CORE (short for COde REvisions), architected using a pair of LLMs organized as a duo comprised of a proposer and a ranker. Providers of static analysis tools recommend ways to mitigate the tool warnings and developers follow them to revise their code. The proposer LLM of CORE takes the same set of recommendations and applies them to generate candidate code revisions. The candidates which pass the static quality checks are retained. However, the LLM may introduce subtle, unintended functionality changes which may go undetected by the static analysis. The ranker LLM evaluates the changes made by the proposer using a rubric that closely follows the acceptance criteria that a developer would enforce. CORE uses the scores assigned by the ranker LLM to rank the candidate revisions before presenting them to the developer.\nWe conduct a variety of experiments on two public benchmarks to show the ability of CORE:\n\nto generate code revisions acceptable to both static analysis tools and human reviewers (the latter evaluated with user study on a subset of the Python benchmark),\nto reduce human review efforts by detecting and eliminating revisions with unintended changes,\nto readily work across multiple languages (Python and Java), static analysis tools (CodeQL and SonarQube) and quality checks (52 and 10 checks, respectively),\nand\nto achieve fix rate comparable to a rule-based automated program repair tool but with much smaller engineering efforts (on the Java benchmark).\n\nCORE could revise 59.2% Python files (across 52 quality checks) so that they pass scrutiny by both a tool and a human reviewer. The ranker LLM reduced false positives by 25.8% in these cases. CORE produced revisions that passed the static analysis tool in 76.8% Java files (across 10 quality checks) comparable to 78.3% of a specialized program repair tool, with significantly much less engineering efforts. We release code, data, and supplementary material publicly at http://aka.ms/COREMSRI (opens in new tab).",
      "url": "https://www.microsoft.com/en-us/research/publication/core-resolving-code-quality-issues-using-llms/"
    },
    {
      "title": "MixDQ: Memory-Efficient Few-Step Text-to-Image Diffusion Models with Metric-Decoupled Mixed Precision Quantization",
      "authors": [
        "Enshu Liu",
        "Guohao Dai",
        "Guyue Huang",
        "Shengen Yan",
        "Tianchen Zhao",
        "Tongcheng Fang",
        "Xuefei Ning",
        "Yu Wang",
        "Zinan Lin"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Computer vision"
      ],
      "publication_date": "July 2024",
      "abstract": "Diffusion models have achieved significant visual generation quality. However, their significant computational and memory costs pose challenge for their application on resource-constrained mobile devices or even desktop GPUs. Recent few-step diffusion models reduces the inference time by reducing the denoising steps. However, their memory consumptions are still excessive. The Post Training Quantization (PTQ) replaces high bit-width FP representation with low-bit integer values (INT4/8) , which is an effective and efficient technique to reduce the memory cost. However, when applying to few-step diffusion models, existing quantization methods face challenges in preserving both the image quality and text alignment. To address this issue, we propose an mixed-precision quantization framework – MixDQ. Firstly, We design specialized BOS-aware quantization method for highly sensitive text embedding quantization. Then, we conduct metric-decoupled sensitivity analysis to measure the sensitivity of each layer. Finally, we develop an integer-programming-based method to conduct bit-width allocation. While existing quantization methods fall short at W8A8, MixDQ could achieve W8A8 without performance loss, and W4A8 with negligible visual degradation. Compared with FP16, we achieve 3-4x reduction in model size and memory cost, and 1.45x latency speedup.",
      "url": "https://www.microsoft.com/en-us/research/publication/mixdq-memory-efficient-few-step-text-to-image-diffusion-models-with-metric-decoupled-mixed-precision-quantization/"
    },
    {
      "title": "Large Language Models for Tabular Data: Progresses and Future Directions",
      "authors": [
        "Haoyu Dong",
        "Zhiruo Wang"
      ],
      "research_areas": [
        "Data platforms and analytics"
      ],
      "publication_date": "July 2024",
      "abstract": "HaoAreYuDong/Large-Language-Models-for-Tabular-Data (opens in new tab)\nTables contain a significant portion of the world’s structured information. The ability to efficiently and accurately understand, process, reason about, analyze, and generate tabular data is critical for achieving Artificial General Intelligence (AGI) systems.\nHowever, despite their prevalence and importance, tables present unique challenges due to their structured nature and the diverse semantics embedded within them. Textual content, numerical values, visual formats, and even formulas in tables carry rich semantic information that is often underutilized due to the complexity of accurately interpreting and integrating.\nFortunately, the advent of Large Language Models (LLMs) has opened new frontiers in natural language processing (NLP) and machine learning (ML), showing remarkable success in understanding and generating text, code, etc. Applying these advanced models to the domain of tabular data holds the promise of significant breakthroughs in how we process and leverage structured information.\nTherefore, this tutorial aims to provide a comprehensive study of the advances, challenges, and opportunities in leveraging cutting-edge LLMs for tabular data. By introducing methods of prompting or training cutting-edge LLMs for table interpreting, processing, reasoning, analytics, and generation, we aim to equip researchers and practitioners with the knowledge and tools needed to unlock the full potential of LLMs for tabular data in their domains.",
      "url": "https://www.microsoft.com/en-us/research/publication/large-language-models-for-tabular-data-progresses-and-future-directions/"
    },
    {
      "title": "Statistic Maximal Leakage",
      "authors": [
        "Giulia Fanti",
        "Shuaiqi Wang",
        "Zinan Lin"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Security, privacy, and cryptography"
      ],
      "publication_date": "July 2024",
      "abstract": "We introduce a privacy metric called statistic maximal leakage that quantifies how much a privacy mechanism leaks about a specific secret, relative to the adversary’s prior information about that secret. Statistic maximal leakage is an extension of the well-known maximal leakage. Unlike maximal leakage, it protects a single, known secret. We show that statistic maximal leakage satisfies composition and post-processing properties. Additionally, we show how to efficiently compute it in the special case of deterministic data release mechanisms. We analyze two important mechanisms under statistic maximal leakage: the quantization mechanism and randomized response. We show theoretically and empirically that the quantization mechanism achieves better privacy-utility tradeoffs in the settings we study.",
      "url": "https://www.microsoft.com/en-us/research/publication/statistic-maximal-leakage/"
    },
    {
      "title": "Class-Level Code Generation from Natural Language Using Iterative, Tool-Enhanced Reasoning over Repository",
      "authors": [
        "Aditya Kanade",
        "Ajinkya Deshpande",
        "Anmol Agarwal",
        "Arun Iyer",
        "Ramakrishna Bairi",
        "Shashank Shet",
        "Suresh Parthasarathy"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Programming languages and software engineering"
      ],
      "publication_date": "July 2024",
      "abstract": "LLMs have demonstrated significant potential in code generation tasks, achieving promising results at the function or statement level across various benchmarks. However, the complexities associated with creating code artifacts like classes, particularly within the context of real-world software repositories, remain underexplored. Prior research treats class-level generation as an isolated task, neglecting the intricate dependencies & interactions that characterize real-world software environments. To address this gap, we introduce RepoClassBench, a comprehensive benchmark designed to rigorously evaluate LLMs in generating complex, class-level code within real-world repositories. RepoClassBench includes “Natural Language to Class generation” tasks across Java, Python & C# from a selection of repositories. We ensure that each class in our dataset not only has cross-file dependencies within the repository but also includes corresponding test cases to verify its functionality. We find that current models struggle with the realistic challenges posed by our benchmark, primarily due to their limited exposure to relevant repository contexts. To address this shortcoming, we introduce Retrieve-Repotools-Reflect (RRR), a novel approach that equips LLMs with static analysis tools to iteratively navigate & reason about repository-level context in an agent-based framework. Our experiments demonstrate that RRR significantly outperforms existing baselines on RepoClassBench, showcasing its effectiveness across programming languages & under various settings. Our findings emphasize the critical need for code-generation benchmarks to incorporate repo-level dependencies to more accurately reflect the complexities of software development. Our work shows the benefits of leveraging specialized tools to enhance LLMs’ understanding of repository context. We plan to make our dataset & evaluation harness public.",
      "url": "https://www.microsoft.com/en-us/research/publication/natural-language-to-class-level-code-generation-by-iterative-tool-augmented-reasoning-over-repository/"
    },
    {
      "title": "The Art of Saying No: Contextual Noncompliance in Language Models",
      "authors": [
        "Abhilasha Ravichander",
        "Faeze Brahman",
        "Hanna Hajishirzi",
        "Jack Hessel",
        "K. Chandu",
        "Noah A. Smith",
        "Nouha Dziri",
        "Pradeep Dasigi",
        "Sachin Kumar",
        "Sarah Wiegreffe",
        "Valentina Pyatkin",
        "Vidhisha Balachandran",
        "Yejin Choi",
        "Yulia Tsvetkov"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "July 2024",
      "abstract": "Chat-based language models are designed to be helpful, yet they should not comply with every user request. While most existing work primarily focuses on refusal of”unsafe”queries, we posit that the scope of noncompliance should be broadened. We introduce a comprehensive taxonomy of contextual noncompliance describing when and how models should not comply with user requests. Our taxonomy spans a wide range of categories including incomplete, unsupported, indeterminate, and humanizing requests (in addition to unsafe requests). To test noncompliance capabilities of language models, we use this taxonomy to develop a new evaluation suite of 1000 noncompliance prompts. We find that most existing models show significantly high compliance rates in certain previously understudied categories with models like GPT-4 incorrectly complying with as many as 30% of requests. To address these gaps, we explore different training strategies using a synthetically-generated training set of requests and expected noncompliant responses. Our experiments demonstrate that while direct finetuning of instruction-tuned models can lead to both over-refusal and a decline in general capabilities, using parameter efficient methods like low rank adapters helps to strike a good balance between appropriate noncompliance and other capabilities.",
      "url": "https://www.microsoft.com/en-us/research/publication/the-art-of-saying-no-contextual-noncompliance-in-language-models/"
    },
    {
      "title": "Ladder: Enabling Efficient Low-Precision Deep Learning Computing through Hardware-aware Tensor Transformation",
      "authors": [
        "Fan Yang",
        "Jilong Xue",
        "Lei Wang",
        "Lingxiao Ma",
        "Mao Yang",
        "Ningxin Zheng",
        "Quanlu Zhang",
        "Shijie Cao",
        "Ting Cao",
        "Yining Shi",
        "Yuqing Yang",
        "Ziming Miao"
      ],
      "research_areas": [
        "Systems and networking"
      ],
      "publication_date": "July 2024",
      "abstract": "The increasing demand for improving deep learning model performance has led to a paradigm shift in supporting low-precision computation to harness the robustness of deep learning to errors. Despite the emergence of new low-precision data types and optimization approaches, existing hardware and software have insufficient and inefficient support for those evolving data types, making it challenging to achieve real performance gains through low-precision computing.\nThis paper introduces Ladder, a novel compiler designed to bridge the gap between evolving custom data types and the fixed precision formats supported by current hardware. Leveraging a general type system, tType, and an extended tensor expression, Ladder transforms deep neural network (DNN) computations into optimized computing pipelines with custom data types as the first-class citizen, exposing an optimization space for efficiently handling data storage, accesses, and type conversions. Ladder employs a new set of tensor scheduling primitives and a hardware-aware optimization policy to navigate the complex transformation space, ensuring optimal performance across different memory layers and DNN operators. Our evaluation demonstrates Ladder’s capability to systematically support a wide array of low-bit precision custom data types, significantly enhancing the performance of DNN computations on modern accelerators without necessitating hardware modifications. This innovation empowers model designers with the ability to explore data type optimizations and offers hardware vendors a flexible solution to expand their support for diverse precision formats.",
      "url": "https://www.microsoft.com/en-us/research/publication/ladder-enabling-efficient-low-precision-deep-learning-computing-through-hardware-aware-tensor-transformation/"
    },
    {
      "title": "The Vision of Autonomic Computing: Can LLMs Make It a Reality?",
      "authors": [
        "Dongmei Zhang",
        "Fangkai Yang",
        "Gong Cheng",
        "Jue Zhang",
        "Qi Zhang",
        "Qingwei Lin 林庆维",
        "Saravan Rajmohan",
        "Xiaoting Qin",
        "Zhiyang Zhang"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "July 2024",
      "abstract": "The Vision of Autonomic Computing (ACV), proposed over two decades ago, envisions computing systems that self-manage akin to biological organisms, adapting seamlessly to changing environments. Despite decades of research, achieving ACV remains challenging due to the dynamic and complex nature of modern computing systems. Recent advancements in Large Language Models (LLMs) offer promising solutions to these challenges by leveraging their extensive knowledge, language understanding, and task automation capabilities. This paper explores the feasibility of realizing ACV through an LLM-based multi-agent framework for microservice management. We introduce a five-level taxonomy for autonomous service maintenance and present an online evaluation benchmark based on the Sock Shop microservice demo project to assess our framework’s performance. Our findings demonstrate significant progress towards achieving Level 3 autonomy, highlighting the effectiveness of LLMs in detecting and resolving issues within microservice architectures. This study contributes to advancing autonomic computing by pioneering the integration of LLMs into microservice management frameworks, paving the way for more adaptive and self-managing computing systems. The code will be made available at this https URL.",
      "url": "https://www.microsoft.com/en-us/research/publication/the-vision-of-autonomic-computing-can-llms-make-it-a-reality/"
    },
    {
      "title": "Streaming Algorithms for Connectivity Augmentation",
      "authors": [
        "Ali Vakilian",
        "Ce Jin",
        "Michael Kapralov",
        "Sepideh Mahabadi"
      ],
      "research_areas": [
        "Algorithms",
        "Systems and networking"
      ],
      "publication_date": "July 2024",
      "abstract": "We study the k-connectivity augmentation problem (k-CAP) in the single-pass streaming model. Given a (k−1)-edge connected graph G=(V,E) that is stored in memory, and a stream of weighted edges L with weights in {0,1,…,W}, the goal is to choose a minimum weight subset L′⊆L such that G′=(V,E∪L′) is k-edge connected. We give a (2+ϵ)-approximation algorithm for this problem which requires to store O(ϵ^(−1) n log n) words. Moreover, we show our result is tight: Any algorithm with better than 2-approximation for the problem requires Ω(n^2) bits of space even when k=2. This establishes a gap between the optimal approximation factor one can obtain in the streaming vs the offline setting for k-CAP.\nWe further consider a natural generalization to the fully streaming model where both E and L arrive in the stream in an arbitrary order. We show that this problem has a space lower bound that matches the best possible size of a spanner of the same approximation ratio. Following this, we give improved results for spanners on weighted graphs: We show a streaming algorithm that finds a (2t−1+ϵ)-approximate weighted spanner of size at most O(ϵ^(−1) n^(1+1/t) log n) for integer t, whereas the best prior streaming algorithm for spanner on weighted graphs had size depending on log W. Using our spanner result, we provide an optimal O(t)-approximation for k-CAP in the fully streaming model with O(nk+n^(1+1/t) ) words of space.\nFinally we apply our results to network design problems such as Steiner tree augmentation problem (STAP), k-edge connected spanning subgraph (k-ECSS), and the general Survivable Network Design problem (SNDP). In particular, we show a single-pass O(t logk)-approximation for SNDP using O(k n^(1+1/t)) words of space, where k is the maximum connectivity requirement",
      "url": "https://www.microsoft.com/en-us/research/publication/streaming-algorithms-for-connectivity-augmentation/"
    },
    {
      "title": "LSKV: A Confidential Distributed Datastore to Protect Critical Data in the Cloud",
      "authors": [
        "Andrew Jeffery",
        "Heidi Howard",
        "Julien Maffre",
        "Richard Mortier"
      ],
      "research_areas": [
        "Security, privacy, and cryptography",
        "Systems and networking"
      ],
      "publication_date": "July 2024",
      "abstract": "Software services are increasingly migrating to the cloud, requiring trust in actors with direct access to the hardware, software and data comprising the service. A distributed datastore storing critical data sits at the core of many services; a prime example being etcd in Kubernetes. Trusted execution environments can secure this data from cloud providers during execution, but it is complex to build trustworthy data storage systems using such mechanisms. We present the design and evaluation of the Ledger-backed Secure Key-Value datastore (LSKV), a distributed datastore that provides an etcd-like API but can use trusted execution mechanisms to keep cloud providers outside the trust boundary. LSKV provides a path to transition traditional systems towards confidential execution, provides competitive performance compared to etcd, and helps clients to gain trust in intermediary services. LSKV forms a foundational core, lowering the barriers to building more trustworthy systems.",
      "url": "https://www.microsoft.com/en-us/research/publication/lskv-a-confidential-distributed-datastore-to-protect-critical-data-in-the-cloud/"
    },
    {
      "title": "Managing Memory Tiers with CXL in Virtualized Environments",
      "authors": [
        "Asaf Cidon",
        "Carl Waldspurger",
        "Daniel S. Berger",
        "Frank Hady",
        "Ishwar Agarwal",
        "Karthik Kumar",
        "Mark D. Hill",
        "Mosharaf Chowdhury",
        "Rajat Agarwal",
        "Yuhong Zhong"
      ],
      "research_areas": [
        "Systems and networking"
      ],
      "publication_date": "July 2024",
      "abstract": "Cloud providers seek to deploy CXL-based memory to increase aggregate memory capacity, reduce costs, and lower carbon emissions. However, CXL accesses incur higher latency than local DRAM. Existing systems use software to manage data placement across memory tiers at page granularity. Cloud providers are reluctant to deploy software-based tiering due to high overheads in virtualized environments. Hardware-based memory tiering could place data at cacheline granularity, mitigating these drawbacks. However, hardware is oblivious to application-level performance.\nWe propose combining hardware-managed tiering with software-managed performance isolation to overcome the pitfalls of either approach. We introduce Intel® Flat Memory Mode, the first hardware-managed tiering system for CXL. Our evaluation on a full-system prototype demonstrates that it provides performance close to regular DRAM, with no more than 5% degradation for more than 82% of workloads. Despite such small slowdowns, we identify two challenges that can still degrade performance by up to 34% for “outlier” workloads: (1) memory contention across tenants, and (2) intra-tenant contention due to conflicting access patterns.\nTo address these challenges, we introduce Memstrata, a lightweight multi-tenant memory allocator. Memstrata employs page coloring to eliminate inter-VM contention. It improves performance for VMs with access patterns that are sensitive to hardware tiering by allocating them more local DRAM using an online slowdown estimator. In multi-VM experiments on prototype hardware, Memstrata is able to identify performance outliers and reduce their degradation from above 30% to below 6%, providing consistent performance across a wide range of workloads.",
      "url": "https://www.microsoft.com/en-us/research/publication/managing-memory-tiers-with-cxl-in-virtualized-environments/"
    },
    {
      "title": "OAK: Enriching Document Representations using Auxiliary Knowledge for Extreme Classification",
      "authors": [
        "Anshul Mittal",
        "Bhawna Paliwal",
        "Deepak Saini",
        "Jian Jiao",
        "Manik Varma",
        "Manish Gupta",
        "Sayak Roy Chowdhury",
        "Shikhar Mohan"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Search and information retrieval"
      ],
      "publication_date": "July 2024",
      "abstract": "The objective in eXtreme Classification (XC) is to find relevant labels for a document from an exceptionally large label space. Most XC application scenarios have rich auxiliary data associated with the input documents, e.g., frequently clicked webpages for search queries in sponsored search. Unfortunately, most of the existing XC methods do not use any auxiliary data. In this paper, we propose a novel framework, Online Auxiliary Knowledge (OAK), which harnesses auxiliary information linked to the document to improve XC accuracy. OAK stores information learnt from the auxiliary data in a knowledge bank and during a forward pass, retrieves relevant auxiliary knowledge embeddings for a given document. An enriched embedding is obtained by fusing these auxiliary knowledge embeddings with the document’s embedding, thereby enabling much more precise candidate label selection and final classification. OAK training involves three stages. (1) Training a linker module to link documents to relevant auxiliary data points. (2) Learning an embedding for documents enriched using linked auxiliary information. (3) Using the enriched document embeddings to learn the final classifiers. OAK outperforms current state-of-the-art XC methods by up to ~5% on academic datasets, and by ~3% on an auxiliary data-augmented variant of LF-ORCAS-800K dataset in Precision@1. OAK also demonstrates statistically significant improvements in sponsored search metrics when deployed on a large-scale search engine.",
      "url": "https://www.microsoft.com/en-us/research/publication/oak-enriching-document-representations-using-auxiliary-knowledge/"
    },
    {
      "title": "The CoExplorer Technology Probe: A Generative AI-Powered Adaptive Interface to Support Intentionality in Planning and Running Video Meetings",
      "authors": [
        "Gun Woo (Warren) Park",
        "Lev Tankelevitch",
        "Payod Panda",
        "Sean Rintel"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Human-computer interaction",
        "Social sciences"
      ],
      "publication_date": "July 2024",
      "abstract": "Effective meetings are effortful, but traditional videoconferencing systems offer little support for reducing this effort across the meeting lifecycle. Generative AI (GenAI) has the potential to radically redefine meetings by augmenting intentional meeting behaviors. CoExplorer, our novel adaptive meeting prototype, preemptively generates likely phases that meetings would undergo, tools that allow capturing attendees’ thoughts before the meeting, and for each phase, window layouts, and appropriate applications and files. Using CoExplorer as a technology probe in a guided walkthrough, we studied its potential in a sample of participants from a global technology company. Our findings suggest that GenAI has the potential to help meetings stay on track and reduce workload, although concerns were raised about users’ agency, trust, and possible disruption to traditional meeting norms. We discuss these concerns and their design implications for the development of GenAI meeting technology. KEYWORDS: video meetings, effectiveness, effort, design, adaptive user interface, windowing system, speech recognition; intent recognition, technology probe\n\nRELATED RESEARCH\n\n\nFormative studies\n\n\n\nMental Models of Meeting Goals: Supporting Intentionality in Meeting Technology\n\n\n\nPrototype studies\n\n\n\nBefore meetings: What Does Success Look Like? Catalyzing Meeting Intentionality with AI-Assisted Prospective Reflection\n\n\nBefore and during meetings: The CoExplorer Technology Probe: A Generative AI-Powered Adaptive Interface to Support Intentionality in Planning and Running Video Meetings\n\n\nDuring meetings: Are We On Track? AI-Assisted Active and Passive Goal Reflection During Meetings – Microsoft Research\n\n\nBetween meetings: Designing Interfaces that Support Temporal Work Across Meetings with Generative AI",
      "url": "https://www.microsoft.com/en-us/research/publication/the-coexplorer-technology-probe-a-generative-ai-powered-adaptive-interface-to-support-intentionality-in-planning-and-running-video-meetings/"
    },
    {
      "title": "A Universal Transfer Theorem for Convex Optimization Algorithms Using Inexact First-order Oracles",
      "authors": [
        "Amitabh Basu",
        "Hongyi Jiang",
        "Marco Molinaro",
        "Phillip Kerger"
      ],
      "research_areas": [
        "Algorithms"
      ],
      "publication_date": "July 2024",
      "abstract": "Given any algorithm for convex optimization that uses exact first-order information (i.e., function values and subgradients), we show how to use such an algorithm to solve the problem with access to inexact first-order information. This is done in a “black-box” manner without knowledge of the internal workings of the algorithm. This complements work done by Devolder-Glineur-Nesterov and Schmidt-Le Roux-Bach who consider the performance of specific algorithms like (accelerated) gradient descent with inexact information. In particular, our results apply to a wider range of algorithms beyond variants of gradient descent, e.g., projection-free methods, cutting-plane methods, or any other first-order methods formulated in the future. Further, they also apply to algorithms that handle structured nonconvexities like mixed-integer decision variables.",
      "url": "https://www.microsoft.com/en-us/research/publication/a-universal-transfer-theorem-for-convex-optimization-algorithms-using-inexact-first-order-oracles/"
    },
    {
      "title": "Optimizing Learning-to-Rank Models for Ex-Post Fair Relevance",
      "authors": [
        "Amit Deshpande",
        "Anand Louis",
        "Eshaan Bhansali",
        "Sruthi Gorantla"
      ],
      "research_areas": [
        "Algorithms",
        "Search and information retrieval"
      ],
      "publication_date": "July 2024",
      "abstract": "Learning-to-rank (LTR) models rank items based on specific features, aiming to maximize ranking utility by prioritizing highly relevant items. However, optimizing only for ranking utility can lead to representational harm and may fail to address implicit bias in relevance scores. Prior studies introduced algorithms to train stochastic ranking models, such as the Plackett-Luce ranking model, that maximize expected ranking utility while achieving fairness in expectation (ex-ante fairness). Still, every sampled ranking may not satisfy group fairness (ex-post fairness). Post-processing methods ensure ex-post fairness; however, the LTR model lacks awareness of this step, creating a mismatch between the objective function the LTR model optimizes and the one it is supposed to optimize. In this paper, we first propose a novel objective where the relevance (or the expected ranking utility) is computed over only those rankings that satisfy given representation constraints for groups of items. We call this the ex-post fair relevance. We then give a framework for training Group-Fair LTR models to maximize our proposed ranking objective.\nLeveraging an efficient sampler for ex-post group-fair rankings and efficient algorithms to train the Plackett-Luce LTR model, we demonstrate their use in training the Group-Fair Plackett-Luce model in our framework. Experiments on MovieLens and Kiva datasets reveal improved fairness and relevance with our group-fair Plackett-Luce model compared to post-processing. In scenarios with implicit bias, our algorithm generally outperforms existing LTR baselines in both fairness and relevance.",
      "url": "https://www.microsoft.com/en-us/research/publication/optimizing-learning-to-rank-models-for-ex-post-fair-relevance-4/"
    },
    {
      "title": "Exploring LLM-based Agents for Root Cause Analysis",
      "authors": [
        "Chetan Bansal",
        "Devjeet Roy",
        "Pedro Las-Casas",
        "Rashi Bhave",
        "Rodrigo Fonseca",
        "Saravan Rajmohan",
        "Xuchao Zhang"
      ],
      "research_areas": [
        "Algorithms",
        "Artificial intelligence"
      ],
      "publication_date": "July 2024",
      "abstract": "The growing complexity of cloud based software systems has resulted in incident management becoming an integral part of the software development lifecycle. Root cause analysis (RCA), a critical part of the incident management process, is a demanding task for on-call engineers, requiring deep domain knowledge and extensive experience with a team’s specific services. Automation of RCA can result in significant savings of time, and ease the burden of incident management on on-call engineers. Recently, researchers have utilized Large Language Models (LLMs) to perform RCA, and have demonstrated promising results. However, these approaches are not able to dynamically collect additional diagnostic information such as incident related logs, metrics or databases, severely restricting their ability to diagnose root causes. In this work, we explore the use of LLM based agents for RCA to address this limitation. We present a thorough empirical evaluation of a ReAct agent equipped with retrieval tools, on an out-of-distribution dataset of production incidents collected at Microsoft. Results show that ReAct performs competitively with strong retrieval and reasoning baselines, but with highly increased factual accuracy. We then extend this evaluation by incorporating discussions associated with incident reports as additional inputs for the models, which surprisingly does not yield significant performance improvements. Lastly, we conduct a case study with a team at Microsoft to equip the ReAct agent with tools that give it access to external diagnostic services that are used by the team for manual RCA. Our results show how agents can overcome the limitations of prior work, and practical considerations for implementing such a system in practice.",
      "url": "https://www.microsoft.com/en-us/research/publication/exploring-llm-based-agents-for-root-cause-analysis/"
    },
    {
      "title": "Sound Borrow-Checking for Rust via Symbolic Semantics",
      "authors": [
        "Aymeric Fromherz",
        "Jonathan Protzenko",
        "Son Ho"
      ],
      "research_areas": [
        "Programming languages and software engineering"
      ],
      "publication_date": "July 2024",
      "abstract": "The Rust programming language continues to rise in popularity, and as such, warrants the close attention of the programming languages community. In this work, we present a new foundational contribution towards the theoretical understanding of Rust’s semantics. We prove that LLBC, a high-level, borrow-centric model previously proposed for Rust’s semantics and execution, is sound with regards to a low-level pointer-based language à la CompCert. Specifically, we prove the following: that LLBC is a correct view over a traditional model of execution; that LLBC’s symbolic semantics are a correct abstraction of LLBC programs; and that LLBC’s symbolic semantics act as a borrow-checker for LLBC, i.e. that symbolically-checked LLBC programs do not get stuck when executed on a heap-and-addresses model of execution.\nTo prove these results, we introduce a new proof style that considerably simplifies our proofs of simulation, which relies on a notion of hybrid states. Equipped with this reasoning framework, we show that a new addition to LLBC’s symbolic semantics, namely a join operation, preserves the abstraction and borrow-checking properties. This in turn allows us to add support for loops to the Aeneas framework; we show, using a series of examples and case studies, that this unlocks new expressive power for Aeneas.",
      "url": "https://www.microsoft.com/en-us/research/publication/sound-borrow-checking-for-rust-via-symbolic-semantics/"
    },
    {
      "title": "nnScaler: Constraint-Guided Parallelization Plan Generation for Deep Learning Training",
      "authors": [
        "Cheng Li",
        "Fan Yang",
        "Lidong Zhou",
        "Lintao Zhang",
        "Mao Yang",
        "Ning Shang",
        "Quanlu Zhang",
        "Saeed Maleki",
        "Weijiang Xu",
        "Xu Cao",
        "Yi Zhu",
        "Yilei Yang",
        "Youshan Miao",
        "Zhiqi Lin"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Systems and networking"
      ],
      "publication_date": "July 2024",
      "abstract": "With the growing model size of deep neural networks (DNN), deep learning training is increasingly relying on handcrafted search spaces to find efficient parallelization execution plans. However, our study shows that existing search spaces exclude plans that significantly impact the training performance of well-known DNN models (e.g., AlphaFold2) under important settings, such as when handling large embedding tables in large language models.\nTo address this problem, we propose nnScaler, a framework that generates efficient parallelization plans for deep learning training. Instead of relying on the existing search space, nnScaler advocates a more general approach that empowers domain experts to construct their own search space through three primitives, op-trans, op-assign, and op-order, which capture model transformation and the temporal-spatial scheduling of the transformed model of any parallelization plans. To avoid space explosion, nnScaler allows the application of constraints to those primitives during space construction. With the proposed primitives and constraints, nnScaler can compose existing search spaces as well as new ones. Experiments show that nnScaler can find new parallelization plans in new search spaces that achieve up to 3.5× speedup compared to solutions such as DeepSpeed, Megatron-LM, and Alpa for popular DNN models like SwinTransformer and AlphaFold2.",
      "url": "https://www.microsoft.com/en-us/research/publication/nnscaler-constraint-guided-parallelization-plan-generation-for-deep-learning-training/"
    },
    {
      "title": "Pre-gated MoE: An Algorithm-System Co-Design for Fast and Scalable Mixture-of-Expert Inference",
      "authors": [
        "Changho Hwang",
        "Jianyu Wei",
        "Mao Yang",
        "Ranggi Hwang",
        "Shijie Cao",
        "Ting Cao",
        "Xiaohu Tang"
      ],
      "research_areas": [
        "Systems and networking"
      ],
      "publication_date": "July 2024",
      "abstract": "Large language models (LLMs) based on transformers have made significant strides in recent years, the success of which is driven by scaling up their model size.\nDespite their high algorithmic performance, the computational and memory requirements of LLMs present unprecedented challenges. To tackle the high compute requirements of LLMs, the Mixture-of-Experts (MoE) architecture was introduced which is able to scale its model size without proportionally scaling up its computational requirements. Unfortunately, MoE’s high memory demands and dynamic activation of sparse experts restrict its applicability to real-world problems. Previous solutions that offload MoE’s memory-hungry expert parameters to CPU memory fall short because the latency to migrate activated experts from CPU to GPU incurs high performance overhead. Our proposed Pre-gated MoE system effectively tackles the compute and memory challenges of conventional MoE architectures using our algorithm-system co-design. Pre-gated MoE employs our novel pre-gating function which alleviates the dynamic nature of sparse expert activation, allowing our proposed system to address the large memory footprint of MoEs while also achieving high performance. We demonstrate that Pre-gated MoE is able to improve performance, reduce GPU memory consumption, while also maintaining the same level of model quality. These features allow our Pre-gated MoE system to cost-effectively deploy large-scale LLMs using just a single GPU with high performance.",
      "url": "https://www.microsoft.com/en-us/research/publication/pre-gated-moe-an-algorithm-system-co-design-for-fast-and-scalable-mixture-of-expert-inference/"
    },
    {
      "title": "MonitorAssistant: Simplifying Cloud Service Monitoring via Large Language Models",
      "authors": [
        "Changhua Pei",
        "Chaoyun Zhang",
        "Chetan Bansal",
        "Dan Pei",
        "Dongmei Zhang",
        "Minghua Ma",
        "Qingwei Lin 林庆维",
        "Saravan Rajmohan",
        "Si Qin",
        "Yingnong Dang",
        "Yu Kang",
        "Zhaoyang Yu"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "July 2024",
      "abstract": "In large-scale cloud service systems, monitoring metric data and conducting anomaly detection is an important way to maintain reliability and stability. However, great disparity exists between academic approaches and industrial practice to anomaly detection. Industry predominantly uses simple, efficient methods due to better interpretability and ease of implementation. In contrast, academically favor deep-learning methods, despite their advanced capabilities, face practical challenges in real-world applications. To address these challenges, this paper introduces MonitorAssistant, an end-to-end practical anomaly detection system via Large Language Models. MonitorAssistant automates model configuration recommendation achieving knowledge inheritance and alarm interpretation with guidance-oriented anomaly reports, facilitating a more intuitive engineer-system interaction through natural language. By deploying MonitorAssistant in Microsoft’s cloud service system, we validate its efficacy and practicality, marking a significant advancement in the field of practical anomaly detection for large-scale cloud services.",
      "url": "https://www.microsoft.com/en-us/research/publication/monitorassistant-simplifying-cloud-service-monitoring-via-large-language-models-2/"
    },
    {
      "title": "SuperBench: Improving Cloud AI Infrastructure Reliability with Proactive Validation",
      "authors": [
        "Boris Pinzur",
        "Dong Zhong",
        "Guoshuai Zhao",
        "Hossein Pourreza",
        "Jeff Baxter",
        "Jie Zhang",
        "Jithin Jose",
        "Joe Chau",
        "Kushal Datta",
        "Lei Qu",
        "Lidong Zhou",
        "Luke Melton",
        "Peng Cheng",
        "Prabhat Ram",
        "Shuguang Liu",
        "Yang Wang",
        "Yifan Xiong",
        "Yongqiang Xiong",
        "Yuting Jiang",
        "Ziyue Yang"
      ],
      "research_areas": [
        "Systems and networking"
      ],
      "publication_date": "July 2024",
      "abstract": "Reliability in cloud AI infrastructure is crucial for cloud service providers, prompting the widespread use of hardware redundancies. However, these redundancies can inadvertently lead to hidden degradation, so called “gray failure”, for AI workloads, significantly affecting end-to-end performance and concealing performance issues, which complicates root cause analysis for failures and regressions.\nWe introduce SuperBench, a proactive validation system for AI infrastructure that mitigates hidden degradation caused by hardware redundancies and enhances overall reliability. SuperBench features a comprehensive benchmark suite, capable of evaluating individual hardware components and representing most real AI workloads. It comprises a Validator which learns benchmark criteria to clearly pinpoint defective components. Additionally, SuperBench incorporates a Selector to balance validation time and issue-related penalties, enabling optimal timing for validation execution with a tailored subset of benchmarks. Through testbed evaluation and simulation, we demonstrate that SuperBench can increase the mean time between incidents by up to 22.61x. SuperBench has been successfully deployed in Azure production, validating hundreds of thousands of GPUs over the last two years.",
      "url": "https://www.microsoft.com/en-us/research/publication/superbench/"
    },
    {
      "title": "Beaver: Practical Partial Snapshots for Distributed Cloud Services",
      "authors": [
        "Dan R. K. Ports",
        "Haoran Zhang",
        "John Sonchack",
        "Liangcheng Yu",
        "Vincent Liu",
        "Xiao Zhang"
      ],
      "research_areas": [
        "Systems and networking"
      ],
      "publication_date": "July 2024",
      "abstract": "Distributed snapshots are a classic class of protocols used for capturing a causally consistent view of states across machines. Although effective, existing protocols presume an isolated universe of processes to snapshot and require instrumentation and coordination of all. This assumption does not match today’s cloud services—it is not always practical to instrument all involved processes nor realistic to assume zero interaction of the machines of interest with the external world.\nTo bridge this gap, this paper presents Beaver, the first practical partial snapshot protocol that ensures causal consistency under external traffic interference. Beaver presents a unique design point that tightly couples its protocol with the regularities of the underlying data center environment. By exploiting the placement of software load balancers in public clouds and their associated communication pattern, Beaver not only requires minimal changes to today’s data center operations but also eliminates any form of blocking to existing communication, thus incurring near-zero overhead to user traffic. We demonstrate the Beaver’s effectiveness through extensive testbed experiments and novel use cases.",
      "url": "https://www.microsoft.com/en-us/research/publication/beaver-practical-partial-snapshots-for-distributed-cloud-services/"
    },
    {
      "title": "VALL-E R: Robust and Efficient Zero-Shot Text-to-Speech Synthesis via Monotonic Alignment",
      "authors": [
        "Bing Han",
        "Furu Wei",
        "Jinyu Li",
        "Lingwei Meng",
        "Long Zhou",
        "Sanyuan Chen",
        "Sheng Zhao",
        "Shujie Liu",
        "Yanming Qian",
        "Yanqing Liu"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Audio and Acoustics"
      ],
      "publication_date": "June 2024",
      "abstract": "With the help of discrete neural audio codecs, large language models (LLM) have increasingly been recognized as a promising methodology for zero-shot Text-to-Speech (TTS) synthesis. However, sampling based decoding strategies bring astonishing diversity to generation, but also pose robustness issues such as typos, omissions and repetition. In addition, the high sampling rate of audio also brings huge computational overhead to the inference process of autoregression. To address these issues, we propose VALL-E R, a robust and efficient zero-shot TTS system, building upon the foundation of VALL-E. Specifically, we introduce a phoneme monotonic alignment strategy to strengthen the connection between phonemes and acoustic sequence, ensuring a more precise alignment by constraining the acoustic tokens to match their associated phonemes. Furthermore, we employ a codec-merging approach to downsample the discrete codes in shallow quantization layer, thereby accelerating the decoding speed while preserving the high quality of speech output. Benefiting from these strategies, VALL-E R obtains controllablity over phonemes and demonstrates its strong robustness by approaching the WER of ground truth. In addition, it requires fewer autoregressive steps, with over 60% time reduction during inference. This research has the potential to be applied to meaningful projects, including the creation of speech for those affected by aphasia. See https://aka.ms/valler for demos of VALL-E R.",
      "url": "https://www.microsoft.com/en-us/research/publication/vall-e-r-robust-and-efficient-zero-shot-text-to-speech-synthesis-via-monotonic-alignment/"
    },
    {
      "title": "Universal Checkpointing: Efficient and Flexible Checkpointing for Large Scale Distributed Training",
      "authors": [
        "Lev Kurilenko",
        "Masahiro Tanaka",
        "Minjia Zhang",
        "Olatunji Ruwase",
        "Sam Ade Jacobs",
        "Stas Bekman",
        "Xinyu Lian"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "June 2024",
      "abstract": "Existing checkpointing approaches seem ill-suited for distributed training even though hardware limitations make model parallelism, i.e., sharding model state across multiple accelerators, a requirement for model scaling. Consolidating distributed model state into a single checkpoint unacceptably slows down training, and is impractical at extreme scales. Distributed checkpoints, in contrast, are tightly coupled to the model parallelism and hardware configurations of the training run, and thus unusable on different configurations. To address this problem, we propose Universal Checkpointing, a technique that enables efficient checkpoint creation while providing the flexibility of resuming on arbitrary parallelism strategy and hardware configurations. Universal Checkpointing unlocks unprecedented capabilities for large-scale training such as improved resilience to hardware failures through continued training on remaining healthy hardware, and reduced training time through opportunistic exploitation of elastic capacity. The key insight of Universal Checkpointing is the selection of the optimal representation in each phase of the checkpointing life cycle: distributed representation for saving, and consolidated representation for loading. This is achieved using two key mechanisms. First, the universal checkpoint format, which consists of a consolidated representation of each model parameter and metadata for mapping parameter fragments into training ranks of arbitrary model-parallelism configuration. Second, the universal checkpoint language, a simple but powerful specification language for converting distributed checkpoints into the universal checkpoint format. Our evaluation demonstrates the effectiveness and generality of Universal Checkpointing on state-of-the-art model architectures and a wide range of parallelism techniques.",
      "url": "https://www.microsoft.com/en-us/research/publication/universal-checkpointing-efficient-and-flexible-checkpointing-for-large-scale-distributed-training/"
    },
    {
      "title": "BatchIt: Optimizing Message-Passing Allocators for Producer-Consumer Workloads: An Intellectual Abstract",
      "authors": [
        "Matthew J. Parkinson",
        "Nathaniel Filardo"
      ],
      "research_areas": [
        "Programming languages and software engineering"
      ],
      "publication_date": "June 2024",
      "abstract": "Modern, high-performance memory allocators must scale\nto a wide array of uses, including producer-consumer workloads.\nIn such workloads, objects are allocated by one thread\nand deallocated by another, which we call remote deallocations.\nThese remote deallocations lead to contention on the\nallocator’s synchronization mechanisms. Message-passing\nallocators, such as mimalloc and snmalloc, use message\nqueues to communicate remote deallocations between threads.\nThese queues work well for producer-consumer workloads,\nbut there is room for optimization.\nWe propose and characterize BatchIt, a conceptually simple\n optimization for such allocators: a per-slab cache of\nremote deallocations that enables batching of objects destined\n for the same slab. This optimization aims to exploit\nnaturally-arising locality of allocations, and it generalizes\nacross particular implementations; we have implementations\nfor both mimalloc and snmalloc. Multi-threaded, producer-consumer\nbenchmarks show improved performance from reduced\nrates of atomic operations and cache misses in the underlying\nallocator. Experimental results using the mimalloc-bench\nsuite and a custom message-passing workload show\nthat some producer-consumer workloads see over 20% performance\nimprovement even based on the high-performance\nthese allocators already provide.",
      "url": "https://www.microsoft.com/en-us/research/publication/batchit-optimizing-message-passing-allocators-for-producer-consumer-workloads-an-intellectual-abstract/"
    },
    {
      "title": "Efficient Expert Pruning for Sparse Mixture-of-Experts Language Models: Enhancing Performance and Reducing Inference Costs",
      "authors": [
        "Enshu Liu",
        "Guohao Dai",
        "Huazhong Yang",
        "Junyi Zhu",
        "Matthew B. Blaschko",
        "Shengen Yan",
        "Xuefei Ning",
        "Yu Wang",
        "Zinan Lin"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "June 2024",
      "abstract": "The rapid advancement of large language models (LLMs) has led to architectures with billions to trillions of parameters, posing significant deployment challenges due to their substantial demands on memory, processing power, and energy consumption. Sparse Mixture-of-Experts (SMoE) architectures have emerged as a solution, activating only a subset of parameters per token, thereby achieving faster inference while maintaining performance. However, SMoE models still face limitations in broader deployment due to their large parameter counts and significant GPU memory requirements. In this work, we introduce a gradient-free evolutionary strategy named EEP (Efficient Expert P}runing) to enhance the pruning of experts in SMoE models. EEP relies solely on model inference (i.e., no gradient computation) and achieves greater sparsity while maintaining or even improving performance on downstream tasks. EEP can be used to reduce both the total number of experts (thus saving GPU memory) and the number of active experts (thus accelerating inference). For example, we demonstrate that pruning up to 75% of experts in Mixtral $8\\times7$B-Instruct results in a substantial reduction in parameters with minimal performance loss. Remarkably, we observe improved performance on certain tasks, such as a significant increase in accuracy on the SQuAD dataset (from 53.4% to 75.4%), when pruning half of the experts. With these results, EEP not only lowers the barrier to deploying SMoE models,but also challenges the conventional understanding of model pruning by showing that fewer experts can lead to better task-specific performance without any fine-tuning. Code is available at https://github.com/imagination-research/EEP.",
      "url": "https://www.microsoft.com/en-us/research/publication/efficient-expert-pruning-for-sparse-mixture-of-experts-language-models-enhancing-performance-and-reducing-inference-costs/"
    },
    {
      "title": "Direct Preference Knowledge Distillation for Large Language Models",
      "authors": [
        "Dequan Wang",
        "Furu Wei",
        "Li Dong",
        "Yixing Li",
        "Yu Cheng",
        "Yuxian Gu"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "June 2024",
      "abstract": "In the field of large language models (LLMs), Knowledge Distillation (KD) is a critical technique for transferring capabilities from teacher models to student models. However, existing KD methods face limitations and challenges in distillation of LLMs, including efficiency and insufficient measurement capabilities of traditional KL divergence. It is shown that LLMs can serve as an implicit reward function, which we define as a supplement to KL divergence. In this work, we propose Direct Preference Knowledge Distillation (DPKD) for LLMs. DPKD utilizes distribution divergence to represent the preference loss and implicit reward function. We re-formulate KD of LLMs into two stages: first optimizing and objective consisting of implicit reward and reverse KL divergence and then improving the preference probability of teacher outputs over student outputs. We conducted experiments and analysis on various datasets with LLM parameters ranging from 120M to 13B and demonstrate the broad applicability and effectiveness of our DPKD approach. Meanwhile, we prove the value and effectiveness of the introduced implicit reward and output preference in KD through experiments and theoretical analysis. The DPKD method outperforms the baseline method in both output response precision and exact match percentage. Code and data are available at https://aka.ms/dpkd (opens in new tab).",
      "url": "https://www.microsoft.com/en-us/research/publication/direct-preference-knowledge-distillation-for-large-language-models/"
    },
    {
      "title": "Language-guided Skill Learning with Temporal Variational Inference",
      "authors": [
        "Elias Stengel-Eskin",
        "George Konidaris",
        "Haotian Fu",
        "Marc-Alexandre Côté",
        "Nicolas Le Roux",
        "Pratyusha Sharma",
        "Xingdi Yuan"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "July 2024",
      "abstract": "We present an algorithm for skill discovery from expert demonstrations. The algorithm first utilizes Large Language Models (LLMs) to propose an initial segmentation of the trajectories. Following that, a hierarchical variational inference framework incorporates the LLM-generated segmentation information to discover reusable skills by merging trajectory segments. To further control the trade-off between compression and reusability, we introduce a novel auxiliary objective based on the Minimum Description Length principle that helps guide this skill discovery process. Our results demonstrate that agents equipped with our method are able to discover skills that help accelerate learning and outperform baseline skill learning approaches on new long-horizon tasks in BabyAI, a grid world navigation environment, as well as ALFRED, a household simulation environment.",
      "url": "https://www.microsoft.com/en-us/research/publication/language-guided-skill-learning-with-temporal-variational-inference/"
    },
    {
      "title": "Reference Counting Deeply Immutable Data Structures with Cycles: an Intellectual Abstract",
      "authors": [
        "Matthew J. Parkinson",
        "Sylvan Clebsch",
        "Tobias Wrigstad"
      ],
      "research_areas": [
        "Programming languages and software engineering"
      ],
      "publication_date": "June 2024",
      "abstract": "Immutable data structures are a powerful tool for building concurrent programs. They allow the sharing of data without the need for locks or other synchronisation mechanisms. This makes it much easier to reason about the correctness of the program.\nIn this paper, we focus on what we call deep immutability from freeze, that is, objects are initially mutable, and then can be frozen, and from that point on the object and everything it refers to (transitively) can no longer be mutated. A key challenge with this form of immutability is “how to manage the memory of cyclic data structures?” The standard approach is to use a garbage collector (GC), or a back-up cycle detector. These approaches sacrifice the promptness of memory reclamation, and the determinism of memory usage.\nIn this paper, we argue that memory underlying an immutable data structure can be efficiently managed using reference counting even in the presence of cycles, based on the observation that the cycles are themselves immutable. Our approach takes a classic algorithm for calculating strongly connected components (SCCs) and managing equivalence classes with union-find (UF), and combines them so that the liveness of each SCC can be tracked efficiently using only a single reference counter. The key observation is that since the graph is unchanging, we can calculate the SCCs once, in time that is almost linear in the size of the graph, and then use the result to reference count at the level of the SCCs. This gives precise reachability information, and does not require any backup mechanism to detect or handle cycles.",
      "url": "https://www.microsoft.com/en-us/research/publication/reference-counting-deeply-immutable-data-structures-with-cycles-an-intellectual-abstract/"
    },
    {
      "title": "Human-Aware Vision-and-Language Navigation: Bridging Simulation to Reality with Dynamic Human Interactions",
      "authors": [
        "Alexander G. Hauptmann",
        "Heng Li",
        "Jun-Yan He",
        "Minghan Li",
        "Qi Dai",
        "Teruko Mitamura",
        "Yifei Dong",
        "Yuxuan Zhou",
        "Zhi-Qi Cheng"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Computer vision"
      ],
      "publication_date": "June 2024",
      "abstract": "Vision-and-Language Navigation (VLN) aims to develop embodied agents that navigate based on human instructions. However, current VLN frameworks often rely on static environments and optimal expert supervision, limiting their real-world applicability. To address this, we introduce Human-Aware Vision-and-Language Navigation (HA-VLN), extending traditional VLN by incorporating dynamic human activities and relaxing key assumptions. We propose the Human-Aware 3D (HA3D) simulator, which combines dynamic human activities with the Matterport3D dataset, and the Human-Aware Room-to-Room (HA-R2R) dataset, extending R2R with human activity descriptions. To tackle HA-VLN challenges, we present the Expert-Supervised Cross-Modal (VLN-CM) and Non-Expert-Supervised Decision Transformer (VLN-DT) agents, utilizing cross-modal fusion and diverse training strategies for effective navigation in dynamic human environments. A comprehensive evaluation, including metrics considering human activities, and systematic analysis of HA-VLN’s unique challenges, underscores the need for further research to enhance HA-VLN agents’ real-world robustness and adaptability. Ultimately, this work provides benchmarks and insights for future research on embodied AI and Sim2Real transfer, paving the way for more realistic and applicable VLN systems in human-populated environments.",
      "url": "https://www.microsoft.com/en-us/research/publication/human-aware-vision-and-language-navigation-bridging-simulation-to-reality-with-dynamic-human-interactions/"
    },
    {
      "title": "Report on The Search Futures Workshop at ECIR 2024",
      "authors": [
        "Bhaskar Mitra",
        "Charles L. A. Clarke",
        "Johanne R. Trippas",
        "Leif Azzopardi",
        "Paul Kantor",
        "Zhaochun Ren"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Search and information retrieval"
      ],
      "publication_date": "July 2024",
      "abstract": "The First Search Futures Workshop, in conjunction with the Forty-sixth European Conference on Information Retrieval (ECIR) 2024, looked into the future of search to ask questions such as:\n\nHow can we harness the power of generative AI to enhance, improve and re-imagine Information Retrieval (IR)?\nWhat are the principles and fundamental rights that the field of Information Retrieval should strive to uphold?\nHow can we build trustworthy IR systems in light of Large Language Models and their ability to generate content at super human speeds?\nWhat new applications and affordances does generative AI offer and enable, and can we go back to the future, and do what we only dreamed of previously?\n\nThe workshop started with seventeen lightning talks from a diverse set speakers. Instead of conventional paper presentations, the lightning talks provided a rapid and concise overview of ideas, allowing speakers to share critical points or novel concepts quickly. This format was designed to encourage discussion and introduce a wide range of topics within a short period, thereby maximising the exchange of ideas and ensuring that participants could gain insights into various future search areas without the deep dive typically required in longer presentations. This report, co-authored by the workshop’s organisers and its participants, summarises the talks and discussions. This report aims to provide the broader IR community with the insights and ideas discussed and debated during the workshop – and to provide a platform for future discussion.\nDate: 24 March 2024.\nWebsite: https://searchfutures.github.io/.",
      "url": "https://www.microsoft.com/en-us/research/publication/report-on-the-search-futures-workshop-at-ecir-2024/"
    },
    {
      "title": "The Functional Essence of Imperative Binary Search Trees",
      "authors": [
        "Anton Lorenzen",
        "Daan Leijen",
        "Sam Lindley",
        "Wouter Swierstra"
      ],
      "research_areas": [
        "Programming languages and software engineering"
      ],
      "publication_date": "June 2024",
      "abstract": "Algorithms on restructuring binary search trees are typically\npresented in imperative pseudocode. Understandably so, as their\nperformance relies on in-place execution, rather than the repeated\nallocation of fresh nodes in memory. Unfortunately, these imperative\nalgorithms are notoriously difficult to verify as their loop\ninvariants must relate the unfinished tree fragments being\nrebalanced. This paper presents several novel functional algorithms\nfor accessing and inserting elements in a restructuring binary search\ntree that are as fast as their imperative counterparts; yet the\ncorrectness of these functional algorithms is established using a\nsimple inductive argument. For each data structure, move-to-root,\nsplay, and zip trees, this paper describes both a bottom-up\nalgorithm using zippers and a top-down algorithm using a novel\nfirst-class constructor context primitive.\nThe functional and imperative algorithms are equivalent:\nwe mechanise the proofs establishing this in the Coq\nproof assistant using the Iris framework. This yields a first fully\nverified implementation of well known algorithms on binary search trees with\nperformance on par with the fastest implementations in C.",
      "url": "https://www.microsoft.com/en-us/research/publication/fiptree-full/"
    },
    {
      "title": "A Tale of Two Paths: Toward a Hybrid Data Plane for Efficient Far-Memory Applications",
      "authors": [
        "Chenggang Wu",
        "Chenxi Wang",
        "Haoran Ma",
        "Harry Xu",
        "Huimin Cui",
        "Lei Chen",
        "Shan Lu",
        "Shiafun Liu",
        "Xiaobing Feng",
        "Yifan Qiao",
        "Youyou Lu",
        "Zhe Wang"
      ],
      "research_areas": [
        "Systems and networking"
      ],
      "publication_date": "June 2024",
      "abstract": "With rapid advances in network hardware, far memory has gained a great deal of traction due to its ability to break the memory capacity wall. Existing far memory systems fall into one of two data paths: one that uses the kernel’s paging system to transparently access far memory at the page granularity, and a second that bypasses the kernel, fetching data at the object granularity. While it is generally believed that object fetching outperforms paging due to its fine-grained access, it requires significantly more compute resources to run object-level LRU and eviction.\nWe built Atlas, a hybrid data plane enabled by a runtime-kernel co-design that simultaneously enables accesses via these two data paths to provide high efficiency for real-world applications. Atlas uses always-on profiling to continuously measure page locality. For workloads already with good locality, paging is used to fetch data, whereas for those without, object fetching is employed. Object fetching moves objects that are accessed close in time to contiguous local space, dynamically improving locality and making the execution increasingly amenable to paging, which is much more resource-efficient. Our evaluation shows that Atlas improves the throughput (e.g., by 1.5x and 3.2x) and reduces the tail latency (e.g., by one and two orders of magnitude) when using remote memory, compared with AIFM and Fastswap, the state-of-the-art techniques respectively in the two categories.",
      "url": "https://www.microsoft.com/en-us/research/publication/a-tale-of-two-paths-toward-a-hybrid-data-plane-for-efficient-far-memory-applications/"
    },
    {
      "title": "Democratizing Protein Language Models with Parameter-Efficient Fine-Tuning",
      "authors": [
        "Bonnie Berger",
        "Juan M. Lavista Ferres",
        "Meghana Kshirsagar",
        "Minkyung Baek",
        "Rahul Dodhia",
        "Samuel Sledzieski"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Medical, health and genomics"
      ],
      "publication_date": "June 2024",
      "abstract": "Proteomics has been revolutionized by large pre-trained protein language models, which learn unsupervised representations from large corpora of sequences. The parameters of these models are then fine-tuned in a supervised setting to tailor the model to a specific downstream task. However, as model size increases, the computational and memory footprint of fine-tuning becomes a barrier for many research groups. In the field of natural language processing, which has seen a similar explosion in the size of models, these challenges have been addressed by methods for parameter-efficient fine-tuning (PEFT). In this work, we newly bring parameter-efficient fine-tuning methods to proteomics. Using the parameter-efficient method LoRA, we train new models for two important proteomic tasks: predicting protein-protein interactions (PPI) and predicting the symmetry of homooligomers. We show that for homooligomer symmetry prediction, these approaches achieve performance competitive with traditional fine-tuning while requiring reduced memory and using three orders of magnitude fewer parameters. On the PPI prediction task, we surprisingly find that PEFT models actually outperform traditional fine-tuning while using two orders of magnitude fewer parameters. Here, we go even further to show that freezing the parameters of the language model and training only a classification head also outperforms fine-tuning, using five orders of magnitude fewer parameters, and that both of these models outperform state-of-the-art PPI prediction methods with substantially reduced compute. We also demonstrate that PEFT is robust to variations in training hyper-parameters, and elucidate where best practices for PEFT in proteomics differ from in natural language processing. Thus, we provide a blueprint to democratize the power of protein language model tuning to groups which have limited computational resources.",
      "url": "https://www.microsoft.com/en-us/research/publication/democratizing-protein-language-models-with-parameter-efficient-fine-tuning/"
    },
    {
      "title": "GeoMFormer: A General Architecture for Geometric Molecular Representation Learning",
      "authors": [
        "Di He",
        "Liwei Wang",
        "Shengjie Luo",
        "Shuxin Zheng",
        "Tianlang Chen",
        "Tie-Yan Liu"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "June 2024",
      "abstract": "Molecular modeling, a central topic in quantum mechanics, aims to accurately calculate the properties and simulate the behaviors of molecular systems. The molecular model is governed by physical laws, which impose geometric constraints such as invariance and equivariance to coordinate rotation and translation. While numerous deep learning approaches have been developed to learn molecular representations under these constraints, most of them are built upon heuristic and costly modules. We argue that there is a strong need for a general and flexible framework for learning both invariant and equivariant features. In this work, we introduce a novel Transformer-based molecular model called GeoMFormer to achieve this goal. Using the standard Transformer modules, two separate streams are developed to maintain and learn invariant and equivariant representations. Carefully designed cross-attention modules bridge the two streams, allowing information fusion and enhancing geometric modeling in each stream. As a general and flexible architecture, we show that many previous architectures can be viewed as special instantiations of GeoMFormer. Extensive experiments are conducted to demonstrate the power of GeoMFormer. All empirical results show that GeoMFormer achieves strong performance on both invariant and equivariant tasks of different types and scales.",
      "url": "https://www.microsoft.com/en-us/research/publication/geomformer-a-general-architecture-for-geometric-molecular-representation-learning/"
    },
    {
      "title": "TorchSpatial: A Location Encoding Framework and Benchmark for Spatial Representation Learning",
      "authors": [
        "Akshay Nambi",
        "Gengchen Mai",
        "Hongxu Ma",
        "Jielu Zhang",
        "Joshua Ni",
        "Lan Mu",
        "Nemin Wu",
        "Ni Lao",
        "Qian Cao",
        "Stefano Ermon",
        "Tanuja Ganu",
        "Xiaobai Yao",
        "Yanlin Qi",
        "Zeping Liu",
        "Zhangyu Wang"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "June 2024",
      "abstract": "Spatial representation learning (SRL) aims at learning general-purpose neural network representations from various types of spatial data (e.g., points, polylines, polygons, networks, images, etc.) in their native formats. Learning good spatial representations is a fundamental problem for various downstream applications such as species distribution modeling, weather forecasting, trajectory generation, geographic question answering, etc. Even though SRL has become the foundation of almost all geospatial artificial intelligence (GeoAI) research, we have not yet seen significant efforts to develop an extensive deep learning framework and benchmark to support SRL model development and evaluation. To fill this gap, we propose TorchSpatial, a learning framework and benchmark for location (point) encoding, which is one of the most fundamental data types of spatial representation learning. TorchSpatial contains three key components: 1) a unified location encoding framework that consolidates 15 commonly recognized location encoders, ensuring scalability and reproducibility of the implementations; 2) the LocBench benchmark tasks encompassing 7 geo-aware image classification and 4 geo-aware image regression datasets; 3) a comprehensive suite of evaluation metrics to quantify geo-aware models’ overall performance as well as their geographic bias, with a novel Geo-Bias Score metric. Finally, we provide a detailed analysis and insights into the model performance and geographic bias of different location encoders. We believe TorchSpatial will foster future advancement of spatial representation learning and spatial fairness in GeoAI research. The TorchSpatial model framework, LocBench, and Geo-Bias Score evaluation framework are available at https://github.com/seai-lab/TorchSpatial.",
      "url": "https://www.microsoft.com/en-us/research/publication/torchspatial-a-location-encoding-framework-and-benchmark-for-spatial-representation-learning/"
    },
    {
      "title": "Impact of Decentralized Learning on Player Utilities in Stackelberg Games",
      "authors": [
        "Aleksandrs Slivkins",
        "Brendan Lucier",
        "Kate Donahue",
        "Meena Jagadeesan",
        "Nicole Immorlica"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "June 2024",
      "abstract": "When deployed in the world, a learning agent such as a recommender system or a chatbot often repeatedly interacts with another learning agent (such as a user) over time. In many such two-agent systems, each agent learns separately and the rewards of the two agents are not perfectly aligned. To better understand such cases, we examine the learning dynamics of the two-agent system and the implications for each agent’s objective. We model these systems as Stackelberg games with decentralized learning and show that standard regret benchmarks (such as Stackelberg equilibrium payoffs) result in worst-case linear regret for at least one player. To better capture these systems, we construct a relaxed regret benchmark that is tolerant to small learning errors by agents. We show that standard learning algorithms fail to provide sublinear regret, and we develop algorithms to achieve near-optimal \\(O(T^{2/3})\\) regret for both players with respect to these benchmarks. We further design relaxed environments under which faster learning \\((O(\\sqrt{T}))\\) is possible. Altogether, our results take a step towards assessing how two-agent interactions in sequential and decentralized learning environments affect the utility of both agents.",
      "url": "https://www.microsoft.com/en-us/research/publication/impact-of-decentralized-learning-on-player-utilities-in-stackelberg-games/"
    },
    {
      "title": "T-MAC: CPU Renaissance via Table Lookup for Low-Bit LLM Deployment on Edge",
      "authors": [
        "Jianyu Wei",
        "Lei Wang",
        "Lingxiao Ma",
        "Mao Yang",
        "Shijie Cao",
        "Ting Cao",
        "Yanyong Zhang"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "June 2024",
      "abstract": "The deployment of Large Language Models (LLMs) on edge devices is increasingly important to enhance on-device intelligence. Weight quantization is crucial for reducing the memory footprint of LLMs on devices. However, low-bit LLMs necessitate mixed precision matrix multiplication (mpGEMM) of low precision weights and high precision activations during inference. Existing systems, lacking native support for mpGEMM, resort to dequantize weights for high precision computation. Such an indirect way can lead to a significant inference overhead. In this paper, we introduce T-MAC, an innovative lookup table(LUT)-based method designed for efficient low-bit LLM (i.e., weight-quantized LLM) inference on CPUs. T-MAC directly supports mpGEMM without dequantization, while simultaneously eliminating multiplications and reducing additions required. Specifically, T-MAC transforms the traditional data-type-centric multiplication to bit-wise table lookup, and enables a unified and scalable mpGEMM solution. Our LUT-based kernels scale linearly to the weight bit-width. Evaluated on low-bit Llama and BitNet models, T-MAC demonstrates up to 4x increase in throughput and 70% reduction in energy consumption compared to llama.cpp. For BitNet-b1.58-3B, T-MAC delivers a token generation throughput of 30 tokens/s with a single core and 71 tokens/s with eight cores on M2-Ultra, and 11 tokens/s on lower-end devices like Raspberry Pi 5, which significantly exceeds the adult average reading speed. T-MAC with LUT-based computing paradigm, paves the way for the practical deployment of low-bit LLMs on resource-constrained edge devices without compromising computational efficiency. The system is open-sourced at https://github.com/microsoft/T-MAC (opens in new tab).",
      "url": "https://www.microsoft.com/en-us/research/publication/t-mac-cpu-renaissance-via-table-lookup-for-low-bit-llm-deployment-on-edge/"
    },
    {
      "title": "CARE: a Benchmark Suite for the Classification and Retrieval of Enzymes",
      "authors": [
        "A. Anandkumar",
        "Ariane Mora",
        "Bruce J. Wittmann",
        "Frances H. Arnold",
        "Jason Yang",
        "Shengchao Liu",
        "Yisong Yue"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "June 2024",
      "abstract": "Enzymes are important proteins that catalyze chemical reactions. In recent years, machine learning methods have emerged to predict enzyme function from sequence; however, there are no standardized benchmarks to evaluate these methods. We introduce CARE, a benchmark and dataset suite for the Classification And Retrieval of Enzymes (CARE). CARE centers on two tasks: (1) classification of a protein sequence by its enzyme commission (EC) number and (2) retrieval of an EC number given a chemical reaction. For each task, we design train-test splits to evaluate different kinds of out-of-distribution generalization that are relevant to real use cases. For the classification task, we provide baselines for state-of-the-art methods. Because the retrieval task has not been previously formalized, we propose a method called Contrastive Reaction-EnzymE Pretraining (CREEP) as one of the first baselines for this task. CARE is available at https://github.com/jsunn-y/CARE/ (opens in new tab).",
      "url": "https://www.microsoft.com/en-us/research/publication/care-a-benchmark-suite-for-the-classification-and-retrieval-of-enzymes/"
    },
    {
      "title": "Jacdac: Service-Based Prototyping of Embedded Systems",
      "authors": [
        "James Devine",
        "Jonathan \"Peli\" de Halleux",
        "Michal Moskal",
        "Steve Hodges",
        "Thomas Ball"
      ],
      "research_areas": [
        "Programming languages and software engineering"
      ],
      "publication_date": "June 2024",
      "abstract": "The traditional approach to programming embedded systems is monolithic: firmware on a microcontroller contains both application code and the drivers needed to communicate with sensors and actuators, using low-level protocols such as I2C, SPI, and RS232. In comparison, software development for the cloud has moved to a service-based development and operation paradigm: a service provides a discrete unit of functionality that can be accessed remotely by an application, or other service, but is independently managed and updated.\nWe propose, design, implement, and evaluate a service-based approach to prototyping embedded systems called Jacdac (opens in new tab). Jacdac defines a service specification language, designed especially for embedded systems, along with a host of specifications for a variety of sensors and actuators. With Jacdac, each sensor/actuator in a system is paired with a low-cost microcontroller that advertises the services that represent the functionality of the underlying hardware over an efficient and low-cost single-wire bus protocol. A separate microcontroller executes the user’s application program, which is a client of the Jacdac services on the bus.\nOur evaluation shows that Jacdac supports a service-based abstraction for sensors/actuators at low cost and reasonable performance, with many benefits for prototyping: ease of use via the automated discovery of devices and their capabilities, substitution of same-service devices for each other, as well as high-level programming, monitoring, and debugging. We also report on the experience of bringing Jacdac to commercial availability via third-party manufacturers.",
      "url": "https://www.microsoft.com/en-us/research/publication/jacdac-pldi2024/"
    },
    {
      "title": "Evolving Roles and Workflows of Creative Practitioners in the Age of Generative AI",
      "authors": [
        "Gonzalo Ramos",
        "Srishti Palani"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Human-computer interaction"
      ],
      "publication_date": "June 2024",
      "abstract": "Creative practitioners (like designers, software developers, and architects) have started to employ Generative AI models (GenAI) to produce text, images, and assets comparable to those made by people. While HCI research explores specific GenAI models and creativity support tools, little is known about practitioners’ evolving roles and workflows with GenAI models across a project’s stages. This knowledge is key to guide the development of the new generation of Creativity Support Tools. We contribute to this knowledge by employing a triangulated method to capture interviews, videos, and survey responses of creative practitioners reflecting on projects they completed with GenAI. Our observations let us derive a set of factors that capture practitioners’ perceived roles, challenges, benefits, and interaction patterns when creating with GenAI. From these factors, we offer insights and propose design opportunities and priorities that serve to encourage reflection from the wider community of Creativity Support Tools and GenAI stakeholders such as systems creators, researchers, and educators on how to develop systems that meet the needs of creatives in human-centered ways.",
      "url": "https://www.microsoft.com/en-us/research/publication/evolving-roles-and-workflows-of-creative-practitioners-in-the-age-of-generative-ai/"
    },
    {
      "title": "Hierarchical Intra-modal Correlation Learning for Label-free 3D Semantic Segmentation",
      "authors": [
        "Jiahao Li",
        "Lei Chu",
        "Xin Kang",
        "Xuejin Chen",
        "Yan Lu"
      ],
      "research_areas": [
        "Computer vision"
      ],
      "publication_date": "June 2024",
      "abstract": "Recent methods for label-free 3D semantic segmentation aim to assist 3D model training by leveraging the open-world recognition ability of pre-trained vision language models. However, these methods usually suffer from inconsistent and noisy pseudo-labels provided by the vision language models. To address this issue, we present a hierarchical intra-modal correlation learning framework that captures visual and geometric correlations in 3D scenes at three levels: intra-set, intra-scene, and inter-scene, to help learn more compact 3D representations. We refine pseudo-labels using intra-set correlations within each geometric consistency set and align features of visually and geometrically similar points using intra-scene and inter-scene correlation learning. We also introduce a feedback mechanism to distill the correlation learning capability into the 3D model. Experiments on both indoor and outdoor datasets show the superiority of our method. We achieve a state-of-the-art 36.6% mIoU on the ScanNet dataset, and a 23.0% mIoU on the nuScenes dataset, with improvements of 7.8% mIoU and 2.2% mIoU compared with previous SOTA. We also provide theoretical analysis and qualitative visualization results to discuss the mechanism and conduct thorough ablation studies to support the effectiveness of our framework.",
      "url": "https://www.microsoft.com/en-us/research/publication/hierarchical-intra-modal-correlation-learning-for-label-free-3d-semantic-segmentation/"
    },
    {
      "title": "Instruction Pre-Training: Language Models are Supervised Multitask Learners",
      "authors": [
        "Daixuan Cheng",
        "Furu Wei",
        "Junyu Bi",
        "Minlie Huang",
        "Shaohan Huang",
        "Yuxian Gu"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "June 2024",
      "abstract": "Unsupervised multitask pre-training has been the critical method behind the recent success of language models (LMs). However, supervised multitask learning still holds significant promise, as scaling it in the post-training stage trends towards better generalization. In this paper, we explore supervised multitask pre-training by proposing Instruction Pre-Training, a framework that scalably augments massive raw corpora with instruction-response pairs to pre-train LMs. The instruction-response pairs are generated by an efficient instruction synthesizer built on open-source models. In our experiments, we synthesize 200M instruction-response pairs covering 40+ task categories to verify the effectiveness of Instruction Pre-Training. In pre-training from scratch, Instruction Pre-Training not only consistently enhances pre-trained base models but also benefits more from further instruction tuning. In continual pre-training, Instruction Pre-Training enables Llama3-8B to be comparable to or even outperform Llama3-70B. Our model, code, and data are available at https://github.com/microsoft/LMOps (opens in new tab).",
      "url": "https://www.microsoft.com/en-us/research/publication/instruction-pre-training-language-models-are-supervised-multitask-learners/"
    },
    {
      "title": "FastPersist: Accelerating Model Checkpointing in Deep Learning",
      "authors": [
        "Bing Xie",
        "Guanhua Wang",
        "Olatunji Ruwase",
        "Yuxiong He"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Systems and networking"
      ],
      "publication_date": "June 2024",
      "abstract": "Model checkpoints are critical Deep Learning (DL) artifacts that enable fault tolerance for training and downstream applications, such as inference. However, writing checkpoints to persistent storage, and other I/O aspects of DL training, are mostly ignored by compute-focused optimization efforts for faster training of rapidly growing models and datasets. Towards addressing this imbalance, we propose FastPersist to accelerate checkpoint creation in DL training. FastPersist combines three novel techniques: (i) NVMe optimizations for faster checkpoint writes to SSDs, (ii) efficient write parallelism using the available SSDs in training environments, and (iii) overlapping checkpointing with independent training computations. Our evaluation using real world dense and sparse DL models shows that FastPersist creates checkpoints in persistent storage up to 116x faster than baseline, and enables per-iteration checkpointing with negligible overhead.",
      "url": "https://www.microsoft.com/en-us/research/publication/fastpersist-accelerating-model-checkpointing-in-deep-learning/"
    },
    {
      "title": "Explaining CLIP’s performance disparities on data from blind/low vision users",
      "authors": [
        "Agnieszka Slowik",
        "Camilla Longden",
        "Cecily Morrison",
        "Daniela Massiceti",
        "Martin Grayson",
        "Samuel Wills"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Computer vision",
        "Human-computer interaction"
      ],
      "publication_date": "June 2024",
      "abstract": "Large multi-modal models (LMMs) hold the potential to usher in a new era of automated visual assistance for people who are blind or low vision (BLV). Yet, these models have not been systematically evaluated on data captured by BLV users. We address this by empirically assessing CLIP, a widely-used LMM likely to underpin many assistive technologies. Testing 25 CLIP variants in a zero-shot classification task, we find that their accuracy is 15 percentage points lower on average for images captured by BLV users than web-crawled images. This disparity stems from CLIP’s sensitivities to 1) image content (e.g. not recognizing disability objects as well as other objects); 2) image quality (e.g. not being robust to lighting variation); and 3) text content (e.g. not recognizing objects described by tactile adjectives as well as visual ones). We delve deeper with a textual analysis of three common pre-training datasets: LAION-400M, LAION-2B and DataComp-1B, showing that disability content is rarely mentioned. We then provide three examples that illustrate how the performance disparities extend to three downstream models underpinned by CLIP: OWL-ViT, CLIPSeg and DALL-E2. We find that few-shot learning with as few as 5 images can mitigate CLIP’s quality-of-service disparities for BLV users in some scenarios, which we discuss alongside a set of other possible mitigations.",
      "url": "https://www.microsoft.com/en-us/research/publication/explaining-clips-performance-disparities-on-data-from-blind-low-vision-users/"
    },
    {
      "title": "ROCOv2: Radiology Objects in COntext Version 2, an Updated Multimodal Image Dataset",
      "authors": [
        "Ahmad Idrissi-Yaghir",
        "Alba García Seco de Herrera",
        "Asma Ben Abacha",
        "Christoph M. Friedrich",
        "Cynthia S. Schmidt",
        "Felix Nensa",
        "Henning Müller",
        "Henning Schäfer",
        "Johannes Rückert",
        "Louise Bloch",
        "Obioma Pelka",
        "Peter A. Horn",
        "Raphael Brüngel",
        "Sven Koitka"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Computer vision",
        "Medical, health and genomics"
      ],
      "publication_date": "June 2024",
      "abstract": "Automated medical image analysis systems often require large amounts of training data with high quality labels, which are difficult and time consuming to generate. This paper introduces Radiology Object in COntext version 2 (ROCOv2), a multimodal dataset consisting of radiological images and associated medical concepts and captions extracted from the PMC Open Access subset. It is an updated version of the ROCO dataset published in 2018, and adds 35,705 new images added to PMC since 2018. It further provides manually curated concepts for imaging modalities with additional anatomical and directional concepts for X-rays. The dataset consists of 79,789 images and has been used, with minor modifications, in the concept detection and caption prediction tasks of ImageCLEFmedical Caption 2023. The dataset is suitable for training image annotation models based on image-caption pairs, or for multi-label image classification using Unified Medical Language System (UMLS) concepts provided with each image. In addition, it can serve for pre-training of medical domain models, and evaluation of deep learning models for multi-task learning.",
      "url": "https://www.microsoft.com/en-us/research/publication/rocov2-radiology-objects-in-context-version-2-an-updated-multimodal-image-dataset/"
    },
    {
      "title": "Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks",
      "authors": [
        "Bin Xiao",
        "Ce Liu",
        "Haiping Wu",
        "Houdong Hu",
        "Lu Yuan",
        "Michael Zeng",
        "Weijian Xu",
        "Xiyang Dai",
        "Yumao Lu"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Computer vision"
      ],
      "publication_date": "June 2024",
      "abstract": "We introduce Florence-2, a novel vision foundation model with a unified, prompt-based representation for a variety of computer vision and vision-language tasks. While existing large vision models excel in transfer learning, they struggle to perform a diversity of tasks with simple instructions, a capability that implies handling the complexity of various spatial hierarchy and semantic granularity. Florence-2 was designed to take text-prompt as task instructions and generate desirable results in text forms, whether it be captioning, object detection, grounding or segmentation. This multi-task learning setup demands large-scale, high-quality annotated data. To this end, we co-developed FLD-5B that consists of 5.4 billion comprehensive visual annotations on 126 million images, using an iterative strategy of automated image annotation and model refinement. We adopted a sequence-to-sequence structure to train Florence-2 to perform versatile and comprehensive vision tasks. Extensive evaluations on numerous tasks demonstrated Florence-2 to be a strong vision foundation model contender with unprecedented zero-shot and fine-tuning capabilities.",
      "url": "https://www.microsoft.com/en-us/research/publication/florence-2-advancing-a-unified-representation-for-a-variety-of-vision-tasks/"
    },
    {
      "title": "Raising the Bar: Investigating the Values of Large Language Models via Generative Evolving Testing",
      "authors": [
        "Han Jiang",
        "Shu Wang",
        "Xiaoyuan Yi",
        "Xing Xie",
        "Zhihua Wei"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "June 2024",
      "abstract": "Warning: Contains harmful model outputs. Despite significant advancements, the propensity of Large Language Models (LLMs) to generate harmful and unethical content poses critical challenges. Measuring value alignment of LLMs becomes crucial for their regulation and responsible deployment. Although numerous benchmarks have been constructed to assess social bias, toxicity, and ethical issues in LLMs, those static benchmarks suffer from evaluation chronoeffect, in which, as models rapidly evolve, existing benchmarks may leak into training data or become saturated, overestimating ever-developing LLMs. To tackle this problem, we propose GETA, a novel generative evolving testing approach based on adaptive testing methods in measurement theory. Unlike traditional adaptive testing methods that rely on a static test item pool, GETA probes the underlying moral boundaries of LLMs by dynamically generating test items tailored to model capability. GETA co-evolves with LLMs by learning a joint distribution of item difficulty and model value conformity, thus effectively addressing evaluation chronoeffect. We evaluated various popular LLMs with GETA and demonstrated that 1) GETA can dynamically create difficulty-tailored test items and 2) GETA’s evaluation results are more consistent with models’ performance on unseen OOD and i.i.d. items, laying the groundwork for future evaluation paradigms.",
      "url": "https://www.microsoft.com/en-us/research/publication/raising-the-bar-investigating-the-values-of-large-language-models-via-generative-evolving-testing/"
    },
    {
      "title": "MMCTAgent: Multi-modal Critical Thinking Agent Framework for Complex Visual Reasoning",
      "authors": [
        "Akshay Nambi",
        "Somnath Kumar",
        "Tanuja Ganu",
        "Yash Gadhia"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Computer vision"
      ],
      "publication_date": "June 2024",
      "abstract": "Recent advancements in Multi-modal Large Language Models (MLLMs) have significantly improved their performance in tasks combining vision and language. However, challenges persist in detailed multi-modal understanding, comprehension of complex tasks, and reasoning over multi-modal information. This paper introduces MMCTAgent, a novel multi-modal critical thinking agent framework designed to address the inherent limitations of current MLLMs in complex visual reasoning tasks. Inspired by human cognitive processes and critical thinking, MMCTAgent iteratively analyzes multi-modal information, decomposes queries, plans strategies, and dynamically evolves its reasoning. Additionally, MMCTAgent incorporates critical thinking elements such as verification of final answers and self-reflection through a novel approach that defines a vision-based critic and identifies task-specific evaluation criteria, thereby enhancing its decision-making abilities. Through rigorous evaluations across various image and video understanding benchmarks, we demonstrate that MMCTAgent (with and without the critic) outperforms both foundational MLLMs and other tool-augmented pipelines.",
      "url": "https://www.microsoft.com/en-us/research/publication/mmctagent-multi-modal-critical-thinking-agent-framework-for-complex-visual-reasoning/"
    },
    {
      "title": "Known pathogenic gene variants and new candidates detected in Sudden Unexpected Infant Death using Whole Genome Sequencing",
      "authors": [
        "Addie Nesbitt",
        "Andrew Timms",
        "Angela M Bard",
        "Avni Santani",
        "Chelsea M Pagan",
        "David Jardine",
        "Diego Martinez",
        "Edwin A Mitchell",
        "Elisabeth A Haas",
        "Erdal Cosgun",
        "Jan-Marino Ramirez",
        "Juan M. Lavista Ferres",
        "Kimberly A Aldinger",
        "Lely A Quina",
        "Lindsay V Clark",
        "Soumitra Barua",
        "Tatiana M Becker",
        "Zakkary McNutt"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Medical, health and genomics"
      ],
      "publication_date": "June 2024",
      "abstract": "The purpose of this study is to gain insights into potential genetic factors contributing to the infant’s vulnerability to Sudden Unexpected Infant Death (SUID). Whole Genome Sequencing (WGS) was performed on 144 infants that succumbed to SUID, and 573 healthy adults. Variants were filtered by gnomAD allele frequencies and predictions of functional consequences. Variants of interest were identified in 88 genes, in 64.6% of our cohort. Seventy-three of these have been previously associated with SIDS/SUID/SUDP. Forty-three can be characterized as cardiac genes and are related to cardiomyopathies, arrhythmias, and other conditions. Variants in 22 genes were associated with neurologic functions. Variants were also found in 13 genes reported to be pathogenic for various systemic disorders and in two genes associated with immunological function. Variants in eight genes are implicated in the response to hypoxia and the regulation of reactive oxygen species (ROS) and have not been previously described in SIDS/SUID/SUDP. Seventy-two infants met the triple risk hypothesis criteria. Our study confirms and further expands the list of genetic variants associated with SUID. The abundance of genes associated with heart disease and the discovery of variants associated with the redox metabolism have important mechanistic implications for the pathophysiology of SUID.\n",
      "url": "https://www.microsoft.com/en-us/research/publication/known-pathogenic-gene-variants-and-new-candidates-detected-in-sudden-unexpected-infant-death-using-whole-genome-sequencing/"
    },
    {
      "title": "Approximately Equivariant Neural Processes",
      "authors": [
        "Adrian Weller",
        "Cristiana-Diana Diaconu",
        "Matthew Ashman",
        "Richard Turner",
        "Wessel Bruinsma"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "June 2024",
      "abstract": "Equivariant deep learning architectures exploit symmetries in learning problems to improve the sample efficiency of neural-network-based models and their ability to generalise. However, when modelling real-world data, learning problems are often not exactly equivariant, but only approximately. For example, when estimating the global temperature field from weather station observations, local topographical features like mountains break translation equivariance. In these scenarios, it is desirable to construct architectures that can flexibly depart from exact equivariance in a data-driven way. In this paper, we develop a general approach to achieving this using existing equivariant architectures. Our approach is agnostic to both the choice of symmetry group and model architecture, making it widely applicable. We consider the use of approximately equivariant architectures in neural processes (NPs), a popular family of meta-learning models. We demonstrate the effectiveness of our approach on a number of synthetic and real-world regression experiments, demonstrating that approximately equivariant NP models can outperform both their non-equivariant and strictly equivariant counterparts.",
      "url": "https://www.microsoft.com/en-us/research/publication/approximately-equivariant-neural-processes/"
    },
    {
      "title": "Multimodal Needle in a Haystack: Benchmarking Long-Context Capability of Multimodal Large Language Models",
      "authors": [
        "Akshay Nambi",
        "Haizhou Shi",
        "Hao Wang",
        "Hengyi Wang",
        "Shiwei Tan",
        "Tanuja Ganu",
        "Tunyu Zhang",
        "Weiyi Qin",
        "Wenyuan Wang"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Computer vision"
      ],
      "publication_date": "June 2024",
      "abstract": "Multimodal Large Language Models (MLLMs) have shown significant promise in various applications, leading to broad interest from researchers and practitioners alike. However, a comprehensive evaluation of their long-context capabilities remains underexplored. To address these gaps, we introduce the MultiModal Needle-in-a-haystack (MMNeedle) benchmark, specifically designed to assess the long-context capabilities of MLLMs. Besides multi-image input, we employ image stitching to further increase the input context length, and develop a protocol to automatically generate labels for sub-image level retrieval. Essentially, MMNeedle evaluates MLLMs by stress-testing their capability to locate a target sub-image (needle) within a set of images (haystack) based on textual instructions and descriptions of image contents. This setup necessitates an advanced understanding of extensive visual contexts and effective information retrieval within long-context image inputs. With this benchmark, we evaluate state-of-the-art MLLMs, encompassing both API-based and open-source models. The findings reveal that GPT-4o consistently surpasses other models in long-context scenarios, but suffers from hallucination problems in negative samples, i.e., when needles are not in the haystacks. Our comprehensive long-context evaluation of MLLMs also sheds lights on the considerable performance gap between API-based and open-source models. ",
      "url": "https://www.microsoft.com/en-us/research/publication/multimodal-needle-in-a-haystack-benchmarking-long-context-capability-of-multimodal-large-language-models/"
    },
    {
      "title": "Exposing the Achilles’ Heel: Evaluating LLMs Ability to Handle Mistakes in Mathematical Reasoning",
      "authors": [
        "Akshay Nambi",
        "Joykirat Singh",
        "Vibhav Vineet"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "June 2024",
      "abstract": "Large Language Models (LLMs) have been applied to Math Word Problems (MWPs) with transformative impacts, revolutionizing how these complex problems are approached and solved in various domains including educational settings. However, the evaluation of these models often prioritizes final accuracy, overlooking the crucial aspect of reasoning capabilities. This work addresses this gap by focusing on the ability of LLMs to detect and correct reasoning mistakes. We introduce a novel dataset MWP-MISTAKE, incorporating MWPs with both correct and incorrect reasoning steps generated through rule-based methods and smaller language models. Our comprehensive benchmarking reveals significant insights into the strengths and weaknesses of state-of-the-art models, such as GPT-4o, GPT-4, GPT-3.5Turbo, and others. We highlight GPT-$o’s superior performance in mistake detection and rectification and the persistent challenges faced by smaller models. Additionally, we identify issues related to data contamination and memorization, impacting the reliability of LLMs in real-world applications. Our findings emphasize the importance of rigorous evaluation of reasoning processes and propose future directions to enhance the generalization and robustness of LLMs in mathematical problem-solving.",
      "url": "https://www.microsoft.com/en-us/research/publication/exposing-the-achilles-heel-evaluating-llms-ability-to-handle-mistakes-in-mathematical-reasoning/"
    },
    {
      "title": "Interpreting User Requests in the Context of Natural Language Standing Instructions",
      "authors": [
        "Ben Van Durme",
        "Harsh Jhamtani",
        "Jacob Andreas",
        "Jason Eisner",
        "Nikita Moghe",
        "Patrick Xia"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "June 2024",
      "abstract": "Users of natural language interfaces, generally powered by Large Language Models (LLMs),often must repeat their preferences each time they make a similar request. We describe an approach to LLM-based dialogue modeling in which persistent user constraints and preferences — collectively termed standing instructions — as additional context for such interfaces. For example, when a user states”I’m hungry”, a previously expressed preference for Persian food can be automatically added to the LLM prompt, influencing the search for relevant restaurants. We develop NLSI, a language-to-program dataset consisting of over 2.4K dialogues spanning 17 domains, where each dialogue is paired with a user profile (a set of users specific standing instructions) and corresponding structured representations (API calls). A key challenge in NLSI is to identify which subset of the standing instructions is applicable to a given dialogue. NLSI contains diverse phenomena, from simple preferences to interdependent instructions such as triggering a hotel search whenever the user is booking tickets to an event. We conduct experiments on NLSI using prompting with large language models and various retrieval approaches, achieving a maximum of 44.7% exact match on API prediction. Our results demonstrate the challenges in identifying the relevant standing instructions and their interpretation into API calls.",
      "url": "https://www.microsoft.com/en-us/research/publication/interpreting-user-requests-in-the-context-of-natural-language-standing-instructions/"
    },
    {
      "title": "VideoGUI: A Benchmark for GUI Automation from Instructional Videos",
      "authors": [
        "Difei Gao",
        "Kevin Qinghong Lin",
        "Lijuan Wang",
        "Linjie Li",
        "Mike Zheng Shou",
        "Mingyi Yan",
        "Qinchen Wu",
        "Zhengyuan Yang"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "June 2024",
      "abstract": "Graphical User Interface (GUI) automation holds significant promise for enhancing human productivity by assisting with computer tasks. Existing task formulations primarily focus on simple tasks that can be specified by a single, language-only instruction, such as”Insert a new slide.”In this work, we introduce VideoGUI, a novel multi-modal benchmark designed to evaluate GUI assistants on visual-centric GUI tasks. Sourced from high-quality web instructional videos, our benchmark focuses on tasks involving professional and novel software (e.g., Adobe Photoshop or Stable Diffusion WebUI) and complex activities (e.g., video editing). VideoGUI evaluates GUI assistants through a hierarchical process, allowing for identification of the specific levels at which they may fail: (i) high-level planning: reconstruct procedural subtasks from visual conditions without language descriptions; (ii) middle-level planning: generate sequences of precise action narrations based on visual state (i.e., screenshot) and goals; (iii) atomic action execution: perform specific actions such as accurately clicking designated elements. For each level, we design evaluation metrics across individual dimensions to provide clear signals, such as individual performance in clicking, dragging, typing, and scrolling for atomic action execution. Our evaluation on VideoGUI reveals that even the SoTA large multimodal model GPT4o performs poorly on visual-centric GUI tasks, especially for high-level planning.",
      "url": "https://www.microsoft.com/en-us/research/publication/videogui-a-benchmark-for-gui-automation-from-instructional-videos/"
    },
    {
      "title": "BIOCLIP: A Vision Foundation Model for the Tree of Life",
      "authors": [
        "Chan Hee Song",
        "Charles Stewart",
        "David Carlyn",
        "Elizabeth G. Campolongo",
        "Jiaman Wu",
        "Li Dong",
        "Matthew J Thompson",
        "Samuel Stevens",
        "Tanya Y. Berger-Wolf",
        "W. Dahdul",
        "Wei-Lun Chao",
        "Yu Su"
      ],
      "research_areas": [
        "Computer vision"
      ],
      "publication_date": "June 2024",
      "abstract": "Images of the natural world, collected by a variety of cameras, from drones to individual phones, are increasingly abundant sources of biological information. There is an explosion of computational methods and tools, particularly computer vision, for extracting biologically relevant information from images for science and conservation. Yet most of these are bespoke approaches designed for a specific task and are not easily adaptable or extendable to new questions, contexts, and datasets. A vision model for general organismal biology questions on images is of timely need. To approach this, we curate and release TreeOfLife-10M, the largest and most diverse ML-ready dataset of biology images. We then develop BioCLIP, a foundation model for the tree of life, leveraging the unique properties of biology captured by TreeOfLife-10M, namely the abundance and variety of images of plants, animals, and fungi, together with the availability of rich structured biological knowledge. We rigorously benchmark our approach on diverse fine-grained biology classification tasks, and find that BioCLIP consistently and substantially outperforms existing baselines (by 17% to 20% absolute). Intrinsic evaluation reveals that BioCLIP has learned a hierarchical representation conforming to the tree of life, shedding light on its strong generalizability. Our code, models and data will be made available at https://github.com/Imageomics/bioclip (opens in new tab).",
      "url": "https://www.microsoft.com/en-us/research/publication/bioclip-a-vision-foundation-model-for-the-tree-of-life/"
    },
    {
      "title": "A Label is Worth a Thousand Images in Dataset Distillation",
      "authors": [
        "David Alvarez-Melis",
        "Tian Qin",
        "Zhiwei Deng"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "June 2024",
      "abstract": "Data *quality* is a crucial factor in the performance of machine learning models, a principle that dataset distillation methods exploit by compressing training datasets into much smaller counterparts that maintain similar downstream performance. Understanding how and why data distillation methods work is vital not only for improving these methods but also for revealing fundamental characteristics of “good” training data. However, a major challenge in achieving this goal is the observation that distillation approaches, which rely on sophisticated but mostly disparate methods to generate synthetic data, have little in common with each other. In this work, we highlight a largely overlooked aspect common to most of these methods: the use of soft (probabilistic) labels. Through a series of ablation experiments, we study the role of soft labels in depth. Our results reveal that the main factor explaining the performance of state-of-the-art distillation methods is not the specific techniques used to generate synthetic data but rather the use of soft labels. Furthermore, we demonstrate that not all soft labels are created equal; they must contain *structured information* to be beneficial. We also provide empirical scaling laws that characterize the effectiveness of soft labels as a function of images-per-class in the distilled dataset and establish an empirical Pareto frontier for data-efficient learning. Combined, our findings challenge conventional wisdom in dataset distillation, underscore the importance of soft labels in learning, and suggest new directions for improving distillation methods. Code for all experiments is available at https://github.com/sunnytqin/no-distillation (opens in new tab).",
      "url": "https://www.microsoft.com/en-us/research/publication/a-label-is-worth-a-thousand-images-in-dataset-distillation/"
    },
    {
      "title": "Meet MicroCode: a Live and Portable Programming Tool for the BBC micro:bit",
      "authors": [
        "Elisa Rubegni",
        "Eric Anderson",
        "James Devine",
        "Joe Finney",
        "Jonathan \"Peli\" de Halleux",
        "Kobi Hartley",
        "Lorraine Underwood",
        "Michal Moskal",
        "Steve Hodges",
        "Thomas Ball"
      ],
      "research_areas": [
        "Human-computer interaction"
      ],
      "publication_date": "June 2024",
      "abstract": "Physical computing has emerged as an effective approach to introducing computing and coding to students. One of the most popular enabling tools is the BBC micro:bit, well-known for its positive impact on teaching programming and driving engagement in the classroom. We extend these benefits by developing a new approach to coding with micro:bit: MicroCode. Unlike other experiences, MicroCode couples the micro:bit with a low-cost handheld accessory to enable live and portable programming via an on-device visual programming language; no separate host computer is needed. We present the design of MicroCode and the findings of a study in which we investigated teachers’ perspectives and children’s experiences. We interviewed five primary school teachers and evaluated MicroCode with 60 children aged 10-11. Our findings show that MicroCode raised children’s engagement and stimulated the development of a strong sense of agency, while teachers felt empowered to adopt situated and cross-curricular learning approaches.",
      "url": "https://www.microsoft.com/en-us/research/publication/meet-microcode-idc2024/"
    },
    {
      "title": "GLACE: Global Local Accelerated Coordinate Encoding",
      "authors": [
        "Christoph Vogel",
        "Fangjinhua Wang",
        "Marc Pollefeys",
        "Silvano Galliani",
        "Xudong Jiang"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Computer vision"
      ],
      "publication_date": "June 2024",
      "abstract": "Scene coordinate regression (SCR) methods are a family of visual localization methods that directly regress 2D-3D matches for camera pose estimation. They are effective in small-scale scenes but face significant challenges in large-scale scenes that are further amplified in the absence of ground truth 3D point clouds for supervision. Here, the model can only rely on reprojection constraints and needs to implicitly triangulate the points. The challenges stem from a fundamental dilemma: The network has to be invariant to observations of the same landmark at different viewpoints and lighting conditions, etc., but at the same time discriminate unrelated but similar observations. The latter becomes more relevant and severe in larger scenes. In this work, we tackle this problem by introducing the concept of co-visibility to the network. We propose GLACE, which integrates pre-trained global and local encodings and enables SCR to scale to large scenes with only a single small-sized network. Specifically, we propose a novel feature diffusion technique that implicitly groups the reprojection constraints with co-visibility and avoids overfitting to trivial solutions. Additionally, our position decoder parameterizes the output positions for large-scale scenes more effectively. Without using 3D models or depth maps for supervision, our method achieves state-of-the-art results on large-scale scenes with a low-map-size model. On Cambridge landmarks, with a single model, we achieve 17% lower median position error than Poker, the ensemble variant of the state-of-the-art SCR method ACE. Code is available at: https://github.com/cvg/glace (opens in new tab).",
      "url": "https://www.microsoft.com/en-us/research/publication/glace-global-local-accelerated-coordinate-encoding/"
    },
    {
      "title": "SeD: Semantic-Aware Discriminator for Image Super-Resolution",
      "authors": [
        "Bingchen Li",
        "Hanxin Zhu",
        "Ruoyu Feng",
        "Xin Li",
        "Yeying Jin",
        "Zhibo Chen",
        "Zhizheng Zhang"
      ],
      "research_areas": [
        "Computer vision"
      ],
      "publication_date": "June 2024",
      "abstract": "Generative Adversarial Networks (GANs) have been widely used to recover vivid textures in image super-resolution (SR) tasks. In particular, one discriminator is utilized to enable the SR network to learn the distribution of real-world high-quality images in an adversarial training manner. However, the distribution learning is overly coarse-grained, which is susceptible to virtual textures and causes counter-intuitive generation results. To mitigate this, we propose the simple and effective Semantic-aware Discriminator (denoted as SeD), which encourages the SR network to learn the fine-grained distributions by introducing the semantics of images as a condition. Concretely, we aim to excavate the semantics of images from a well-trained semantic extractor. Under different semantics, the discriminator is able to distinguish the real-fake images individually and adaptively, which guides the SR network to learn the more fine-grained semantic-aware textures. To obtain accurate and abundant semantics, we take full advantage of recently popular pretrained vision models (PVMs) with extensive datasets, and then incorporate its semantic features into the discriminator through a well-designed spatial cross-attention module. In this way, our proposed semantic-aware discriminator empowered the SR network to produce more photo-realistic and pleasing images. Extensive experiments on two typical tasks, i.e., SR and Real SR have demonstrated the effectiveness of our proposed methods.",
      "url": "https://www.microsoft.com/en-us/research/publication/sed-semantic-aware-discriminator-for-image-super-resolution/"
    },
    {
      "title": "From RAGs to rich parameters: Probing how language models utilize external knowledge over parametric information for factual queries",
      "authors": [
        "Ehsan Aghazadeh",
        "Hitesh Wadhwa",
        "Rahul Seetharaman",
        "Reshmi Ghosh",
        "Samyadeep Basu",
        "Shreyas Chaudhari",
        "Somyaa Aggarwal",
        "Soundararajan Srinivasan",
        "Wenlong Zhao"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Human language technologies",
        "Search and information retrieval",
        "Technology for emerging markets"
      ],
      "publication_date": "June 2024",
      "abstract": "Retrieval Augmented Generation (RAG) enriches the ability of language models to reason using external context to augment responses for a given user prompt. This approach has risen in popularity due to practical applications in various applications of language models in search, question/answering, and chat-bots. However, the exact nature of how this approach works isn’t clearly understood. In this paper, we mechanistically examine the RAG pipeline to highlight that language models take shortcut and have a strong bias towards utilizing only the context information to answer the question, while relying minimally on their parametric memory. We probe this mechanistic behavior in language models with: (i) Causal Mediation Analysis to show that the parametric memory is minimally utilized when answering a question and (ii) Attention Contributions and Knockouts to show that the last token residual stream do not get enriched from the subject token in the question, but gets enriched from other informative tokens in the context. We find this pronounced shortcut behaviour true across both LLaMa and Phi family of models.",
      "url": "https://www.microsoft.com/en-us/research/publication/from-rags-to-rich-parameters-probing-how-language-models-utilize-external-knowledge-over-parametric-information-for-factual-queries/"
    },
    {
      "title": "MMWorld: Towards Multi-discipline Multi-faceted World Model Evaluation in Videos",
      "authors": [
        "Jiachen Li",
        "Jianfeng Wang",
        "K. Lin",
        "Kaizhi Zheng",
        "Lijuan Wang",
        "Linjie Li",
        "Wanrong Zhu",
        "Weixi Feng",
        "William Yang Wang",
        "Xin Eric Wang",
        "Xuehai He",
        "Yue Fan",
        "Yujie Lu",
        "Zhengyuan Yang"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Computer vision"
      ],
      "publication_date": "June 2024",
      "abstract": "Multimodal Language Language Models (MLLMs) demonstrate the emerging abilities of “world models”– interpreting and reasoning about complex real-world dynamics. To assess these abilities, we posit videos are the ideal medium, as they encapsulate rich representations of real-world dynamics and causalities. To this end, we introduce MMWorld, a new benchmark for multi-discipline, multi-faceted multimodal video understanding. MMWorld distinguishes itself from previous video understanding benchmarks with two unique advantages: (1) multi-discipline, covering various disciplines that often require domain expertise for comprehensive understanding; (2) multi-faceted reasoning, including explanation, counterfactual thinking, future prediction, etc. MMWorld consists of a human-annotated dataset to evaluate MLLMs with questions about the whole videos and a synthetic dataset to analyze MLLMs within a single modality of perception. Together, MMWorld encompasses 1,910 videos across seven broad disciplines and 69 subdisciplines, complete with 6,627 question-answer pairs and associated captions. The evaluation includes 2 proprietary and 10 open-source MLLMs, which struggle on MMWorld (e.g., GPT-4V performs the best with only 52.3\\% accuracy), showing large room for improvement. Further ablation studies reveal other interesting findings such as models’ different skill sets from humans. We hope MMWorld can serve as an essential step towards world model evaluation in videos.",
      "url": "https://www.microsoft.com/en-us/research/publication/mmworld-towards-multi-discipline-multi-faceted-world-model-evaluation-in-videos/"
    },
    {
      "title": "DISCOVERYWORLD: A Virtual Environment for Developing and Evaluating Automated Scientific Discovery Agents",
      "authors": [
        "Bhavana Dalvi",
        "Bodhisattwa Prasad Majumder",
        "Erin Bransom",
        "Marc-Alexandre Côté",
        "Oyvind Tafjord",
        "Peter Alexander Jansen",
        "Peter Clark",
        "Tushar Khot"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "June 2024",
      "abstract": "Automated scientific discovery promises to accelerate progress across scientific domains. However, developing and evaluating an AI agent’s capacity for end-to-end scientific reasoning is challenging as running real-world experiments is often prohibitively expensive or infeasible. In this work we introduce DISCOVERYWORLD, the first virtual environment for developing and benchmarking an agent’s ability to perform complete cycles of novel scientific discovery. DISCOVERYWORLD contains a variety of different challenges, covering topics as diverse as radioisotope dating, rocket science, and proteomics, to encourage development of general discovery skills rather than task-specific solutions. DISCOVERYWORLD itself is an inexpensive, simulated, text-based environment (with optional 2D visual overlay). It includes 120 different challenge tasks, spanning eight topics each with three levels of difficulty and several parametric variations. Each task requires an agent to form hypotheses, design and run experiments, analyze results, and act on conclusions. DISCOVERYWORLD further provides three automatic metrics for evaluating performance, based on (a) task completion, (b) task-relevant actions taken, and (c) the discovered explanatory knowledge. We find that strong baseline agents, that perform well in prior published environments, struggle on most DISCOVERYWORLD tasks, suggesting that DISCOVERYWORLD captures some of the novel challenges of discovery, and thus that DISCOVERYWORLD may help accelerate near-term development and assessment of scientific discovery competency in agents. Code available at: www.github.com/allenai/discoveryworld",
      "url": "https://www.microsoft.com/en-us/research/publication/discoveryworld-a-virtual-environment-for-developing-and-evaluating-automated-scientific-discovery-agents/"
    },
    {
      "title": "Aligning Vision Models with Human Aesthetics in Retrieval: Benchmarks and Algorithms",
      "authors": [
        "Baining Guo",
        "Chong Luo",
        "Ji Li",
        "Miaosen Zhang",
        "Qi Dai",
        "Xin Geng",
        "Yifei Ma",
        "Yixuan Wei",
        "Zhen Xing",
        "Zheng Zhang",
        "Zuxuan Wu"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "June 2024",
      "abstract": "Modern vision models are trained on very large noisy datasets. While these models acquire strong capabilities, they may not follow the user’s intent to output the desired results in certain aspects, e.g., visual aesthetic, preferred style, and responsibility. In this paper, we target the realm of visual aesthetics and aim to align vision models with human aesthetic standards in a retrieval system. Advanced retrieval systems usually adopt a cascade of aesthetic models as re-rankers or filters, which are limited to low-level features like saturation and perform poorly when stylistic, cultural or knowledge contexts are involved. We find that utilizing the reasoning ability of large language models (LLMs) to rephrase the search query and extend the aesthetic expectations can make up for this shortcoming. Based on the above findings, we propose a preference-based reinforcement learning method that fine-tunes the vision models to distill the knowledge from both LLMs reasoning and the aesthetic models to better align the vision models with human aesthetics. Meanwhile, with rare benchmarks designed for evaluating retrieval systems, we leverage large multi-modality model (LMM) to evaluate the aesthetic performance with their strong abilities. As aesthetic assessment is one of the most subjective tasks, to validate the robustness of LMM, we further propose a novel dataset named HPIR to benchmark the alignment with human aesthetics. Experiments demonstrate that our method significantly enhances the aesthetic behaviors of the vision models, under several metrics. We believe the proposed algorithm can be a general practice for aligning vision models with human values.",
      "url": "https://www.microsoft.com/en-us/research/publication/aligning-vision-models-with-human-aesthetics-in-retrieval-benchmarks-and-algorithms/"
    },
    {
      "title": "Motion Consistency Model: Accelerating Video Diffusion with Disentangled Motion-Appearance Distillation",
      "authors": [
        "Chung-Ching Lin",
        "David Doermann",
        "Jianfeng Wang",
        "Junsong Yuan",
        "Kevin Lin",
        "Lijuan Wang",
        "Linjie Li",
        "Yuanhao Zhai",
        "Zhengyuan Yang"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Computer vision"
      ],
      "publication_date": "June 2024",
      "abstract": "Image diffusion distillation achieves high-fidelity generation with very few sampling steps. However, applying these techniques directly to video diffusion often results in unsatisfactory frame quality due to the limited visual quality in public video datasets. This affects the performance of both teacher and student video diffusion models. Our study aims to improve video diffusion distillation while improving frame appearance using abundant high-quality image data. We propose motion consistency model (MCM), a single-stage video diffusion distillation method that disentangles motion and appearance learning. Specifically, MCM includes a video consistency model that distills motion from the video teacher model, and an image discriminator that enhances frame appearance to match high-quality image data. This combination presents two challenges: (1) conflicting frame learning objectives, as video distillation learns from low-quality video frames while the image discriminator targets high-quality images; and (2) training-inference discrepancies due to the differing quality of video samples used during training and inference. To address these challenges, we introduce disentangled motion distillation and mixed trajectory distillation. The former applies the distillation objective solely to the motion representation, while the latter mitigates training-inference discrepancies by mixing distillation trajectories from both the low- and high-quality video domains. Extensive experiments show that our MCM achieves the state-of-the-art video diffusion distillation performance. Additionally, our method can enhance frame quality in video diffusion models, producing frames with high aesthetic scores or specific styles without corresponding video data.",
      "url": "https://www.microsoft.com/en-us/research/publication/motion-consistency-model-accelerating-video-diffusion-with-disentangled-motion-appearance-distillation/"
    },
    {
      "title": "Noise-Aware Differentially Private Regression via Meta-Learning",
      "authors": [
        "Antti Honkela",
        "Marlon Tobaben",
        "Matthew Ashman",
        "Ossi Räisä",
        "Richard Turner",
        "Stratis Markou",
        "Wessel Bruinsma"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "June 2024",
      "abstract": "Many high-stakes applications require machine learning models that protect user privacy and provide well-calibrated, accurate predictions. While Differential Privacy (DP) is the gold standard for protecting user privacy, standard DP mechanisms typically significantly impair performance. One approach to mitigating this issue is pre-training models on simulated data before DP learning on the private data. In this work we go a step further, using simulated data to train a meta-learning model that combines the Convolutional Conditional Neural Process (ConvCNP) with an improved functional DP mechanism of Hall et al. [2013] yielding the DPConvCNP. DPConvCNP learns from simulated data how to map private data to a DP predictive model in one forward pass, and then provides accurate, well-calibrated predictions. We compare DPConvCNP with a DP Gaussian Process (GP) baseline with carefully tuned hyperparameters. The DPConvCNP outperforms the GP baseline, especially on non-Gaussian data, yet is much faster at test time and requires less tuning.",
      "url": "https://www.microsoft.com/en-us/research/publication/noise-aware-differentially-private-regression-via-meta-learning/"
    },
    {
      "title": "Dataset and Lessons Learned from the 2024 SaTML LLM Capture-the-Flag Competition",
      "authors": [
        "Ahmed Salem",
        "Chenhao Li",
        "Daniel Paleka",
        "Dragos Albastroiu",
        "Edoardo Debenedetti",
        "Fineas Silaghi",
        "Florian Tramer",
        "Giovanni Cherubin",
        "Javier Rando",
        "Lea Schönherr",
        "Mario Fritz",
        "Niv Cohen",
        "Reshmi Ghosh",
        "Robin Schmid",
        "Rui Wen",
        "Sahar Abdelnabi",
        "Santiago Zanella-Béguelin",
        "Stefan Kraft",
        "Takahiro Miki",
        "Victor Klemm",
        "Yuval Lemberg"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Security, privacy, and cryptography"
      ],
      "publication_date": "June 2024",
      "abstract": "Large language model systems face important security risks from maliciously crafted messages that aim to overwrite the system’s original instructions or leak private data. To study this problem, we organized a capture-the-flag competition at IEEE SaTML 2024, where the flag is a secret string in the LLM system prompt. The competition was organized in two phases. In the first phase, teams developed defenses to prevent the model from leaking the secret. During the second phase, teams were challenged to extract the secrets hidden for defenses proposed by the other teams. This report summarizes the main insights from the competition. Notably, we found that all defenses were bypassed at least once, highlighting the difficulty of designing a successful defense and the necessity for additional research to protect LLM systems. To foster future research in this direction, we compiled a dataset with over 137k multi-turn attack chats and open-sourced the platform.",
      "url": "https://www.microsoft.com/en-us/research/publication/dataset-and-lessons-learned-from-the-2024-satml-llm-capture-the-flag-competition/"
    },
    {
      "title": "CARES: A Comprehensive Benchmark of Trustworthiness in Medical Vision Language Models",
      "authors": [
        "Chetan Bansal",
        "Gang Li",
        "Hongtu Zhu",
        "Huaxiu Yao",
        "James Zou",
        "Jimeng Sun",
        "Juanxi Tian",
        "Junzhou Huang",
        "Kangyu Zhu",
        "Marc Niethammer",
        "Peng Xia",
        "Ruibo Hou",
        "Wenhao Zheng",
        "Xiao Wang",
        "Xuchao Zhang",
        "Yangrui Gong",
        "Yiyang Zhou",
        "Yue Xu",
        "Yun Li",
        "Ze Chen",
        "Zhaoyang Wang",
        "Zhenbang Wu",
        "Zhiyuan Fan",
        "Zongyuan Ge"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Medical, health and genomics"
      ],
      "publication_date": "June 2024",
      "abstract": "Artificial intelligence has significantly impacted medical applications, particularly with the advent of Medical Large Vision Language Models (Med-LVLMs), sparking optimism for the future of automated and personalized healthcare. However, the trustworthiness of Med-LVLMs remains unverified, posing significant risks for future model deployment. In this paper, we introduce CARES and aim to comprehensively evaluate the Trustworthiness of Med-LVLMs across the medical domain. We assess the trustworthiness of Med-LVLMs across five dimensions, including trustfulness, fairness, safety, privacy, and robustness. CARES comprises about 41K question-answer pairs in both closed and open-ended formats, covering 16 medical image modalities and 27 anatomical regions. Our analysis reveals that the models consistently exhibit concerns regarding trustworthiness, often displaying factual inaccuracies and failing to maintain fairness across different demographic groups. Furthermore, they are vulnerable to attacks and demonstrate a lack of privacy awareness. We publicly release our benchmark and code in https://github.com/richard-peng-xia/CARES (opens in new tab).",
      "url": "https://www.microsoft.com/en-us/research/publication/cares-a-comprehensive-benchmark-of-trustworthiness-in-medical-vision-language-models/"
    },
    {
      "title": "Mutation and cell state compatibility is required and targetable in Ph+ acute lymphoblastic leukemia minimal residual disease",
      "authors": [
        "Alan DenAdel",
        "Alejandro J. Gupta",
        "Alex K. Shalek",
        "Alexandria van Scoyk",
        "Andrew W. Navia",
        "Ava P. Amini",
        "Catharine S. Leahy",
        "Charles P. Couturier",
        "David M. Weinstock",
        "Foster Powers",
        "Haley Strouf",
        "Huiyun Liu",
        "Hyun Hwan An",
        "Jennyfer Galves-Reyes",
        "Kay Shigemori",
        "Kristen Jones",
        "Kristen Stevenson",
        "Laura L. Bilal",
        "Lorin Crawford",
        "M. Luskin",
        "Mahnoor Mirza",
        "Mark A. Murakami",
        "Mark M. Stevens",
        "Michelle L. Ramseier",
        "Nezha Senhaji",
        "Nicholas L. Calistri",
        "Nolawit Mulugeta",
        "Peter Dennis",
        "Peter S. Winter",
        "Robert J. Kimmerling",
        "S. Ren",
        "Sachit D. Saksena",
        "Scott R. Manalis",
        "Srivatsan Raghavan",
        "Ye Zhang"
      ],
      "research_areas": [
        "Medical, health and genomics"
      ],
      "publication_date": "June 2024",
      "abstract": "Efforts to cure BCR::ABL1 B cell acute lymphoblastic leukemia (Ph+ ALL) solely through inhibition of ABL1 kinase activity have thus far been insufficient despite the availability of tyrosine kinase inhibitors (TKIs) with broad activity against resistance mutants. The mechanisms that drive persistence within minimal residual disease (MRD) remain poorly understood and therefore untargeted. Utilizing 13 patient-derived xenograft (PDX) models and clinical trial specimens of Ph+ ALL, we examined how genetic and transcriptional features co-evolve to drive progression during prolonged TKI response. Our work reveals a landscape of cooperative mutational and transcriptional escape mechanisms that differ from those causing resistance to first generation TKIs. By analyzing MRD during remission, we show that the same resistance mutation can either increase or decrease cellular fitness depending on transcriptional state. We further demonstrate that directly targeting transcriptional state-associated vulnerabilities at MRD can overcome BCR::ABL1 independence, suggesting a new paradigm for rationally eradicating MRD prior to relapse. Finally, we illustrate how cell mass measurements of leukemia cells can be used to rapidly monitor dominant transcriptional features of Ph+ ALL to help rationally guide therapeutic selection from low-input samples. HIGHLIGHTS Relapse after remission on TKI can harbor mutations in ABL1, RAS, or neither Mutations and development-like cell state dictate fitness in residual disease Co-targeting cell state and ABL1 markedly reduces MRD Biophysical measurements provide an integrative, rapid measurement of cell state",
      "url": "https://www.microsoft.com/en-us/research/publication/mutation-and-cell-state-compatibility-is-required-and-targetable-in-ph-acute-lymphoblastic-leukemia-minimal-residual-disease/"
    },
    {
      "title": "ORES-Inspect: A technology probe for machine learning audits on enwiki",
      "authors": [
        "Aaron L Halfaker",
        "Jada Lilleboe",
        "Lauren Hagen",
        "Loren G. Terveen",
        "Lu Li",
        "Solvejg Wastvedt",
        "Zachary Levonian"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "June 2024",
      "abstract": "Auditing the machine learning (ML) models used on Wikipedia is important for ensuring that vandalism-detection processes remain fair and effective. However, conducting audits is challenging because stakeholders have diverse priorities and assembling evidence for a model’s [in]efficacy is technically complex. We designed an interface to enable editors to learn about and audit the performance of the ORES edit quality model. ORES-Inspect is an open-source web tool and a provocative technology probe for researching how editors think about auditing the many ML models used on Wikipedia. We describe the design of ORES-Inspect and our plans for further research with this system.",
      "url": "https://www.microsoft.com/en-us/research/publication/ores-inspect-a-technology-probe-for-machine-learning-audits-on-enwiki/"
    },
    {
      "title": "Optimal online discrepancy minimization",
      "authors": [
        "Janardhan (Jana) Kulkarni",
        "Thomas Rothvoss",
        "Victor Reis"
      ],
      "research_areas": [
        "Algorithms"
      ],
      "publication_date": "June 2024",
      "abstract": "We prove that there exists an online algorithm that for any sequence of vectors \\(v_1,…,v_T \\in R^n\\) with \\(∥v_i∥_2 ≤1,\\) arriving one at a time, decides random signs \\(x_1,…,x_T \\in {−1,1}\\) so that for every \\(t≤T,\\) the prefix sum \\(∑_{i=1}^t x_1v_i\\) is 10-subgaussian. This improves over the work of Alweiss, Liu and Sawhney who kept prefix sums \\(O(\\sqrt{log(nT)})\\)-subgaussian, and gives a \\(O(\\sqrt{logT})\\) bound on the discrepancy \\(max_{t∈T} ∥∑_{i=1}^t x_iv_i∥_∞.\\) Our proof combines a generalization of Banaszczyk’s prefix balancing result to trees with a cloning argument to find distributions rather than single colorings. We also show a matching \\(Ω(\\sqrt{logT})\\) strategy for an oblivious adversary.",
      "url": "https://www.microsoft.com/en-us/research/publication/optimal-online-discrepancy-minimization/"
    },
    {
      "title": "CVQA: Culturally-diverse Multilingual Visual Question Answering Benchmark",
      "authors": [
        "A. Tonja",
        "Aditya Nanda Kishore",
        "Aishik Mandal",
        "Alham Fikri Aji",
        "Alina Dragonetti",
        "Artem Abzaliev",
        "Bontu Fufa Balcha",
        "Chenxi Whitehouse",
        "Chenyang Lyu",
        "Christian Salamea",
        "D. Adelani",
        "D. Meur",
        "Dan John Velasco",
        "David Romero",
        "Emilio Villa-Cueva",
        "Fajri Koto",
        "Fauzan Farooqui",
        "Frederico Belcavello",
        "Ganzorig Batnasan",
        "Gisela Vallejo",
        "Grainne Caulfield",
        "Guido Ivetta",
        "Haiyue Song",
        "Haryo Akbarianto Wibowo",
        "Henok Biadglign Ademtew",
        "Hernán Maina",
        "Holy Lovenia",
        "Injy Hamed",
        "Israel Abebe Azime",
        "Jan Christian Blaise Cruz",
        "Jay Gala",
        "Jesús-Germán Ortiz-Barajas",
        "Jiahui Geng",
        "Jinheon Baek",
        "Joan Nwatu",
        "Jocelyn Dunstan",
        "Kumaranage Ravindu Yasas Nagasinghe",
        "L. A. Alemany",
        "L. F. D’Haro",
        "Luciana Benotti",
        "M. Mihaylov",
        "Marcelo Viridiano",
        "Marcos Estecha-Garitagoitia",
        "Maria Camila Buitrago Cabrera",
        "Mario Rodr'iguez-Cantelar",
        "Mohamed Fazli Mohamed Imam",
        "Muhammad Farid Adilazuarda",
        "Munkh-Erdene Otgonbold",
        "Munkhjargal Gochoo",
        "Mélanie Jouitteau",
        "Naome A. Etori",
        "Oana Ignat",
        "Olivier Niyomugisha",
        "Paula M'onica Silva",
        "Pranjal A. Chitale",
        "Rada Mihalcea",
        "Raj Dabre",
        "Rendi Chevi",
        "Ruochen Zhang",
        "Ryandito Diandaru",
        "Samuel Cahyawijaya",
        "Santiago G'ongora",
        "Soyeong Jeong",
        "Sukannya Purkayastha",
        "T. Solorio",
        "T. Torrent",
        "Tatsuki Kuribayashi",
        "Teresa Lynn",
        "Thanmay Jayakumar",
        "Toqeer Ehsan",
        "Vladimir Araujo",
        "Yova Kementchedjhieva",
        "Zara Burzo",
        "Zheng Wei Lim",
        "Zheng-Xin Yong"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "June 2024",
      "abstract": "Visual Question Answering (VQA) is an important task in multimodal AI, and it is often used to test the ability of vision-language models to understand and reason on knowledge present in both visual and textual data. However, most of the current VQA models use datasets that are primarily focused on English and a few major world languages, with images that are typically Western-centric. While recent efforts have tried to increase the number of languages covered on VQA datasets, they still lack diversity in low-resource languages. More importantly, although these datasets often extend their linguistic range via translation or some other approaches, they usually keep images the same, resulting in narrow cultural representation. To address these limitations, we construct CVQA, a new Culturally-diverse multilingual Visual Question Answering benchmark, designed to cover a rich set of languages and cultures, where we engage native speakers and cultural experts in the data collection process. As a result, CVQA includes culturally-driven images and questions from across 30 countries on four continents, covering 31 languages with 13 scripts, providing a total of 10k questions. We then benchmark several Multimodal Large Language Models (MLLMs) on CVQA, and show that the dataset is challenging for the current state-of-the-art models. This benchmark can serve as a probing evaluation suite for assessing the cultural capability and bias of multimodal models and hopefully encourage more research efforts toward increasing cultural awareness and linguistic diversity in this field.",
      "url": "https://www.microsoft.com/en-us/research/publication/cvqa-culturally-diverse-multilingual-visual-question-answering-benchmark/"
    },
    {
      "title": "GeoGen: Geometry-Aware Generative Modeling via Signed Distance Functions",
      "authors": [
        "Arno Onken",
        "Charlie Hewitt",
        "Julien Valentin",
        "Kacper Kania",
        "Lohit Petikam",
        "Octave Mariotti",
        "Oisin Mac Aodha",
        "Qingshan Xu",
        "Salvatore Esposito"
      ],
      "research_areas": [
        "Graphics and multimedia"
      ],
      "publication_date": "June 2024",
      "abstract": "We introduce a new generative approach for synthesizing 3D geometry and images from single-view collections. Most existing approaches predict volumetric density to render multi-view consistent images. By employing volumetric rendering using neural radiance fields, they inherit a key limitation: the generated geometry is noisy and unconstrained, limiting the quality and utility of the output meshes. To address this issue, we propose GeoGen, a new SDF-based 3D generative model trained in an end-to-end manner. Initially, we reinterpret the volumetric density as a Signed Distance Function (SDF). This allows us to introduce useful priors to generate valid meshes. However, those priors prevent the generative model from learning details, limiting the applicability of the method to real-world scenarios. To alleviate that problem, we make the transformation learnable and constrain the rendered depth map to be consistent with the zero-level set of the SDF. Through the lens of adversarial training, we encourage the network to produce higher fidelity details on the output meshes. For evaluation, we introduce a synthetic dataset of human avatars captured from 360-degree camera angles, to overcome the challenges presented by real-world datasets, which often lack 3D consistency and do not cover all camera angles. Our experiments on multiple datasets show that GeoGen produces visually and quantitatively better geometry than the previous generative models based on neural radiance fields.",
      "url": "https://www.microsoft.com/en-us/research/publication/geogen/"
    },
    {
      "title": "Understanding Information Storage and Transfer in Multi-modal Large Language Models",
      "authors": [
        "Besmira Nushi",
        "Cecily Morrison",
        "Daniela Massiceti",
        "Martin Grayson",
        "S. Feizi",
        "Samyadeep Basu"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "June 2024",
      "abstract": "Understanding the mechanisms of information storage and transfer in Transformer-based models is important for driving model understanding progress. Recent work has studied these mechanisms for Large Language Models (LLMs), revealing insights on how information is stored in a model’s parameters and how information flows to and from these parameters in response to specific prompts. However, these studies have not yet been extended to Multi-modal Large Language Models (MLLMs). Given their expanding capabilities and real-world use, we start by studying one aspect of these models — how MLLMs process information in a factual visual question answering task. We use a constraint-based formulation which views a visual question as having a set of visual or textual constraints that the model’s generated answer must satisfy to be correct (e.g. What movie directed by the director in this photo has won a Golden Globe?). Under this setting, we contribute i) a method that extends causal information tracing from pure language to the multi-modal setting, and ii) VQA-Constraints, a test-bed of 9.7K visual questions annotated with constraints. We use these tools to study two open-source MLLMs, LLaVa and multi-modal Phi-2. Our key findings show that these MLLMs rely on MLP and self-attention blocks in much earlier layers for information storage, compared to LLMs whose mid-layer MLPs are more important. We also show that a consistent small subset of visual tokens output by the vision encoder are responsible for transferring information from the image to these causal blocks. We validate these mechanisms by introducing MultEdit, a model-editing algorithm that can correct errors and insert new long-tailed information into MLLMs by targeting these causal blocks.",
      "url": "https://www.microsoft.com/en-us/research/publication/understanding-information-storage-and-transfer-in-multi-modal-large-language-models/"
    },
    {
      "title": "VALL-E 2: Neural Codec Language Models are Human Parity Zero-Shot Text to Speech Synthesizers",
      "authors": [
        "Furu Wei",
        "Jinyu Li",
        "Long Zhou",
        "Sanyuan Chen",
        "Sheng Zhao",
        "Shujie Liu",
        "Xu Tan",
        "Yanqing Liu",
        "Yao Qian"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Audio and Acoustics"
      ],
      "publication_date": "June 2024",
      "abstract": "This paper introduces VALL-E 2, the latest advancement in neural codec language models that marks a milestone in zero-shot text-to-speech synthesis (TTS), achieving human parity for the first time. Based on its predecessor, VALL-E, the new iteration introduces two significant enhancements: Repetition Aware Sampling refines the original nucleus sampling process by accounting for token repetition in the decoding history. It not only stabilizes the decoding but also circumvents the infinite loop issue. Grouped Code Modeling organizes codec codes into groups to effectively shorten the sequence length, which not only boosts inference speed but also addresses the challenges of long sequence modeling. Our experiments on the LibriSpeech and VCTK datasets show that VALL-E 2 surpasses previous systems in speech robustness, naturalness, and speaker similarity. It is the first of its kind to reach human parity on these benchmarks. Moreover, VALL-E 2 consistently synthesizes high-quality speech, even for sentences that are traditionally challenging due to their complexity or repetitive phrases. The advantages of this work could contribute to valuable endeavors, such as generating speech for individuals with aphasia or people with amyotrophic lateral sclerosis. See https://aka.ms/valle2 (opens in new tab) for demos of VALL-E 2.",
      "url": "https://www.microsoft.com/en-us/research/publication/vall-e-2-neural-codec-language-models-are-human-parity-zero-shot-text-to-speech-synthesizers-2/"
    },
    {
      "title": "MAIRA-2: Grounded Radiology Report Generation",
      "authors": [
        "Anja Thieme",
        "Anton Schwaighofer",
        "Daniel Coelho de Castro",
        "Fabian Falck",
        "Felix Meissen",
        "Fernando Pérez-García",
        "Harshita Sharma",
        "Javier Alvarez-Valle",
        "Julia Gong",
        "Kenza Bouzid",
        "Maria Teodora Wetscherek",
        "Matthew P Lungren",
        "Maximilian Ilse",
        "Mercy Ranjit",
        "Ozan Oktay",
        "Sam Bond-Taylor",
        "Shaury Srivastav",
        "Shruthi Bannur",
        "Stephanie Hyland",
        "Valentina Salvatelli"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Medical, health and genomics"
      ],
      "publication_date": "June 2024",
      "abstract": "Radiology reporting is a complex task that requires detailed image understanding, integration of multiple inputs, including comparison with prior imaging, and precise language generation. This makes it ideal for the development and use of generative multimodal models. Here, we extend report generation to include the localisation of individual findings on the image – a task we call grounded report generation. Prior work indicates that grounding is important for clarifying image understanding and interpreting AI-generated text. Therefore, grounded reporting stands to improve the utility and transparency of automated report drafting. To enable evaluation of grounded reporting, we propose a novel evaluation framework – RadFact – leveraging the reasoning capabilities of large language models (LLMs). RadFact assesses the factuality of individual generated sentences, as well as correctness of generated spatial localisations when present. We introduce MAIRA-2, a large multimodal model combining a radiology-specific image encoder with a LLM, and trained for the new task of grounded report generation on chest X-rays. MAIRA-2 uses more comprehensive inputs than explored previously: the current frontal image, the current lateral image, the prior frontal image and prior report, as well as the Indication, Technique and Comparison sections of the current report. We demonstrate that these additions significantly improve report quality and reduce hallucinations, establishing a new state of the art on findings generation (without grounding) on MIMIC-CXR while demonstrating the feasibility of grounded reporting as a novel and richer task.",
      "url": "https://www.microsoft.com/en-us/research/publication/maira-2-grounded-radiology-report-generation/"
    },
    {
      "title": "Variational Flow Matching for Graph Generation",
      "authors": [
        "C. A. Naesseth",
        "Floor Eijkelboom",
        "Grigory Bartosh",
        "Jan-Willem van de Meent",
        "Max Welling"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "June 2024",
      "abstract": "We present a formulation of flow matching as variational inference, which we refer to as variational flow matching (VFM). Based on this formulation we develop CatFlow, a flow matching method for categorical data. CatFlow is easy to implement, computationally efficient, and achieves strong results on graph generation tasks. In VFM, the objective is to approximate the posterior probability path, which is a distribution over possible end points of a trajectory. We show that VFM admits both the CatFlow objective and the original flow matching objective as special cases. We also relate VFM to score-based models, in which the dynamics are stochastic rather than deterministic, and derive a bound on the model likelihood based on a reweighted VFM objective. We evaluate CatFlow on one abstract graph generation task and two molecular generation tasks. In all cases, CatFlow exceeds or matches performance of the current state-of-the-art models.",
      "url": "https://www.microsoft.com/en-us/research/publication/variational-flow-matching-for-graph-generation/"
    },
    {
      "title": "Infusing Self-Consistency into Density Functional Theory Hamiltonian Prediction via Deep Equilibrium Models",
      "authors": [
        "Bin Shao",
        "Chang Liu",
        "He Zhang",
        "Lijun Wu",
        "Lin Huang",
        "Nianlong Zou",
        "Xinran wei",
        "Zun Wang"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "June 2024",
      "abstract": "In this study, we introduce a unified neural network architecture, the Deep Equilibrium Density Functional Theory Hamiltonian (DEQH) model, which incorporates Deep Equilibrium Models (DEQs) for predicting Density Functional Theory (DFT) Hamiltonians. The DEQH model inherently captures the self-consistency nature of Hamiltonian, a critical aspect often overlooked by traditional machine learning approaches for Hamiltonian prediction. By employing DEQ within our model architecture, we circumvent the need for DFT calculations during the training phase to introduce the Hamiltonian’s self-consistency, thus addressing computational bottlenecks associated with large or complex systems. We propose a versatile framework that combines DEQ with off-the-shelf machine learning models for predicting Hamiltonians. When benchmarked on the MD17 and QH9 datasets, DEQHNet, an instantiation of the DEQH framework, has demonstrated a significant improvement in prediction accuracy. Beyond a predictor, the DEQH model is a Hamiltonian solver, in the sense that it uses the fixed-point solving capability of the deep equilibrium model to iteratively solve for the Hamiltonian. Ablation studies of DEQHNet further elucidate the network’s effectiveness, offering insights into the potential of DEQ-integrated networks for Hamiltonian learning.",
      "url": "https://www.microsoft.com/en-us/research/publication/infusing-self-consistency-into-density-functional-theory-hamiltonian-prediction-via-deep-equilibrium-models/"
    },
    {
      "title": "Submodular Framework for Structured-Sparse Optimal Transport",
      "authors": [
        "Bamdev Mishra",
        "Karthik S. Gurumoorthy",
        "Piyushi Manupriya",
        "Pratik Jawanpuria",
        "S. Jagarlapudi"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "June 2024",
      "abstract": "Unbalanced optimal transport (UOT) has recently gained much attention due to its flexible framework for handling un-normalized measures and its robustness properties. In this work, we explore learning (structured) sparse transport plans in the UOT setting, i.e., transport plans have an upper bound on the number of non-sparse entries in each column (structured sparse pattern) or in the whole plan (general sparse pattern). We propose novel sparsity-constrained UOT formulations building on the recently explored maximum mean discrepancy based UOT. We show that the proposed optimization problem is equivalent to the maximization of a weakly submodular function over a uniform matroid or a partition matroid. We develop efficient gradient-based discrete greedy algorithms and provide the corresponding theoretical guarantees. Empirically, we observe that our proposed greedy algorithms select a diverse support set and we illustrate the efficacy of the proposed approach in various applications.",
      "url": "https://www.microsoft.com/en-us/research/publication/submodular-framework-for-structured-sparse-optimal-transport/"
    },
    {
      "title": "LLM-Vectorizer: LLM-Based Verified Loop Vectorizer",
      "authors": [
        "Avery Laird",
        "Cong Yan",
        "Jubi Taneja",
        "Madan Musuvathi",
        "Shuvendu Lahiri"
      ],
      "research_areas": [
        "Programming languages and software engineering"
      ],
      "publication_date": "June 2024",
      "abstract": "Vectorization is a powerful optimization technique that significantly boosts the performance of high performance computing applications operating on large data arrays. Despite decades of research on auto-vectorization, compilers frequently miss opportunities to vectorize code. On the other hand, writing vectorized code manually using compiler intrinsics is still a complex, error-prone task that demands deep knowledge of specific architecture and compilers. In this paper, we evaluate the potential of large-language models (LLMs) to generate vectorized (Single Instruction Multiple Data) code from scalar programs that process individual array elements. We propose a novel finite-state-machine multi-agents based approach that harnesses LLMs and test-based feedback to generate vectorized code. Our findings indicate that LLMs are capable of producing high-performance vectorized code with run-time speedup ranging from 1.1x to 9.4x as compared to the state-of-the-art compilers such as Intel Compiler, GCC, and Clang. To verify the correctness of vectorized code, we use Alive2, a leading bounded translation validation tool for LLVM IR. We describe a few domain-specific techniques to improve the scalability of Alive2 on our benchmark dataset. Overall, our approach is able to verify 38.2% of vectorizations as correct on the TSVC benchmark dataset.",
      "url": "https://www.microsoft.com/en-us/research/publication/llm-vectorizer-llm-based-verified-loop-vectorizer/"
    },
    {
      "title": "Offline Training of Language Model Agents with Functions as Learnable Weights",
      "authors": [
        "Chi Wang",
        "Jiale Liu",
        "Jieyu Zhang",
        "Linxin Song",
        "Qingyun Wu",
        "Ranjay Krishna",
        "Shaokun Zhang"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "June 2024",
      "abstract": "Researchers and practitioners have recently reframed powerful Large Language Models (LLMs) as agents, enabling them to automate complex tasks largely via the use of specialized functions. To facilitate the development of LLM agents, we present a novel paradigm of training LLM agents without modifying the LLM weights, which is particularly useful when the LLMs are difficult or inaccessible for modifications. Inspired by how humans continuously forge tools to adapt to real-world tasks, rather than change our biological structure to fit a static set of tools, we propose to progressively forge agent’s functions to better solve the downstream tasks instead of modifying the LLM weights. By treating the functions as learnable `agent parameters’ and leveraging the fundamental idea of model training in artificial intelligence, we develop AgentOptimizer that employs the LLM to update agents’ functions and devise an agent training algorithm with two strategies, roll-back, and early-stop, to streamline the training process. With extensive experiments, we showcase that the agent training paradigm could significantly improve the performance of representative LLM agents in various downstream tasks. We also study the behavior of the agent training regarding aspects like the learning curve and domain transferability.",
      "url": "https://www.microsoft.com/en-us/research/publication/offline-training-of-language-model-agents-with-functions-as-learnable-weights/"
    },
    {
      "title": "Mitigate Position Bias in Large Language Models via Scaling a Single Dimension",
      "authors": [
        "Chin-Yew Lin",
        "Dongsheng Li",
        "Huiqiang Jiang",
        "Lili Qiu",
        "Qianhui Wu",
        "Xufang Luo",
        "Yijiong Yu",
        "Yongfeng Huang",
        "Yuqing Yang"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Human language technologies"
      ],
      "publication_date": "June 2024",
      "abstract": "Large Language Models (LLMs) are increasingly applied in various real-world scenarios due to their excellent generalization capabilities and robust generative abilities. However, they exhibit position bias, also known as”lost in the middle”, a phenomenon that is especially pronounced in long-context scenarios, which indicates the placement of the key information in different positions of a prompt can significantly affect accuracy. This paper first explores the micro-level manifestations of position bias, concluding that attention weights are a micro-level expression of position bias. It further identifies that, in addition to position embeddings, causal attention mask also contributes to position bias by creating position-specific hidden states. Based on these insights, we propose a method to mitigate position bias by scaling this positional hidden states. Experiments on the NaturalQuestions Multi-document QA, KV retrieval, LongBench and timeline reorder tasks, using various models including RoPE models, context windowextended models, and Alibi models, demonstrate the effectiveness and generalizability of our approach. Our method can improve performance by up to 15.2% by modifying just one dimension of hidden states. Our code is available at https://aka.ms/PositionalHidden (opens in new tab).",
      "url": "https://www.microsoft.com/en-us/research/publication/mitigate-position-bias-in-large-language-models-via-scaling-a-single-dimension/"
    },
    {
      "title": "Measuring and shaping the nutritional environment via food sales logs: case studies of campus-wide food choice and a call to action",
      "authors": [
        "Arnaud Chiolero",
        "Emre Kiciman",
        "Eric Horvitz",
        "Kristina Gligorić",
        "Robert West",
        "Robin Zbinden",
        "Ryen W. White"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Social sciences"
      ],
      "publication_date": "June 2024",
      "abstract": "Although diets influence health and the environment, measuring and changing nutrition is challenging. Traditional measurement methods face challenges, and designing and conducting behavior-changing interventions is conceptually and logistically complicated. Situated local communities such as university campuses offer unique opportunities to shape the nutritional environment and promote health and sustainability. The present study investigates how passively sensed food purchase logs typically collected as part of regular business operations can be used to monitor and measure on-campus food consumption and understand food choice determinants. First, based on 38 million sales logs collected on a large university campus over eight years, we perform statistical analyses to quantify spatio-temporal determinants of food choice and characterize harmful patterns in dietary behaviors, in a case study of food purchasing at EPFL campus. We identify spatial proximity, food item pairing, and academic schedules (yearly and daily) as important determinants driving the on-campus food choice. The case studies demonstrate the potential of food sales logs for measuring nutrition and highlight the breadth and depth of future possibilities to study individual food-choice determinants. We describe how these insights provide an opportunity for stakeholders, such as campus offices responsible for managing food services, to shape the nutritional environment and improve health and sustainability by designing policies and behavioral interventions. Finally, based on the insights derived through the case study of food purchases at EPFL campus, we identify five future opportunities and offer a call to action for the nutrition research community to contribute to ensuring the health and sustainability of on-campus populations—the very communities to which many researchers belong.",
      "url": "https://www.microsoft.com/en-us/research/publication/measuring-and-shaping-the-nutritional-environment-via-food-sales-logs-case-studies-of-campus-wide-food-choice-and-a-call-to-action/"
    },
    {
      "title": "Empowering In-Browser Deep Learning Inference on Edge Devices with Just-in-Time Kernel Optimizations",
      "authors": [
        "Deyu Zhang",
        "Fucheng Jia",
        "Ju Ren",
        "Lili Qiu",
        "Mao Yang",
        "Qipeng Wang",
        "Shiqi Jiang",
        "Tianrui Xia",
        "Ting Cao",
        "Wei Cui",
        "Xu Cao",
        "Yuanchun Li",
        "Yunxin Liu"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Systems and networking"
      ],
      "publication_date": "June 2024",
      "abstract": "Web is increasingly becoming the primary platform to deliver AI services onto edge devices, making in-browser deep learning (DL) inference more prominent. Nevertheless, the heterogeneity of edge devices, combined with the underdeveloped state of Web hardware acceleration practices, hinders current in-browser inference from achieving its full performance potential on target devices.\n\nTo address this issue, this paper presents the pioneering inbrowser inference system, nnJIT, which enables just-in-time (JIT) auto-generation of optimized computing kernels for edge devices. nnJIT is built upon two novel techniques that significantly reduce kernel search and compilation overhead while improving performance firmly: Tensor-Web Compiling Co-Design lowers compiling costs by around 100× through eliminating redundant and ineffective compiling passes; Web-Specific Lite Kernel Optimization Space reduces kernel tuning costs by focusing on Web programming requirements and efficient device resource utilization, pruning the optimization space from millions to only dozens.\n\nnnJIT is evaluated for modern models, e.g., BART, T5, and Llama 2, on a range of edge devices including laptops and smartphones using different browsers and hardware from ARM, Intel, AMD and Nvidia. The results show that nnJIT can achieve up to 8.2× faster within 30 seconds compared to the existing baselines.",
      "url": "https://www.microsoft.com/en-us/research/publication/empowering-in-browser-deep-learning-inference-on-edge-devices-with-just-in-time-kernel-optimizations/"
    },
    {
      "title": "Leveraging Visual Tokens for Extended Text Contexts in Multi-Modal Learning",
      "authors": [
        "Alex Jinpeng Wang",
        "Lijuan Wang",
        "Linjie Li",
        "Mike Zheng Shou",
        "Min Li",
        "Yiqi Lin"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "June 2024",
      "abstract": "Training models with longer in-context lengths is a significant challenge for multimodal model due to substantial GPU memory and computational costs. This exploratory study does not present state-of-the-art models; rather, it introduces an innovative method designed to increase in-context text length in multi-modality large language models (MLLMs) efficiently. We present Visualized In-Context Text Processing (VisInContext), which processes long in-context text using visual tokens. This technique significantly reduces GPU memory usage and floating point operations (FLOPs) for both training and inferenceing stage. For instance, our method expands the pre-training in-context text length from 256 to 2048 tokens with nearly same FLOPs for a 56 billion parameter MOE model. Experimental results demonstrate that model trained with VisInContext delivers superior performance on common downstream benchmarks for in-context few-shot evaluation. Additionally, VisInContext is complementary to existing methods for increasing in-context text length and enhances document understanding capabilities, showing great potential in document QA tasks and sequential document retrieval.",
      "url": "https://www.microsoft.com/en-us/research/publication/leveraging-visual-tokens-for-extended-text-contexts-in-multi-modal-learning/"
    },
    {
      "title": "MEDIQ: Question-Asking LLMs for Adaptive and Reliable Clinical Reasoning",
      "authors": [
        "Emma Pierson",
        "Jonathan Ilgen",
        "Pang Wei Koh",
        "Shangbin Feng",
        "Shuyue Stella Li",
        "Vidhisha Balachandran",
        "Yulia Tsvetkov"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Medical, health and genomics"
      ],
      "publication_date": "June 2024",
      "abstract": "In high-stakes domains like clinical reasoning, AI assistants powered by large language models (LLMs) are yet to be reliable and safe. We identify a key obstacle towards reliability: existing LLMs are trained to answer any question, even with incomplete context in the prompt or insufficient parametric knowledge. We propose to change this paradigm to develop more careful LLMs that ask follow-up questions to gather necessary and sufficient information and respond reliably. We introduce MEDIQ, a framework to simulate realistic clinical interactions, which incorporates a Patient System and an adaptive Expert System. The Patient may provide incomplete information in the beginning; the Expert refrains from making diagnostic decisions when unconfident, and instead elicits missing details from the Patient via follow-up questions. To evaluate MEDIQ, we convert MEDQA and CRAFT-MD — medical benchmarks for diagnostic question answering — into an interactive setup. We develop a reliable Patient system and prototype several Expert systems, first showing that directly prompting state-of-the-art LLMs to ask questions degrades the quality of clinical reasoning, indicating that adapting LLMs to interactive information-seeking settings is nontrivial. We then augment the Expert with a novel abstention module to better estimate model confidence and decide whether to ask more questions, thereby improving diagnostic accuracy by 20.3%; however, performance still lags compared to an (unrealistic in practice) upper bound when full information is given upfront. Further analyses reveal that interactive performance can be improved by filtering irrelevant contexts and reformatting conversations. Overall, our paper introduces a novel problem towards LLM reliability, a novel MEDIQ framework, and highlights important future directions to extend the information-seeking abilities of LLM assistants in critical domains.",
      "url": "https://www.microsoft.com/en-us/research/publication/mediq-question-asking-llms-for-adaptive-and-reliable-clinical-reasoning/"
    },
    {
      "title": "MedFuzz: Exploring the Robustness of Large Language Models in Medical Question Answering",
      "authors": [
        "Carey E. Priebe",
        "Eric Horvitz",
        "Hayden Helm",
        "Junaid Bajwa",
        "Katie Matton",
        "Robert Osazuwa Ness",
        "Sheng Zhang"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "June 2024",
      "abstract": "Large language models (LLM) have achieved impressive performance on medical question-answering benchmarks. However, high benchmark accuracy does not imply that the performance generalizes to real-world clinical settings. Medical question-answering benchmarks rely on assumptions consistent with quantifying LLM performance but that may not hold in the open world of the clinic. Yet LLMs learn broad knowledge that can help the LLM generalize to practical conditions regardless of unrealistic assumptions in celebrated benchmarks. We seek to quantify how well LLM medical question-answering benchmark performance generalizes when benchmark assumptions are violated. Specifically, we present an adversarial method that we call MedFuzz (for medical fuzzing). MedFuzz attempts to modify benchmark questions in ways aimed at confounding the LLM. We demonstrate the approach by targeting strong assumptions about patient characteristics presented in the MedQA benchmark. Successful “attacks” modify a benchmark item in ways that would be unlikely to fool a medical expert but nonetheless “trick” the LLM into changing from a correct to an incorrect answer. Further, we present a permutation test technique that can ensure a successful attack is statistically significant. We show how to use performance on a “MedFuzzed” benchmark, as well as individual successful attacks. The methods show promise at providing insights into the ability of an LLM to operate robustly in more realistic settings.",
      "url": "https://www.microsoft.com/en-us/research/publication/medfuzz-exploring-the-robustness-of-large-language-models-in-medical-question-answering/"
    },
    {
      "title": "Rapidly Adapting Policies to the Real World via Simulation-Guided Fine-Tuning",
      "authors": [
        "Abhishek Gupta",
        "Andrey Kolobov",
        "Ching-An Cheng",
        "Kevin Huang",
        "Patrick Yin",
        "Simran Bagaria",
        "Tyler Westenbroek"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "June 2024",
      "abstract": "Robot learning requires a considerable amount of data to realize it’s promise of generalization. However, it can be challenging to actually collect the magnitude of data necessary for generalization entirely in the real world. Simulation can serve as a source of plentiful data with coverage over relevant states and actions, without requiring the burden of human data collection. However, the high-fidelity physics simulators are fundamentally misspecified approximations to reality, making direct zero-shot transfer challenging. This makes real-world finetuning of policies pretrained in simulation an attractive approach to robot learning. However, current finetuning methods simply use the simulator to provide a reasonable initialization for real-world learning. We go beyond this paradigm by demonstrating how the task structure extracted from its simulation can be used to effectively guide and accelerate learning in the real world. Specifically, we argue that dynamics models and value functions learned in simulation can provide gradient information for the real-world learning problem, substantially reducing the complexity of learning a finetuned policy in real. We demonstrate our approach across several tabletop manipulation tasks in simulation and the real world, learning successful policies for problems that are challenging to handle using purely real-world data.",
      "url": "https://www.microsoft.com/en-us/research/publication/transferring-world-models-from-simulation-for-efficient-real-world-finetuning/"
    },
    {
      "title": "BiomedParse: a biomedical foundation model for image parsing of everything everywhere all at once",
      "authors": [
        "Angela Crabtree",
        "B. Piening",
        "Carlo Bifulco",
        "Ho Hin Lee",
        "Hoifung Poon",
        "Jianfeng Gao",
        "Jianwei Yang",
        "Mu-Hsin Wei",
        "Naoto Usuyama",
        "Sheng Wang",
        "Theodore Zhao",
        "Tristan Naumann",
        "Yu Gu"
      ],
      "research_areas": [
        "Computer vision"
      ],
      "publication_date": "June 2024",
      "abstract": "Biomedical image analysis is fundamental for biomedical discovery in cell biology, pathology, radiology, and many other biomedical domains. Holistic image analysis comprises interdependent subtasks such as segmentation, detection, and recognition of relevant objects. Here, we propose BiomedParse, a biomedical foundation model for imaging parsing that can jointly conduct segmentation, detection, and recognition for 82 object types across 9 imaging modalities. Through joint learning, we can improve accuracy for individual tasks and enable novel applications such as segmenting all relevant objects in an image through a text prompt, rather than requiring users to laboriously specify the bounding box for each object. We leveraged readily available natural-language labels or descriptions accompanying those datasets and use GPT-4 to harmonize the noisy, unstructured text information with established biomedical object ontologies. We created a large dataset comprising over six million triples of image, segmentation mask, and textual description. On image segmentation, we showed that BiomedParse is broadly applicable, outperforming state-of-the-art methods on 102,855 test image-mask-label triples across 9 imaging modalities (everything). On object detection, which aims to locate a specific object of interest, BiomedParse again attained state-of-the-art performance, especially on objects with irregular shapes (everywhere). On object recognition, which aims to identify all objects in a given image along with their semantic types, we showed that BiomedParse can simultaneously segment and label all biomedical objects in an image (all at once). In summary, BiomedParse is an all-in-one tool for biomedical image analysis by jointly solving segmentation, detection, and recognition for all major biomedical image modalities, paving the path for efficient and accurate image-based biomedical discovery.",
      "url": "https://www.microsoft.com/en-us/research/publication/biomedparse-a-biomedical-foundation-model-for-image-parsing-of-everything-everywhere-all-at-once/"
    },
    {
      "title": "PRISE: LLM-Style Sequence Compression for Learning Temporal Action Abstractions in Control",
      "authors": [
        "Andrey Kolobov",
        "Ching-An Cheng",
        "Furong Huang",
        "Hal Daumé III",
        "Ruijie Zheng"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "June 2024",
      "abstract": "Temporal action abstractions, along with belief state representations, are a powerful knowledge sharing mechanism for sequential decision making. In this work, we propose a novel view that treats inducing temporal action abstractions as a sequence compression problem. To do so, we bring a subtle but critical component of LLM training pipelines — input tokenization via byte pair encoding (BPE) — to the seemingly distant task of learning skills of variable time span in continuous control domains. We introduce an approach called Primitive Sequence Encoding (PRISE) that combines continuous action quantization with BPE to learn powerful action abstractions. We empirically show that high-level skills discovered by PRISE from a multitask set of robotic manipulation demonstrations significantly boost the learning performance of Behavior Cloning on downstream tasks.",
      "url": "https://www.microsoft.com/en-us/research/publication/prise-learning-temporal-action-abstractions-as-a-sequence-compression-problem/"
    },
    {
      "title": "SLTrain: a sparse plus low-rank approach for parameter and memory efficient pretraining",
      "authors": [
        "Akiko Takeda",
        "Andi Han",
        "Bamdev Mishra",
        "Jiaxiang Li",
        "Mingyi Hong",
        "Pratik Jawanpuria",
        "Wei Huang"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "June 2024",
      "abstract": "Large language models (LLMs) have shown impressive capabilities across various tasks. However, training LLMs from scratch requires significant computational power and extensive memory capacity. Recent studies have explored low-rank structures on weights for efficient fine-tuning in terms of parameters and memory, either through low-rank adaptation or factorization. While effective for fine-tuning, low-rank structures are generally less suitable for pretraining because they restrict parameters to a low-dimensional subspace. In this work, we propose to parameterize the weights as a sum of low-rank and sparse matrices for pretraining, which we call SLTrain. The low-rank component is learned via matrix factorization, while for the sparse component, we employ a simple strategy of uniformly selecting the sparsity support at random and learning only the non-zero entries with the fixed support. While being simple, the random fixed-support sparse learning strategy significantly enhances pretraining when combined with low-rank learning. Our results show that SLTrain adds minimal extra parameters and memory costs compared to pretraining with low-rank parameterization, yet achieves substantially better performance, which is comparable to full-rank training. Remarkably, when combined with quantization and per-layer updates, SLTrain can reduce memory requirements by up to 73% when pretraining the LLaMA 7B model.",
      "url": "https://www.microsoft.com/en-us/research/publication/sltrain-a-sparse-plus-low-rank-approach-for-parameter-and-memory-efficient-pretraining/"
    },
    {
      "title": "Evaluating the Feasibility of Visual Imagery for an EEG-Based Brain–Computer Interface",
      "authors": [
        "Ivan Tashev",
        "James Sulzer",
        "Jarrod Lewis-Peacock",
        "José del R. Millán",
        "Justin Kilmarx"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "June 2024",
      "abstract": "Visual imagery, or the mental simulation of visual information from memory, could serve as an effective control paradigm for a brain-computer interface (BCI) due to its ability to directly convey the user’s intention with many natural ways of envisioning an intended action. However, multiple initial investigations into using visual imagery as a BCI control strategies have been unable to fully evaluate the capabilities of true spontaneous visual mental imagery. One major limitation in these prior works is that the target image is typically displayed immediately preceding the imagery period. This paradigm does not capture spontaneous mental imagery as would be necessary in an actual BCI application but something more akin to short-term retention in visual working memory. Results from the present study show that short-term visual imagery following the presentation of a specific target image provides a stronger, more easily classifiable neural signature in EEG than spontaneous visual imagery from long-term memory following an auditory cue for the image. We also show that short-term visual imagery and visual perception share commonalities in the most predictive electrodes and spectral features. However, visual imagery received greater influence from frontal electrodes whereas perception was mostly confined to occipital electrodes. This suggests that visual perception is primarily driven by sensory information whereas visual imagery has greater contributions from areas associated with memory and attention. This work provides the first direct comparison of short-term and long-term visual imagery tasks and provides greater insight into the feasibility of using visual imagery as a BCI control strategy.",
      "url": "https://www.microsoft.com/en-us/research/publication/evaluating-the-feasibility-of-visual-imagery-for-an-eeg-based-brain-computer-interface/"
    },
    {
      "title": "“One-Size-Fits-All”? Examining Expectations around What Constitute “Fair” or “Good” NLG System Behaviors",
      "authors": [
        "Alexandra Olteanu",
        "Hanna Wallach",
        "Li Lucy",
        "Milad Shokouhi",
        "Su Lin Blodgett"
      ],
      "research_areas": [
        "Human language technologies"
      ],
      "publication_date": "June 2024",
      "abstract": "Fairness-related assumptions about what constitute appropriate NLG system behaviors range from invariance, where systems are expected to behave identically for social groups, to adaptation, where behaviors should instead vary across them. To illuminate tensions around invariance and adaptation, we conduct five case studies, in which we perturb different types of identity-related language features (names, roles, locations, dialect, and style) in NLG system inputs. Through these cases studies, we examine people’s expectations of system behaviors, and surface potential caveats of these contrasting yet commonly held assumptions. We find that motivations for adaptation include social norms, cultural differences, feature-specific information, and accommodation; in contrast, motivations for invariance include perspectives that favor prescriptivism, view adaptation as unnecessary or too difficult for NLG systems to do appropriately, and are wary of false assumptions. Our findings highlight open challenges around what constitute “fair” or “good” NLG system behaviors.",
      "url": "https://www.microsoft.com/en-us/research/publication/one-size-fits-all-examining-expectations-around-what-constitute-fair-or-good-nlg-system-behaviors/"
    },
    {
      "title": "Auto-Formula: Recommend Formulas in Spreadsheets using Contrastive Learning for Table Representations",
      "authors": [
        "Dongmei Zhang",
        "Haidong Zhang",
        "Ju Fan",
        "Sibei Chen",
        "Song Ge",
        "Surajit Chaudhuri",
        "Weiwei Cui",
        "Yeye He"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Data platforms and analytics"
      ],
      "publication_date": "June 2024",
      "abstract": "Spreadsheets are widely recognized as the most popular end-user programming tools, which blend the power of formula-based computation, with an intuitive table-based interface. Today, spreadsheets are used by billions of users to manipulate tables, most of whom are neither database experts nor professional programmers.\nDespite the success of spreadsheets, authoring complex formulas remains challenging, as non-technical users need to look up and understand non-trivial formula syntax. To address this pain point, we leverage the observation that there is often an abundance of similar-looking spreadsheets in the same organization, which not only have similar data, but also share similar computation logic encoded as formulas. We develop an Auto-Formula system that can accurately predict formulas that users want to author in a target spreadsheet cell, by learning and adapting formulas that already exist in similar spreadsheets, using contrastive-learning techniques inspired by “similar-face recognition” from compute vision.\nExtensive evaluations on over 2K test formulas extracted from real enterprise spreadsheets show the effectiveness of Auto-Formula over alternatives. Our benchmark data is available at \\url{https://github.com/microsoft/Auto-Formula} to facilitate future research.",
      "url": "https://www.microsoft.com/en-us/research/publication/auto-formula-recommend-formulas-in-spreadsheets-using-contrastive-learning-for-table-representations/"
    },
    {
      "title": "Solving Data-centric Tasks using Large Language Models",
      "authors": [
        "Advait Sarkar",
        "Andy Gordon",
        "Ben Zorn",
        "Brian Slininger",
        "Carina Negreanu",
        "Christian Poelitz",
        "Elnaz Nouri",
        "Jack Williams",
        "José Cambronero",
        "Nadia Polikarpova",
        "Neil Toronto",
        "Shraddha Barke",
        "Vu Le"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "June 2024",
      "abstract": "Large language models are rapidly replacing help forums like StackOverflow, and are especially helpful to non-professional programmers and end users. These users are often interested in data-centric tasks, like spreadsheet manipulation and data wrangling, which are hard to solve if the intent is only communicated using a natural-language description, without including data. But how do we decide how much data and which data to include in the prompt?This paper makes two contributions towards answering this question. First, we create a dataset of real-world NL-to-code tasks manipulating tabular data, mined from StackOverflow posts. Second, we introduce a novel cluster-then-select prompting technique, which adds the most representative rows from the input data to the LLM prompt. Our experiments show that LLM performance is indeed sensitive to the amount of data passed in the prompt, and that for tasks with a lot of syntactic variation in the input table,our cluster-then-select technique outperforms a random selection baseline.",
      "url": "https://www.microsoft.com/en-us/research/publication/solving-data-centric-tasks-using-large-language-models/"
    },
    {
      "title": "Overview of the MEDIQA-CORR 2024 Shared Task on Medical Error Detection and Correction",
      "authors": [
        "Asma Ben Abacha",
        "Fei Xia",
        "Meliha Yetisgen",
        "Wen-wai Yim",
        "Yujuan Fu",
        "Zhaoyi Sun"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Medical, health and genomics"
      ],
      "publication_date": "June 2024",
      "abstract": "Automatic detection and correction of medical errors enables a more rigorous validation of medical documentation as well as clinical notes generated by large language models. Such solutions can ensure the accuracy and medical coherence of clinical texts and enhance patient care and health outcomes. The MEDIQA-CORR 2024 shared task focused on detecting and correcting different types of medical errors in clinical texts. Seventeen teams participated in the shared task and experimented with a broad range of approaches and models. In this paper, we describe the MEDIQA-CORR task, datasets, and the participants’ results and methods.",
      "url": "https://www.microsoft.com/en-us/research/publication/overview-of-the-mediqa-corr-2024-shared-task-on-medical-error-detection-and-correction/"
    },
    {
      "title": "Position: What Can Large Language Models Tell Us about Time Series Analysis",
      "authors": [
        "Bin Yang",
        "Jindong Wang",
        "Kexin Zhang",
        "Ming Jin",
        "Qingsong Wen",
        "Shirui Pan",
        "Wei Chen",
        "Yifan Zhang",
        "Yuxuan Liang"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "June 2024",
      "abstract": "Time series analysis is essential for comprehending the complexities inherent in various realworld systems and applications. Although large language models (LLMs) have recently made significant strides, the development of artificial general intelligence (AGI) equipped with time series analysis capabilities remains in its nascent phase. Most existing time series models heavily rely on domain knowledge and extensive model tuning, predominantly focusing on prediction tasks. In this paper, we argue that current LLMs have the potential to revolutionize time series analysis, thereby promoting efficient decision-making and advancing towards a more universal form of time series analytical intelligence. Such advancement could unlock a wide range of possibilities, including time series modality switching and question answering. We encourage researchers and practitioners to recognize the potential of LLMs in advancing time series analysis and emphasize the need for trust in these related efforts. Furthermore, we detail the seamless integration of time series analysis with existing LLM technologies and outline promising avenues for future research.",
      "url": "https://www.microsoft.com/en-us/research/publication/position-what-can-large-language-models-tell-us-about-time-series-analysis/"
    },
    {
      "title": "Memories are One-to-Many Mapping Alleviators in Talking Face Generation",
      "authors": [
        "Jiang Bian",
        "Sheng Zhao",
        "Tianyu He"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Computer vision"
      ],
      "publication_date": "June 2024",
      "abstract": "Talking face generation aims at generating photo-realistic video portraits of a target person driven by input audio. Due to its nature of one-to-many mapping from the input audio to the output video (e.g., one speech content may have multiple feasible visual appearances), learning a deterministic mapping like previous works brings ambiguity during training, and thus causes inferior visual results. Although this one-to-many mapping could be alleviated in part by a two-stage framework (i.e., an audio-to-expression model followed by a neural-rendering model), it is still insufficient since the prediction is produced without enough information (e.g., emotions, wrinkles, etc.). In this paper, we propose MemFace to complement the missing information with an implicit memory and an explicit memory that follow the sense of the two stages respectively. More specifically, the implicit memory is employed in the audio-to-expression model to capture high-level semantics in the audio-expression shared space, while the explicit memory is employed in the neural-rendering model to help synthesize pixel-level details. Our experimental results show that our proposed MemFace surpasses all the state-of-the-art results across multiple scenarios consistently and significantly.",
      "url": "https://www.microsoft.com/en-us/research/publication/memories-are-one-to-many-mapping-alleviators-in-talking-face-generation/"
    },
    {
      "title": "Language Models can be Deductive Solvers",
      "authors": [
        "Dongyan Zhao",
        "Hiteshi Sharma",
        "Jiazhan Feng",
        "Junheng Hao",
        "Ruochen Xu",
        "Weizhu Chen",
        "Yelong Shen"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Human language technologies"
      ],
      "publication_date": "June 2024",
      "abstract": "Logical reasoning is a fundamental aspect of human intelligence and a key component of tasks like problem-solving and decision-making. Recent advancements have enabled Large Language Models (LLMs) to potentially exhibit reasoning capabilities, but complex logical reasoning remains a challenge. The state-of-the-art, solver-augmented language models, use LLMs to parse natural language logical questions into symbolic representations first and then adopt external logical solvers to take in the symbolic representations and output the answers. Despite their impressive performance, any parsing errors will inevitably result in the failure of the execution of external logical solvers and no answer to the logical questions. In this paper, we introduce LoGiPT, a novel language model that directly internalizes and emulates the reasoning processes of logical solvers and avoids parsing errors by learning strict adherence to solver syntax and grammar. LoGiPT is fine-tuned on a newly constructed instruction-tuning dataset derived from revealing and refining the invisible reasoning process of deductive solvers. Experimental results on two public deductive reasoning benchmarks show that LoGiPT outperforms state-of-the-art solver-augmented LMs and few-shot prompting methods on competitive LLMs like GPT-4.",
      "url": "https://www.microsoft.com/en-us/research/publication/language-models-can-be-deductive-solvers/"
    },
    {
      "title": "Concurrent Immediate Reference Counting",
      "authors": [
        "Jaehwang Jung",
        "Jeehoon Kang",
        "Jeonghyeon Kim",
        "Matthew J. Parkinson"
      ],
      "research_areas": [
        "Programming languages and software engineering",
        "Systems and networking"
      ],
      "publication_date": "June 2024",
      "abstract": "Memory management for optimistic concurrency in unmanaged programming languages is challenging. Safe\nmemory reclamation (SMR) algorithms help address this, but they are difficult to use correctly. Automatic\nreference counting provides a simpler interface, but it has been less efficient than SMR algorithms. Recently,\nthere has been a push to apply the optimizations used in garbage collectors for managed languages to elide\nreference count updates from local references. Notably, Fast Reference Counter, OrcGC, and Concurrent\nDeferred Reference Counting use SMR algorithms to protect local references by deferring decrements or\nreclamation. While they show a significant performance improvement, their use of deferral may result\nin growing memory usage due to slow reclamation of linked structures, and suboptimal performance in\nupdate-heavy workloads.\nWe present Concurrent Immediate Reference Counting (CIRC), a new combination of SMR algorithms with\nreference counting. CIRC employs deferral like other modern methods, but it avoids their problems with\nnovel algorithms for (1) immediately reclaiming linked structures recursively by tracking the reachability of\neach object, and (2) applying decrements immediately and deferring only the reclamation. Our experiments\nshow that CIRC’s memory usage does not grow over time and is only slightly higher than the underlying\nSMR. Moreover, CIRC further narrows the performance gap between the underlying SMR, positioning it\nas a promising solution to safe automatic memory management for highly concurrent data structures in\nunmanaged languages.",
      "url": "https://www.microsoft.com/en-us/research/publication/concurrent-immediate-reference-counting/"
    },
    {
      "title": "ReNeuIR at SIGIR 2024: The Third Workshop on Reaching Efficiency in Neural Information Retrieval",
      "authors": [
        "Bhaskar Mitra",
        "Franco Maria Nardini",
        "Joel Mackenzie",
        "Maik Fröbe",
        "Martin Potthast"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Search and information retrieval"
      ],
      "publication_date": "June 2024",
      "abstract": "The Information Retrieval (IR) community has a rich history of empirically measuring novel retrieval methods in terms of effectiveness and efficiency. However, as the search ecosystem is developing rapidly, comparatively little attention has been paid to evaluating efficiency in recent years, which raises the question of the cost-benefit ratio between effectiveness and efficiency. In this regard, it has become difficult to compare and contrast systems in an empirically fair way. Factors including hardware configurations, software versioning, experimental settings, and measurement methods all contribute to the difficulty of meaningfully comparing search systems, especially where efficiency is a key component of the evaluation. Furthermore, efficiency is no longer limited to time and space but has found new, challenging dimensions that stretch to resource, sample, and energy efficiency and have implications for users, researchers, and the environment. Examining algorithms and models through the lens of efficiency and its trade-off with effectiveness requires revisiting and establishing new standards and principles, from defining relevant concepts, to designing measures, to creating guidelines for making sense of the significance of findings. The third iteration of ReNeuIR aims to bring the community together to debate these questions and collaboratively test and improve a benchmarking framework for efficiency derived from the discussions of the first two iterations of this workshop. We provide a first prototype of this framework by organizing a shared task track focused on comparability and reproducibility at the workshop.",
      "url": "https://www.microsoft.com/en-us/research/publication/reneuir-at-sigir-2024-the-third-workshop-on-reaching-efficiency-in-neural-information-retrieval/"
    },
    {
      "title": "Splitwise: Efficient generative LLM inference using phase splitting",
      "authors": [
        "Aashaka Shah",
        "Chaojie Zhang",
        "Esha Choukse",
        "Pratyush Patel",
        "Ricardo Bianchini",
        "Saeed Maleki",
        "Íñigo Goiri"
      ],
      "research_areas": [
        "Systems and networking"
      ],
      "publication_date": "June 2024",
      "abstract": "Generative large language model (LLM) applications are growing rapidly, leading to large-scale deployments of expensive and power-hungry GPUs. Our characterization of LLM inference shows that each inference request undergoes two phases: a compute-intensive prompt computation phase and a memory-intensive token generation phase, each with distinct latency, throughput, memory, and power characteristics. Despite state of-the-art batching and scheduling, the token generation phase underutilizes compute resources. Unlike prompt computation, token generation does not need the compute capability of the latest GPUs and can be run with lower power and cost.\nBased on these insights, we propose Splitwise, a model deployment and scheduling technique that splits the two phases of LLM inference requests on to separate machines. Splitwise enables phase-specific resource management using hardware that is well suited for each phase. Request state is transferred efficiently between machines using optimized network libraries on the fast back-plane interconnects available in today’s GPU clusters. Using Splitwise, we design homogeneous and heterogeneous LLM inference clusters optimized for throughput, cost, and power. Compared to current designs, Splitwise clusters achieve up to 1.4x higher throughput at 20% lower cost. Alternatively, they can deliver 2.35x more throughput under the same power and cost budgets.",
      "url": "https://www.microsoft.com/en-us/research/publication/splitwise-efficient-generative-llm-inference-using-phase-splitting/"
    },
    {
      "title": "“It’s like a rubber duck that talks back”: Understanding Generative AI-Assisted Data Analysis Workflows through a Participatory Prompting Study",
      "authors": [
        "Advait Sarkar",
        "Carina Negreanu",
        "Ian Drosos",
        "Lev Tankelevitch",
        "Sean Rintel",
        "Xiaotong Xu"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Human-computer interaction",
        "Social sciences"
      ],
      "publication_date": "June 2024",
      "abstract": "Generative AI tools can help users with many tasks. One such task is data analysis, which is notoriously challenging for non-expert end-users due to its expertise requirements, and where AI holds much potential, such as finding relevant data sources, proposing analysis strategies, and writing analysis code. To understand how data analysis workflows can be assisted or impaired by generative AI, we conducted a study (n=15) using Bing Chat via participatory prompting. Participatory prompting is a recently developed methodology in which users and researchers reflect together on tasks through co-engagement with generative AI. In this paper we demonstrate the value of the participatory prompting method. We found that generative AI benefits the information foraging and sensemaking loops of data analysis in specific ways, but also introduces its own barriers and challenges, arising from the difficulties of query formulation, specifying context, and verifying results.",
      "url": "https://www.microsoft.com/en-us/research/publication/its-like-a-rubber-duck-that-talks-back-understanding-generative-ai-assisted-data-analysis-workflows-through-a-participatory-prompting-study/"
    },
    {
      "title": "Safe and Robust Subgame Exploitation in Imperfect Information Games",
      "authors": [
        "Bo An",
        "Linjian Meng",
        "Tianyu Ding",
        "Wenbin Li",
        "Yang Gao",
        "Zheng Xu",
        "Zhenxing Ge"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "June 2024",
      "abstract": "Opponent exploitation is an important task for players to exploit the weaknesses of others in games. Existing approaches mainly focus on balancing between exploitation and exploitability but are often vulnerable to modeling errors and deceptive adversaries. To address this problem, our paper offers a novel perspective on the safety of opponent exploitation, named Adaptation Safety. This concept leverages the insight that strategies, even those not explicitly aimed at opponent exploitation, may inherently be exploitable due to computational complexities, rendering traditional safety overly rigorous. In contrast, adaptation safety requires that the strategy should not be more exploitable than it would be in scenarios where opponent exploitation is not considered. Building on such adaptation safety, we further propose an Opponent eXploitation Search (OX-Search) framework by incorporating real-time search techniques for efficient online opponent exploitation. Moreover, we provide theoretical analyses to show the adaptation safety and robust exploitation of OX-Search, even with inaccurate opponent models. Empirical evaluations in popular poker games demonstrate OX-Search’s superiority in both exploitability and exploitation compared to previous methods.",
      "url": "https://www.microsoft.com/en-us/research/publication/safe-and-robust-subgame-exploitation-in-imperfect-information-games/"
    },
    {
      "title": "Investigating the Effect of Misalignment on Membership Privacy in the White-box Setting",
      "authors": [
        "Ana-Maria Cretu",
        "Daniel Jones",
        "Shruti Tople",
        "Yves-Alexandre de Montjoye"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Security, privacy, and cryptography"
      ],
      "publication_date": "June 2024",
      "abstract": "Machine learning models have been shown to leak sensitive information about their training datasets. Models are increasingly deployed on devices, raising concerns that white-box access to the model parameters increases the attack surface compared to black-box access which only provides query access. Directly extending the shadow modelling technique from the black-box to the white-box setting has been shown, in general, not to perform better than black-box only attacks. A potential reason is misalignment, a known characteristic of deep neural networks. In the shadow modelling context, misalignment means that, while the shadow models learn similar features in each layer, the features are located in different positions. We here present the first systematic analysis of the causes of misalignment in shadow models and show the use of a different weight initialisation to be the main cause. We then extend several re-alignment techniques, previously developed in the model fusion literature, to the shadow modelling context, where the goal is to re-align the layers of a shadow model to those of the target model. We show re-alignment techniques to significantly reduce the measured misalignment between the target and shadow models. Finally, we perform a comprehensive evaluation of white-box membership inference attacks (MIA). Our analysis reveals that internal layer activation-based MIAs suffer strongly from shadow model misalignment, while gradient-based MIAs are only sometimes significantly affected. We show that re-aligning the shadow models strongly improves the former’s performance and can also improve the latter’s performance, although less frequently. On the CIFAR10 dataset with a false positive rate of 1%, white-box MIA using re-aligned shadow models improves the true positive rate by 4.5%. Taken together, our results highlight that on-device deployment increases the attack surface and that the newly available information can be used to build more powerful attacks.",
      "url": "https://www.microsoft.com/en-us/research/publication/investigating-the-effect-of-misalignment-on-membership-privacy-in-the-white-box-setting/"
    },
    {
      "title": "Knowledge Distillation in Automated Annotation: Supervised Text Classification with LLM-Generated Training Labels",
      "authors": [
        "Nick Pangakis",
        "Sam Wolken"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Social sciences"
      ],
      "publication_date": "June 2024",
      "abstract": "Computational social science (CSS) practitioners often rely on human-labeled data to fine-tune supervised text classifiers. We assess the potential for researchers to augment or replace human-generated training data with surrogate training labels from generative large language models (LLMs). We introduce a recommended workflow and test this LLM application by replicating 14 classification tasks and measuring performance. We employ a novel corpus of English-language text classification data sets from recent CSS articles in high-impact journals. Because these data sets are stored in password-protected archives, our analyses are less prone to issues of contamination. For each task, we compare supervised classifiers fine-tuned using GPT-4 labels against classifiers fine-tuned with human annotations and against labels from GPT-4 and Mistral-7B with few-shot in-context learning. Our findings indicate that supervised classification models fine-tuned on LLM-generated labels perform comparably to models fine-tuned with labels from human annotators. Fine-tuning models using LLM-generated labels can be a fast, efficient and cost-effective method of building supervised text classifiers.",
      "url": "https://www.microsoft.com/en-us/research/publication/knowledge-distillation-in-automated-annotation-supervised-text-classification-with-llm-generated-training-labels/"
    },
    {
      "title": "Wii: Dynamic Budget Reallocation In Index Tuning (Extended Version)",
      "authors": [
        "Chi Wang",
        "Surajit Chaudhuri",
        "Vivek Narasayya",
        "Wentao Wu",
        "Xiaoying Wang"
      ],
      "research_areas": [
        "Data platforms and analytics"
      ],
      "publication_date": "June 2024",
      "abstract": "Index tuning aims to find the optimal index configuration for an input workload. It is often a time-consuming and resource-intensive process, largely attributed to the huge amount of “what-if” calls made to the query optimizer during configuration enumeration. Therefore, in practice it is desirable to set a budget constraint that limits the number of what-if calls allowed. This yields a new problem of budget allocation, namely, deciding on which query-configuration pairs (QCP’s) to issue what-if calls. Unfortunately, optimal budget allocation is NP-hard, and budget allocation decisions made by existing solutions can be inferior. In particular, many of the what-if calls allocated by using existing solutions are devoted to QCP’s whose what-if costs can be approximated by using cost derivation, a well-known technique that is computationally much more efficient and has been adopted by commercial index tuning software. This results in considerable waste of the budget, as these what-if calls are unnecessary. In this paper, we propose “Wii,” a lightweight mechanism that aims to avoid such spurious what-if calls. It can be seamlessly integrated with existing configuration enumeration algorithms. Experimental evaluation on top of both standard industrial benchmarks and real workloads demonstrates that Wii can eliminate significant number of spurious what-if calls. Moreover, by reallocating the saved budget to QCP’s where cost derivation is less accurate, existing algorithms can be significantly improved in terms of the final configuration found.",
      "url": "https://www.microsoft.com/en-us/research/publication/wii-dynamic-budget-reallocation-in-index-tuning-extended-version/"
    },
    {
      "title": "ProbTS: Benchmarking Point and Distributional Forecasting across Diverse Prediction Horizons",
      "authors": [
        "Jia Li",
        "Jiang Bian",
        "Jiawen Zhang",
        "Shun Zheng",
        "Xumeng Wen",
        "Zhenwei Zhang"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "June 2024",
      "abstract": "Delivering precise point and distributional forecasts across a spectrum of prediction horizons represents a significant and enduring challenge in the application of time-series forecasting within various industries. Prior research on developing deep learning models for time-series forecasting has often concentrated on isolated aspects, such as long-term point forecasting or short-term probabilistic estimations. This narrow focus may result in skewed methodological choices and hinder the adaptability of these models to uncharted scenarios. While there is a rising trend in developing universal forecasting models, a thorough understanding of their advantages and drawbacks, especially regarding essential forecasting needs like point and distributional forecasts across short and long horizons, is still lacking. In this paper, we present ProbTS, a benchmark tool designed as a unified platform to evaluate these fundamental forecasting needs and to conduct a rigorous comparative analysis of numerous cutting-edge studies from recent years. We dissect the distinctive data characteristics arising from disparate forecasting requirements and elucidate how these characteristics can skew methodological preferences in typical research trajectories, which often fail to fully accommodate essential forecasting needs. Building on this, we examine the latest models for universal time-series forecasting and discover that our analyses of methodological strengths and weaknesses are also applicable to these universal models. Finally, we outline the limitations inherent in current research and underscore several avenues for future exploration.",
      "url": "https://www.microsoft.com/en-us/research/publication/probts-benchmarking-point-and-distributional-forecasting-across-diverse-prediction-horizons/"
    },
    {
      "title": "Can LLMs Learn by Teaching for Better Reasoning? A Preliminary Study",
      "authors": [
        "Guohao Dai",
        "Huazhong Yang",
        "Matthew B. Blaschko",
        "Peiran Yao",
        "Shiyao Li",
        "Tianyu Fu",
        "Xuefei Ning",
        "Yu Wang",
        "Zifu Wang",
        "Zinan Lin"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "June 2024",
      "abstract": "Teaching to improve student models (e.g., knowledge distillation) is an extensively studied methodology in LLMs. However, for humans, teaching not only improves students but also improves teachers. We ask: Can LLMs also learn by teaching (LbT)? If yes, we can potentially unlock the possibility of continuously advancing the models without solely relying on human-produced data or stronger models. In this paper, we provide a preliminary exploration of this ambitious agenda. We show that LbT ideas can be incorporated into existing LLM training/prompting pipelines and provide noticeable improvements. Specifically, we design three methods, each mimicking one of the three levels of LbT in humans: observing students’ feedback, learning from the feedback, and learning iteratively, with the goals of improving answer accuracy without training and improving models’ inherent capability with fine-tuning. The findings are encouraging. For example, similar to LbT in human, we see that: (1) LbT can induce weak-to-strong generalization: strong models can improve themselves by teaching other weak models; (2) Diversity in students might help: teaching multiple students could be better than teaching one student or the teacher itself. We hope that this early promise can inspire future research on LbT and more broadly adopting the advanced techniques in education to improve LLMs. The code is available at https://github.com/imagination-research/lbt (opens in new tab).",
      "url": "https://www.microsoft.com/en-us/research/publication/can-llms-learn-by-teaching-a-preliminary-study/"
    },
    {
      "title": "DyLeCT: Achieving Huge-page-like Translation Performance For Hardware-compressed Memory",
      "authors": [
        "Esha Choukse",
        "Gagan Panwar",
        "Muhammad Laghari",
        "Xun Jian"
      ],
      "research_areas": [
        "Hardware and devices"
      ],
      "publication_date": "June 2024",
      "abstract": "As DRAM scaling slows, a promising solution is to logically scale up memory capacity through hardware-memory compression, where the CPU-side memory controller (MC) dynamically compresses and packs data more densely into DRAM. However, this requires introducing a new layer of hardware-managed address translation in the MC; for large and irregular workloads that already suffer from frequent virtual address translation misses in the TLB, adding a new translation can double the translation misses (e.g., by adding a new miss in the MC per TLB miss). Worse, while TLB misses can be drastically reduced by using huge pages, no work has explored huge-page-like translation in the MC for hardware memory compression.\nThis paper explores how to achieve huge-page-like translation performance in the MC for hardware memory compression. To minimize data movement, we let the MC still manage everything at page granularity, instead of huge page. We propose dynamically shortening the translations for hot pages to only a few bits (e.g., 2) by dynamically migrating them to locations encodable by these few bits at the cost of displacing cold pages to locations that require full-length translations to encode. As translation caches favor hot pages, switching to space-efficient short translations for hot pages alone is enough to pack many translations into the cache, so that just 128KB can provide similar (e.g., up to 2GB) total translation reach as a TLB that entirely uses huge page entries. Evaluations show our proposal — Dynamic Length Compressed-Memory Translations (DyLeCT) — improves average performance by 10.5% over the prior art.",
      "url": "https://www.microsoft.com/en-us/research/publication/dylect-achieving-huge-page-like-translation-performance-for-hardware-compressed-memory/"
    },
    {
      "title": "SIGMA: Secure GPT Inference with Function Secret Sharing",
      "authors": [
        "Ananta Mukherjee",
        "Ashish Panwar",
        "Divya Gupta",
        "Kanav Gupta",
        "Neha Jawalkar",
        "Nishanth Chandran",
        "Rahul Sharma"
      ],
      "research_areas": [
        "Security, privacy, and cryptography"
      ],
      "publication_date": "June 2024",
      "abstract": "Secure 2-party computation (2PC) enables secure inference that offers protection for both proprietary machine learning (ML) models and sensitive inputs to them. However, the existing secure inference solutions suffer from high latency and communication overheads, particularly for transformers. Function secret sharing (FSS) is a recent paradigm for obtaining efficient 2PC protocols with a preprocessing phase. We provide SIGMA, the first end-to-end system for secure transformer inference based on FSS. By constructing new FSS-based protocols for complex machine learning functionalities, such as Softmax and GeLU, and also accelerating their computation on GPUs, SIGMA improves the latency of secure inference of transformers by 11-19xover the state-of-the-art that uses preprocessing and GPUs. We present the first secure inference of generative pre-trained transformer (GPT) models. In particular, SIGMA executes GPT-Neo with 1.3 billion parameters in 7.4s and HuggingFace’s GPT2 in 1.6s.",
      "url": "https://www.microsoft.com/en-us/research/publication/sigma-secure-gpt-inference-with-function-secret-sharing/"
    },
    {
      "title": "Open X-Embodiment: Robotic Learning Datasets and RT-X Models",
      "authors": [
        "Andrey Kolobov",
        "Anikait Singh, Animesh Garg, Aniruddha Kembhavi, Annie Xie, Anthony Brohan, Antonin Raffin, Archit Sharma, Arefeh Yavary, Arhan Jain, Ashwin Balakrishna, Ayzaan Wahid, Ben Burgess-Limerick, Beomjoon Kim, Bernhard Schölkopf, Blake Wulfe, Brian Ichter, Cewu Lu, Charles Xu, Charlotte Le, Chelsea Finn, Chen Wang, Chenfeng Xu, Cheng Chi, Chenguang Huang, Christine Chan, Christopher Agia, Chuer Pan, Chuyuan Fu, Coline Devin, Danfei Xu, Daniel Morton, Danny Driess, Daphne Chen, Deepak Pathak, Dhruv Shah, Dieter Büchler, Dinesh Jayaraman, Dmitry Kalashnikov, Dorsa Sadigh, Edward Johns, Ethan Foster, Fangchen Liu, Federico Ceola, Fei Xia, Feiyu Zhao, Felipe Vieira Frujeri, Freek Stulp, Gaoyue Zhou, Gaurav S. Sukhatme, Gautam Salhotra, Ge Yan, Gilbert Feng, Giulio Schiavi, Glen Berseth, Gregory Kahn, Guangwen Yang, Guanzhi Wang, Hao Su, Hao-Shu Fang, Haochen Shi, Henghui Bao, Heni Ben Amor, Henrik I Christensen, Hiroki Furuta, Homanga Bharadhwaj, Homer Walke, Hongjie Fang, Huy Ha, Igor Mordatch, Ilija Radosavovic, Isabel Leal, Jacky Liang, Jad Abou-Chakra, Jaehyung Kim, Jaimyn Drake, Jan Peters, Jan Schneider, Jasmine Hsu, Jay Vakil, Jeannette Bohg, Jeffrey Bingham, Jeffrey Wu, Jensen Gao, Jiaheng Hu, Jiajun Wu, Jialin Wu, Jiankai Sun, Jianlan Luo, Jiayuan Gu, Jie Tan, Jihoon Oh, Jimmy Wu, Jingpei Lu, Jingyun Yang, Jitendra Malik, João Silvério, Joey Hejna, Jonathan Booher, Jonathan Tompson, Jonathan Yang, Jordi Salvador, Joseph J. Lim, Junhyek Han, Kaiyuan Wang, Kanishka Rao, Karl Pertsch, Karol Hausman, Keegan Go, Keerthana Gopalakrishnan, Ken Goldberg, Kendra Byrne, Kenneth Oslund, Kento Kawaharazuka, Kevin Black, Kevin Lin, Kevin Zhang, Kiana Ehsani, Kiran Lekkala, Kirsty Ellis, Krishan Rana, Krishnan Srinivasan, Kuan Fang, Kunal Pratap Singh, Kuo-Hao Zeng, Kyle Hatch, Kyle Hsu, Laurent Itti, Lawrence Yunliang Chen, Lerrel Pinto, Li Fei-Fei, Liam Tan, Linxi \"Jim\" Fan, Lionel Ott, Lisa Lee, Luca Weihs, Magnum Chen, Marion Lepert, Marius Memmel, Masayoshi Tomizuka, Masha Itkina, Mateo Guaman Castro, Max Spero, Maximilian Du, Michael Ahn, Michael C. Yip, Mingtong Zhang, Mingyu Ding, Minho Heo, Mohan Kumar Srirama, Mohit Sharma, Moo Jin Kim, Naoaki Kanazawa, Nicklas Hansen, Nicolas Heess, Nikhil J Joshi, Niko Suenderhauf, Ning Liu, Norman Di Palo, Nur Muhammad Mahi Shafiullah, Oier Mees, Oliver Kroemer, Osbert Bastani, Pannag R Sanketi, Patrick \"Tree\" Miller, Patrick Yin, Paul Wohlhart, Peng Xu, Peter David Fagan, Peter Mitrano, Pierre Sermanet, Pieter Abbeel, Priya Sundaresan, Qiuyu Chen, Quan Vuong, Rafael Rafailov, Ran Tian, Ria Doshi, Roberto Mart'in-Mart'in, Rohan Baijal, Rosario Scalise, Rose Hendrix, Roy Lin, Runjia Qian, Ruohan Zhang, Russell Mendonca, Rutav Shah, Ryan Hoque, Ryan Julian, Samuel Bustamante, Sean Kirmani, Sergey Levine, Shan Lin, Sherry Moore, Shikhar Bahl, Shivin Dass, Shubham Sonawani, Shubham Tulsiani, Shuran Song, Sichun Xu, Siddhant Haldar, Siddharth Karamcheti, Simeon Adebola, Simon Guist, Soroush Nasiriany, Stefan Schaal, Stefan Welker, Stephen Tian, Subramanian Ramamoorthy, Sudeep Dasari, Suneel Belkhale, Sungjae Park, Suraj Nair, Suvir Mirchandani, Takayuki Osa, Tanmay Gupta, Tatsuya Harada, Tatsuya Matsushima, Ted Xiao, Thomas Kollar, Tianhe Yu, Tianli Ding, Todor Davchev, Tony Z. Zhao, Travis Armstrong, Trevor Darrell, Trinity Chung, Vidhi Jain, Vikash Kumar, Vincent Vanhoucke, Wei Zhan, Wenxuan Zhou, Wolfram Burgard, Xi Chen, Xiangyu Chen, Xiaolong Wang, Xinghao Zhu, Xinyang Geng, Xiyuan Liu, Xu Liangwei, Xuanlin Li, Yansong Pang, Yao Lu, Yecheng Jason Ma, Yejin Kim, Yevgen Chebotar, Yifan Zhou, Yifeng Zhu, Yilin Wu, Ying Xu, Yixuan Wang, Yonatan Bisk, Yongqiang Dou, Yoonyoung Cho, Youngwoon Lee, Yuchen Cui, Yue Cao, Yueh-Hua Wu, Yujin Tang, Yuke Zhu, Yunchu Zhang, Yunfan Jiang, Yunshuang Li, Yunzhu Li, Yusuke Iwasawa, Yutaka Matsuo, Zehan Ma, Zhuo Xu, Zichen Jeff Cui, Zichen Zhang, Zipeng Fu, Zipeng Lin",
        "Open X-Embodiment Collaboration, Abby O'Neill, Abdul Rehman, Abhinav Gupta, Abhiram Maddukuri, Abhishek Gupta, Abhishek Padalkar, Abraham Lee, Acorn Pooley, Agrim Gupta, Ajay Mandlekar, Ajinkya Jain, Albert Tung, Alex Bewley, Alex Herzog, Alex Irpan, Alexander Khazatsky, Anant Rai, Anchit Gupta, Andrew Wang"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "June 2024",
      "abstract": "Large, high-capacity models trained on diverse datasets have shown remarkable successes on efficiently tackling downstream applications. In domains from NLP to Computer Vision, this has led to a consolidation of pretrained models, with general pretrained backbones serving as a starting point for many applications. Can such a consolidation happen in robotics? Conventionally, robotic learning methods train a separate model for every application, every robot, and even every environment. Can we instead train generalist X-robot policy that can be adapted efficiently to new robots, tasks, and environments? In this paper, we provide datasets in standardized data formats and models to make it possible to explore this possibility in the context of robotic manipulation, alongside experimental results that provide an example of effective X-robot policies. We assemble a dataset from 22 different robots collected through a collaboration between 21 institutions, demonstrating 527 skills (160266 tasks). We show that a high-capacity model trained on this data, which we call RT-X, exhibits positive transfer and improves the capabilities of multiple robots by leveraging experience from other platforms. More details can be found on the project website this https URL.",
      "url": "https://www.microsoft.com/en-us/research/publication/open-x-embodiment-robotic-learning-datasets-and-rt-x-models/"
    },
    {
      "title": "Diffy: Data-Driven Bug Finding for Configurations",
      "authors": [
        "Francis Y. Yan",
        "Ryan Beckett",
        "Siva Kesava Reddy Kakarla"
      ],
      "research_areas": [
        "Systems and networking"
      ],
      "publication_date": "June 2024",
      "abstract": "Configuration errors remain a major cause of system failures and service outages. One promising approach to identify configuration errors automatically is to learn common usage patterns (and anti-patterns) using data-driven methods. However, existing data-driven learning approaches analyze only simple configurations (e.g., those with no hierarchical structure), identify only simple types of issues (e.g., type errors), or require extensive domain-specific tuning. In this paper, we present Diffy, the first push-button configuration analyzer that detects likely bugs in structured configurations. From example configurations, Diffy learns a common template, with “holes” that capture their variation. It then applies unsupervised learning to identify anomalous template parameters as likely bugs. We evaluate Diffy on a large cloud provider’s wide-area network, an operational 5G network testbed, and MySQL configurations, demonstrating its versatility, performance, and accuracy. During Diffy’s development, it caught and prevented a bug in a configuration timer value that had previously caused an outage for the cloud provider.",
      "url": "https://www.microsoft.com/en-us/research/publication/diffy-data-driven-bug-finding-for-configurations/"
    },
    {
      "title": "Vertically Autoscaling Monolithic Applications with CaaSPER: Scalable Container-as-a-Service Performance Enhanced Resizing Algorithm for the Cloud",
      "authors": [
        "Andrew Carter",
        "Anna Pavlenko",
        "Brian Kroth",
        "David Liao",
        "Jesús Camacho-Rodríguez",
        "Joyce Cahoon",
        "Karla Saur",
        "Michael Nelson",
        "Travis Wright",
        "Yiwen Zhu"
      ],
      "research_areas": [
        "Data platforms and analytics"
      ],
      "publication_date": "June 2024",
      "abstract": "Kubernetes has emerged as a prominent open-source platform for managing cloud applications, including stateful databases. These monolithic applications rely on vertical scaling, adjusting CPU cores based on load fluctuations. However, our analysis of Kubernetes-based Database-as-a-Service (DBaaS) offerings at Microsoft revealed that many customers consistently over-provision resources for peak workloads, neglecting cost-saving opportunities through resource scale-down. We found that there is a gap in the ability of existing vertical autoscaling tools to minimize resource slack and respond promptly to throttling, leading to increased costs and impacting crucial metrics such as throughput and availability.\nTo address this challenge, we propose CaaSPER, a vertical autoscaling algorithm that blends reactive and proactive strategies. By dynamically adjusting CPU resources, CaaSPER minimizes resource slack, maintains optimal CPU utilization, and reduces throttling. Importantly, customers have the flexibility to prioritize either cost savings or high performance based on their preferences. Extensive testing demonstrates that CaaSPER effectively reduces throttling and keeps CPU utilization within target levels. CaaSPER is designed to be application-agnostic and platform-agnostic, with potential for extension to other applications requiring vertical autoscaling.",
      "url": "https://www.microsoft.com/en-us/research/publication/caasper-vertical-autoscaling/"
    },
    {
      "title": "Riemannian coordinate descent algorithms on matrix manifolds",
      "authors": [
        "Andi Han",
        "Bamdev Mishra",
        "Pratik Jawanpuria"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "June 2024",
      "abstract": "Many machine learning applications are naturally formulated as optimization problems on Riemannian manifolds. The main idea behind Riemannian optimization is to maintain the feasibility of the variables while moving along a descent direction on the manifold. This results in updating all the variables at every iteration. In this work, we provide a general framework for developing computationally efficient coordinate descent (CD) algorithms on matrix manifolds that allows updating only a few variables at every iteration while adhering to the manifold constraint. In particular, we propose CD algorithms for various manifolds such as Stiefel, Grassmann, (generalized) hyperbolic, symplectic, and symmetric positive (semi)definite. While the cost per iteration of the proposed CD algorithms is low, we further develop a more efficient variant via a first-order approximation of the objective function. We analyze their convergence and complexity, and empirically illustrate their efficacy in several applications.",
      "url": "https://www.microsoft.com/en-us/research/publication/riemannian-coordinate-descent-algorithms-on-matrix-manifolds/"
    },
    {
      "title": "Table-GPT: Table Fine-tuned GPT for Diverse Table Tasks",
      "authors": [
        "Danielle Rifinski Fainman",
        "Dongmei Zhang",
        "Dror Yashar",
        "Haidong Zhang",
        "Peng Li",
        "Song Ge",
        "Surajit Chaudhuri",
        "Weiwei Cui",
        "Yeye He"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Data platforms and analytics"
      ],
      "publication_date": "June 2024",
      "abstract": "Language models, such as GPT-3 and ChatGPT, demonstrate remarkable abilities to follow diverse human instructions and perform a wide range of tasks, using instruction fine-tuning. However, when probing language models using a range of basic table-understanding tasks, we observe that today’s language models are still sub-optimal in many table-related tasks, likely because they are pre-trained predominantly on one-dimensional natural-language texts, whereas relational tables are two-dimensional objects.\n \nIn this work, we propose a new “table fine-tuning” paradigm, where we continue to train/fine-tune language models like GPT-3.5 and ChatGPT, using diverse table-tasks synthesized from real tables as training data, which is analogous to “instruction fine-tuning”, but with the goal of enhancing language models’ ability to understand tables and perform table tasks. We show that our resulting Table-GPT models demonstrate (1) better table-understanding capabilities, by consistently outperforming the vanilla GPT-3.5 and ChatGPT, on a wide range of table tasks, including holdout unseen tasks, and (2) strong generalizability, in its ability to respond to diverse human instructions to perform new table-tasks, in a manner similar to GPT-3.5 and ChatGPT.",
      "url": "https://www.microsoft.com/en-us/research/publication/table-gpt-table-fine-tuned-gpt-for-diverse-table-tasks/"
    },
    {
      "title": "Designing Cloud Servers for Lower Carbon",
      "authors": [
        "Akshitha Sriraman",
        "Brijesh Warrier",
        "Celine Irvene",
        "Chaojie Zhang",
        "Chetan Bansal",
        "Daniel S. Berger",
        "Esha Choukse",
        "Fiodar Kazhamiaka",
        "Jaylen Wang",
        "Jonathan Stern",
        "Kali Frost",
        "Ricardo Bianchini",
        "Rodrigo Fonseca"
      ],
      "research_areas": [
        "Systems and networking"
      ],
      "publication_date": "June 2024",
      "abstract": "Major cloud providers intend to reduce carbon emissions by 2030, which requires effective interventions with short deployment timelines. We find that designing carbon-efficient compute server SKUs, or GreenSKUs, is a promising avenue as compute servers cause the majority of cloud emissions. However, designing GreenSKUs has several adoption challenges.\nTo address GreenSKU design challenges for the first time, we develop a systematic methodology and associated framework, GSF, that helps cloud providers make informed GreenSKU design and deployment decisions. GSF enables designing GreenSKU servers and evaluating their end-to-end performance and carbon impacts. GSF accounts for tradeoffs between different emission types and application performance requirements. We apply GSF within a leading cloud provider’s production constraints to make a systematic case for designing and deploying GreenSKUs.\nWe build a new GreenSKU and use GSF to show that it reduces carbon emissions per customer core by 29% compared to currently-deployed cloud servers. When deploying GreenSKUs in a way that meets applications’ performance requirements, we reduce emissions by 16%. When incorporating overall data center overheads, GreenSKU reduces cloud emissions by 9%.",
      "url": "https://www.microsoft.com/en-us/research/publication/designing-cloud-servers-for-lower-carbon/"
    },
    {
      "title": "SmartOClock: Workload- and Risk-Aware Overclocking in the Cloud",
      "authors": [
        "Ashish Raniwala",
        "Brijesh Warrier",
        "Chetan Bansal",
        "Esha Choukse",
        "Haoran Qiu",
        "Jason Lee",
        "Jovan Stojkovic",
        "Mayukh Das",
        "Pulkit Misra",
        "Reed Zimmermann",
        "Ricardo Bianchini",
        "Sam Whitlock",
        "Savyasachi Samal",
        "Zoey Sun",
        "Íñigo Goiri"
      ],
      "research_areas": [
        "Systems and networking"
      ],
      "publication_date": "June 2024",
      "abstract": "Operating server components beyond their voltage and power design limits (i.e., overclocking) enables improving performance and lowering cost for cloud workloads. However, overclocking can significantly degrade component lifetime, increase power consumption, and cause power capping events, eventually diminishing the performance benefits.\nIn this paper, we characterize the impact of overclocking on cloud workloads by studying their profiles from production deployments. Based on the characterization insights, we propose SmartOClock, the first distributed overclocking management platform specifically designed for cloud environments. SmartOClock is a workload-aware scheme that relies on power predictions to heterogeneously distribute the power budgets across its servers based on their needs and then enforce budget compliance locally, per-server, in a decentralized manner.\nSmartOClock reduces the tail latency by 9%, application cost by 30% and total energy consumption by 10% for latency-sensitive microservices on a 36-server deployment. Simulation analysis using production traces show that SmartOClock reduces the number of power capping events by up to 95% while increasing the overclocking success rate by up to 62%. We also describe lessons from building a first-of-its-kind overclockable cluster at a cloud provider for production experiments.",
      "url": "https://www.microsoft.com/en-us/research/publication/smartoclock-workload-and-risk-aware-overclocking-in-the-cloud/"
    },
    {
      "title": "LST-Bench: Benchmarking Log-Structured Tables in the Cloud",
      "authors": [
        "Anja Gruenheid",
        "Ashit Gosalia",
        "Ashvin Agrawal",
        "Avrilia Floratou",
        "Carlo Curino",
        "Cristian Petculescu",
        "Jesús Camacho-Rodríguez",
        "Josep Aguilar-Saborit",
        "Raghu Ramakrishnan"
      ],
      "research_areas": [
        "Data platforms and analytics"
      ],
      "publication_date": "June 2024",
      "abstract": "Data processing engines increasingly leverage distributed file systems for scalable, cost-effective storage. While the Apache Parquet columnar format has become a popular choice for data storage and retrieval, the immutability of Parquet files renders it impractical to meet the demands of frequent updates in contemporary analytical workloads. Log-Structured Tables (LSTs), such as Delta Lake, Apache Iceberg, and Apache Hudi, offer an alternative for scenarios requiring data mutability, providing a balance between efficient updates and the benefits of columnar storage. They provide features like transactions, time-travel, and schema evolution, enhancing usability and enabling access from multiple engines. Moreover, engines like Apache Spark and Trino can be configured to leverage the optimizations and controls offered by LSTs to meet specific business needs. Conventional benchmarks and tools are inadequate for evaluating the transformative changes in the storage layer resulting from these advancements, as they do not allow us to measure the impact of design and optimization choices in this new setting.\nIn this paper, we propose a novel benchmarking approach and metrics that build upon existing benchmarks, aiming to systematically assess LSTs. We develop a framework, LST-Bench, which facilitates effective exploration and evaluation of the collaborative functioning of LSTs and data processing engines through tailored benchmark packages. A package is a mix of use patterns reflecting a target workload; LST-Bench makes it easy to define a wide range of use patterns and combine them into a package, and we include a baseline package for completeness. Our assessment demonstrates the effectiveness of our framework and benchmark packages in extracting valuable insights across diverse environments. The code for LST-Bench is open-sourced and is available at https://github.com/microsoft/lst-bench/ (opens in new tab).",
      "url": "https://www.microsoft.com/en-us/research/publication/lst-bench-benchmarking-log-structured-tables-in-the-cloud/"
    },
    {
      "title": "Sibyl: Forecasting Time-Evolving Query Workloads",
      "authors": [
        "Alekh Jindal",
        "Carlo Curino",
        "Hanxian Huang",
        "Jesús Camacho-Rodríguez",
        "Jishen Zhao",
        "Jyoti Leeka",
        "Rana Alotaibi",
        "Tarique Siddiqui",
        "Yuanyuan Tian"
      ],
      "research_areas": [
        "Data platforms and analytics"
      ],
      "publication_date": "June 2024",
      "abstract": "Database systems often rely on historical query traces to perform workload-based performance tuning. However, real production workloads are time-evolving, making historical queries ineffective for optimizing future workloads. To address this challenge, we propose Sibyl, an end-to-end machine learning-based framework that accurately forecasts a sequence of future queries, with the entire query statements, in various prediction windows. Drawing insights from real-workloads, we propose template-based featurization techniques and develop a stacked-LSTM with an encoder-decoder architecture for accurate forecasting of query workloads. We also develop techniques to improve forecasting accuracy over large prediction windows and achieve high scalability over large workloads with high variability in arrival rates of queries. Finally, we propose techniques to handle workload drifts. Our evaluation on four real workloads demonstrates that Sibyl can forecast workloads with an 87.3% median F1 score, and can result in 1.7× and 1.3× performance improvement when applied to materialized view selection and index selection applications, respectively.",
      "url": "https://www.microsoft.com/en-us/research/publication/sibyl-forecasting-time-evolving-query-workloads/"
    },
    {
      "title": "Is In-Context Learning in Large Language Models Bayesian? A Martingale Perspective",
      "authors": [
        "Chris Holmes",
        "Fabian Falck",
        "Ziyu Wang"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "June 2024",
      "abstract": "In-context learning (ICL) has emerged as a particularly remarkable characteristic of Large Language Models (LLM): given a pretrained LLM and an observed dataset, LLMs can make predictions for new data points from the same distribution without fine-tuning. Numerous works have postulated ICL as approximately Bayesian inference, rendering this a natural hypothesis. In this work, we analyse this hypothesis from a new angle through the martingale property, a fundamental requirement of a Bayesian learning system for exchangeable data. We show that the martingale property is a necessary condition for unambiguous predictions in such scenarios, and enables a principled, decomposed notion of uncertainty vital in trustworthy, safety-critical systems. We derive actionable checks with corresponding theory and test statistics which must hold if the martingale property is satisfied. We also examine if uncertainty in LLMs decreases as expected in Bayesian learning when more data is observed. In three experiments, we provide evidence for violations of the martingale property, and deviations from a Bayesian scaling behaviour of uncertainty, falsifying the hypothesis that ICL is Bayesian.",
      "url": "https://www.microsoft.com/en-us/research/publication/is-in-context-learning-in-large-language-models-bayesian-a-martingale-perspective/"
    },
    {
      "title": "Peekaboo: Interactive video generation via masked-diffusion",
      "authors": [
        "Anshul Nasery",
        "Harkirat Behl",
        "Vibhav Vineet",
        "Yash Jain"
      ],
      "research_areas": [
        "Computer vision"
      ],
      "publication_date": "June 2024",
      "abstract": "Modern video generation models like Sora have achieved remarkable success in producing high-quality videos. However, a significant limitation is their inability to offer interactive control to users, a feature that promises to open up unprecedented applications and creativity. In this work, we introduce the first solution to equip diffusion-based video generation models with spatio-temporal control. We present PEEKABOO, a novel masked attention module, which seamlessly integrates with current video generation models offering control without the need for additional training or inference overhead. To facilitate future research, we also introduce a comprehensive benchmark for interactive video generation. This benchmark offers a standardized framework for the community to assess the efficacy of emerging interactive video generation models. Our extensive qualitative and quantitative assessments reveal that PEEKABOO achieves up to a 3.8x improvement in mIoU over baseline models, all while maintaining the same latency. Code and benchmark are available on the webpage.",
      "url": "https://www.microsoft.com/en-us/research/publication/peekaboo-interactive-video-generation-via-masked-diffusion-2/"
    },
    {
      "title": "NaturalSpeech 3: Zero-Shot Speech Synthesis with Factorized Codec and Diffusion Models",
      "authors": [
        "Detai Xin",
        "Dongchao Yang",
        "Jiang Bian",
        "Jinyu Li",
        "Kai Shen",
        "Kaitao Song",
        "Lei He",
        "Sheng Zhao",
        "Shikun Zhang",
        "Siliang Tang",
        "Tao Qin",
        "Wei Ye",
        "Xiang-Yang Li",
        "Xu Tan",
        "Yanqing Liu",
        "Yichong Leng",
        "Yuancheng Wang",
        "Zeqian Ju",
        "Zhizheng Wu"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Audio and Acoustics"
      ],
      "publication_date": "June 2024",
      "abstract": "While recent large-scale text-to-speech (TTS) models have achieved significant progress, they still fall shorts in speech quality, similarity, and prosody. Considering that speech intricately encompasses various attributes (e.g., content, prosody, timbre, and acoustic details) that pose\nsignificant challenges for generation, a natural idea is to factorize speech into individual subspaces representing different attributes and generate them individually. Motivated by it, we propose NaturalSpeech 3, a TTS system with novel factorized diffusion models to generate natural speech in a zero-shot way. Specifically, 1) we design a neural codec with factorized vector quantization (FVQ) to disentangle speech waveform into subspaces of content, prosody, timbre, and acoustic details; 2) we propose a factorized diffusion model, which generates attributes in each subspace following its corresponding prompt. With this factorization design, NaturalSpeech 3 can effectively and efficiently model the intricate speech with disentangled subspaces in a divide-and-conquer way. Experimental results show that NaturalSpeech 3 outperforms the state-of-the-art TTS systems on quality, similarity, prosody, and intelligibility.",
      "url": "https://www.microsoft.com/en-us/research/publication/naturalspeech-3-zero-shot-speech-synthesis-with-factorized-codec-and-diffusion-models/"
    },
    {
      "title": "FlashEval: Towards Fast and Accurate Evaluation of Text-to-image Diffusion Generative Models",
      "authors": [
        "Guohao Dai",
        "Huazhong Yang",
        "Lin Zhao",
        "Tianchen Zhao",
        "Xuefei Ning",
        "Yu Wang",
        "Zinan Lin"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Computer vision"
      ],
      "publication_date": "June 2024",
      "abstract": "In recent years, there has been significant progress in the development of text-to-image generative models. Evaluating the quality of the generative models is one essential step in the development process. Unfortunately, the evaluation process could consume a significant amount of computational resources, making the required periodic evaluation of model performance (e.g., monitoring training progress) impractical. Therefore, we seek to improve the evaluation efficiency by selecting the representative subset of the text-image dataset. We systematically investigate the design choices, including the section criteria (textural features or image-based metrics) and the selection granularity (prompt-level or set-level). We find that the insights from prior work on subset selection for training data do not generalize to this problem, and we propose FlashEval, an iterative search algorithm tailored to evaluation data selection. We demonstrate the effectiveness of FlashEval on ranking diffusion models with various configurations including architectures, quantization levels, and sampler schedules on COCO and DiffusionDB datasets. Our searched 50-item subset could achieve comparable evaluation quality to the randomly sampled 500-item subset for COCO annotations on unseen models, achieving a 10x evaluation speedup. We will release the condensed subset of these commonly used datasets to help accelerate and facilitate diffusion algorithm design and evaluation, and open-source FlashEval as a tool for condensing future datasets.",
      "url": "https://www.microsoft.com/en-us/research/publication/flasheval-towards-fast-and-accurate-evaluation-of-text-to-image-diffusion-generative-models/"
    },
    {
      "title": "“I’m Not Sure, But…”: Examining the Impact of Large Language Models’ Uncertainty Expression on User Reliance and Trust",
      "authors": [
        "Jennifer Wortman Vaughan",
        "Mihaela Vorvoreanu",
        "Q. Vera Liao",
        "Stephanie Ballard",
        "Sunnie S. Y. Kim"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "June 2024",
      "abstract": "Widely deployed large language models (LLMs) can produce convincing yet incorrect outputs, potentially misleading users who may rely on them as if they were correct. To reduce such overreliance, there have been calls for LLMs to communicate their uncertainty to end users. However, there has been little empirical work examining how users perceive and act upon LLMs’ expressions of uncertainty. We explore this question through a large-scale, pre-registered, human-subject experiment (N=404) in which participants answer medical questions with or without access to responses from a fictional LLM-infused search engine. Using both behavioral and self-reported measures, we examine how different natural language expressions of uncertainty impact participants’ reliance, trust, and overall task performance. We find that first-person expressions (e.g., “I’m not sure, but…”) decrease participants’ confidence in the system and tendency to agree with the system’s answers, while increasing participants’ accuracy. An exploratory analysis suggests that this increase can be attributed to reduced (but not fully eliminated) overreliance on incorrect answers. While we observe similar effects for uncertainty expressed from a general perspective (e.g., “It’s not clear, but…”), these effects are weaker and not statistically significant. Our findings suggest that using natural language expressions of uncertainty may be an effective approach for reducing overreliance on LLMs, but that the precise language used matters. This highlights the importance of user testing before deploying LLMs at scale.",
      "url": "https://www.microsoft.com/en-us/research/publication/im-not-sure-but-examining-the-impact-of-large-language-models-uncertainty-expression-on-user-reliance-and-trust/"
    },
    {
      "title": "Is Vanilla MLP in Neural Radiance Field Enough for Few-shot View Synthesis?",
      "authors": [
        "Bingchen Li",
        "Hanxin Zhu",
        "Tianyu He",
        "Xin Li",
        "Zhibo Chen"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Computer vision"
      ],
      "publication_date": "June 2024",
      "abstract": "Neural Radiance Field (NeRF) has achieved superior performance for novel view synthesis by modeling the scene with a Multi-Layer Perception (MLP) and a volume rendering procedure however when fewer known views are given (i.e. few-shot view synthesis) the model is prone to overfit the given views. To handle this issue previous efforts have been made towards leveraging learned priors or introducing additional regularizations. In contrast in this paper we for the first time provide an orthogonal method from the perspective of network structure. Given the observation that trivially reducing the number of model parameters alleviates the overfitting issue but at the cost of missing details we propose the multi-input MLP (mi-MLP) that incorporates the inputs (i.e. location and viewing direction) of the vanilla MLP into each layer to prevent the overfitting issue without harming detailed synthesis. To further reduce the artifacts we propose to model colors and volume density separately and present two regularization terms. Extensive experiments on multiple datasets demonstrate that: 1) although the proposed mi-MLP is easy to implement it is surprisingly effective as it boosts the PSNR of the baseline from 14.73 to 24.23. 2) the overall framework achieves state-of-the-art results on a wide range of benchmarks. We will release the code upon publication.",
      "url": "https://www.microsoft.com/en-us/research/publication/is-vanilla-mlp-in-neural-radiance-field-enough-for-few-shot-view-synthesis/"
    },
    {
      "title": "Position: Rethinking Post-Hoc Search-Based Neural Approaches for Solving Large-Scale Traveling Salesman Problems",
      "authors": [
        "Jiang Bian",
        "Lei Song",
        "Xianliang Yang",
        "Yifan Xia",
        "Zhihao Liu",
        "Zichuan Liu"
      ],
      "research_areas": [
        "Algorithms"
      ],
      "publication_date": "June 2024",
      "abstract": "Recent advancements in solving large-scale traveling salesman problems (TSP) utilize the heatmap-guided Monte Carlo tree search (MCTS) paradigm, where machine learning (ML) models generate heatmaps, indicating the probability distribution of each edge being part of the optimal solution, to guide MCTS in solution finding. However, our theoretical and experimental analysis raises doubts about the effectiveness of ML-based heatmap generation. In support of this, we demonstrate that a simple baseline method can outperform complex ML approaches in heatmap generation. Furthermore, we question the practical value of the heatmap-guided MCTS paradigm. To substantiate this, our findings show its inferiority to the LKH-3 heuristic despite the paradigm’s reliance on problem-specific, hand-crafted strategies. For the future, we suggest research directions focused on developing more theoretically sound heatmap generation methods and exploring autonomous, generalizable ML approaches for combinatorial problems. The code is available for review: https://github.com/xyfffff/rethink_mcts_for_tsp.",
      "url": "https://www.microsoft.com/en-us/research/publication/position-rethinking-post-hoc-search-based-neural-approaches-for-solving-large-scale-traveling-salesman-problems/"
    },
    {
      "title": "WindSeer: Real-time volumetric wind prediction over complex terrain aboard a small uncrewed aerial vehicle",
      "authors": [
        "Andrey Kolobov",
        "Bogdan Danciu",
        "Florian Achermann",
        "Jen Jen Chung",
        "Nicholas Lawrance",
        "Roland Siegwart",
        "Thomas Stastny"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "May 2024",
      "abstract": "Real-time high-resolution wind predictions are beneficial for various applications including safe crewed and uncrewed aviation. Current weather models require too much compute and lack the necessary predictive capabilities as they are valid only at the scale of multiple kilometers and hours — much lower spatial and temporal resolutions than these applications require. Our work demonstrates the ability to predict low-altitude time-averaged wind fields in real time on limited-compute devices, from only sparse measurement data. We train a deep neural network-based model, WindSeer, using only synthetic data from computational fluid dynamics simulations and show that it can successfully predict real wind fields over terrain with known topography from just a few noisy and spatially clustered wind measurements. WindSeer can generate accurate predictions at different resolutions and domain sizes on previously unseen topography without retraining. We demonstrate that the model successfully predicts historical wind data collected by weather stations and wind measured by drones during flight.",
      "url": "https://www.microsoft.com/en-us/research/publication/windseer-real-time-volumetric-wind-prediction-over-complex-terrain-aboard-a-small-uav/"
    },
    {
      "title": "Optimizing Distributed Protocols with Query Rewrites",
      "authors": [
        "Chris Liu",
        "David Chu",
        "Heidi Howard",
        "Joe Hellerstein",
        "Kaushik Shivakumar",
        "Lucky Katahanas",
        "Natacha Crooks",
        "Rithvik Panchapakesan",
        "Shadaj Laddad"
      ],
      "research_areas": [
        "Programming languages and software engineering",
        "Systems and networking"
      ],
      "publication_date": "June 2024",
      "abstract": "Distributed protocols such as 2PC and Paxos lie at the core of many systems in the cloud, but standard implementations do not scale. New scalable distributed protocols are developed through careful analysis and rewrites, but this process is ad hoc and error-prone. This paper presents an approach for scaling any distributed protocol by applying rule-driven rewrites, borrowing from query optimization. Distributed protocol rewrites entail a new burden: reasoning about spatiotemporal correctness. We leverage order-insensitivity and data dependency analysis to systematically identify correct coordination-free scaling opportunities. We apply this analysis to create preconditions and mechanisms for coordination-free decoupling and partitioning, two fundamental vertical and horizontal scaling techniques. Manual rule-driven applications of decoupling and partitioning improve the throughput of 2PC by 5× and Paxos by 3×, and match state-of-the-art throughput in recent work. These results point the way toward automated optimizers for distributed protocols based on correct-by-construction rewrite rules.",
      "url": "https://www.microsoft.com/en-us/research/publication/optimizing-distributed-protocols-with-query-rewrites/"
    },
    {
      "title": "Overview of the MEDIQA-M3G 2024 Shared Task on Multilingual Multimodal Medical Answer Generation",
      "authors": [
        "Asma Ben Abacha",
        "Fei Xia",
        "Martin Krallinger",
        "Meliha Yetisgen",
        "Wen-wai Yim",
        "Yujuan Fu",
        "Zhaoyi Sun"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Computer vision",
        "Medical, health and genomics"
      ],
      "publication_date": "June 2024",
      "abstract": "Remote patient care provides opportunities for expanding medical access, saving healthcare costs, and offering on-demand convenient services. In the MEDIQA-M3G 2024 Shared Task, researchers explored solutions for the specific task of dermatological consumer health visual question answering, where user generated queries and images are used as input and a free-text answer response is generated as output. In this novel challenge, eight teams with a total of 48 submissions were evaluated across three language test sets. In this work, we provide a summary of the dataset, as well as results and approaches. We hope that the insights learned here will inspire future research directions that can lead to technology that deburdens clinical workload and improves care.",
      "url": "https://www.microsoft.com/en-us/research/publication/overview-of-the-mediqa-m3g-2024-shared-task-on-multilingual-multimodal-medical-answer-generation/"
    },
    {
      "title": "The Perspectivist Paradigm Shift: Assumptions and Challenges of Capturing Human Labels",
      "authors": [
        "Dan Klein",
        "Eve Fleisig",
        "Su Lin Blodgett",
        "Zeerak Talat"
      ],
      "research_areas": [
        "Human language technologies"
      ],
      "publication_date": "June 2024",
      "abstract": "Longstanding data labeling practices in machine learning involve collecting and aggregating labels from multiple annotators. But what should we do when annotators disagree? Though annotator disagreement has long been seen as a problem to minimize, new perspectivist approaches challenge this assumption by treating disagreement as a valuable source of information. In this position paper, we examine practices and assumptions surrounding the causes of disagreement–some challenged by perspectivist approaches, and some that remain to be addressed–as well as practical and normative challenges for work operating under these assumptions. We conclude with recommendations for the data labeling pipeline and avenues for future research engaging with subjectivity and disagreement.",
      "url": "https://www.microsoft.com/en-us/research/publication/the-perspectivist-paradigm-shift-assumptions-and-challenges-of-capturing-human-labels/"
    },
    {
      "title": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
      "authors": [
        "Chaofan Lin",
        "Chen Chen",
        "Chengruidong Zhang",
        "Fan Yang",
        "Lili Qiu",
        "Yuqing Yang",
        "Zhenhua Han"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "May 2024",
      "abstract": "The rise of large language models (LLMs) has enabled LLM-based applications (a.k.a. AI agents or co-pilots), a new software paradigm that combines the strength of LLM and conventional software. Diverse LLM applications from different tenants could design complex workflows using multiple LLM requests to accomplish one task. However, they have to use the over-simplified request-level API provided by today’s public LLM services, losing essential application-level information. Public LLM services have to blindly optimize individual LLM requests, leading to sub-optimal end-to-end performance of LLM applications.\nThis paper introduces Parrot, an LLM service system that focuses on the end-to-end experience of LLM-based applications. Parrot proposes Semantic Variable, a unified abstraction to expose application-level knowledge to public LLM services. A Semantic Variable annotates an input/output variable in the prompt of a request, and creates the data pipeline when connecting multiple LLM requests, providing a natural way to program LLM applications. Exposing Semantic Variables to the public LLM service allows it to perform conventional data flow analysis to uncover the correlation across multiple LLM requests. This correlation opens a brand-new optimization space for the end-to-end performance of LLM-based applications. Extensive evaluations demonstrate that Parrot can achieve up to an order-of-magnitude improvement for popular and practical use cases of LLM applications.",
      "url": "https://www.microsoft.com/en-us/research/publication/parrot-efficient-serving-of-llm-based-applications-with-semantic-variable/"
    },
    {
      "title": "Provably Efficient Interactive-Grounded Learning with Personalized Reward",
      "authors": [
        "Haipeng Luo",
        "Mengxiao Zhang",
        "Paul Mineiro",
        "Yuheng Zhang"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "May 2024",
      "abstract": "Interactive-Grounded Learning (IGL) [Xie et al., 2021] is a powerful framework in which a learner aims at maximizing unobservable rewards through interacting with an environment and observing reward-dependent feedback on the taken actions. To deal with personalized rewards that are ubiquitous in applications such as recommendation systems, Maghakian et al. [2022] study a version of IGL with context-dependent feedback, but their algorithm does not come with theoretical guarantees. In this work, we consider the same problem and provide the first provably efficient algorithms with sublinear regret under realizability. Our analysis reveals that the step-function estimator of prior work can deviate uncontrollably due to finite-sample effects. Our solution is a novel Lipschitz reward estimator which underestimates the true reward and enjoys favorable generalization performances. Building on this estimator, we propose two algorithms, one based on explore-then-exploit and the other based on inverse-gap weighting. We apply IGL to learning from image feedback and learning from text feedback, which are reward-free settings that arise in practice. Experimental results showcase the importance of using our Lipschitz reward estimator and the overall effectiveness of our algorithms.",
      "url": "https://www.microsoft.com/en-us/research/publication/provably-efficient-interactive-grounded-learning-with-personalized-reward/"
    },
    {
      "title": "Instruction-Guided Visual Masking",
      "authors": [
        "Jiaming Li",
        "Jianxiong Li",
        "Jihao Liu",
        "Jingjing Liu",
        "Jinliang Zheng",
        "Si Cheng",
        "Xianyuan Zhan",
        "Yinan Zheng",
        "Yu Liu"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "May 2024",
      "abstract": "Instruction following is crucial in contemporary LLM. However, when extended to multimodal setting, it often suffers from misalignment between specific textual instruction and targeted local region of an image. To achieve more accurate and nuanced multimodal instruction following, we introduce Instruction-guided Visual Masking (IVM), a new versatile visual grounding model that is compatible with diverse multimodal models, such as LMM and robot model. By constructing visual masks for instruction-irrelevant regions, IVM-enhanced multimodal models can effectively focus on task-relevant image regions to better align with complex instructions. Specifically, we design a visual masking data generation pipeline and create an IVM-Mix-1M dataset with 1 million image-instruction pairs. We further introduce a new learning technique, Discriminator Weighted Supervised Learning (DWSL) for preferential IVM training that prioritizes high-quality data samples. Experimental results on generic multimodal tasks such as VQA and embodied robotic control demonstrate the versatility of IVM, which as a plug-and-play tool, significantly boosts the performance of diverse multimodal models, yielding new state-of-the-art results across challenging multimodal benchmarks. Code is available at https://github.com/2toinf/IVM.",
      "url": "https://www.microsoft.com/en-us/research/publication/instruction-guided-visual-masking/"
    },
    {
      "title": "Rich-Observation Reinforcement Learning with Continuous Latent Dynamics",
      "authors": [
        "Akshay Krishnamurthy",
        "Dylan J. Foster",
        "Lili Wu",
        "Yuda Song"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "May 2024",
      "abstract": "Sample-efficiency and reliability remain major bottlenecks toward wide adoption of reinforcement learning algorithms in continuous settings with high-dimensional perceptual inputs. Toward addressing these challenges, we introduce a new theoretical framework, RichCLD (Rich-Observation RL with Continuous Latent Dynamics), in which the agent performs control based on high-dimensional observations, but the environment is governed by low-dimensional latent states and Lipschitz continuous dynamics. Our main contribution is a new algorithm for this setting that is provably statistically and computationally efficient. The core of our algorithm is a new representation learning objective; we show that prior representation learning schemes tailored to discrete dynamics do not naturally extend to the continuous setting. Our new objective is amenable to practical implementation, and empirically, we find that it compares favorably to prior schemes in a standard evaluation protocol. We further provide several insights into the statistical complexity of the RichCLD framework, in particular proving that certain notions of Lipschitzness that admit sample-efficient learning in the absence of rich observations are insufficient in the rich-observation setting.",
      "url": "https://www.microsoft.com/en-us/research/publication/rich-observation-reinforcement-learning-with-continuous-latent-dynamics/"
    },
    {
      "title": "Divide-and-Conquer Meets Consensus: Unleashing the Power of Functions in Code Generation",
      "authors": [
        "Bing Qin",
        "Hongxuan Tang",
        "Jingchang Chen",
        "Ming Liu",
        "Qianglong Chen",
        "Zekun Wang",
        "Zheng Chu"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "May 2024",
      "abstract": "Despite recent progress made by large language models in code generation, they still struggle with programs that meet complex requirements. Recent work utilizes plan-and-solve decomposition to decrease the complexity and leverage self-tests to refine the generated program. Yet, planning deep-inside requirements in advance can be challenging, and the tests need to be accurate to accomplish self-improvement. To this end, we propose FunCoder, a code generation framework incorporating the divide-and-conquer strategy with functional consensus. Specifically, FunCoder recursively branches off sub-functions as smaller goals during code generation, represented by a tree hierarchy. These sub-functions are then composited to attain more complex objectives. Additionally, we designate functions via a consensus formed by identifying similarities in program behavior, mitigating error propagation. FunCoder outperforms state-of-the-art methods by +9.8% on average in HumanEval, MBPP, xCodeEval and MATH with GPT-3.5 and GPT-4. Moreover, our method demonstrates superiority on smaller models: With FunCoder, StableCode-3b surpasses GPT-3.5 by +18.6% and achieves 97.7% of GPT-4’s performance on HumanEval. Further analysis reveals that our proposed dynamic function decomposition is capable of handling complex requirements, and the functional consensus prevails over self-testing in correctness evaluation.",
      "url": "https://www.microsoft.com/en-us/research/publication/divide-and-conquer-meets-consensus-unleashing-the-power-of-functions-in-code-generation/"
    },
    {
      "title": "Autodroid: LLM-Powered Task Automation in Android",
      "authors": [
        "Guohong Liu",
        "Hao Wen",
        "Shanhui Zhao",
        "Shiqi Jiang",
        "Tao Yu",
        "Toby Jia-Jun Li",
        "Yaqin Zhang",
        "Yuanchun Li",
        "Yunhao Liu",
        "Yunxin Liu"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Systems and networking"
      ],
      "publication_date": "May 2024",
      "abstract": "Mobile task automation is an attractive technique that aims to enable voice-based hands-free user interaction with smartphones. However, existing approaches suffer from poor scalability due to the limited language understanding ability and the non-trivial manual efforts required from developers or endusers. The recent advance of large language models (LLMs) in language understanding and reasoning inspires us to rethink the problem from a model-centric perspective, where task preparation, comprehension, and execution are handled by a unified language model. In this work, we introduce AutoDroid, a mobile task automation system capable of handling arbitrary tasks on any Android application without manual efforts. The key insight is to combine the commonsense knowledge of LLMs and domain-specific knowledge of apps through automated dynamic analysis. The main components include a functionality-aware UI representation method that bridges the UI with the LLM, exploration-based memory injection techniques that augment the app-specific domain knowledge of LLM, and a multi-granularity query optimization module that reduces the cost of model inference. We integrate AutoDroid with off-the-shelf LLMs including online GPT-4/GPT-3.5 and on-device Vicuna, and evaluate its performance on a new benchmark for memory-augmented Android task automation with 158 common tasks. The results demonstrated that AutoDroid is able to precisely generate actions with an accuracy of 90.9%, and complete tasks with a success rate of 71.3%, outperforming the GPT-4-powered baselines by 36.4% and 39.7%.",
      "url": "https://www.microsoft.com/en-us/research/publication/autodroid-llm-powered-task-automation-in-android/"
    },
    {
      "title": "3D Neural Edge Reconstruction",
      "authors": [
        "Lei Li",
        "Marc Pollefeys",
        "Remi Pautrat",
        "Shaohui Liu",
        "Songyou Peng",
        "Xiaochuan Yin",
        "Zehao Yu"
      ],
      "research_areas": [
        "Computer vision"
      ],
      "publication_date": "May 2024",
      "abstract": "Real-world objects and environments are predominantly composed of edge features, including straight lines and curves. Such edges are crucial elements for various applications, such as CAD modeling, surface meshing, lane mapping, etc. However, existing traditional methods only prioritize lines over curves for simplicity in geometric modeling. To this end, we introduce EMAP, a new method for learning 3D edge representations with a focus on both lines and curves. Our method implicitly encodes 3D edge distance and direction in Unsigned Distance Functions (UDF) from multi-view edge maps. On top of this neural representation, we propose an edge extraction algorithm that robustly abstracts parametric 3D edges from the inferred edge points and their directions. Comprehensive evaluations demonstrate that our method achieves better 3D edge reconstruction on multiple challenging datasets. We further show that our learned UDF field enhances neural surface reconstruction by capturing more details.",
      "url": "https://www.microsoft.com/en-us/research/publication/3d-neural-edge-reconstruction-2/"
    },
    {
      "title": "Think Before You Act: Decision Transformers with Internal Working Memory",
      "authors": [
        "Adam Trischler",
        "Jie Fu",
        "Jikun Kang",
        "Romain Laroche",
        "Xingdi Yuan",
        "Xuefei Liu"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "May 2024",
      "abstract": "Large language model (LLM)-based decision-making agents have shown the ability to generalize across multiple tasks. However, their performance relies on massive data and compute. We argue that this inefficiency stems from the forgetting phenomenon, in which a model memorizes its behaviors in parameters throughout training. As a result, training on a new task may deteriorate the model’s performance on previous tasks. In contrast to LLMs’ implicit memory mechanism, the human brain utilizes distributed memory storage, which helps manage and organize multiple skills efficiently, mitigating the forgetting phenomenon. Thus inspired, we propose an internal working memory module to store, blend, and retrieve information for different downstream tasks. Evaluation results show that the proposed method improves training efficiency and generalization in both Atari games and meta-world object manipulation tasks. Moreover, we demonstrate that memory fine-tuning further enhances the adaptability of the proposed architecture.",
      "url": "https://www.microsoft.com/en-us/research/publication/think-before-you-act-decision-transformers-with-internal-working-memory/"
    },
    {
      "title": "A Study of Plasticity Loss in On-Policy Deep Reinforcement Learning",
      "authors": [
        "Arthur Juliani",
        "Jordan Ash"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "May 2024",
      "abstract": "Continual learning with deep neural networks presents challenges distinct from both the fixed-dataset and convex continual learning regimes. One such challenge is plasticity loss, wherein a neural network trained in an online fashion displays a degraded ability to fit new tasks. This problem has been extensively studied in both supervised learning and off-policy reinforcement learning (RL), where a number of remedies have been proposed. Still, plasticity loss has received less attention in the on-policy deep RL setting. Here we perform an extensive set of experiments examining plasticity loss and a variety of mitigation methods in on-policy deep RL. We demonstrate that plasticity loss is pervasive under domain shift in this regime, and that a number of methods developed to resolve it in other settings fail, sometimes even resulting in performance that is worse than performing no intervention at all. In contrast, we find that a class of “regenerative” methods are able to consistently mitigate plasticity loss in a variety of contexts, including in gridworld tasks and more challenging environments like Montezuma’s Revenge and ProcGen.",
      "url": "https://www.microsoft.com/en-us/research/publication/a-study-of-plasticity-loss-in-on-policy-deep-reinforcement-learning/"
    },
    {
      "title": "Participation in the age of foundation models",
      "authors": [
        "Emily Tseng",
        "Emma Pierson",
        "Harini Suresh",
        "Karen Levy",
        "Mary L. Gray",
        "Meg Young"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Human-computer interaction",
        "Social sciences"
      ],
      "publication_date": "May 2024",
      "abstract": "Growing interest and investment in the capabilities of foundation models has positioned such systems to impact a wide array of public services. Alongside these opportunities is the risk that these systems reify existing power imbalances and cause disproportionate harm to marginalized communities. Participatory approaches hold promise to instead lend agency and decision-making power to marginalized stakeholders. But existing approaches in participatory AI/ML are typically deeply grounded in context – how do we apply these approaches to foundation models, which are, by design, disconnected from context? Our paper interrogates this question.\nFirst, we examine existing attempts at incorporating participation into foundation models. We highlight the tension between participation and scale, demonstrating that it is intractable for impacted communities to meaningfully shape a foundation model that is intended to be universally applicable. In response, we develop a blueprint for participatory foundation models that identifies more local, application-oriented opportunities for meaningful participation. In addition to the”foundation”layer, our framework proposes the”subfloor” layer, in which stakeholders develop shared technical infrastructure, norms and governance for a grounded domain, and the”surface” layer, in which affected communities shape the use of a foundation model for a specific downstream task. The intermediate”subfloor” layer scopes the range of potential harms to consider, and affords communities more concrete avenues for deliberation and intervention. At the same time, it avoids duplicative effort by scaling input across relevant use cases. Through three case studies in clinical care, financial services, and journalism, we illustrate how this multi-layer model can create more meaningful opportunities for participation than solely intervening at the foundation layer.",
      "url": "https://www.microsoft.com/en-us/research/publication/participation-in-the-age-of-foundation-models/"
    },
    {
      "title": "PromptWizard: Task-Aware Agent-driven Prompt Optimization Framework",
      "authors": [
        "Akshay Nambi",
        "Eshaan Agarwal",
        "Joykirat Singh",
        "Raghav Magazine",
        "Tanuja Ganu",
        "Vivek Dani"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "May 2024",
      "abstract": "Large language models (LLMs) have transformed AI across diverse domains, with prompting being central to their success in guiding model outputs. However, manual prompt engineering is both labor-intensive and domain-specific, necessitating the need for automated solutions. We introduce PromptWizard, a novel, fully automated framework for discrete prompt optimization, utilizing a self-evolving, self-adapting mechanism. Through a feedback-driven critique and synthesis process, PromptWizard achieves an effective balance between exploration and exploitation, iteratively refining both prompt instructions and in-context examples to generate human-readable, task-specific prompts. This guided approach systematically improves prompt quality, resulting in superior performance across 45 tasks. PromptWizard excels even with limited training data, smaller LLMs, and various LLM architectures. Additionally, our cost analysis reveals a substantial reduction in API calls, token usage, and overall cost, demonstrating PromptWizard’s efficiency, scalability, and advantages over existing prompt optimization strategies.",
      "url": "https://www.microsoft.com/en-us/research/publication/promptwizard-task-aware-agent-driven-prompt-optimization-framework/"
    },
    {
      "title": "An Outlook into the Future of Egocentric Vision",
      "authors": [
        "Antonino Furnari",
        "Chiara Plizzari",
        "D. Damen",
        "Francesco Ragusa",
        "G. Farinella",
        "Gabriele Goletto",
        "Siddhant Bansal",
        "T. Tommasi"
      ],
      "research_areas": [
        "Computer vision"
      ],
      "publication_date": "May 2024",
      "abstract": "What will the future be? We wonder! In this survey, we explore the gap between current research in egocentric vision and the ever-anticipated future, where wearable computing, with outward facing cameras and digital overlays, is expected to be integrated in our every day lives. To understand this gap, the article starts by envisaging the future through character-based stories, showcasing through examples the limitations of current technology. We then provide a mapping between this future and previously defined research tasks. For each task, we survey its seminal works, current state-of-the-art methodologies and available datasets, then reflect on shortcomings that limit its applicability to future research. Note that this survey focuses on software models for egocentric vision, independent of any specific hardware. The paper concludes with recommendations for areas of immediate explorations so as to unlock our path to the future always-on, personalised and life-enhancing egocentric vision.",
      "url": "https://www.microsoft.com/en-us/research/publication/an-outlook-into-the-future-of-egocentric-vision/"
    },
    {
      "title": "InversionView: A General-Purpose Method for Reading Information from Neural Activations",
      "authors": [
        "Madhur Panwar",
        "Michael Hahn",
        "Navin Goyal",
        "Xinting Huang"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "May 2024",
      "abstract": "The inner workings of neural networks can be better understood if we can fully decipher the information encoded in neural activations. In this paper, we argue that this information is embodied by the subset of inputs that give rise to similar activations. Computing such subsets is nontrivial as the input space is exponentially large. We propose InversionView, which allows us to practically inspect this subset by sampling from a trained decoder model conditioned on activations. This helps uncover the information content of activation vectors, and facilitates understanding of the algorithms implemented by transformer models. We present four case studies where we investigate models ranging from small transformers to GPT-2. In these studies, we demonstrate the characteristics of our method, show the distinctive advantages it offers, and provide causally verified circuits.",
      "url": "https://www.microsoft.com/en-us/research/publication/inversionview-a-general-purpose-method-for-reading-information-from-neural-activations/"
    },
    {
      "title": "TransVIP: Speech to Speech Translation System with Voice and Isochrony Preservation",
      "authors": [
        "Chenyang Le",
        "Dongmei Wang",
        "Jinyu Li",
        "Long Zhou",
        "Michael Zeng",
        "Midia Yousefi",
        "Sheng Zhao",
        "Shujie Liu",
        "Xiaofei Wang",
        "Yanmin Qian",
        "Yao Qian"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Audio and Acoustics"
      ],
      "publication_date": "May 2024",
      "abstract": "There is a rising interest and trend in research towards directly translating speech from one language to another, known as end-to-end speech-to-speech translation. However, most end-to-end models struggle to outperform cascade models, i.e., a pipeline framework by concatenating speech recognition, machine translation and text-to-speech models. The primary challenges stem from the inherent complexities involved in direct translation tasks and the scarcity of data. In this study, we introduce a novel model framework TransVIP that leverages diverse datasets in a cascade fashion yet facilitates end-to-end inference through joint probability. Furthermore, we propose two separated encoders to preserve the speaker’s voice characteristics and isochrony from the source speech during the translation process, making it highly suitable for scenarios such as video dubbing. Our experiments on the French-English language pair demonstrate that our model outperforms the current state-of-the-art speech-to-speech translation model.",
      "url": "https://www.microsoft.com/en-us/research/publication/transvip-speech-to-speech-translation-system-with-voice-and-isochrony-preservation/"
    },
    {
      "title": "PromptFix: You Prompt and We Fix the Photo",
      "authors": [
        "Hang Hua",
        "Jianlong Fu",
        "Jiebo Luo",
        "Yongsheng Yu",
        "Ziyun Zeng"
      ],
      "research_areas": [
        "Computer vision"
      ],
      "publication_date": "May 2024",
      "abstract": "Diffusion models equipped with language models demonstrate excellent controllability in image generation tasks, allowing image processing to adhere to human instructions. However, the lack of diverse instruction-following data hampers the development of models that effectively recognize and execute user-customized instructions, particularly in low-level tasks. Moreover, the stochastic nature of the diffusion process leads to deficiencies in image generation or editing tasks that require the detailed preservation of the generated images. To address these limitations, we propose PromptFix, a comprehensive framework that enables diffusion models to follow human instructions to perform a wide variety of image-processing tasks. First, we construct a large-scale instruction-following dataset that covers comprehensive image-processing tasks, including low-level tasks, image editing, and object creation. Next, we propose a high-frequency guidance sampling method to explicitly control the denoising process and preserve high-frequency details in unprocessed areas. Finally, we design an auxiliary prompting adapter, utilizing Vision-Language Models (VLMs) to enhance text prompts and improve the model’s task generalization. Experimental results show that PromptFix outperforms previous methods in various image-processing tasks. Our proposed model also achieves comparable inference efficiency with these baseline models and exhibits superior zero-shot capabilities in blind restoration and combination tasks. The dataset and code are available at https://www.yongshengyu.com/PromptFix-Page.",
      "url": "https://www.microsoft.com/en-us/research/publication/promptfix-you-prompt-and-we-fix-the-photo/"
    },
    {
      "title": "Artificial intelligence and radiomics in the diagnosis of intraosseous lesions of the gnathic bones: A systematic review.",
      "authors": [
        "A. D. de Carvalho",
        "A. L. D. Araújo",
        "A. Santos-Silva",
        "C. Saldivia-Siracusa",
        "Daniela Giraldo-Roldán",
        "Erin Crespo Cordeiro Ribeiro",
        "L. P. Kowalski",
        "M. A. Lopes",
        "Maria Eduarda Pérez-de-Oliveira",
        "Matheus Cardoso Moraes",
        "Matheus Cerqueira",
        "Pablo Agustin Vargas",
        "S. S. Sousa-Neto",
        "Viviane Mariano da Silva"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Medical, health and genomics"
      ],
      "publication_date": "May 2024",
      "abstract": "BACKGROUND The purpose of this systematic review (SR) is to gather evidence on the use of machine learning (ML) models in the diagnosis of intraosseous lesions in gnathic bones and to analyze the reliability, impact, and usefulness of such models. This SR was performed in accordance with the PRISMA 2022 guidelines and was registered in the PROSPERO database (CRD42022379298). METHODS The acronym PICOS was used to structure the inquiry-focused review question “Is Artificial Intelligence reliable for the diagnosis of intraosseous lesions in gnathic bones?” The literature search was conducted in various electronic databases, including PubMed, Embase, Scopus, Cochrane Library, Web of Science, Lilacs, IEEE Xplore, and Gray Literature (Google Scholar and ProQuest). Risk of bias assessment was performed using PROBAST, and the results were synthesized by considering the task and sampling strategy of the dataset. RESULTS Twenty-six studies were included (21 146 radiographic images). Ameloblastomas, odontogenic keratocysts, dentigerous cysts, and periapical cysts were the most frequently investigated lesions. According to TRIPOD, most studies were classified as type 2 (randomly divided). The F1 score was presented in only 13 studies, which provided the metrics for 20 trials, with a mean of 0.71 (±0.25). CONCLUSION There is no conclusive evidence to support the usefulness of ML-based models in the detection, segmentation, and classification of intraosseous lesions in gnathic bones for routine clinical application. The lack of detail about data sampling, the lack of a comprehensive set of metrics for training and validation, and the absence of external testing limit experiments and hinder proper evaluation of model performance.",
      "url": "https://www.microsoft.com/en-us/research/publication/artificial-intelligence-and-radiomics-in-the-diagnosis-of-intraosseous-lesions-of-the-gnathic-bones-a-systematic-review/"
    },
    {
      "title": "Bridging the Gap: Dynamic Learning Strategies for Improving Multilingual Performance in LLMs",
      "authors": [
        "Akshay Nambi",
        "Kabir Ahuja",
        "Kalika Bali",
        "Mercy Ranjit",
        "Somnath Kumar",
        "Sunayana Sitaram",
        "Tanuja Ganu",
        "Vaibhav Balloli"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "May 2024",
      "abstract": "Large language models (LLMs) are at the forefront of transforming numerous domains globally. However, their inclusivity and effectiveness remain limited for non-Latin scripts and low-resource languages. This paper tackles the imperative challenge of enhancing the multilingual performance of LLMs without extensive training or fine-tuning. Through systematic investigation and evaluation of diverse languages using popular question-answering (QA) datasets, we present novel techniques that unlock the true potential of LLMs in a polyglot landscape. Our approach encompasses three key strategies that yield significant improvements in multilingual proficiency. First, by meticulously optimizing prompts tailored for polyglot LLMs, we unlock their latent capabilities, resulting in substantial performance boosts across languages. Second, we introduce a new hybrid approach that synergizes LLM Retrieval Augmented Generation (RAG) with multilingual embeddings and achieves improved multilingual task performance. Finally, we introduce a novel learning approach that dynamically selects the optimal prompt strategy, LLM model, and embedding model per query at run-time. This dynamic adaptation maximizes the efficacy of LLMs across languages, outperforming best static and random strategies. Additionally, our approach adapts configurations in both offline and online settings, and can seamlessly adapt to new languages and datasets, leading to substantial advancements in multilingual understanding and generation across diverse languages.",
      "url": "https://www.microsoft.com/en-us/research/publication/bridging-the-gap-dynamic-learning-strategies-for-improving-multilingual-performance-in-llms/"
    },
    {
      "title": "Blind Image Restoration via Fast Diffusion Inversion",
      "authors": [
        "Abdelhak Lemkhenter",
        "Hamadi Chihaoui",
        "Paolo Favaro"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "May 2024",
      "abstract": "Recently, various methods have been proposed to solve Image Restoration (IR) tasks using a pre-trained diffusion model leading to state-of-the-art performance. However, most of these methods assume that the degradation operator in the IR task is completely known. Furthermore, a common characteristic among these approaches is that they alter the diffusion sampling process in order to satisfy the consistency with the degraded input image. This choice has recently been shown to be sub-optimal and to cause the restored image to deviate from the data manifold. To address these issues, we propose Blind Image Restoration via fast Diffusion inversion (BIRD) a blind IR method that jointly optimizes for the degradation model parameters and the restored image. To ensure that the restored images lie onto the data manifold, we propose a novel sampling technique on a pre-trained diffusion model. A key idea in our method is not to modify the reverse sampling, i.e., not to alter all the intermediate latents, once an initial noise is sampled. This is ultimately equivalent to casting the IR task as an optimization problem in the space of the input noise. Moreover, to mitigate the computational cost associated with inverting a fully unrolled diffusion model, we leverage the inherent capability of these models to skip ahead in the forward diffusion process using large time steps. We experimentally validate BIRD on several image restoration tasks and show that it achieves state of the art performance on all of them. Our code is available at https://github.com/hamadichihaoui/BIRD.",
      "url": "https://www.microsoft.com/en-us/research/publication/blind-image-restoration-via-fast-diffusion-inversion/"
    },
    {
      "title": "Lorentz: Learned SKU Recommendation Using Profile Data",
      "authors": [
        "Helen Serr",
        "Matthew Gleeson",
        "Nick Glaze",
        "Rajeev Bhopi",
        "Subru Krishnan",
        "Tria McNeely",
        "Yiwen Zhu"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Data platforms and analytics"
      ],
      "publication_date": "May 2024",
      "abstract": "In response to diverse demands, cloud operators have significantly expanded the array of service offerings, often referred to as Stock Keeping Units (SKUs) available for computing resource configurations. Such diversity has led to increased complexity for customers to choose the appropriate SKU. In the analyzed system, only 43% of the resource capacity was rightly chosen. Although various automated solutions have attempted to resolve this issue, they often rely on the availability of enriched data, such as workload traces, which are unavailable for newly established services. Since these services amass a substantial volume of telemetry from existing users, cloud operators can leverage this information to better understand customer needs and mitigate the risk of over- or under-provisioning. Furthermore, customer satisfaction feedback serves as a crucial resource for continuous learning and improving the recommendation mechanism. In this paper, we present Lorentz, an intelligent SKU recommender for provisioning new compute resources that circumvents the need for workload traces. Lorentz leverages customer profile data to forecast resource capacities for new users based on detailed profiling of existing users. Furthermore, using a continuous learned feedback loop, Lorentz tailors capacity recommendations according to customer performance vs. cost preferences captured through satisfaction signals. Validated using the production data from provisioned VMs supporting Database Platform X, we demonstrate that Lorentz outperforms user selections and existing defaults, reducing slack by >60% without increasing throttling. Evaluated using synthetic data, Lorentz’s personalization stage iteratively learns the user preferences over time with high accuracy.",
      "url": "https://www.microsoft.com/en-us/research/publication/lorentz-learned-sku-recommendation-using-profile-data/"
    },
    {
      "title": "Wavefront Threading Enables Effective High-Level Synthesis",
      "authors": [
        "Adam Sapek",
        "Adrian Caulfield",
        "Alessandro Forin",
        "Blake Pelton",
        "Daniel Lo",
        "David Cox",
        "Doug Burger",
        "Evgeny Babin",
        "Jinwen Xi",
        "Johannes de Fine Licht",
        "Ken Eguro",
        "Matt Humphrey",
        "Rajas H. Karandikar"
      ],
      "research_areas": [
        "Hardware and devices",
        "Programming languages and software engineering"
      ],
      "publication_date": "May 2024",
      "abstract": "Digital systems are growing in importance and computing hardware is growing more heterogeneous. Hardware design, however, remains laborious and expensive, in part due to the limitations of conventional hardware description languages (HDLs) like VHDL and Verilog. A longstanding research goal has been programming hardware like software, with high-level languages that can generate efficient hardware designs. This paper describes Kanagawa, a language that takes a new approach to combine the programmer productivity benefits of traditional High-Level Synthesis (HLS) approaches with the expressibility and hardware efficiency of Register-Transfer Level (RTL) design. The language’s concise syntax, matched with a hardware design-friendly execution model, permits a relatively simple toolchain to map high-level code into efficient hardware implementations.",
      "url": "https://www.microsoft.com/en-us/research/publication/wavefront-threading-enables-effective-high-level-synthesis/"
    },
    {
      "title": "CulturePark: Boosting Cross-cultural Understanding in Large Language Models",
      "authors": [
        "Cheng Li",
        "Damien Teney",
        "Jindong Wang",
        "Linyi Yang",
        "Qingsong Wen",
        "Xing Xie"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "May 2024",
      "abstract": "Cultural bias is pervasive in many large language models (LLMs), largely due to the deficiency of data representative of different cultures. Typically, cultural datasets and benchmarks are constructed either by extracting subsets of existing datasets or by aggregating from platforms such as Wikipedia and social media. However, these approaches are highly dependent on real-world data and human annotations, making them costly and difficult to scale. Inspired by cognitive theories on social communication, this paper introduces CulturePark, an LLM-powered multi-agent communication framework for cultural data collection. CulturePark simulates cross-cultural human communication with LLM-based agents playing roles in different cultures. It generates high-quality cross-cultural dialogues encapsulating human beliefs, norms, and customs. Using CulturePark, we generated 41,000 cultural samples to fine-tune eight culture-specific LLMs. We evaluated these models across three downstream tasks: content moderation, cultural alignment, and cultural education. Results show that for content moderation, our GPT-3.5-based models either match or outperform GPT-4 on datasets. Regarding cultural alignment, our models surpass GPT-4 on Hofstede’s VSM 13 framework. Furthermore, for cultural education of human participants, our models demonstrate superior outcomes in both learning efficacy and user experience compared to GPT-4. CulturePark proves an important step in addressing cultural bias and advancing the democratization of AI, highlighting the critical role of culturally inclusive data in model training.",
      "url": "https://www.microsoft.com/en-us/research/publication/culturepark-boosting-cross-cultural-understanding-in-large-language-models/"
    },
    {
      "title": "GMConv: Modulating Effective Receptive Fields for Convolutional Kernels.",
      "authors": [
        "Chao Li",
        "Jia Ning",
        "Kun He",
        "Qi Chen",
        "Stephen Lin"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Computer vision"
      ],
      "publication_date": "May 2024",
      "abstract": "In convolutional neural networks (CNNs), the convolutions are conventionally performed using a square kernel with a fixed N × N receptive field (RF). However, what matters most to the network is the effective receptive field (ERF), which indicates the extent to which input pixels contribute to an output pixel. Inspired by the property that ERFs typically exhibit a Gaussian distribution, we propose a Gaussian Mask convolutional kernel (GMConv). Specifically, GMConv utilizes the Gaussian function to generate a concentric symmetry mask that is placed over the kernel to refine the RF. We analyze the RFs of CNN kernels in different CNN layers and evaluate our approach through extensive experiments on image classification and object detection tasks. Over several tasks and standard base models, our approach compares favorably against the standard convolution. For instance, using GMConv for AlexNet and ResNet-50, the top-1 accuracy on ImageNet classification is boosted by 0.98% and 0.85% , respectively.",
      "url": "https://www.microsoft.com/en-us/research/publication/gmconv-modulating-effective-receptive-fields-for-convolutional-kernels/"
    },
    {
      "title": "Crafting Interpretable Embeddings by Asking LLMs Questions",
      "authors": [
        "Alexander Huth",
        "Chandan Singh",
        "Ion Stoica",
        "Jianfeng Gao",
        "John X. Morris",
        "Richard Antonello",
        "Vinamra Benara"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "May 2024",
      "abstract": "Large language models (LLMs) have rapidly improved text embeddings for a growing array of natural-language processing tasks. However, their opaqueness and proliferation into scientific domains such as neuroscience have created a growing need for interpretability. Here, we ask whether we can obtain interpretable embeddings through LLM prompting. We introduce question-answering embeddings (QA-Emb), embeddings where each feature represents an answer to a yes/no question asked to an LLM. Training QA-Emb reduces to selecting a set of underlying questions rather than learning model weights. We use QA-Emb to flexibly generate interpretable models for predicting fMRI voxel responses to language stimuli. QA-Emb significantly outperforms an established interpretable baseline, and does so while requiring very few questions. This paves the way towards building flexible feature spaces that can concretize and evaluate our understanding of semantic brain representations. We additionally find that QA-Emb can be effectively approximated with an efficient model, and we explore broader applications in simple NLP tasks.",
      "url": "https://www.microsoft.com/en-us/research/publication/crafting-interpretable-embeddings-by-asking-llms-questions/"
    },
    {
      "title": "Quantifying the Gain in Weak-to-Strong Generalization",
      "authors": [
        "Chirag Pabbaraju",
        "Kirankumar Shiragur",
        "Moses Charikar"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "May 2024",
      "abstract": "Recent advances in large language models have shown capabilities that are extraordinary and near-superhuman. These models operate with such complexity that reliably evaluating and aligning them proves challenging for humans. This leads to the natural question: can guidance from weak models (like humans) adequately direct the capabilities of strong models? In a recent and somewhat surprising work, Burns et al. (2023) empirically demonstrated that when strong models (like GPT-4) are finetuned using labels generated by weak supervisors (like GPT-2), the strong models outperform their weaker counterparts — a phenomenon they term weak-to-strong generalization. In this work, we present a theoretical framework for understanding weak-to-strong generalization. Specifically, we show that the improvement in performance achieved by strong models over their weaker counterparts is quantified by the misfit error incurred by the strong model on labels generated by the weaker model. Our theory reveals several curious algorithmic insights. For instance, we can predict the amount by which the strong model will improve over the weak model, and also choose among different weak models to train the strong model, based on its misfit error. We validate our theoretical findings through various empirical assessments.",
      "url": "https://www.microsoft.com/en-us/research/publication/quantifying-the-gain-in-weak-to-strong-generalization/"
    },
    {
      "title": "A whole-slide foundation model for digital pathology from real-world data",
      "authors": [
        "Ari Robicsek",
        "Bill Wright",
        "Brian Piening",
        "Carlo Bifulco",
        "Chunyuan Li",
        "Cliff Wong",
        "Furu Wei",
        "Hanwen Xu",
        "Hoifung Poon",
        "Jaspreet Bagga",
        "Javier González",
        "Jaylen Rosemon",
        "Jianfeng Gao",
        "Jianwei Yang",
        "Mu-Hsin Wei",
        "Naoto Usuyama",
        "R. Weerasinghe",
        "Rajesh Rao",
        "Sheng Wang",
        "Sheng Zhang",
        "Shuming Ma",
        "Soohee Lee",
        "Tristan Naumann",
        "Tucker Bower",
        "Wenhui Wang",
        "Yanbo Xu",
        "Yu Gu",
        "Zelalem Gero"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Medical, health and genomics"
      ],
      "publication_date": "May 2024",
      "abstract": "Digital pathology poses unique computational challenges, as a standard gigapixel slide may comprise tens of thousands of image tiles. Prior models have often resorted to subsampling a small portion of tiles for each slide, thus missing the important slide-level context. Here we present Prov-GigaPath, a whole-slide pathology foundation model pretrained on 1.3 billion 256 × 256 pathology image tiles in 171,189 whole slides from Providence, a large US health network comprising 28 cancer centres.",
      "url": "https://www.microsoft.com/en-us/research/publication/a-whole-slide-foundation-model-for-digital-pathology-from-real-world-data/"
    },
    {
      "title": "Small Language Models for Application Interactions: A Case Study",
      "authors": [
        "Beibin Li",
        "Ishai Menache",
        "Jeevan Pathuri",
        "Sébastien Bubeck",
        "Yi Zhang"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "May 2024",
      "abstract": "We study the efficacy of Small Language Models (SLMs) in facilitating application usage through natural language interactions. Our focus here is on a particular internal application used in Microsoft for cloud supply chain fulfilment. Our experiments show that small models can outperform much larger ones in terms of both accuracy and running time, even when fine-tuned on small datasets. Alongside these results, we also highlight SLM-based system design considerations.",
      "url": "https://www.microsoft.com/en-us/research/publication/small-language-models-for-application-interactions-a-case-study/"
    },
    {
      "title": "Amortized Active Causal Induction with Deep Reinforcement Learning",
      "authors": [
        "Adam Foster",
        "P. Tigas",
        "Stefan Bauer",
        "Yashas Annadani"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "May 2024",
      "abstract": "We present Causal Amortized Active Structure Learning (CAASL), an active intervention design policy that can select interventions that are adaptive, real-time and that does not require access to the likelihood. This policy, an amortized network based on the transformer, is trained with reinforcement learning on a simulator of the design environment, and a reward function that measures how close the true causal graph is to a causal graph posterior inferred from the gathered data. On synthetic data and a single-cell gene expression simulator, we demonstrate empirically that the data acquired through our policy results in a better estimate of the underlying causal graph than alternative strategies. Our design policy successfully achieves amortized intervention design on the distribution of the training environment while also generalizing well to distribution shifts in test-time design environments. Further, our policy also demonstrates excellent zero-shot generalization to design environments with dimensionality higher than that during training, and to intervention types that it has not been trained on.",
      "url": "https://www.microsoft.com/en-us/research/publication/amortized-active-causal-induction-with-deep-reinforcement-learning/"
    },
    {
      "title": "Opportunities and risks of large language models in psychiatry",
      "authors": [
        "Jina Suh",
        "Martin P. Paulus",
        "Nick Obradovich",
        "Olusola A Ajilore",
        "Roy H. Perlis",
        "S. Khalsa",
        "Waqas U. Khan"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Human-computer interaction"
      ],
      "publication_date": "May 2024",
      "abstract": "The integration of large language models (LLMs) into mental healthcare and research heralds a potentially transformative shift, one offering enhanced access to care, efficient data collection, and innovative therapeutic tools. This paper reviews the development, function, and burgeoning use of LLMs in psychiatry, highlighting their potential to enhance mental healthcare through improved diagnostic accuracy, personalized care, and streamlined administrative processes. It is also acknowledged that LLMs introduce challenges related to computational demands, potential for misinterpretation, and ethical concerns, necessitating the development of pragmatic frameworks to ensure their safe deployment. We explore both the promise of LLMs in enriching psychiatric care and research through examples such as predictive analytics and therapy chatbots and risks including labor substitution, privacy concerns, and the necessity for responsible AI practices. We conclude by advocating for processes to develop responsible guardrails, including red-teaming, multi-stakeholder-oriented safety, and ethical guidelines/frameworks, to mitigate risks and harness the full potential of LLMs for advancing mental health.",
      "url": "https://www.microsoft.com/en-us/research/publication/opportunities-and-risks-of-large-language-models-in-psychiatry/"
    },
    {
      "title": "Efficient Adversarial Training in LLMs with Continuous Attacks",
      "authors": [
        "Alessandro Sordoni",
        "Gauthier Gidel",
        "Leo Schwinn",
        "Sophie Xhonneux",
        "Stephan Günnemann"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "May 2024",
      "abstract": "Large language models (LLMs) are vulnerable to adversarial attacks that can bypass their safety guardrails. In many domains, adversarial training has proven to be one of the most promising methods to reliably improve robustness against such attacks. Yet, in the context of LLMs, current methods for adversarial training are hindered by the high computational costs required to perform discrete adversarial attacks at each training iteration. We address this problem by instead calculating adversarial attacks in the continuous embedding space of the LLM, which is orders of magnitudes more efficient. We propose a fast adversarial training algorithm (C-AdvUL) composed of two losses: the first makes the model robust on continuous embedding attacks computed on an adversarial behaviour dataset; the second ensures the usefulness of the final model by fine-tuning on utility data. Moreover, we introduce C-AdvIPO, an adversarial variant of IPO that does not require utility data for adversarially robust alignment. Our empirical evaluation on four models from different families (Gemma, Phi3, Mistral, Zephyr) and at different scales (2B, 3.8B, 7B) shows that both algorithms substantially enhance LLM robustness against discrete attacks (GCG, AutoDAN, PAIR), while maintaining utility. Our results demonstrate that robustness to continuous perturbations can extrapolate to discrete threat models. Thereby, we present a path toward scalable adversarial training algorithms for robustly aligning LLMs.",
      "url": "https://www.microsoft.com/en-us/research/publication/efficient-adversarial-training-in-llms-with-continuous-attacks/"
    },
    {
      "title": "Reading Between the Lines: Modeling User Behavior and Costs in AI-Assisted Programming",
      "authors": [
        "Adam Fourney",
        "E. Horvitz",
        "Gagan Bansal",
        "Hussein Mozannar"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Human-computer interaction"
      ],
      "publication_date": "May 2024",
      "abstract": "Code-recommendation systems, such as Copilot and CodeWhisperer, have the potential to improve programmer productivity by suggesting and auto-completing code. However, to fully realize their potential, we must understand how programmers interact with these systems and identify ways to improve that interaction. To make progress, we studied GitHub Copilot, a code-recommendation system used by millions of programmers daily. We developed CUPS, a taxonomy of common programmer activities when interacting with Copilot. Our study of 21 programmers, who completed coding tasks and retrospectively labeled their sessions with CUPS, showed that CUPS can help us understand how programmers interact with code-recommendation systems, revealing inefficiencies and time costs. Our insights reveal how programmers interact with Copilot and motivate new interface designs and metrics.",
      "url": "https://www.microsoft.com/en-us/research/publication/reading-between-the-lines-modeling-user-behavior-and-costs-in-ai-assisted-programming-2/"
    },
    {
      "title": "Synthetic Test Collections for Retrieval Evaluation",
      "authors": [
        "Bhaskar Mitra",
        "Daniel Campos",
        "Emine Yilmaz",
        "Hossein A. Rahmani",
        "Nick Craswell"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Search and information retrieval"
      ],
      "publication_date": "May 2024",
      "abstract": "Test collections play a vital role in evaluation of information retrieval (IR) systems. Obtaining a diverse set of user queries for test collection construction can be challenging, and acquiring relevance judgments, which indicate the appropriateness of retrieved documents to a query, is often costly and resource-intensive. Generating synthetic datasets using Large Language Models (LLMs) has recently gained significant attention in various applications. In IR, while previous work exploited the capabilities of LLMs to generate synthetic queries or documents to augment training data and improve the performance of ranking models, using LLMs for constructing synthetic test collections is relatively unexplored. Previous studies demonstrate that LLMs have the potential to generate synthetic relevance judgments for use in the evaluation of IR systems. In this paper, we comprehensively investigate whether it is possible to use LLMs to construct fully synthetic test collections by generating not only synthetic judgments but also synthetic queries. In particular, we analyse whether it is possible to construct reliable synthetic test collections and the potential risks of bias such test collections may exhibit towards LLM-based models. Our experiments indicate that using LLMs it is possible to construct synthetic test collections that can reliably be used for retrieval evaluation.",
      "url": "https://www.microsoft.com/en-us/research/publication/synthetic-test-collections-for-retrieval-evaluation/"
    },
    {
      "title": "You Only Need Less Attention Each Stage in Vision Transformers",
      "authors": [
        "Hanpeng Liu",
        "Kun He",
        "Shuoxi Zhang",
        "Stephen Lin"
      ],
      "research_areas": [
        "Computer vision"
      ],
      "publication_date": "May 2024",
      "abstract": "The advent of Vision Transformers (ViTs) marks a substantial paradigm shift in the realm of computer vision. ViTs capture the global information of images through self-attention modules, which perform dot product computations among patchified image tokens. While self-attention modules empower ViTs to capture long-range dependencies, the computational complexity grows quadratically with the number of tokens, which is a major hindrance to the practical application of ViTs. Moreover, the self-attention mechanism in deep ViTs is also susceptible to the attention saturation issue. Accordingly, we argue against the necessity of computing the attention scores in every layer, and we propose the Less-Attention Vision Transformer (LaViT), which computes only a few attention operations at each stage and calculates the subsequent feature alignments in other layers via attention transformations that leverage the previously calculated attention scores. This novel approach can mitigate two primary issues plaguing traditional self-attention modules: the heavy computational burden and attention saturation. Our proposed architecture offers superior efficiency and ease of implementation, merely requiring matrix multiplications that are highly optimized in contemporary deep learning frameworks. Moreover, our architecture demonstrates exceptional performance across various vision tasks including classification, detection and segmentation.",
      "url": "https://www.microsoft.com/en-us/research/publication/you-only-need-less-attention-each-stage-in-vision-transformers/"
    },
    {
      "title": "To Err Is Human, How about Medical Large Language Models? Comparing Pre-trained Language Models for Medical Assessment Errors and Reliability",
      "authors": [
        "Asma Ben Abacha",
        "Meliha Yetisgen",
        "Wen-wai Yim",
        "Yujuan Fu"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Medical, health and genomics"
      ],
      "publication_date": "May 2024",
      "abstract": "Unpredictability, especially unpredictability with unknown error characteristics, is a highly undesirable trait, particularly in medical patient care applications. Although large pre-trained language models (LLM) have been applied to a variety of unseen tasks with highly competitive and successful results, their sensitivity to language inputs and resulting performance variability is not well-studied. In this work, we test state-of-the-art pre-trained language models from a variety of families to characterize their error generation and reliability in medical assessment ability. Particularly, we experiment with general medical assessment multiple choice tests, as well as their open-ended and true-false alternatives. We also profile model consistency, error agreements with each other and to humans; and finally, quantify their ability to recover and explain errors. The findings in this work can be used to give further information about medical models so that modelers can make better-informed decisions rather than relying on standalone performance metrics alone.",
      "url": "https://www.microsoft.com/en-us/research/publication/to-err-is-human-how-about-medical-large-language-models-comparing-pre-trained-language-models-for-medical-assessment-errors-and-reliability/"
    },
    {
      "title": "MoRA: High-Rank Updating for Parameter-Efficient Fine-Tuning",
      "authors": [
        "Deqing Wang",
        "Feng Sun",
        "Furu Wei",
        "Fuzhen Zhuang",
        "Haizhen Huang",
        "Qi Zhang",
        "Shaohan Huang",
        "Shengyue Luo",
        "Ting Jiang",
        "Weiwei Deng",
        "Zihan Zhang"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "May 2024",
      "abstract": "Low-rank adaptation is a popular parameter-efficient fine-tuning method for large language models. In this paper, we analyze the impact of low-rank updating, as implemented in LoRA. Our findings suggest that the low-rank updating mechanism may limit the ability of LLMs to effectively learn and memorize new knowledge. Inspired by this observation, we propose a new method called MoRA, which employs a square matrix to achieve high-rank updating while maintaining the same number of trainable parameters. To achieve it, we introduce the corresponding non-parameter operators to reduce the input dimension and increase the output dimension for the square matrix. Furthermore, these operators ensure that the weight can be merged back into LLMs, which makes our method can be deployed like LoRA. We perform a comprehensive evaluation of our method across five tasks: instruction tuning, mathematical reasoning, continual pretraining, memory and pretraining. Our method outperforms LoRA on memory-intensive tasks and achieves comparable performance on other tasks.",
      "url": "https://www.microsoft.com/en-us/research/publication/mora-high-rank-updating-for-parameter-efficient-fine-tuning/"
    },
    {
      "title": "Diffusion for World Modeling: Visual Details Matter in Atari",
      "authors": [
        "A. Storkey",
        "Adam Jelley",
        "Anssi Kanervisto",
        "Eloi Alonso",
        "Franccois Fleuret",
        "Tim Pearce",
        "Vincent Micheli"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "May 2024",
      "abstract": "World models constitute a promising approach for training reinforcement learning agents in a safe and sample-efficient manner. Recent world models predominantly operate on sequences of discrete latent variables to model environment dynamics. However, this compression into a compact discrete representation may ignore visual details that are important for reinforcement learning. Concurrently, diffusion models have become a dominant approach for image generation, challenging well-established methods modeling discrete latents. Motivated by this paradigm shift, we introduce DIAMOND (DIffusion As a Model Of eNvironment Dreams), a reinforcement learning agent trained in a diffusion world model. We analyze the key design choices that are required to make diffusion suitable for world modeling, and demonstrate how improved visual details can lead to improved agent performance. DIAMOND achieves a mean human normalized score of 1.46 on the competitive Atari 100k benchmark; a new best for agents trained entirely within a world model. To foster future research on diffusion for world modeling, we release our code, agents and playable world models at https://github.com/eloialonso/diamond.",
      "url": "https://www.microsoft.com/en-us/research/publication/diffusion-for-world-modeling-visual-details-matter-in-atari/"
    },
    {
      "title": "Implicit Motion Function",
      "authors": [
        "Jiahao Li",
        "Lei Chu",
        "Yan Lu",
        "Yue Gao"
      ],
      "research_areas": [
        "Computer vision"
      ],
      "publication_date": "May 2024",
      "abstract": "Abstract pending…",
      "url": "https://www.microsoft.com/en-us/research/publication/implicit-motion-function/"
    },
    {
      "title": "Assouad, Fano, and Le Cam with Interaction: A Unifying Lower Bound Framework and Characterization for Bandit Learnability",
      "authors": [
        "Alexander Rakhlin",
        "Dylan Foster",
        "Fan Chen",
        "Jian Qian",
        "Yanjun Han",
        "Yunbei Xu"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "May 2024",
      "abstract": "In this paper, we develop a unified framework for lower bound methods in statistical estimation and interactive decision making. Classical lower bound techniques—such as Fano’s inequality, Le Cam’s method, and Assouad’s lemma—have been central to the study of minimax risk in statistical estimation, yet they are insufficient for the analysis of methods that collect data in an interactive manner. The recent minimax lower bounds for interactive decision making via the Decision-Estimation Coefficient (DEC) appear to be genuinely different from the classical methods. We propose a unified view of these distinct methodologies through a general algorithmic lower bound method. We further introduce a novel complexity measure, decision coverage, which facilitates the derivation of new lower bounds for interactive decision making. In particular, we establish necessary and sufficient complexity measures for convex model classes, addressing the remaining gap between upper and lower bounds in Foster et al.",
      "url": "https://www.microsoft.com/en-us/research/publication/beyond-assouad-fano-and-le-cam-toward-unified-lower-bounds-for-statistical-estimation-and-interactive-decision-making/"
    },
    {
      "title": "Watching the Air Rise: Learning-Based Single-Frame Schlieren Detection",
      "authors": [
        "Andrey Kolobov",
        "Florian Achermann",
        "Jen Jen Chung",
        "Julian Andreas Haug",
        "Nicholas Lawrance",
        "Roland Siegwart",
        "Tobias Zumsteg"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "May 2024",
      "abstract": "Detecting air flows caused by phenomena such as heat convection is valuable in multiple scenarios, including leak identification and locating thermal updrafts for extending UAVs’ flight duration. Unfortunately, these flows’ heat signature is often too subtle to be seen by a thermal camera. While convection also leads to fluctuations in air density and hence causes so-called schlieren – intensity and color variations in images – existing techniques such as Background-oriented schlieren (BOS) allow detecting them only against a known background and from a static camera, making these approaches unsuitable for moving vehicles. In this work we demonstrate the feasibility of visualizing air movement by predicting the corresponding schlieren-induced optical flow from a single greyscale image captured by a moving camera against an unfamiliar background. We first record and label a set of optical flows in an indoor setup using standard BOS techniques. We then train a convolutional neural network (CNN) by applying the previously collected optical flow distortions to a dataset containing a mixture of real and synthetically generated images to predict the two-dimensional optical flow from a single image. Finally, we evaluate our approach on the task of extracting the optical flow caused by schlieren from both a static and moving camera on previously unseen flow patterns and background images.",
      "url": "https://www.microsoft.com/en-us/research/publication/watching-the-air-rise-learning-based-single-frame-schlieren-detection/"
    },
    {
      "title": "xRAG: Extreme Context Compression for Retrieval-augmented Generation with One Token",
      "authors": [
        "Dongyan Zhao",
        "Furu Wei",
        "Huishuai Zhang",
        "Si-Qing Chen",
        "Tao Ge",
        "Xin Cheng",
        "Xingxing Zhang",
        "Xun Wang"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "May 2024",
      "abstract": "This paper introduces xRAG, an innovative context compression method tailored for retrieval-augmented generation. xRAG reinterprets document embeddings in dense retrieval–traditionally used solely for retrieval–as features from the retrieval modality. By employing a modality fusion methodology, xRAG seamlessly integrates these embeddings into the language model representation space, effectively eliminating the need for their textual counterparts and achieving an extreme compression rate. In xRAG, the only trainable component is the modality bridge, while both the retriever and the language model remain frozen. This design choice allows for the reuse of offline-constructed document embeddings and preserves the plug-and-play nature of retrieval augmentation. Experimental results demonstrate that xRAG achieves an average improvement of over 10% across six knowledge-intensive tasks, adaptable to various language model backbones, ranging from a dense 7B model to an 8x7B Mixture of Experts configuration. xRAG not only significantly outperforms previous context compression methods but also matches the performance of uncompressed models on several datasets, while reducing overall FLOPs by a factor of 3.53. Our work pioneers new directions in retrieval-augmented generation from the perspective of multimodality fusion, and we hope it lays the foundation for future efficient and scalable retrieval-augmented systems",
      "url": "https://www.microsoft.com/en-us/research/publication/xrag-extreme-context-compression-for-retrieval-augmented-generation-with-one-token/"
    },
    {
      "title": "Towards Modular LLMs by Building and Reusing a Library of LoRAs",
      "authors": [
        "Alessandro Sordoni",
        "E. Ponti",
        "Laurent Charlin",
        "Lucas Caccia",
        "Matheus Pereira",
        "Nicolas Le Roux",
        "O. Ostapenko",
        "Zhan Su"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "May 2024",
      "abstract": "The growing number of parameter-efficient adaptations of a base large language model (LLM) calls for studying whether we can reuse such trained adapters to improve performance for new tasks. We study how to best build a library of adapters given multi-task data and devise techniques for both zero-shot and supervised task generalization through routing in such library. We benchmark existing approaches to build this library and introduce model-based clustering, MBC, a method that groups tasks based on the similarity of their adapter parameters, indirectly optimizing for transfer across the multi-task dataset. To re-use the library, we present a novel zero-shot routing mechanism, Arrow, which enables dynamic selection of the most relevant adapters for new inputs without the need for retraining. We experiment with several LLMs, such as Phi-2 and Mistral, on a wide array of held-out tasks, verifying that MBC-based adapters and Arrow routing lead to superior generalization to new tasks. We make steps towards creating modular, adaptable LLMs that can match or outperform traditional joint training.",
      "url": "https://www.microsoft.com/en-us/research/publication/towards-modular-llms-by-building-and-reusing-a-library-of-loras/"
    },
    {
      "title": "Generalizing Knowledge Graph Embedding with Universal Orthogonal Parameterization",
      "authors": [
        "Chaozhuo Li",
        "Rui Li",
        "Xu Chen",
        "Yanming Shen",
        "Zeyu Zhang"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "May 2024",
      "abstract": "Recent advances in knowledge graph embedding (KGE) rely on Euclidean/hyperbolic orthogonal relation transformations to model intrinsic logical patterns and topological structures. However, existing approaches are confined to rigid relational orthogonalization with restricted dimension and homogeneous geometry, leading to deficient modeling capability. In this work, we move beyond these approaches in terms of both dimension and geometry by introducing a powerful framework named GoldE, which features a universal orthogonal parameterization based on a generalized form of Householder reflection. Such parameterization can naturally achieve dimensional extension and geometric unification with theoretical guarantees, enabling our framework to simultaneously capture crucial logical patterns and inherent topological heterogeneity of knowledge graphs. Empirically, GoldE achieves state-of-the-art performance on three standard benchmarks. Codes are available at https://github.com/xxrep/GoldE (opens in new tab).",
      "url": "https://www.microsoft.com/en-us/research/publication/generalizing-knowledge-graph-embedding-with-universal-orthogonal-parameterization/"
    },
    {
      "title": "SharedNeRF: Leveraging Photorealistic and View-dependent Rendering for Real-time and Remote Collaboration",
      "authors": [
        "Andrew D. Wilson",
        "Bala Kumaravel",
        "Mose Sakashita",
        "Nicolai Marquardt"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Human-computer interaction"
      ],
      "publication_date": "May 2024",
      "abstract": "Collaborating around physical objects necessitates examining different aspects of design or hardware in detail when reviewing or inspecting physical artifacts or prototypes. When collaborators are remote, coordinating the sharing of views of their physical environment becomes challenging. Video-conferencing tools often do not provide the desired viewpoints for a remote viewer. While RGB-D cameras offer 3D views, they lack the necessary fidelity. We introduce SharedNeRF, designed to enhance synchronous remote collaboration by leveraging the photorealistic and view-dependent nature of Neural Radiance Field (NeRF). The system complements the higher visual quality of the NeRF rendering with the instantaneity of a point cloud and combines them through carefully accommodating the dynamic elements within the shared space, such as hand gestures and moving objects. The system employs a head-mounted camera for data collection, creating a volumetric task space on the fly and updating it as the task space changes. In our preliminary study, participants successfully completed a flower arrangement task, benefiting from SharedNeRF’s ability to render the space in high fidelity from various viewpoints.",
      "url": "https://www.microsoft.com/en-us/research/publication/sharednerf-leveraging-photorealistic-and-view-dependent-rendering-for-real-time-and-remote-collaboration/"
    },
    {
      "title": "VASIM: Vertical Autoscaling Simulator Toolkit",
      "authors": [
        "Anna Pavlenko",
        "Brian Kroth",
        "Jesús Camacho-Rodríguez",
        "Joyce Cahoon",
        "Karla Saur",
        "Yiwen Zhu"
      ],
      "research_areas": [
        "Data platforms and analytics",
        "Programming languages and software engineering"
      ],
      "publication_date": "May 2024",
      "abstract": "In recent years, autoscaling has garnered significant attention in cloud computing, emphasizing cost efficiency, performance optimization, and availability for dynamic workloads. New algorithms for horizontal, vertical, and hybrid scaling, targeting instances, VM specifications, and resources like CPU, memory, and IO, have emerged. Various approaches, including forecasting and custom autoscaling functions, are used. However, conducting comprehensive end-to-end testing remains a complex and costly endeavor due to the variety of technology constraints involved.\nThis paper introduces VASIM, an autoscaling simulator toolkit designed for testing recommendation algorithms, with a particular focus on CPU usage in VMs and Kubernetes pods. The toolkit replicates common components found in autoscaler architectures, including the controller, metrics collector, recommender, and resource updater. It enables a comprehensive simulation of the entire autoscaling system’s behavior, with the flexibility to customize various parameters. In our demonstration, we showcase VASIM’s versatility across multiple use cases, highlighting its effectiveness in evaluating autoscaling strategies, fine-tuning parameters, comparing algorithm performance, and addressing autoscaling-related challenges. This underscores VASIM’s critical role in expediting algorithm development and refinement by providing a controlled environment for testing and experimentation.",
      "url": "https://www.microsoft.com/en-us/research/publication/vasim-vertical-autoscaling-simulator-toolkit/"
    },
    {
      "title": "Big or Small, It’s All in Your Head: Visuo-Haptic Illusion of Size-Change Using Finger-Repositioning",
      "authors": [
        "Andrea Bianchi",
        "Eyal Ofek",
        "Michel Pahud",
        "Mike Sinclair",
        "Myung Jin Kim"
      ],
      "research_areas": [
        "Human-computer interaction"
      ],
      "publication_date": "May 2024",
      "abstract": "Haptic perception of physical sizes increases the realism and immersion in Virtual Reality (VR). Prior work rendered sizes by exerting pressure on the user’s fingertips or employing tangible, shape-changing devices. These interfaces are constrained by the physical shapes they can assume, making it challenging to simulate objects growing larger or smaller than the perceived size of the interface. Motivated by literature on pseudo-haptics describing the strong influence of visuals over haptic perception, this work investigates modulating the perception of size beyond this range. We developed a fixed-sized VR controller leveraging finger-repositioning to create a visuo-haptic illusion of dynamic size-change of handheld virtual objects. Through two user studies, we found that with an accompanying size-changing visual context, users can perceive virtual object sizes up to 44.2% smaller to 160.4%larger than the perceived size of the device. Without the accompanying visuals, a constant size (141.4% of device size) was perceived.",
      "url": "https://www.microsoft.com/en-us/research/publication/big-or-small-its-all-in-your-head-visuo-haptic-illusion-of-size-change-using-finger-repositioning/"
    },
    {
      "title": "LeanAttention: Hardware-Aware Scalable Attention Mechanism for the Decode-Phase of Transformers",
      "authors": [
        "Renee St. Amant",
        "Rya Sanovar",
        "Saravan Rajmohan",
        "Srikant Bharadwaj",
        "Victor Ruehle"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Hardware and devices",
        "Systems and networking"
      ],
      "publication_date": "May 2024",
      "abstract": "Transformer-based models have emerged as one of the most widely used architectures for natural language processing, natural language generation, and image generation. The size of the state-of-the-art models has increased steadily reaching billions of parameters. These huge models are memory hungry and incur significant inference latency even on cutting edge AI-accelerators, such as GPUs. Specifically, the time and memory complexity of the attention operation is quadratic in terms of the total context length, i.e., prompt and output tokens. Thus, several optimizations such as key-value tensor caching and FlashAttention computation have been proposed to deliver the low latency demands of applications relying on such large models. However, these techniques do not cater to the computationally distinct nature of different phases during inference. To that end, we propose LeanAttention, a scalable technique of computing self-attention for the token-generation phase (decode-phase) of decoder-only transformer models. LeanAttention enables scaling the attention mechanism implementation for the challenging case of long context lengths by re-designing the execution flow for the decode-phase. We identify that the associative property of online softmax can be treated as a reduction operation thus allowing us to parallelize the attention computation over these large context lengths. We extend the”stream-K”style reduction of tiled calculation to self-attention to enable parallel computation resulting in an average of 2.6x attention execution speedup over FlashAttention-2 and up to 8.33x speedup for 512k context lengths.",
      "url": "https://www.microsoft.com/en-us/research/publication/lean-attention-hardware-aware-scalable-attention-mechanism-for-the-decode-phase-of-transformers/"
    },
    {
      "title": "TimeX++: Learning Time-Series Explanations with Information Bottleneck",
      "authors": [
        "Dongsheng Luo",
        "Farhad Shirani",
        "J. Obeysekera",
        "Jimeng Shi",
        "Lei Song",
        "Tianchun Wang",
        "Wenqian Dong",
        "Xu Zheng",
        "Zhuomin Chen",
        "Zichuan Liu"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "May 2024",
      "abstract": "Explaining deep learning models operating on time series data is crucial in various applications of interest which require interpretable and transparent insights from time series signals. In this work, we investigate this problem from an information theoretic perspective and show that most existing measures of explainability may suffer from trivial solutions and distributional shift issues. To address these issues, we introduce a simple yet practical objective function for time series explainable learning. The design of the objective function builds upon the principle of information bottleneck (IB), and modifies the IB objective function to avoid trivial solutions and distributional shift issues. We further present TimeX++, a novel explanation framework that leverages a parametric network to produce explanation-embedded instances that are both in-distributed and label-preserving. We evaluate TimeX++ on both synthetic and real-world datasets comparing its performance against leading baselines, and validate its practical efficacy through case studies in a real-world environmental application. Quantitative and qualitative evaluations show that TimeX++ outperforms baselines across all datasets, demonstrating a substantial improvement in explanation quality for time series data. The source code is available at \\url{https://github.com/zichuan-liu/TimeXplusplus}.",
      "url": "https://www.microsoft.com/en-us/research/publication/timex-learning-time-series-explanations-with-information-bottleneck/"
    },
    {
      "title": "Dependency Aware Incident Linking in Large Cloud Systems",
      "authors": [
        "Chetan Bansal",
        "Jimmy Wong",
        "Karish Grover",
        "Mohit Verma",
        "Rakesh Namineni",
        "Saravan Rajmohan",
        "Supriyo GHOSH"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "May 2024",
      "abstract": "Despite significant reliability efforts, large-scale cloud services inevitably experience production incidents that can significantly impact service availability and customer’s satisfaction. Worse, in many cases one incident can lead to multiple downstream failures due to cascading effects that creates several related incidents across different dependent services. Often time On-call Engineers (OCEs) examine these incidents in silos that lead to significant amount of manual toil and increase the overall time-to-mitigate incidents. Therefore, developing efficient incident linking models is of paramount importance for grouping related incidents into clusters so as to quickly resolve major outages and reduce on-call fatigue. Existing incident linking methods mostly leverages textual and contextual information of incidents (e.g., title, description, severity, impacted components), thus failing to leverage the inter-dependencies between services. In this paper, we propose the dependency-aware incident linking (DiLink) framework which leverages both textual and service dependency graph information to improve the accuracy and coverage of incident links not only coming from same service, but also from different services and workloads. Furthermore, we propose a novel method to align the embeddings of multi-modal (i.e., textual and graphical) data using Orthogonal Procrustes. Extensive experimental results on real-world incidents from 5 workloads of Microsoft demonstrate that our alignment method has an F1-score of 0.96 (14% gain over current state-of-the-art methods). We are also in the process of deploying this solution across 610 services from these 5 workloads for continuously supporting OCEs improving incident management and reducing manual toil.",
      "url": "https://www.microsoft.com/en-us/research/publication/dependency-aware-incident-linking-in-large-cloud-systems/"
    },
    {
      "title": "A Graph-based Framework for Reducing False Positives in Authentication Alerts in Security Systems",
      "authors": [
        "Jonathan Larson",
        "Karl Hallgren",
        "Yanbang Wang"
      ],
      "research_areas": [
        "Artificial intelligence",
        "Security, privacy, and cryptography"
      ],
      "publication_date": "May 2024",
      "abstract": "The high false positive (FP) rate of authentication alerts remains to be a prominent challenge in cybersecurity nowadays. We identify two problems that cause this issue, which are unaddressed in existing learning-based anomaly detection methods. First, in industrial applications, ground-truth labels for malicious authentication events are extremely scarce. Therefore, learning-based methods must optimize their procedures for auto-generating high-quality training instances, an aspect that existing works have overlooked. Second, every existing model is based on a single form of data representation, either stream or graph snapshot, which may not be expressive enough to identify heterogeneity in behaviors of networked entities. This results in misclassifying a legitimate but differently-behaved authentication event into an anomalous one. We address these problems by proposing a new framework based on self-supervised link prediction on dynamic authentication networks, with two highlighted features: (1) our framework is based on the unification of two most popular views of dynamic interconnected systems: graph snapshots and link stream, ensuring the best coverage of behavioral heterogeneity; (2) to generate high-quality training samples, we propose a carefully designed negative sampling procedure called filtered rewiring, to ensure that the negative samples used for training are both truly negative and instructive. We validate our framework on 4 months of authentication data of 125 randomly selected, real organizations that subscribe to Microsoft’s defense services.",
      "url": "https://www.microsoft.com/en-us/research/publication/a-graph-based-framework-for-reducing-false-positives-in-authentication-alerts-in-security-systems/"
    },
    {
      "title": "MS MARCO Web Search: A Large-scale Information-rich Web Dataset with Millions of Real Click Labels",
      "authors": [
        "Bryan Tower",
        "Carolyn Buractaon",
        "Ce Zhang",
        "Chenyan Xiong",
        "Chuanjie Liu",
        "Corby Rosset",
        "Fan Yang",
        "Harsha Simhadri",
        "Jason (Zengzhong) Li",
        "Jennifer Neville",
        "Jingwen Lu",
        "Kun Zhou",
        "Linjun Yang",
        "Manik Varma",
        "Mao Yang",
        "Mingqin Li",
        "Nick Craswell",
        "Paul Bennett",
        "Qi Chen",
        "Rangan Majumder",
        "Tao Shen",
        "Xing Xie",
        "Xiubo Geng",
        "Yeyun Gong",
        "Yujing Wang",
        "Zheng Liu"
      ],
      "research_areas": [
        "Search and information retrieval",
        "Systems and networking"
      ],
      "publication_date": "May 2024",
      "abstract": "Recent breakthroughs in large models have highlighted the critical significance of data scale, labels and modals. In this paper, we introduce MS MARCO Web Search, the first large-scale information-rich web dataset, featuring millions of real clicked query-document labels. This dataset closely mimics real-world web document and query distribution, provides rich information for various kinds of downstream tasks and encourages research in various areas, such as generic end-to-end neural indexer models, generic embedding models, and next generation information access system with large language models. MS MARCO Web Search offers a retrieval benchmark with three web retrieval challenge tasks that demands innovations in both machine learning and information retrieval system research domains. As the first dataset that meets large, real and rich data requirements, MS MARCO Web Search paves the way for future advancements in AI and system research. MS MARCO Web Search dataset is available at: https://github.com/microsoft/MS-MARCO-Web-Search (opens in new tab).",
      "url": "https://www.microsoft.com/en-us/research/publication/ms-marco-web-search-a-large-scale-information-rich-web-dataset-with-millions-of-real-click-labels/"
    },
    {
      "title": "A Game-theoretic Framework for Privacy-preserving Federated Learning",
      "authors": [
        "Kai Chen",
        "Lixin Fan",
        "Qiang Yang",
        "Siwei Wang",
        "Wenjie Li",
        "Xiaojin Zhang"
      ],
      "research_areas": [
        "Artificial intelligence"
      ],
      "publication_date": "May 2024",
      "abstract": "In federated learning, benign participants aim to optimize a global model collaboratively. However, the risk of privacy leakage cannot be ignored in the presence of semi-honest adversaries. Existing research has focused either on designing protection mechanisms or on inventing attacking mechanisms. While the battle between defenders and attackers seems never-ending, we are concerned with one critical question: Is it possible to prevent potential attacks in advance? To address this, we propose the first game-theoretic framework that considers both FL defenders and attackers in terms of their respective payoffs, which include computational costs, FL model utilities, and privacy leakage risks. We name this game the federated learning privacy game (FLPG), in which neither defenders nor attackers are aware of all participants’ payoffs. To handle the incomplete information inherent in this situation, we propose associating the FLPG with an oracle that has two primary responsibilities. First, the oracle provides lower and upper bounds of the payoffs for the players. Second, the oracle acts as a correlation device, privately providing suggested actions to each player. With this novel framework, we analyze the optimal strategies of defenders and attackers. Furthermore, we derive and demonstrate conditions under which the attacker, as a rational decision-maker, should always follow the oracle’s suggestion not to attack.",
      "url": "https://www.microsoft.com/en-us/research/publication/a-game-theoretic-framework-for-privacy-preserving-federated-learning/"
    }
  ]
}